---

# Content from: https://ai.google.dev/gemini-api/docs/gemini-for-research

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)

# Accelerate discovery with Gemini for Research

[Get a Gemini API Key](https://aistudio.google.com/apikey)

Gemini models can be used to advance foundational research across disciplines. Here are ways that you can explore Gemini for your research:

*   **Fine-tuning**: You can fine-tune Gemini models for a variety of modalities to advance your research. [Learn more](/gemini-api/docs/model-tuning/tutorial).
*   **Analyze and control model outputs**: For further analysis, you can examine a response candidate generated by the model using tools like `Logprobs` and `CitationMetadata`. You can also configure options for model generation and outputs, such as `responseSchema`, `topP`, and `topK`. [Learn more](/api/generate-content).
*   **Multimodal inputs**: Gemini can process images, audio, and videos, enabling a multitude of exciting research directions. [Learn more](/gemini-api/docs/vision).
*   **Long-context capabilities**: Gemini 1.5 Flash comes with a 1-million-token context window, and Gemini 1.5 Pro comes with a 2-million-token context window. [Learn more](/gemini-api/docs/long-context).
*   **Grow with Google**: Quickly access Gemini models through the API and Google AI Studio for production use cases. If you're looking for a Google Cloud-based platform, Vertex AI can provide additional supporting infrastructure.

To support academic research and drive cutting-edge research, Google provides access to Gemini API credits for scientists and academic researchers through the [Gemini Academic Program](/gemini-api/docs/gemini-for-research#gemini-academic-program).

## Get started with Gemini

The Gemini API and Google AI Studio help you start working with Google's latest models and turn your ideas into applications that scale.

### Python

```
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="How large is the universe?",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "How large is the universe?",
  });
  console.log(response.text);
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [{
    "parts":[{"text": "How large is the universe?"}]
    }]
   }'
```

## Featured academics

![](/static/site-assets/images/diyi-yang.png)

"Our research investigates Gemini as a visual language model (VLM) and its agentic behaviors in diverse environments from robustness and safety perspectives. So far, we have evaluated Gemini's robustness against distractions such as pop-up windows when VLM agents perform computer tasks, and have leveraged Gemini to analyze social interaction, temporal events as well as risk factors based on video input."

[](https://cs.stanford.edu/~diyiy/)

![](/static/site-assets/images/lerrel-pinto.png)

"Gemini Pro and Flash, with their long context window, have been helping us in OK-Robot, our open-vocabulary mobile manipulation project. Gemini enables complex natural language queries and commands over the robot's "memory": in this case, previous observations made by the robot over a long operation duration. Mahi Shafiullah and I are also using Gemini to decompose tasks into code that the robot can execute in the real world."

[](https://www.lerrelpinto.com/)

## Gemini Academic Program

Qualified academic researchers (such as faculty, staff, and PhD students) in [supported countries](/gemini-api/docs/available-regions) can apply to receive Gemini API credits and higher rate limits for research projects. This support enables higher throughput for scientific experiments and advances research.

We are particularly interested in the research areas in the following section, but we welcome applications from diverse scientific disciplines:

*   **Evaluations and benchmarks**: Community-endorsed evaluation methods that can provide a strong performance signal in areas such as factuality, safety, instruction following, reasoning, and planning.
    
*   **Accelerating scientific discovery to benefit humanity**: Potential applications of AI in interdisciplinary scientific research, including areas such as rare and neglected diseases, experimental biology, materials science, and sustainability.
    
*   **Embodiment and interactions**: Utilizing large language models to investigate novel interactions within the fields of embodied AI, ambient interactions, robotics, and human-computer interaction.
    
*   **Emergent capabilities**: Exploring new agentic capabilities required to enhance reasoning and planning, and how capabilities can be expanded during inference (e.g., by utilizing Gemini Flash).
    
*   **Multimodal interaction and understanding**: Identifying gaps and opportunities for multimodal foundational models for analysis, reasoning, and planning across a variety of tasks.
    

Eligibility: Only individuals (faculty members, researchers or equivalent) affiliated with a valid academic institution, or academic research organization can apply. Note that API access and credits will be granted and removed at Google's discretion. We review applications on a monthly basis.

### Start researching with the Gemini API

[Apply now](https://forms.gle/HMviQstU8PxC5iCt5)

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/video

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Generate video using Veo

The Gemini API provides access to [Veo 2](https://deepmind.google/technologies/veo/), Google's most capable video generation model to date. Veo generates videos in a wide range of cinematic and visual styles, capturing prompt nuance to render intricate details consistently across frames. This guide will help you get started with Veo using the Gemini API.

For video prompting guidance, check out the [Veo prompt guide](/gemini-api/docs/video#prompt-guide) section.

### Before you begin

Before calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.

To use Veo with the Google Gen AI SDKs, ensure that you have one of the following versions installed:

*   [Python](https://pypi.org/project/google-genai/) v1.10.0 or later
*   [TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) v0.8.0 or later
*   [Go](https://pkg.go.dev/google.golang.org/genai) v1.0.0 or later

## Generate videos

This section provides code examples for generating videos [using text prompts](#generate-from-text) and [using images](#generate-from-images).

### Generate from text

You can use the following code to generate videos with Veo:

### Python

```
import time
from google import genai
from google.genai import types

client = genai.Client()

operation = client.models.generate_videos(
    model="veo-2.0-generate-001",
    prompt="Panning wide shot of a calico kitten sleeping in the sunshine",
    config=types.GenerateVideosConfig(
        person_generation="dont_allow",  # "dont_allow" or "allow_adult"
        aspect_ratio="16:9",  # "16:9" or "9:16"
    ),
)

while not operation.done:
    time.sleep(20)
    operation = client.operations.get(operation)

for n, generated_video in enumerate(operation.response.generated_videos):
    client.files.download(file=generated_video.video)
    generated_video.video.save(f"video{n}.mp4")  # save the video

```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import { createWriteStream } from "fs";
import { Readable } from "stream";

const ai = new GoogleGenAI({});

async function main() {
  let operation = await ai.models.generateVideos({
    model: "veo-2.0-generate-001",
    prompt: "Panning wide shot of a calico kitten sleeping in the sunshine",
    config: {
      personGeneration: "dont_allow",
      aspectRatio: "16:9",
    },
  });

  while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
      operation: operation,
    });
  }

  operation.response?.generatedVideos?.forEach(async (generatedVideo, n) => {
    const resp = await fetch(`${generatedVideo.video?.uri}&key=GEMINI_API_KEY`); // append your API key
    const writer = createWriteStream(`video${n}.mp4`);
    Readable.fromWeb(resp.body).pipe(writer);
  });
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "time"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  videoConfig := &genai.GenerateVideosConfig{
      AspectRatio:      "16:9",
      PersonGeneration: "dont_allow",
  }

  operation, _ := client.Models.GenerateVideos(
      ctx,
      "veo-2.0-generate-001",
      "Panning wide shot of a calico kitten sleeping in the sunshine",
      nil,
      videoConfig,
  )

  for !operation.Done {
      time.Sleep(20 * time.Second)
      operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
  }

  for n, video := range operation.Response.GeneratedVideos {
      client.Files.Download(ctx, video.Video, nil)
      fname := fmt.Sprintf("video_%d.mp4", n)
      _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
  }
}
```

### REST

```
# Use curl to send a POST request to the predictLongRunning endpoint.
# The request body includes the prompt for video generation.
curl "${BASE_URL}/models/veo-2.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "Panning wide shot of a calico kitten sleeping in the sunshine"
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "personGeneration": "dont_allow",
    }
  }' | tee result.json | jq .name | sed 's/"//g' > op_name

# Obtain operation name to download video.
op_name=$(cat op_name)

# Check against status of operation.
while true; do
  is_done=$(curl -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${op_name}" | tee op_check.json | jq .done)

  if [ "${is_done}" = "true" ]; then
    cat op_check.json
    echo "** Attach API_KEY to download video, or examine error message."
    break
  fi

  echo "** Video ${op_name} has not downloaded yet!  Check again after 5 seconds..."

  # Wait for 5 seoncds to check again.
  sleep 5

done
```

![Kitten sleeping in the sun.](https://storage.googleapis.com/generativeai-downloads/images/calico.gif)

This code takes about 2-3 minutes to run, though it may take longer if resources are constrained. Once it's done running, you should see a video that looks something like this:

If you see an error message instead of a video, this means that resources are constrained and your request couldn't be completed. In this case, run the code again.

Generated videos are stored on the server for 2 days, after which they are removed. If you want to save a local copy of your generated video, you must run `result()` and `save()` within 2 days of generation.

### Generate from images

You can also generate videos using images. The following code generates an image using Imagen, then uses the generated image as the starting frame for the generated video.

First, generate an image using [Imagen](/gemini-api/docs/image-generation#imagen):

### Python

```
prompt="Panning wide shot of a calico kitten sleeping in the sunshine",

imagen = client.models.generate_images(
    model="imagen-3.0-generate-002",
    prompt=prompt,
    config=types.GenerateImagesConfig(
      aspect_ratio="16:9",
      number_of_images=1
    )
)

imagen.generated_images[0].image
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});
const response = await ai.models.generateImages({
  model: "imagen-3.0-generate-002",
  prompt: "Panning wide shot of a calico kitten sleeping in the sunshine",
  config: {
    numberOfImages: 1,
  },
});

// you'll pass response.generatedImages[0].image.imageBytes to Veo
```

### Go

```
package main

import (
    "context"
    "fmt"
    "os"
    "time"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

    config := &genai.GenerateImagesConfig{
        AspectRatio:    "16:9",
        NumberOfImages: 1,
    }

    response, _ := client.Models.GenerateImages(
        ctx,
        "imagen-3.0-generate-002",
        "Panning wide shot of a calico kitten sleeping in the sunshine",
        config,
    )

    // you'll pass response.GeneratedImages[0].Image to Veo
}
```

Then, generate a video using the resulting image as the first frame:

### Python

```
operation = client.models.generate_videos(
    model="veo-2.0-generate-001",
    prompt=prompt,
    image = imagen.generated_images[0].image,
    config=types.GenerateVideosConfig(
      person_generation="dont_allow",  # "dont_allow" or "allow_adult"
      aspect_ratio="16:9",  # "16:9" or "9:16"
      number_of_videos=2
    ),
)

# Wait for videos to generate
 while not operation.done:
  time.sleep(20)
  operation = client.operations.get(operation)

for n, video in enumerate(operation.response.generated_videos):
    fname = f'with_image_input{n}.mp4'
    print(fname)
    client.files.download(file=video.video)
    video.video.save(fname)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import { createWriteStream } from "fs";
import { Readable } from "stream";

const ai = new GoogleGenAI({});

async function main() {
  // get image bytes from Imagen, as shown above

  let operation = await ai.models.generateVideos({
    model: "veo-2.0-generate-001",
    prompt: "Panning wide shot of a calico kitten sleeping in the sunshine",
    image: {
      imageBytes: response.generatedImages[0].image.imageBytes, // response from Imagen
      mimeType: "image/png",
    },
    config: {
      aspectRatio: "16:9",
      numberOfVideos: 2,
    },
  });

  while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
      operation: operation,
    });
  }

  operation.response?.generatedVideos?.forEach(async (generatedVideo, n) => {
    const resp = await fetch(
      `${generatedVideo.video?.uri}&key=GEMINI_API_KEY`, // append your API key
    );
    const writer = createWriteStream(`video${n}.mp4`);
    Readable.fromWeb(resp.body).pipe(writer);
  });
}

main();
```

### Go

    ```
    image := response.GeneratedImages[0].Image

    videoConfig := &genai.GenerateVideosConfig{
      AspectRatio:    "16:9",
      NumberOfVideos: 2,
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-2.0-generate-001",
        "A dramatic scene based on the input image",
        image,
        videoConfig,
    )

    for !operation.Done {
        time.Sleep(20 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    for n, video := range operation.Response.GeneratedVideos {
        client.Files.Download(ctx, video.Video, nil)
        fname := fmt.Sprintf("video_with_image_input_%d.mp4", n)
        _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    }
```

## Veo model parameters

(Naming conventions vary by programming language.)

*   `prompt`: The text prompt for the video. When present, the `image` parameter is optional.
*   `image`: The image to use as the first frame for the video. When present, the `prompt` parameter is optional.
*   `negativePrompt`: Text string that describes anything you want to _discourage_ the model from generating
*   `aspectRatio`: Changes the aspect ratio of the generated video. Supported values are `"16:9"` and `"9:16"`. The default is `"16:9"`.
*   `personGeneration`: Allow the model to generate videos of people. The following values are supported:
    *   Text-to-video generation:
        *   `"dont_allow"`: Don't allow the inclusion of people or faces.
        *   `"allow_adult"`: Generate videos that include adults, but not children.
        *   `"allow_all"`: Generate videos that include adults and children.
    *   Image-to-video generation:
        *   `"dont_allow"`: Don't allow the inclusion of people or faces.
        *   `"allow_adult"`: Generate videos that include adults, but not children.
    *   See [Limitations](#limitations).
*   `numberOfVideos`: Output videos requested, either `1` or `2`.
*   `durationSeconds`: Length of each output video in seconds, between `5` and `8`.
*   `enhance_prompt`: Enable or disable the prompt rewriter. Enabled by default.

## Specifications

**Modalities**

*   Text-to-video generation
*   Image-to-video generation

**Request latency**

*   Min: 11 seconds
*   Max: 6 minutes (during peak hours)

**Variable length generation**

5-8 seconds

**Resolution**

720p

**Frame rate**

24fps

**Aspect ratio**

*   16:9 - landscape
*   9:16 - portrait

**Input languages (text-to-video)**

English

**Limitations**

*   Image-to-video `personGeneration` is not allowed in EU, UK, CH, MENA locations
*   Text-to-video `personGeneration: "allow_all"` is not allowed in EU, UK, CH, MENA locations

Videos created by Veo are watermarked using [SynthID](https://deepmind.google/technologies/synthid/), our tool for watermarking and identifying AI-generated content, and are passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks.

## Things to try

To get the most out of Veo, incorporate video-specific terminology into your prompts. Veo understands a wide range of terms related to:

*   **Shot composition:** Specify the framing and number of subjects in the shot (e.g., "single shot," "two shot," "over-the-shoulder shot").
*   **Camera positioning and movement:** Control the camera's location and movement using terms like "eye level," "high angle," "worms eye," "dolly shot," "zoom shot," "pan shot," and "tracking shot."
*   **Focus and lens effects:** Use terms like "shallow focus," "deep focus," "soft focus," "macro lens," and "wide-angle lens" to achieve specific visual effects.
*   **Overall style and subject:** Guide Veo's creative direction by specifying styles like "sci-fi," "romantic comedy," "action movie," or "animation." You can also describe the subjects and backgrounds you want, such as "cityscape," "nature," "vehicles," or "animals."

## Veo prompt guide

This section of the Veo guide contains examples of videos you can create using Veo, and shows you how to modify prompts to produce distinct results.

### Safety filters

Veo applies safety filters across Gemini to help ensure that generated videos and uploaded photos don't contain offensive content. Prompts that violate our [terms and guidelines](/gemini-api/docs/usage-policies#abuse-monitoring) are blocked.

### Prompt writing basics

Good prompts are descriptive and clear. To get your generated video as close as possible to what you want, start with identifying your core idea, and then refine your idea by adding keywords and modifiers.

The following elements should be included in your prompt:

*   **Subject**: The object, person, animal, or scenery that you want in your video.
*   **Context**: The background or context in which the subject is placed.
*   **Action**: What the subject is doing (for example, _walking_, _running_, or _turning their head_).
*   **Style**: This can be general or very specific. Consider using specific film style keywords, such as _horror film_, _film noir_, or animated styles like _cartoon_ style.
*   **Camera motion**: \[Optional\] What the camera is doing, such as _aerial view_, _eye-level_, _top-down shot_, or _low-angle shot_.
*   **Composition**: \[Optional\] How the shot is framed, such as _wide shot_, _close-up_, or _extreme close-up_.
*   **Ambiance**: \[Optional\] How the color and light contribute to the scene, such as _blue tones_, _night_, or _warm tones_.

#### More tips for writing prompts

The following tips help you write prompts that generate your videos:

*   **Use descriptive language**: Use adjectives and adverbs to paint a clear picture for Veo.
*   **Provide context**: If necessary, include background information to help your model understand what you want.
*   **Reference specific artistic styles**: If you have a particular aesthetic in mind, reference specific artistic styles or art movements.
*   **Utilize prompt engineering tools**: Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results. For more information, visit [Introduction to prompt design](/gemini-api/docs/prompting-intro).
*   **Enhance the facial details in your personal and group images**: Specify facial details as a focus of the photo like using the word _portrait_ in the prompt.

### Example prompts and output

This section presents several prompts, highlighting how descriptive details can elevate the outcome of each video.

#### Icicles

This video demonstrates how you can use the elements of [prompt writing basics](#basics) in your prompt.

**Prompt**

**Generated output**

Close up shot (composition) of melting icicles (subject) on a frozen rock wall (context) with cool blue tones (ambiance), zoomed in (camera motion) maintaining close-up detail of water drips (action).

![Dripping icicles with a blue background.](/static/gemini-api/docs/video/images/icicles2.gif)

#### Man on the phone

These videos demonstrate how you can revise your prompt with increasingly specific details to get Veo to refine the output to your liking.

**Prompt**

**Generated output**

**Analysis**

The camera dollies to show a close up of a desperate man in a green trench coat. He's making a call on a rotary-style wall phone with a green neon light. It looks like a movie scene.

![Man talking on the phone.](/static/gemini-api/docs/video/images/phonebooth.gif)

This is the first generated video based on the prompt.

A close-up cinematic shot follows a desperate man in a weathered green trench coat as he dials a rotary phone mounted on a gritty brick wall, bathed in the eerie glow of a green neon sign. The camera dollies in, revealing the tension in his jaw and the desperation etched on his face as he struggles to make the call. The shallow depth of field focuses on his furrowed brow and the black rotary phone, blurring the background into a sea of neon colors and indistinct shadows, creating a sense of urgency and isolation.

![Man talking on the phone](/static/gemini-api/docs/video/images/phonebooth2.gif)

A more detailed prompt results in a video that is more focused with a richer environment.

A video with smooth motion that dollies in on a desperate man in a green trench coat, using a vintage rotary phone against a wall bathed in an eerie green neon glow. The camera starts from a medium distance, slowly moving closer to the man's face, revealing his frantic expression and the sweat on his brow as he urgently dials the phone. The focus is on the man's hands, his fingers fumbling with the dial as he desperately tries to connect. The green neon light casts long shadows on the wall, adding to the tense atmosphere. The scene is framed to emphasize the isolation and desperation of the man, highlighting the stark contrast between the vibrant glow of the neon and the man's grim determination.

![Man talking on the phone.](/static/gemini-api/docs/video/images/phonebooth3.gif)

Adding more detail gives the subject a realistic expression and creates an intense and vibrant scene.

#### Snow leopard

This example demonstrates the output Veo might generate for a simple prompt.

**Prompt**

**Generated output**

A cute creature with snow leopard-like fur is walking in winter forest, 3D cartoon style render.

![Snow leopard is lethargic.](/static/gemini-api/docs/video/images/snow_leopard_short.gif)

#### Running snow leopard

This prompt has more detail and demonstrates generated output that might be closer to what you want in your video.

**Prompt**

**Generated output**

Create a short 3D animated scene in a joyful cartoon style. A cute creature with snow leopard-like fur, large expressive eyes, and a friendly, rounded form happily prances through a whimsical winter forest. The scene should feature rounded, snow-covered trees, gentle falling snowflakes, and warm sunlight filtering through the branches. The creature's bouncy movements and wide smile should convey pure delight. Aim for an upbeat, heartwarming tone with bright, cheerful colors and playful animation.

![Snow leopard is running faster.](/static/gemini-api/docs/video/images/running_snow_leopard.gif)

### Examples by writing elements

These examples show you how to refine your prompts by each basic element.

#### Subject

This example shows you how to specify a subject description.

**Subject description**

**Prompt**

**Generated output**

The description can include a subject, or multiple subjects and actions. Here, our subject is "white concrete apartment building."

An architectural rendering of a white concrete apartment building with flowing organic shapes, seamlessly blending with lush greenery and futuristic elements

![Placeholder.](/static/gemini-api/docs/video/images/white_building.gif)

#### Context

This example shows you how to specify context.

**Context**

**Prompt**

**Generated output**

The background or context in which the subject will be placed is very important. Try placing your subject in a variety of backgrounds like on a busy street, or in outer space.

A satellite floating through outer space with the moon and some stars in the background.

![Satellite floating in the atmosphere.](/static/gemini-api/docs/video/images/satellite2.gif)

#### Action

This example shows you how to specify action.

**Action**

**Prompt**

**Generated output**

What is the subject doing like walking, running, or turning their head.

A wide shot of a woman walking along the beach, looking content and relaxed towards the horizon at sunset.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/sunset.gif)

#### Style

This example shows you how to specify style.

**Style**

**Prompt**

**Generated output**

You can add keywords to improve generation quality and steer it closer to intended style, such as shallow depth of field, movie still, minimalistic, surreal, vintage, futuristic, or double-exposure.

Film noir style, man and woman walk on the street, mystery, cinematic, black and white.

![Film noir style is absolutely beautiful.](/static/gemini-api/docs/video/images/film_noir.gif)

#### Camera motion

This example shows you how to specify camera motion.

**Camera motion**

**Prompt**

**Generated output**

Options for camera motion include POV shot, aerial view, tracking drone view, or tracking shot.

A POV shot from a vintage car driving in the rain, Canada at night, cinematic.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/pov_shot.gif)

#### Composition

This example shows you how to specify composition.

**Composition**

**Prompt**

**Generated output**

How the shot is framed (wide shot, close-up, low angle).

Extreme close-up of a an eye with city reflected in it.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/composition_eye_close_up.gif)

Create a video of a wide shot of surfer walking on a beach with a surfboard, beautiful sunset, cinematic.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/composition_surfer.gif)

#### Ambiance

This example shows you how to specify ambiance.

**Ambiance**

**Prompt**

**Generated output**

Color palettes play a vital role in photography, influencing the mood and conveying intended emotions. Try things like "muted orange warm tones," "natural light," "sunrise" or "sunset". For example, a warm, golden palette can infuse a romantic and atmospheric feel into a photograph.

A close-up of a girl holding adorable golden retriever puppy in the park, sunlight.

![A puppy in a young girl's arms.](/static/gemini-api/docs/video/images/ambiance_puppy.gif)

Cinematic close-up shot of a sad woman riding a bus in the rain, cool blue tones, sad mood.

![A woman riding on a bus that feels sad.](/static/gemini-api/docs/video/images/ambiance_sad.gif)

### Use reference images to generate videos

You can bring images to life by using Veo's [image-to-video](/gemini-api/docs/video#generate-from-images) capability. You can use existing assets, or try [Imagen](/gemini-api/docs/image-generation) to generate something new.

**Prompt**

**Generated output**

Bunny with a chocolate candy bar.

![Bunny is running away.](/static/gemini-api/docs/video/images/static_bunny.png)

Bunny runs away.

![Bunny is running away.](/static/gemini-api/docs/video/images/bunny_runs_away.gif)

### Negative prompts

Negative prompts can be a powerful tool to help specify elements you _don't_ want in the video. Describe what you want to discourage the model from generating after the phrase "Negative prompt". Follow these tips:

*   ❌ Don't use instructive language or words like _no_ or _don't_. For example, "No walls" or "don't show walls".
    
*   ✅ Do describe what you don't want to see. For example, "wall, frame", which means that you don't want a wall or a frame in the video.
    

**Prompt**

**Generated output**

Generate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette.

![Tree with using words.](/static/gemini-api/docs/video/images/tree_with_no_negative.gif)

Generate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette.  
  
With negative prompt - urban background, man-made structures, dark, stormy, or threatening atmosphere.

![Tree with no negative words.](/static/gemini-api/docs/video/images/tree_with_negative.gif)

### Aspect ratios

Gemini Veo video generation supports the following two aspect ratios:

**Aspect ratio**

**Description**

Widescreen or 16:9

The most common aspect ratio for televisions, monitors, and mobile phone screens (landscape). Use this when you want to capture more of the background, like in scenic landscapes.

Portrait or 9:16

Rotated widescreen. This aspect ratio has been popularized by short form video applications, such as Youtube shorts. Use this for portraits or tall objects with strong vertical orientations, such as buildings, trees, waterfall, or buildings.  

#### Widescreen

This prompt is an example of the widescreen aspect ratio of 16:9.

**Prompt**

**Generated output**

Create a video with a tracking drone view of a man driving a red convertible car in Palm Springs, 1970s, warm sunlight, long shadows.

![Waterfall is absolutely beautiful.](/static/gemini-api/docs/video/images/widescreen_palm_springs.gif)

#### Portrait

This prompt is an example of the portrait aspect ratio of 9:16.

**Prompt**

**Generated output**

Create a video highlighting the smooth motion of a majestic Hawaiian waterfall within a lush rainforest. Focus on realistic water flow, detailed foliage, and natural lighting to convey tranquility. Capture the rushing water, misty atmosphere, and dappled sunlight filtering through the dense canopy. Use smooth, cinematic camera movements to showcase the waterfall and its surroundings. Aim for a peaceful, realistic tone, transporting the viewer to the serene beauty of the Hawaiian rainforest.

![Waterfall is absolutely beautiful.](/static/gemini-api/docs/video/images/waterfall.gif)

## What's next

*   Gain more experience generating AI videos with the [Veo Colab](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_Veo.ipynb).
*   Check out cool examples using Veo 2 on the [Google DeepMind site](https://deepmind.google/technologies/veo/)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs

Nous vous présentons les mises à jour de notre famille de modèles de pensée 2.5. [En savoir plus](https://ai.google.dev/gemini-api/docs/models?hl=fr)

![](https://ai.google.dev/_static/images/translated.svg?hl=fr) Cette page a été traduite par l'[API Cloud Translation](//cloud.google.com/translate/?hl=fr).

*   [Accueil](https://ai.google.dev/?hl=fr)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=fr)
*   [Modèles](https://ai.google.dev/gemini-api/docs?hl=fr)

# API Gemini Developer

[Obtenir une clé API Gemini](https://aistudio.google.com/apikey?hl=fr)

Obtenez une clé API Gemini et effectuez votre première requête API en quelques minutes.

Si vous [définissez votre clé API](https://ai.google.dev/gemini-api/docs/api-key?hl=fr#set-api-env-var) comme variable d'environnement `GEMINI_API_KEY`, elle sera automatiquement détectée par le client lors de l'utilisation des [bibliothèques d'API Gemini](https://ai.google.dev/gemini-api/docs/libraries?hl=fr). Sinon, vous devrez [transmettre votre clé API](https://ai.google.dev/gemini-api/docs/api-key?hl=fr#provide-api-key-explicitly) en tant qu'argument lors de l'initialisation du client.

### Python

```
from google import genai

# The client gets the API key from the environment variable `GEMINI_API_KEY`.
client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

// The client gets the API key from the environment variable `GEMINI_API_KEY`.
const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    client, err := genai.NewClient(ctx, nil))
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Java

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## Rencontrez les modèles

[Utiliser Gemini dans Google AI Studio](https://aistudio.google.com?hl=fr)

2.5 Pro spark

Notre modèle de pensée le plus puissant, avec des fonctionnalités de raisonnement complexe et bien plus encore

[](https://ai.google.dev/gemini-api/docs/models?hl=fr#gemini-2.5-pro)

2.5 Flash spark

Notre tout dernier modèle multimodal, avec des fonctionnalités de nouvelle génération et des fonctionnalités améliorées

[](https://ai.google.dev/gemini-api/docs/models/gemini?hl=fr#gemini-2.5-flash)

2.5 Flash-Lite spark

Notre modèle multimodal le plus rapide et le plus économique, offrant d'excellentes performances pour les tâches à haute fréquence

[](https://ai.google.dev/gemini-api/docs/models/gemini?hl=fr#gemini-2.5-flash-lite)

## Explorer l'API

![](https://ai.google.dev/static/site-assets/images/image-generation-index.png?hl=fr)

### Génération d'images natives

Générez et modifiez des images hautement contextuelles en mode natif avec Gemini 2.0 Flash.

[](https://ai.google.dev/gemini-api/docs/image-generation?hl=fr)

![](https://ai.google.dev/static/site-assets/images/long-context-overview.png?hl=fr)

### Explorer le contexte long

Importez des millions de jetons dans les modèles Gemini et tirez des insights des images, vidéos et documents non structurés.

[](https://ai.google.dev/gemini-api/docs/long-context?hl=fr)

![](https://ai.google.dev/static/site-assets/images/structured-outputs-index.png?hl=fr)

### Générer des sorties structurées

Contraignez Gemini à répondre au format JSON, un format de données structurées adapté au traitement automatisé.

[](https://ai.google.dev/gemini-api/docs/structured-output?hl=fr)

### Commencer à créer avec l'API Gemini

[Commencer](https://ai.google.dev/gemini-api/docs/quickstart?hl=fr)

Sauf indication contraire, le contenu de cette page est régi par une licence [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/), et les échantillons de code sont régis par une licence [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Pour en savoir plus, consultez les [Règles du site Google Developers](https://developers.google.com/site-policies?hl=fr). Java est une marque déposée d'Oracle et/ou de ses sociétés affiliées.

Dernière mise à jour le 2025/06/27 (UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/image-generation

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# יצירת תמונות

אתם יכולים ליצור תמונות באמצעות Gemini API באמצעות היכולות המולטימודליות המובנות של Gemini או באמצעות Imagen, המודלים של Google ליצירת תמונות. ברוב התרחישים לדוגמה, כדאי להתחיל עם [Gemini](https://ai.google.dev/gemini-api/docs/image-generation?hl=he#gemini). כדאי לבחור ב-[Imagen](https://ai.google.dev/gemini-api/docs/image-generation?hl=he#imagen) למשימות מיוחדות שבהן איכות התמונה קריטית. הנחיות נוספות זמינות בקטע [בחירת המודל המתאים](https://ai.google.dev/gemini-api/docs/image-generation?hl=he#choose-a-model).

כל התמונות שנוצרו כוללות [סימן מים של SynthID](https://ai.google.dev/responsible/docs/safeguards/synthid?hl=he).

## לפני שמתחילים

חשוב לוודא שאתם משתמשים בגרסת מודל נתמכת ליצירת תמונות:

*   ב-**Gemini**, משתמשים באפשרות 'יצירת תמונה של תצוגה מקדימה ב-Gemini 2.0 Flash'.
    
*   ב-**Imagen**, משתמשים באחד מהמודלים של Imagen‏ (Imagen 3, ‏ Imagen 4 או Imagen 4 Ultra).
    
    *   חשוב לדעת שהמודלים האלה זמינים רק ב[תוכנית בתשלום](https://ai.google.dev/gemini-api/docs/pricing?hl=he).

אפשר לגשת למודלים של Gemini ושל Imagen באמצעות אותן ספריות.

## יצירת תמונות באמצעות Gemini

Gemini יכול ליצור ולעבד תמונות בשיחה. אתם יכולים להנחות את Gemini באמצעות טקסט, תמונות או שילוב של שניהם כדי לבצע משימות שונות שקשורות לתמונות, כמו יצירת תמונות ועריכתן.

צריך לכלול את `responseModalities`: `["TEXT", "IMAGE"]` בהגדרה. אין תמיכה בפלט של תמונות בלבד במודלים האלה.

### יצירת תמונות (טקסט לתמונה)

הקוד הבא מדגים איך ליצור תמונה על סמך הנחיה תיאורית:

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO
import base64

client = genai.Client()

contents = ('Hi, can you create a 3d rendered image of a pig '
            'with wings and a top hat flying over a happy '
            'futuristic scifi city with lots of greenery?')

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=contents,
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.save('gemini-native-image.png')
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const contents =
    "Hi, can you create a 3d rendered image of a pig " +
    "with wings and a top hat flying over a happy " +
    "futuristic scifi city with lots of greenery?";

  // Set responseModalities to include "Image" so the model can generate  an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      ResponseModalities: []string{"TEXT", "IMAGE"},
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.0-flash-preview-image-generation",
      genai.Text("Hi, can you create a 3d rendered image of a pig " +
                 "with wings and a top hat flying over a happy " +
                 "futuristic scifi city with lots of greenery?"),
      config,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "gemini_generated_image.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

### REST

```
curl -s -X POST 
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?"}
      ]
    }],
    "generationConfig":{"responseModalities":["TEXT","IMAGE"]}
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-native-image.png
```

![תמונה שנוצרה על ידי AI של חזיר מעופף פנטסטי](https://ai.google.dev/static/gemini-api/docs/images/flying-pig.png?hl=he)

תמונה שנוצרה על ידי AI של חזיר מעופף פנטסטי

### עריכת תמונות (טקסט ותמונה לתמונה)

כדי לערוך תמונה, מוסיפים תמונה כקלט. בדוגמה הבאה מוסבר איך מעלים תמונות בקידוד base64. כדי לשלוח כמה תמונות ועומסי נתונים גדולים יותר, אפשר לעיין בקטע [הזנת תמונה](https://ai.google.dev/gemini-api/docs/image-understanding?hl=he#image-input).

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

import PIL.Image

image = PIL.Image.open('/path/to/image.png')

client = genai.Client()

text_input = ('Hi, This is a picture of me.'
            'Can you add a llama next to me?',)

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=[text_input, image],
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  // Load the image from the local file system
  const imagePath = "path/to/image.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  // Prepare the content parts
  const contents = [
    { text: "Can you add a llama next to the image?" },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  // Set responseModalities to include "Image" so the model can generate an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Go

```
package main

import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
)

func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
     log.Fatal(err)
 }

 imagePath := "/path/to/image.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
   genai.NewPartFromText("Hi, This is a picture of me. Can you add a llama next to me?"),
   &genai.Part{
     InlineData: &genai.Blob{
       MIMEType: "image/png",
       Data:     imgData,
     },
   },
 }

 contents := []*genai.Content{
   genai.NewContentFromParts(parts, genai.RoleUser),
 }

 config := &genai.GenerateContentConfig{
     ResponseModalities: []string{"TEXT", "IMAGE"},
 }

 result, _ := client.Models.GenerateContent(
     ctx,
     "gemini-2.0-flash-preview-image-generation",
     contents,
     config,
 )

 for _, part := range result.Candidates[0].Content.Parts {
     if part.Text != "" {
         fmt.Println(part.Text)
     } else if part.InlineData != nil {
         imageBytes := part.InlineData.Data
         outputFilename := "gemini_generated_image.png"
         _ = os.WriteFile(outputFilename, imageBytes, 0644)
     }
 }
}
```

### REST

```
IMG_PATH=/path/to/your/image1.jpeg

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"'Hi, This is a picture of me. Can you add a llama next to me\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/jpeg\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }],
      \"generationConfig\": {\"responseModalities\": [\"TEXT\", \"IMAGE\"]}
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-edited-image.png
```

### מצבי יצירת תמונות אחרים

Gemini תומך במצבי אינטראקציה אחרים עם תמונות על סמך המבנה וההקשר של ההנחיה, כולל:

*   **טקסט לתמונות וטקסט (משולבים):** הפלט הוא תמונות עם טקסט קשור.
    *   הנחיה לדוגמה: "יצירת מתכון מאויר לפאייה".
*   **תמונות וטקסט לתמונות וטקסט (משולבים)**: המערכת משתמשת בתמונות ובטקסט שהזנתם כדי ליצור תמונות וטקסט קשורים חדשים.
    *   דוגמה להנחיה: (עם תמונה של חדר מרוהט) "אילו ספות בצבעים אחרים יתאימו למרחב שלי? יש לך אפשרות לעדכן את התמונה?"
*   **עריכת תמונות במספר שלבים (צ'אט):** אפשר להמשיך ליצור או לערוך תמונות בצורה רציפה.
    *   הנחיות לדוגמה: \[upload an image of a blue car.\] , "Turn this car into a convertible", "עכשיו משנים את הצבע לצהוב".

### מגבלות

*   כדי להשיג את הביצועים הטובים ביותר, מומלץ להשתמש בשפות הבאות: EN, ‏ es-MX, ‏ ja-JP, ‏ zh-CN,‏ hi-IN.
*   יצירת תמונות לא תומכת בקלט של אודיו או וידאו.
*   יכול להיות שיצירת התמונות לא תמיד תגרום לפעולות הבאות:
    *   המודל יכול להפיק פלט של טקסט בלבד. נסו לבקש פלט של תמונות באופן מפורש (למשל, 'יצירת תמונה', 'הוספת תמונות תוך כדי עבודה', 'עדכון התמונה').
    *   יכול להיות שהמודל יפסיק ליצור תמונות באמצע התהליך. אפשר לנסות שוב או לנסות הנחיה אחרת.
*   כשאתם יוצרים טקסט לתמונה, הכי טוב ליצור קודם את הטקסט ואז לבקש תמונה עם הטקסט.
*   יש אזורים או מדינות שבהם התכונה 'יצירת תמונות' לא זמינה. מידע נוסף זמין במאמר [מודלים](https://ai.google.dev/gemini-api/docs/models?hl=he).

## יצירת תמונות באמצעות המודלים של Imagen

בדוגמה הזו מוצגת יצירת תמונות באמצעות [מודל Imagen](https://deepmind.google/technologies/imagen-3/?hl=he):

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

response = client.models.generate_images(
    model='imagen-4.0-generate-preview-06-06',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 4,
    )
)
for generated_image in response.generated_images:
  generated_image.image.show()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-preview-06-06',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateImagesConfig{
      NumberOfImages: 4,
  }

  response, _ := client.Models.GenerateImages(
      ctx,
      "imagen-4.0-generate-preview-06-06",
      "Robot holding a red skateboard",
      config,
  )

  for n, image := range response.GeneratedImages {
      fname := fmt.Sprintf("imagen-%d.png", n)
          _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
  }
}
```

### REST

```
curl -X POST \
    "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-preview-06-06:predict" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "instances": [
          {
            "prompt": "Robot holding a red skateboard"
          }
        ],
        "parameters": {
          "sampleCount": 4
        }
      }'
```

![תמונה שנוצרה על ידי AI של רובוט שמחזיק סקייטבורד אדום](https://ai.google.dev/static/gemini-api/docs/images/robot-skateboard.png?hl=he)

תמונה שנוצרה על ידי AI של רובוט עם סקייטבורד אדום

### הגדרת Imagen

בשלב הזה, Imagen תומך רק בהנחיות באנגלית ובפרמטרים הבאים:

*   `numberOfImages`: מספר התמונות שייווצרו, מ-1 עד 4 (כולל). ערך ברירת המחדל הוא 4. ב-Imagen 4 Ultra, הערך שמוגדר כברירת מחדל הוא 1 כי אפשר ליצור רק תמונה אחת בכל פעם.
*   `aspectRatio`: שינוי יחס הגובה-רוחב של התמונה שנוצרה. הערכים הנתמכים הם `"1:1"`,‏ `"3:4"`,‏ `"4:3"`,‏ `"9:16"` ו-`"16:9"`. ערך ברירת המחדל הוא `"1:1"`.
*   `personGeneration`: מתן הרשאה למודל ליצור תמונות של אנשים. יש תמיכה בערכים הבאים:
    
    *   `"dont_allow"`: חסימה של יצירת תמונות של אנשים.
    *   `"allow_adult"`: יצירת תמונות של מבוגרים, אבל לא של ילדים. זוהי ברירת המחדל.
    *   `"allow_all"`: יצירת תמונות שכוללות מבוגרים וילדים.

## בחירת המודל המתאים

כדאי לבחור ב-**Gemini** במקרים הבאים:

*   אתם צריכים תמונות שרלוונטיות להקשר, שמנצלות את הידע וההיגיון של העולם.
*   חשוב לשלב בצורה חלקה בין טקסט לתמונות.
*   אתם רוצים להטמיע רכיבים חזותיים מדויקים ברצפי טקסט ארוכים.
*   אתם רוצים לערוך תמונות בצורה שיחה תוך שמירה על ההקשר.

כדאי לבחור ב-**Imagen** במקרים הבאים:

*   איכות התמונה, ריאליזם צילום, פרטים אומנותיים או סגנונות ספציפיים (למשל, אימפרסיוניזם, אנימה) הם בעדיפות עליונה.
*   ביצוע משימות עריכה מיוחדות, כמו עדכוני רקע של מוצרים או התאמת תמונות לרזולוציה גבוהה יותר.
*   הוספת מיתוג, סגנון או יצירת סמלי לוגו ועיצובים של מוצרים.

מודל Imagen 4 הוא המודל המומלץ לשימוש כשמתחילים ליצור תמונות באמצעות Imagen. מומלץ לבחור ב-Imagen 4 Ultra לתרחישים מתקדמים של שימוש או כשצריך את איכות התמונה הטובה ביותר. חשוב לדעת ש-Imagen 4 Ultra יכול ליצור רק תמונה אחת בכל פעם.

## מדריך ליצירת הנחיות ב-Imagen

בקטע הזה במדריך של Imagen מוסבר איך שינוי של הנחיה להמרת טקסט לתמונה יכול להניב תוצאות שונות, ומוצגות דוגמאות לתמונות שאפשר ליצור.

### יסודות לכתיבת הנחיות

הנחיה טובה היא תיאורית וברורה, ומכילה מילות מפתח ומשתני שינוי משמעותיים. קודם כול, כדאי לחשוב על **הנושא**, **ההקשר** ו**הסגנון**.

![הנחיה עם הדגשה של הנושא, ההקשר והסגנון](https://ai.google.dev/static/gemini-api/docs/images/imagen/style-subject-context.png?hl=he)

טקסט תמונה: _רישום_ (**סגנון**) של _בניין דירות מודרני_ (**נושא**) מוקף _גורדי שחקים_ (**הקשר ורקע**).

1.  **נושא**: הדבר הראשון שצריך לחשוב עליו בכל הנחיה הוא _הנושא_: האובייקט, האדם, החיה או הנוף שרוצים לצלם.
    
2.  **הקשר ורקע:** חשוב לא פחות הוא _הרקע או ההקשר_ שבהם הנושא יוצג. נסו למקם את מושא הצילום במגוון רקעים. לדוגמה, סטודיו עם רקע לבן, סביבה חיצונית או פנימית.
    
3.  **סגנון:** לסיום, מוסיפים את סגנון התמונה הרצוי. _הסגנונות_ יכולים להיות כלליים (ציור, צילום, סקיצות) או ספציפיים מאוד (ציור בפסטל, ציור בפחם, איזומטרי תלת-ממדי). אפשר גם לשלב סגנונות.
    

אחרי שכותבים גרסה ראשונה של ההנחיה, מוסיפים פרטים נוספים כדי לשפר אותה עד שמגיעים לתמונה הרצויה. חשוב לבצע איטרציות. קודם מגדירים את הרעיון המרכזי, ואז משפרים ומרחיבים אותו עד שהתמונה שנוצרת קרובה לחזון שלכם.

![תמונה פוטוריאליסטית לדוגמה 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_short.png?hl=he)

הנחיה: פארק באביב ליד אגם

![תמונה פוטוריאליסטית לדוגמה 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_medium.png?hl=he)

הנחיה: פארק באביב ליד אגם, **השמש שוקעת מעבר לאגם, שעת הזהב**

![תמונה פוטוריאליסטית לדוגמה 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_long.png?hl=he)

הנחיה: פארק באביב ליד אגם, _**השמש שוקעת מעבר לאגם, שעת הזהב, פרחים אדומים של פרחים בר**_

מודלים של Imagen יכולים להפוך את הרעיונות שלכם לתמונות מפורטות, גם אם ההנחיות שלכם קצרות וגם אם הן ארוכות ומפורטות. תוכלו לשפר את התוצאה באמצעות הנחיות חוזרות, ולהוסיף פרטים עד שתגיעו לתוצאה המושלמת.

הנחיות קצרות מאפשרות לכם ליצור תמונה במהירות.

![דוגמה להנחיה קצרה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_short-prompt.png?hl=he)

הנחיה: תמונה מקרוב של אישה בשנות ה-20 לחייה, צילום רחוב, תמונה סטילס מסרט, גוונים כתומים חמים עמוקים

הנחיות ארוכות יותר מאפשרות לכם להוסיף פרטים ספציפיים ולבנות את התמונה.

![דוגמה להנחיה ארוכה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_long-prompt.png?hl=he)

הנחיה: תמונה מרתקת של אישה בשנות ה-20 שלה בסגנון צילום רחוב. התמונה אמורה להיראות כמו צילום סטילס מסרט עם גוונים כתומים חמים עמוקים.

טיפים נוספים לכתיבת הנחיות ב-Imagen:

*   **שימוש בשפה תיאורי**: השתמשו במילים מפורטות כמו תארים ומילות יחס כדי לתאר את התמונה בצורה ברורה ל-Imagen.
*   **הוספת הקשר**: אם צריך, מוסיפים מידע רקע כדי לעזור ל-AI להבין את התוכן.
*   **הפניה לאומנים או לסגנונות ספציפיים**: אם יש לכם חזון אסתטי מסוים, כדאי להיעזר בהפניה לאומנים או לתנועות אמנותיות ספציפיים.
*   **שימוש בכלים להנדסת הנחיות**: כדאי לבדוק את הכלים או המשאבים להנדסת הנחיות שיעזרו לכם לשפר את ההנחיות ולקבל תוצאות אופטימליות.
*   **שיפור פרטי הפנים בתמונות אישיות וקבוצתיות**: מציינים את פרטי הפנים כמיקוד התמונה (לדוגמה, משתמשים במילה 'פורטרט' בהנחיה).

### יצירת טקסט בתמונות

מודלים של Imagen יכולים להוסיף טקסט לתמונות, וכך פותחים אפשרויות יצירת תמונות יצירתיות יותר. כדי להפיק את המקסימום מהתכונה הזו, מומלץ לפעול לפי ההנחיות הבאות:

*   **חזרה על תהליך היצירה בביטחון**: יכול להיות שתצטרכו ליצור מחדש את התמונות עד שתגיעו למראה הרצוי. שילוב הטקסט ב-Imagen עדיין מתפתח, ולפעמים כמה ניסיונות מניב את התוצאות הטובות ביותר.
*   **קצר וקולע**: כדי שהיצירה תהיה אופטימלית, כדאי להגביל את הטקסט ל-25 תווים או פחות.
*   **כמה ביטויים**: אפשר לנסות להשתמש בשני או שלושה ביטויים נפרדים כדי לספק מידע נוסף. כדי ליצור קומפוזיציות נקיות יותר, מומלץ להימנע משימוש ביותר משלושה ביטויים.
    
    ![דוגמה לטקסט שנוצר על ידי Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_generate-text.png?hl=he)
    
    הנחיה: פוסטרים עם הטקסט 'Summerland' בגופן מודגש בתור כותרת, מתחת לטקסט הזה מופיע הסלוגן 'Summer never felt so good'
    
*   **הנחיה למיקום**: מערכת Imagen יכולה לנסות למקם את הטקסט לפי ההוראות, אבל יכול להיות שתבחינו בפעמים מסוימות בתנודות. אנחנו כל הזמן משפרים את התכונה הזו.
    
*   **השפעה על סגנון הגופן**: אפשר לציין סגנון גופן כללי כדי להשפיע בעדינות על הבחירות של Imagen. אל תצפו לשכפול מדויק של הגופן, אלא לפרשנויות יצירתיות.
    
*   **גודל גופן**: מציינים גודל גופן או אינדיקציה כללית לגודל (לדוגמה, _קטן_, _בינוני_, _גדול_) כדי להשפיע על יצירת גודל הגופן.
    

### הגדרת פרמטרים להנחיה

כדי לשלוט טוב יותר בתוצאות הפלט, כדאי להגדיר פרמטרים של הקלט ב-Imagen. לדוגמה, נניח שאתם רוצים לאפשר ללקוחות ליצור סמלי לוגו לעסק שלהם, ואתם רוצים לוודא שהסמלים תמיד נוצרים על רקע בצבע אחיד. בנוסף, אתם רוצים להגביל את האפשרויות שהלקוח יכול לבחור בתפריט.

בדוגמה הזו אפשר ליצור הנחיה עם פרמטרים, בדומה לדוגמה הבאה:

A {logo\_style} logo for a {company\_area} company on a solid color background. Include the text {company\_name}.

בממשק המשתמש המותאם אישית, הלקוח יכול להזין את הפרמטרים באמצעות תפריט, והערך שבחר מאכלס את ההנחיה ש-Imagen מקבלת.

לדוגמה:

1.  הנחיה: `A minimalist logo for a health care company on a solid color background. Include the text Journey.`
    
    ![דוגמה 1 להגדרת פרמטרים של הנחיה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_healthcare.png?hl=he)
    
2.  הנחיה: `A modern logo for a software company on a solid color background. Include the text Silo.`
    
    ![דוגמה 2 להגדרת פרמטרים של הנחיה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_software.png?hl=he)
    
3.  הנחיה: `A traditional logo for a baking company on a solid color background. Include the text Seed.`
    
    ![דוגמה 3 להגדרת פרמטרים של הנחיה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_baking.png?hl=he)
    

### שיטות מתקדמות לכתיבת הנחיות

תוכלו להיעזר בדוגמאות הבאות כדי ליצור הנחיות ספציפיות יותר על סמך מאפיינים כמו תיאורים של צילום, צורות וחומרים, תנועות היסטוריות באמנות ומאפייני שינוי של איכות התמונה.

#### צילום

*   ההנחיה כוללת: _"תמונה של..."_

כדי להשתמש בסגנון הזה, כדאי להתחיל להשתמש במילות מפתח שמציינות בבירור ל-Imagen שאתם מחפשים תמונה. נותנים הנחיות שמתחילות ב-_"תמונה של. . ‎."_. לדוגמה:

![תמונה פוטוריאליסטית לדוגמה 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_coffee-beans.png?hl=he)

הנחיה: **תמונה של** פולי קפה במטבח על משטח עץ

![תמונה פוטוריאליסטית לדוגמה 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_chocolate-bar.png?hl=he)

הנחיה: **תמונה של** חפיסת שוקולד על משטח מטבח

![תמונה פוטוריאליסטית לדוגמה 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_modern-building.png?hl=he)

הנחיה: **תמונה של** בניין מודרני עם מים ברקע

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

##### אביזרי צילום

בדוגמאות הבאות אפשר לראות כמה פרמטרים ומשתני אופן פעולה ספציפיים לצילום. אפשר לשלב כמה משתני אופן פעולה כדי לקבל שליטה מדויקת יותר.

1.  **קרבה למצלמה** – _צילום תקריב, שצולם מרחוק_
    
    ![תמונה לדוגמה של מצלמה מקרוב](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_close-up.png?hl=he)
    
    הנחיה: תמונה **מקרוב** של פולי קפה
    
    ![תמונה לדוגמה של מצלמה מורחקת](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_zoomed-out.png?hl=he)
    
    הנחיה: תמונה **לא מוגדלת** של שקית קטנה של  
    פולי קפה במטבח מבולגן
    
2.  **מיקום המצלמה** – _אווירי, מלמטה_
    
    ![תמונה לדוגמה של תצלום אווירי](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_aerial-photo.png?hl=he)
    
    הנחיה: **תמונה אווירית** של עיר עירונית עם גורדי שחקים
    
    ![תצוגה מלמטה של תמונה לדוגמה](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_from-below.png?hl=he)
    
    הנחיה: תמונה של חופת יער עם שמיים כחולים **מלמטה**
    
3.  **תאורה** – _טבעית, דרמטית, חמה, קרה_
    
    ![תמונה לדוגמה עם תאורה טבעית](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_natural-lighting.png?hl=he)
    
    הנחיה: תמונה בסטודיו של כורסה מודרנית, **תאורה טבעית**
    
    ![תמונה לדוגמה של תאורה דרמטית](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_dramatic-lighting.png?hl=he)
    
    הנחיה: תמונה בסטודיו של כורסה מודרנית, **תאורה דרמטית**
    
4.  **הגדרות המצלמה** _\- טשטוש תנועה, מיקוד רך, בוקה, דיוקן_
    
    ![תמונה לדוגמה של טשטוש תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_motion-blur.png?hl=he)
    
    הנחיה: תמונה של עיר עם גורדי שחקים מתוך מכונית עם **טשטוש תנועה**
    
    ![תמונה לדוגמה עם פוקוס רך](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_soft-focus.png?hl=he)
    
    הנחיה: תמונה עם **מיקוד רך** של גשר בעיר אורבנית בלילה
    
5.  **סוגי עדשות** – _35 מ"מ, 50 מ"מ, עדשת עין הדג, עדשת זווית רחבה, עדשת מאקרו_
    
    ![תמונה לדוגמה עם עדשת מאקרו](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_macro-lens.png?hl=he)
    
    הנחיה: תמונה של עלה, **עדשת מאקרו**
    
    ![תמונה לדוגמה של עדשת עין הדג](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_fisheye-lens.png?hl=he)
    
    הנחיה: צילום רחוב, ניו יורק, **עדשת עין הדג**
    
6.  **סוגי סרטים** – _שחור-לבן, פולרואיד_
    
    ![תמונה לדוגמה של תמונה בפורמט פולרואיד](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_polaroid-portrait.png?hl=he)
    
    הנחיה: **דיוקן פולרואיד** של כלב עם משקפי שמש
    
    ![תמונה לדוגמה של תמונה בשחור-לבן](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_bw-photo.png?hl=he)
    
    הנחיה: **תמונה בשחור-לבן** של כלב עם משקפי שמש
    

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

### איור ואומנות

*   ההנחיה כוללת: _"painting של..."_, _'sketch של…'_

סגנונות האמנות משתנים מסגנונות מונוכרום כמו סקיצות בעיפרון ועד לאומנות דיגיטלית היפר-ריאליסטית. לדוגמה, בתמונות הבאות נעשה שימוש באותה הנחיה עם סגנונות שונים:

_"\[art style or creation technique\] של מכונית סדאן ספורטיבית חשמלית עם גורדי שחקים ברקע"_

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1A.png?hl=he)

הנחיה: **רישום טכני בעיפרון** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1B.png?hl=he)

הנחיה: **ציור בפחם** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1C.png?hl=he)

הנחיה: **ציור בעיפרון צבעוני** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2E.png?hl=he)

הנחיה: **ציור בפסטל** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2F.png?hl=he)

הנחיה: **אומנות דיגיטלית** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2G.png?hl=he)

הנחיה: **פוסטר** בסגנון אר דקו של...

מקור התמונות: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 2.

##### צורות וחומרים

*   ההנחיה כוללת: _"...עשוי מ..."_, _'…בצורה של…'_

אחד מהיתרונות של הטכנולוגיה הזו הוא שאפשר ליצור תמונות שקשה או בלתי אפשרי ליצור אותן בדרכים אחרות. לדוגמה, תוכלו ליצור מחדש את הלוגו של החברה בחומרים ובמרקמים שונים.

![תמונה 1 של דוגמה לצורות ולחומרים](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_duffel.png?hl=he)

הנחיה: תיק דאפל **שעשוי** גבינה

![תמונה 2 של דוגמה לצורות ולחומרים](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_bird.png?hl=he)

הנחיה: צינורות ניאון **בצורת** ציפור

![תמונה 3 של דוגמה לצורות ולחומרים](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_paper.png?hl=he)

הנחיה: כורסה **עשויה נייר**, תמונה בסטודיו, בסגנון אוריגמי

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

#### מקורות היסטוריים לאמנות

*   ההנחיה כוללת: _"…בסגנון של…"_

סגנונות מסוימים הפכו לאיקונים עם השנים. ריכזנו כאן כמה רעיונות לציור היסטורי או לסגנונות אמנות היסטוריים שתוכלו לנסות.

_"generate an image in the style of \[art period or movement\] : a wind farm"_

![תמונה לדוגמה של אימפרסיוניזם](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_impressionism.png?hl=he)

הנחיה: ליצור תמונה **בסגנון של _ציור אימפרסיוניסטי_**: חוות רוח

![תמונה לדוגמה של הרנסנס](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_renaissance.png?hl=he)

הנחיה: ליצור תמונה **בסגנון של _ציור מתקופת הרנסנס_**: חוות רוח

![תמונה לדוגמה של פופ ארט](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_pop-art.png?hl=he)

הנחיה: יצירת תמונה **בסגנון _פופ ארט_**: חוות רוח

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

#### משתני איכות תמונה

מילות מפתח מסוימות יכולות להעביר למנוע החיפוש את המסר שאתם מחפשים נכס באיכות גבוהה. דוגמאות למשתני איכות:

*   **תוספי הגדרה כלליים** – _באיכות גבוהה, יפים, בסגנון_
*   **תמונות** – _4K, ‏ HDR, ‏ Studio Photo_
*   **אמנות, איור** – _מקצועי ומפורט_

בהמשך מפורטות כמה דוגמאות להנחיות ללא משתני איכות, ואותה הנחייה עם משתני איכות.

![תמונה לדוגמה של תירס ללא מודификаторים](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_no-mods.png?hl=he)

הנחיה (ללא מודפי איכות): תמונה של גבעול תירס

![תמונה לדוגמה של תירס עם מודификаторים](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_4k-hdr.png?hl=he)

הנחיה (עם משתני איכות): **תמונה יפה**  
ב-4K HDR של גבעול תירס **שצולמה על ידי  
צלם מקצועי**

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

#### יחסי גובה-רוחב

יצירת תמונות באמצעות Imagen מאפשרת להגדיר חמישה יחסי גובה-רוחב שונים של תמונות.

1.  **ריבוע** (1:1, ברירת המחדל) – תמונה ריבועית רגילה. יחס גובה-רוחב נפוץ נוסף הוא 1:1. הוא מתאים לפוסטים ברשתות חברתיות.
2.  **מסך מלא** (4:3) – יחס הגובה-רוחב הזה נפוץ בתקשורת או בסרטים. אלה גם המידות של רוב הטלוויזיות הישנות (לא מסך רחב) ומצלמות בפורמט בינוני. ביחס גובה-רוחב כזה, אפשר לצלם יותר מהסצנה אופקית (בהשוואה ליחס גובה-רוחב של 1:1), ולכן זהו יחס הגובה-רוחב המועדף לצילום.
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_piano.png?hl=he)
    
    הנחיה: תקריב על אצבעות של מוזיקאי שמנגן בפסנתר, סרט בשחור לבן, וינטג'י (יחס גובה-רוחב 4:3)
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_fries.png?hl=he)
    
    הנחיה: תמונה מקצועית בסטודיו של צ'יפס למסעדה יוקרתית, בסגנון של מגזין אוכל (יחס גובה-רוחב של 4:3)
    
3.  **מסך מלא לאורך** (3:4) – זהו יחס הגובה-רוחב של המסך המלא, לאחר סיבוב של 90 מעלות. כך אפשר לצלם יותר מהסצנה אנכית בהשוואה ליחס גובה-רוחב של 1:1.
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_hiking.png?hl=he)
    
    הנחיה: אישה בטיול, תקריב של המגפיים שלה שמשתקפים בבור מים, הרים גדולים ברקע, בסגנון של מודעה, זוויות דרמטיות (יחס גובה-רוחב של 3:4)
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_valley.png?hl=he)
    
    הנחיה: צילום אווירי של נהר שזורם בעמק מיסטיקלי (יחס גובה-רוחב 3:4)
    
4.  **מסך רחב** (16:9) – היחס הזה החליף את היחס 4:3 והוא עכשיו יחס הגובה-רוחב הנפוץ ביותר בטלוויזיות, במסכים ובמסכי טלפונים ניידים (לרוחב). כדאי להשתמש ביחס גובה-רוחב כזה כשרוצים לצלם יותר מהרקע (לדוגמה, נופים).
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_16-9_man.png?hl=he)
    
    הנחיה: גבר לבוש לבן יושב על החוף, תקריב, תאורה בשעת הזהב (יחס גובה-רוחב 16:9)
    
5.  **לאורך** (9:16) – יחס גובה-רוחב של מסך רחב, אבל מסובב. זהו יחס גובה-רוחב חדש יחסית שנעשה פופולרי באפליקציות של סרטונים קצרים (לדוגמה, YouTube Shorts). משתמשים באפשרות הזו עבור אובייקטים גבוהים עם כיוון אנכי בולט, כמו בניינים, עצים, מפלים או אובייקטים דומים אחרים.
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_9-16_skyscraper.png?hl=he)
    
    הנחיה: עיבוד דיגיטלי של גורד שחקים ענק, מודרני, מפואר ואפי, עם שקיעה יפהפייה ברקע (יחס גובה-רוחב של 9:16)
    

#### תמונות פוטוריאליסטיות

גרסאות שונות של מודל יצירת התמונות עשויות להציע שילוב של פלט אמנותי ופלט פוטו-ריאליסטי. כדי ליצור פלט ריאליסטי יותר, על סמך הנושא שרוצים ליצור, מומלץ להשתמש בניסוח הבא בהנחיות.

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אנשים (דיוקנאות)

Prime, zoom

24-35 מ״מ

סרט בשחור-לבן, סרט נואר, עומק שדה, דו-גוני (ציינו שני צבעים)

אוכל, חרקים, צמחים (אובייקטים, טבע דומם)

Macro

60-105 מ״מ

רמת פירוט גבוהה, מיקוד מדויק, תאורה מבוקרת

ספורט, חיות בר (תנועה)

זום טלפוטו

100-400 מ״מ

מהירות תריס מהירה, מעקב אחר תנועה או פעולה

אסטרונומי, לרוחב (רחב-זווית)

זווית רחבה

10-24 מ״מ

זמני חשיפה ארוכים, מיקוד חד, חשיפה ארוכה, מים או עננים חלקים

##### צילומי פורטרט

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אנשים (דיוקנאות)

Prime, zoom

24-35 מ״מ

סרט בשחור-לבן, סרט נואר, עומק שדה, דו-גוני (ציינו שני צבעים)

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את הדיוקנאות הבאים:

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray1.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray2.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray3.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray4.png?hl=he)

הנחיה: _אישה, דיוקן 35 מ"מ, דו-גווני כחול ואפור_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir1.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir2.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir3.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir4.png?hl=he)

הנחיה: _אישה, פורטרט 35 מ"מ, שחור-לבן_  
דגם: `imagen-3.0-generate-002`

##### אובייקטים

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אוכל, חרקים, צמחים (אובייקטים, טבע דומם)

Macro

60-105 מ״מ

רמת פירוט גבוהה, מיקוד מדויק, תאורה מבוקרת

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את תמונות האובייקטים הבאות:

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf1.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf2.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf3.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf4.png?hl=he)

הנחיה: _עלה של צמח תפילה, עדשת מאקרו, 60 מ"מ_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta1.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta2.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta3.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta4.png?hl=he)

הנחיה: _צלחת פסטה, עדשת מאקרו 100 מ"מ_  
דגם: `imagen-3.0-generate-002`

##### נסיעה, תנועה

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

ספורט, חיות בר (תנועה)

זום טלפוטו

100-400 מ״מ

מהירות תריס מהירה, מעקב אחר תנועה או פעולה

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את קובצי ה-GIF הבאים:

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football1.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football2.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football3.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football4.png?hl=he)

הנחיה: _טאצ'דאון מנצח, מהירות תריס מהירה, מעקב אחר תנועה_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer1.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer2.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer3.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer4.png?hl=he)

הנחיה: _צבי שרץ ביער, מהירות תריס מהירה, מעקב אחר תנועה_  
דגם: `imagen-3.0-generate-002`

##### זווית רחבה

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אסטרונומי, לרוחב (רחב-זווית)

זווית רחבה

10-24 מ״מ

זמני חשיפה ארוכים, מיקוד חד, חשיפה ארוכה, מים או עננים חלקים

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את התמונות הרחבות הבאות:

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain1.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain2.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain3.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain4.png?hl=he)

הנחיה: _רכס הרים רחב ידיים, זווית רחבה 10 מ"מ, פריסה לרוחב_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro1.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro2.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro3.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro4.png?hl=he)

הנחיה: _תמונה של הירח, צילום אסטרונומי, זווית רחבה 10mm_  
דגם: `imagen-3.0-generate-002`

## המאמרים הבאים

*   ב[מדריך Veo](https://ai.google.dev/gemini-api/docs/video?hl=he) מוסבר איך ליצור סרטונים באמצעות Gemini API.
*   מידע נוסף על מודלים של Gemini זמין במאמרים [מודלים של Gemini](https://ai.google.dev/gemini-api/docs/models/gemini?hl=he) ו[מודלים ניסיוניים](https://ai.google.dev/gemini-api/docs/models/experimental-models?hl=he).

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/quickstart

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Краткое руководство по API Gemini

В этом кратком руководстве показано, как установить наши [библиотеки](https://ai.google.dev/gemini-api/docs/libraries?hl=ru) и сделать первый запрос API Gemini.

## Прежде чем начать

Вам нужен ключ API Gemini. Если у вас его еще нет, вы можете [получить его бесплатно в Google AI Studio](https://aistudio.google.com/app/apikey?hl=ru) .

## Установите Google GenAI SDK

### Питон

Используя [Python 3.9+](https://www.python.org/downloads/) , установите [пакет `google-genai`](https://pypi.org/project/google-genai/) с помощью следующей [команды pip](https://packaging.python.org/en/latest/tutorials/installing-packages/) :

```
pip install -q -U google-genai
```

### JavaScript

Используя [Node.js v18+](https://nodejs.org/en/download/package-manager) , установите [Google Gen AI SDK для TypeScript и JavaScript](https://www.npmjs.com/package/@google/genai) с помощью следующей [команды npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) :

```
npm install @google/genai
```

### Идти

Установите [google.golang.org/genai](https://pkg.go.dev/google.golang.org/genai) в каталог вашего модуля с помощью [команды go get](https://go.dev/doc/code) :

```
go get google.golang.org/genai
```

### Ява

Если вы используете Maven, вы можете установить [google-genai](https://github.com/googleapis/java-genai) , добавив следующее в свои зависимости:

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

### Скрипт приложений

1.  Чтобы создать новый проект Apps Script, перейдите по адресу [script.new](https://script.google.com/u/0/home/projects/create?hl=ru) .
2.  Нажмите **«Проект без названия»** .
3.  Переименуйте проект Apps Script **в AI Studio** и нажмите **«Переименовать»** .
4.  Установите свой [ключ API](https://developers.google.com/apps-script/guides/properties?hl=ru#manage_script_properties_manually)
    1.  Слева нажмите **«Настройки проекта»** . ![Значок настроек проекта](https://fonts.gstatic.com/s/i/short-term/release/googlesymbols/settings/default/24px.svg) .
    2.  В разделе **«Свойства скрипта»** нажмите **«Добавить свойство скрипта»** .
    3.  Для **свойства** введите имя ключа: `GEMINI_API_KEY` .
    4.  В поле **Значение** введите значение ключа API.
    5.  Нажмите **Сохранить свойства скрипта** .
5.  Замените содержимое файла `Code.gs` следующим кодом:

## Сделайте свой первый запрос

Вот пример, в котором метод [`generateContent`](https://ai.google.dev/api/generate-content?hl=ru#method:-models.generatecontent) используется для отправки запроса к API Gemini с использованием модели Gemini 2.5 Flash.

Если вы [установите свой ключ API](https://ai.google.dev/gemini-api/docs/api-key?hl=ru#set-api-env-var) как переменную среды `GEMINI_API_KEY` , он будет автоматически выбран клиентом при использовании [библиотек API Gemini](https://ai.google.dev/gemini-api/docs/libraries?hl=ru) . В противном случае вам нужно будет [передать свой ключ API](https://ai.google.dev/gemini-api/docs/api-key?hl=ru#provide-api-key-explicitly) в качестве аргумента при инициализации клиента.

Обратите внимание, что все примеры кода в документации API Gemini предполагают, что вы установили переменную среды `GEMINI_API_KEY` .

### Питон

```
from google import genai

# The client gets the API key from the environment variable `GEMINI_API_KEY`.
client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

// The client gets the API key from the environment variable `GEMINI_API_KEY`.
const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Идти

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Ява

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');
function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## «Мышление» включено по умолчанию во многих наших примерах кода.

Многие примеры кода на этом сайте используют модель [Gemini 2.5 Flash](https://ai.google.dev/gemini-api/docs/models?hl=ru#gemini-2.5-flash) , в которой функция [«мышления»](https://ai.google.dev/gemini-api/docs/thinking?hl=ru) включена по умолчанию для повышения качества ответа. Вы должны знать, что это может увеличить время ответа и использование токенов. Если вы отдаете приоритет скорости или хотите минимизировать затраты, вы можете отключить эту функцию, установив бюджет мышления на ноль, как показано в примерах ниже. Для получения более подробной информации см. [руководство по мышлению](https://ai.google.dev/gemini-api/docs/thinking?hl=ru#set-budget) .

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Что дальше?

Теперь, когда вы сделали свой первый запрос к API, вам, возможно, захочется изучить следующие руководства, демонстрирующие Gemini в действии:

*   [Думая](https://ai.google.dev/gemini-api/docs/thinking?hl=ru)
*   [Генерация текста](https://ai.google.dev/gemini-api/docs/text-generation?hl=ru)
*   [Зрение](https://ai.google.dev/gemini-api/docs/vision?hl=ru)
*   [Длинный контекст](https://ai.google.dev/gemini-api/docs/long-context?hl=ru)

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/models

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini models

2.5 Pro spark

Our most powerful thinking model with maximum response accuracy and state-of-the-art performance

*   Input audio, images, video, and text, get text responses
*   Tackle difficult problems, analyze large databases, and more
*   Best for complex coding, reasoning, and multimodal understanding

[](#gemini-2.5-pro)

2.5 Flash spark

Our best model in terms of price-performance, offering well-rounded capabilities.

*   Input audio, images, video, and text, and get text responses
*   Model thinks as needed; or, you can configure a thinking budget
*   Best for low latency, high volume tasks that require thinking

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

*   Input audio, images, video, and text, and get text responses
*   Most cost-efficient model supporting high throughput
*   Best for real time, low latency use cases

[](#gemini-2.5-flash-lite)

## Model variants

The Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:

Model variant

Input(s)

Output

Optimized for

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

Audio, images, videos, text, and PDF

Text

Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

Audio, images, videos, and text

Text

Adaptive thinking, cost efficiency

[Gemini 2.5 Flash-Lite Preview](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

Text, image, video, audio

Text

Most cost-efficient model supporting high throughput

[Gemini 2.5 Flash Native Audio](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog` &  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

Audio, videos, and text

Text and audio, interleaved

High quality, natural conversational audio outputs, with or without thinking

[Gemini 2.5 Flash Preview TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.5 Pro Preview TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

Audio, images, videos, and text

Text

Next generation features, speed, and realtime streaming.

[Gemini 2.0 Flash Preview Image Generation](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

Audio, images, videos, and text

Text, images

Conversational image generation and editing

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

Audio, images, videos, and text

Text

Cost efficiency and low latency

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

Audio, images, videos, and text

Text

Fast and versatile performance across a diverse variety of tasks

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

Audio, images, videos, and text

Text

High volume and lower intelligence tasks

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

Audio, images, videos, and text

Text

Complex reasoning tasks requiring more intelligence

[Gemini Embedding](#gemini-embedding)  
`gemini-embedding-exp`

Text

Text embeddings

Measuring the relatedness of text strings

[Imagen 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

Text

Images

Our most up-to-date image generation model

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

Text

Images

High quality image generation model

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

Text, images

Video

High quality video generation

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

[Gemini 2.0 Flash Live](#live-api-2.0)  
`gemini-2.0-flash-live-001`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

You can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).

### Gemini 2.5 Pro

Gemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro)

#### Model details

Property

Description

id\_cardModel code

`gemini-2.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, text, and PDF

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash

Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash`

saveSupported data types

**Inputs**

Text, images, video, audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Audio generation**

Not supported

**Caching**

Supported

**Code execution**

Supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Stable: `gemini-2.5-flash`
*   Preview: `gemini-2.5-flash-preview-05-20`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash-Lite Preview

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-lite-preview-06-17`

saveSupported data types

**Inputs**

Text, images, video, and audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,000,000

**Output token limit**

64,000

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**URL Context**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-lite-preview-06-17`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Native Audio

Our native audio dialog models, with and without thinking, available through the [Live API](/gemini-api/docs/live). These models provide interactive and unstructured conversational experiences, with style and control prompting.

[Try native audio in Google AI Studio](https://aistudio.google.com/app/live)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-native-audio-dialog` &  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

saveSupported data types

**Inputs**

Audio, video, text

**Output**

Audio and text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

128,000

**Output token limit**

8,000

handymanCapabilities

**Audio generation**

Supported

**Caching**

Not supported

**Code execution**

Not supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Not supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-preview-05-20`
*   Experimental: `gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Preview Text-to-Speech

Gemini 2.5 Flash Preview TTS is our price-performant text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Flash rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-flash-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.5 Pro Preview Text-to-Speech

Gemini 2.5 Pro Preview TTS is our most powerful text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Pro rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-pro-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-pro-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.0 Flash

Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Supported

**Thinking**

Experimental

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash`
*   Stable: `gemini-2.0-flash-001`
*   Experimental: `gemini-2.0-flash-exp`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash Preview Image Generation

Gemini 2.0 Flash Preview Image Generation delivers improved image generation features, including generating and editing images conversationally.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-preview-image-generation`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text and images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

32,000

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not Supported

**Search**

Not Supported

**Image generation**

Supported

**Audio generation**

Not supported

**Live API**

Not Supported

**Thinking**

Not Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-preview-image-generation`

gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash-Lite

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-lite`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Not supported

**Search**

Not supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash-lite`
*   Stable: `gemini-2.0-flash-lite-001`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 1.5 Flash

Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-latest`
*   Latest stable: `gemini-1.5-flash`
*   Stable:

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_monthLatest update

September 2024

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash-8b`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-8b-latest`
*   Latest stable: `gemini-1.5-flash-8b`
*   Stable:

*   `gemini-1.5-flash-8b-001`

calendar\_monthLatest update

October 2024

### Gemini 1.5 Pro

Try [Gemini 2.5 Pro Preview](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date.

Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,097,152

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

7,200

**Maximum video length**

2 hours

**Maximum audio length**

Approximately 19 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-pro-latest`
*   Latest stable: `gemini-1.5-pro`
*   Stable:

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_monthLatest update

September 2024

### Imagen 4

Imagen 4 is our latest image model, capable of generating highly detailed images with rich lighting, significantly better text rendering, and higher resolution output than previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

480 tokens (text)

**Output images**

1 (Ultra)  
1 to 4 (Standard)

calendar\_monthLatest update

June 2025

### Imagen 3

Imagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-3.0-generate-002`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

N/A

**Output images**

Up to 4

calendar\_monthLatest update

February 2025

### Veo 2

Veo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`veo-2.0-generate-001`

saveSupported data types

**Input**

Text, image

**Output**

Video

token\_autoLimits

**Text input**

N/A

**Image input**

Any image resolution and aspect ratio up to 20MB file size

**Output video**

Up to 2

calendar\_monthLatest update

April 2025

### Gemini 2.5 Flash Live

The Gemini 2.5 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-live-2.5-flash-preview)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-live-2.5-flash-preview`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-live-2.5-flash-preview`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.0 Flash Live

The Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-live-001`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-live-001`

calendar\_monthLatest update

April 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini Embedding Experimental

`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. Gemini Embedding rate limits are more restricted since it is an experimental model.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`gemini-embedding-exp-03-07`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,192

**Output dimension size**

Elastic, supports: 3072, 1536, or 768

calendar\_monthLatest update

March 2025

### Text Embedding and Embedding

#### Text Embedding

Try our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance.

[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications.

`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`models/text-embedding-004`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

April 2024

#### Embedding

You can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.

The Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens.

##### Embedding model details

Property

Description

id\_cardModel code

`models/embedding-001`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

December 2023

### AQA

You can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability.

#### Model details

Property

Description

id\_cardModel code

`models/aqa`

saveSupported data types

**Input**

Text

**Output**

Text

languageSupported language

English

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

7,168

**Output token limit**

1,024

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Supported

calendar\_monthLatest update

December 2023

See the [examples](/examples) to explore the capabilities of these model variations.

\[\*\] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words.

## Model version name patterns

Gemini models are available in either _stable_, _preview_, or _experimental_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.

### Latest stable

Points to the most recent stable version released for the specified model generation and variation.

To specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-2.0-flash`.

### Stable

Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.

To specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-flash-001`.

### Preview

Points to a preview model which may not be suitable for production use, come with more restrictive rate limits, but may have billing enabled.

To specify a preview version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.5-pro-preview-06-05`.

### Experimental

Points to an experimental model which may not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.

To specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.

## Experimental models

In addition to stable models, the Gemini API offers experimental models which may not be suitable for production use and come with more restrictive rate limits.

We release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.

### Previous experimental models

As new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:

Model code

Base model

Replacement version

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## Supported languages

Gemini models are trained to work with the following languages:

*   Arabic (`ar`)
*   Bengali (`bn`)
*   Bulgarian (`bg`)
*   Chinese simplified and traditional (`zh`)
*   Croatian (`hr`)
*   Czech (`cs`)
*   Danish (`da`)
*   Dutch (`nl`)
*   English (`en`)
*   Estonian (`et`)
*   Finnish (`fi`)
*   French (`fr`)
*   German (`de`)
*   Greek (`el`)
*   Hebrew (`iw`)
*   Hindi (`hi`)
*   Hungarian (`hu`)
*   Indonesian (`id`)
*   Italian (`it`)
*   Japanese (`ja`)
*   Korean (`ko`)
*   Latvian (`lv`)
*   Lithuanian (`lt`)
*   Norwegian (`no`)
*   Polish (`pl`)
*   Portuguese (`pt`)
*   Romanian (`ro`)
*   Russian (`ru`)
*   Serbian (`sr`)
*   Slovak (`sk`)
*   Slovenian (`sl`)
*   Spanish (`es`)
*   Swahili (`sw`)
*   Swedish (`sv`)
*   Thai (`th`)
*   Turkish (`tr`)
*   Ukrainian (`uk`)
*   Vietnamese (`vi`)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/libraries

2.5 사고 모델 제품군의 업데이트를 소개합니다. [자세히 알아보기](https://ai.google.dev/gemini-api/docs/models?hl=ko)

![](https://ai.google.dev/_static/images/translated.svg?hl=ko) 이 페이지는 [Cloud Translation API](//cloud.google.com/translate/?hl=ko)를 통해 번역되었습니다.

*   [홈](https://ai.google.dev/?hl=ko)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ko)
*   [모델](https://ai.google.dev/gemini-api/docs?hl=ko)

의견 보내기

# Gemini API 라이브러리

이 페이지에서는 Gemini API용 최신 라이브러리를 다운로드하고 설치하는 방법을 설명합니다. Gemini API를 처음 사용하는 경우 [API 빠른 시작](https://ai.google.dev/gemini-api/docs/quickstart?hl=ko)을 참고하세요.

## 새 라이브러리 관련 중요사항

2024년 말부터 Gemini Developer API와 Vertex AI를 통해 Google의 생성형 AI 모델에 액세스하기 위한 통합 인터페이스를 제공하는 새로운 라이브러리 세트인 **Google 생성형 AI SDK**를 점진적으로 출시하고 있습니다.

이제 Google Gen AI SDK가 모든 Google 모델에 액세스하는 데 권장되는 경로이며 문서의 모든 코드 스니펫은 이러한 라이브러리를 활용합니다.

**주요 라이브러리 업데이트**

  

언어

새 라이브러리 (권장)

이전 라이브러리

**Python**

`google-genai`

`google-generativeai`

**JavaScript  
및 TypeScript**

`@google/genai`

`@google/generative-ai`

**Go**

`google.golang.org/genai`

`google.golang.org/generative-ai`

**자바**

`google-genai`

해당 사항 없음, 이전에 존재하지 않음

모든 사용자는 [Google GenAI SDK로 이전](https://ai.google.dev/gemini-api/docs/migrate?hl=ko)하는 것이 좋습니다. 멀티모달 출력 (예: 이미지 및 동영상 생성), 실시간 API, 장기 유지보수와 같은 새로운 기능을 사용할 수 있기 때문입니다. 시간이 지남에 따라 이전 SDK와 새 SDK 간의 기능 격차가 커질 것으로 예상됩니다.

## 라이브러리 설치

### Python

다음을 실행하여 [Python 라이브러리](https://pypi.org/project/google-genai)를 설치할 수 있습니다.

```
pip install google-genai
```

### 자바스크립트

다음을 실행하여 [JavaScript 및 TypeScript 라이브러리](https://www.npmjs.com/package/@google/genai)를 설치할 수 있습니다.

```
npm install @google/genai
```

### Go

다음을 실행하여 [Go 라이브러리](https://pkg.go.dev/google.golang.org/genai)를 설치할 수 있습니다.

```
go get google.golang.org/genai
```

### 자바

Maven을 사용하는 경우 종속 항목에 다음을 추가하여 [Java 라이브러리](https://github.com/googleapis/java-genai)를 설치할 수 있습니다.

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

## 이전 라이브러리 및 SDK

새 라이브러리를 사용할 수 있는 플랫폼의 경우 [새 라이브러리로 이전](https://ai.google.dev/gemini-api/docs/migrate?hl=ko)하는 것이 좋습니다. 이전 라이브러리는 새 기능을 수신하지 않으며 아래에 자세히 설명된 대로 구체적인 지원 상태가 다릅니다.

공식 Google 생성형 AI 라이브러리를 아직 사용할 수 없는 플랫폼의 경우 Google API를 직접 호출하거나, 커뮤니티에서 유지관리하는 라이브러리를 사용하거나, 신뢰할 수 있는 서드 파티 옵션을 사용할 수 있습니다. Google은 공식 SDK 적용 범위를 확대하기 위해 노력하고 있으며, 새로운 라이브러리가 출시되면 여기에 표시됩니다.

이전 라이브러리는 다음과 같습니다.

*   [Python (이전)](https://github.com/google-gemini/deprecated-generative-ai-python): 버그 수정을 비롯한 모든 지원이 2025년 9월에 종료됩니다.
*   [Node.js (이전)](https://github.com/google-gemini/generative-ai-js): 버그 수정을 비롯한 모든 지원이 2025년 9월에 종료됩니다.
*   [Go (이전)](https://github.com/google/generative-ai-go): 버그 수정을 비롯한 모든 지원이 2025년 9월에 종료됩니다.
*   [Dart 및 Flutter (이전)](https://pub.dev/packages/google_generative_ai/install): 활발히 유지관리되지 않음
*   [Swift (이전)](https://github.com/google/generative-ai-swift): 적극적으로 유지보수되지 않음
*   [Android (이전)](https://github.com/google-gemini/generative-ai-android): 적극적으로 유지보수되지 않음

의견 보내기

달리 명시되지 않는 한 이 페이지의 콘텐츠에는 [Creative Commons Attribution 4.0 라이선스](https://creativecommons.org/licenses/by/4.0/)에 따라 라이선스가 부여되며, 코드 샘플에는 [Apache 2.0 라이선스](https://www.apache.org/licenses/LICENSE-2.0)에 따라 라이선스가 부여됩니다. 자세한 내용은 [Google Developers 사이트 정책](https://developers.google.com/site-policies?hl=ko)을 참조하세요. 자바는 Oracle 및/또는 Oracle 계열사의 등록 상표입니다.

최종 업데이트: 2025-06-13(UTC)

---

# Content from: https://ai.google.dev/gemini-api/docs/openai

पेश हैं थिंकिंग मॉडल के 2.5 वर्शन में किए गए अपडेट. [ज़्यादा जानें](https://ai.google.dev/gemini-api/docs/models?hl=hi)

![](https://ai.google.dev/_static/images/translated.svg?hl=hi) इस पेज का अनुवाद [Cloud Translation API](//cloud.google.com/translate/?hl=hi) से किया गया है.

*   [होम पेज](https://ai.google.dev/?hl=hi)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=hi)
*   [मॉडल](https://ai.google.dev/gemini-api/docs?hl=hi)

सुझाव भेजें

# OpenAI की सुविधा के साथ काम करता है

Gemini मॉडल को REST API के साथ-साथ OpenAI लाइब्रेरी (Python और TypeScript / Javascript) का इस्तेमाल करके ऐक्सेस किया जा सकता है. इसके लिए, कोड की तीन लाइनों को अपडेट करें और अपनी [Gemini API पासकोड](https://aistudio.google.com/apikey?hl=hi) का इस्तेमाल करें. अगर पहले से ही OpenAI लाइब्रेरी का इस्तेमाल नहीं किया जा रहा है, तो हमारा सुझाव है कि आप सीधे [Gemini API](https://ai.google.dev/gemini-api/docs/quickstart?hl=hi) को कॉल करें.

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ]
    }'
```

क्या बदलाव हुआ है? सिर्फ़ तीन लाइनें!

*   **`api_key="GEMINI_API_KEY"`**: "`GEMINI_API_KEY`" को अपने असली Gemini API पासकोड से बदलें. यह पासकोड, [Google AI Studio](https://aistudio.google.com?hl=hi) में मिल सकता है.
    
*   **`base_url="https://generativelanguage.googleapis.com/v1beta/openai/"`:** इससे, OpenAI लाइब्रेरी को डिफ़ॉल्ट यूआरएल के बजाय, Gemini API एंडपॉइंट पर अनुरोध भेजने के लिए कहा जाता है.
    
*   **`model="gemini-2.0-flash"`**: काम करने वाला Gemini मॉडल चुनना
    

## Gemini जवाब ढूँढ रहा है

Gemini 2.5 मॉडल को मुश्किल समस्याओं को हल करने के लिए ट्रेन किया गया है. इससे, इन मॉडल की रिज़निंग की क्षमता काफ़ी बेहतर हो गई है. Gemini API में ["थिंकिंग बजट" पैरामीटर](https://ai.google.dev/gemini-api/docs/thinking?hl=hi#set-budget) होता है. इससे यह तय किया जा सकता है कि मॉडल को कितना सोचना है.

Gemini API के मुकाबले, OpenAI API में सोचने के कंट्रोल के तीन लेवल होते हैं: `"low"`, `"medium"`, और `"high"`. ये लेवल, क्रमशः 1,024, 8,192, और 24,576 टोकन पर मैप होते हैं.

अगर आपको सोचने की सुविधा बंद करनी है, तो `reasoning_effort` को `"none"` पर सेट करें (ध्यान दें कि 2.5 Pro मॉडल के लिए, तर्क करने की सुविधा बंद नहीं की जा सकती).

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    reasoning_effort="low",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    reasoning_effort: "low",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
    "reasoning_effort": "low",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
      ]
    }'
```

Gemini के थिंकिंग मॉडल, [विचारों की खास जानकारी](https://ai.google.dev/gemini-api/docs/thinking?hl=hi#summaries) भी देते हैं. साथ ही, [थिंकिंग बजट](https://ai.google.dev/gemini-api/docs/thinking?hl=hi#set-budget) का सटीक इस्तेमाल कर सकते हैं. अपने अनुरोध में इन फ़ील्ड को शामिल करने के लिए, [`extra_body`](#extra-body) फ़ील्ड का इस्तेमाल किया जा सकता है.

ध्यान दें कि `reasoning_effort` और `thinking_budget` की सुविधाएं एक जैसी हैं. इसलिए, इन्हें एक साथ इस्तेमाल नहीं किया जा सकता.

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[{"role": "user", "content": "Explain to me how AI works"}],
    extra_body={
      'extra_body': {
        "google": {
          "thinking_config": {
            "thinking_budget": 800,
            "include_thoughts": True
          }
        }
      }
    }
)

print(response.choices[0].message)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    messages: [{role: "user", content: "Explain to me how AI works",}],
    extra_body: {
      "google": {
        "thinking_config": {
          "thinking_budget": 800,
          "include_thoughts": true
        }
      }
    }
});

console.log(response.choices[0].message);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
      "messages": [{"role": "user", "content": "Explain to me how AI works"}],
      "extra_body": {
        "google": {
           "thinking_config": {
             "include_thoughts": true
           }
        }
      }
    }'
```

## स्ट्रीमिंग

Gemini API, [स्ट्रीमिंग रिस्पॉन्स](https://ai.google.dev/gemini-api/docs/text-generation?lang=python&hl=hi#generate-a-text-stream) के साथ काम करता है.

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  stream=True
)

for chunk in response:
    print(chunk.choices[0].delta)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    stream: true,
  });

  for await (const chunk of completion) {
    console.log(chunk.choices[0].delta.content);
  }
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ],
    "stream": true
  }'
```

## फ़ंक्शन कॉल करना

फ़ंक्शन कॉल करने से, जनरेटिव मॉडल से स्ट्रक्चर्ड डेटा आउटपुट पाना आसान हो जाता है. साथ ही, यह [Gemini API के साथ काम करता है](https://ai.google.dev/gemini-api/docs/function-calling/tutorial?hl=hi).

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

tools = [
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "Get the weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. Chicago, IL",
          },
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
        },
        "required": ["location"],
      },
    }
  }
]

messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}]
response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=messages,
  tools=tools,
  tool_choice="auto"
)

print(response)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}];
  const tools = [
      {
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Get the weather in a given location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. Chicago, IL",
              },
              "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
          },
        }
      }
  ];

  const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: messages,
    tools: tools,
    tool_choice: "auto",
  });

  console.log(response);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
  "model": "gemini-2.0-flash",
  "messages": [
    {
      "role": "user",
      "content": "What'\''s the weather like in Chicago today?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. Chicago, IL"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "tool_choice": "auto"
}'
```

## इमेज की बारीक़ी से पहचान

Gemini मॉडल, नेटिव तौर पर कई मोड में काम करते हैं. साथ ही, [विज़न से जुड़े कई सामान्य टास्क](https://ai.google.dev/gemini-api/docs/vision?hl=hi) में सबसे बेहतर परफ़ॉर्म करते हैं.

### Python

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Getting the base64 string
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

### JavaScript

```
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

### REST

```
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

## इमेज जनरेट करें

इमेज जनरेट करने के लिए:

### Python

```
import base64
from openai import OpenAI
from PIL import Image
from io import BytesIO

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

response = client.images.generate(
    model="imagen-3.0-generate-002",
    prompt="a portrait of a sheepadoodle wearing a cape",
    response_format='b64_json',
    n=1,
)

for image_data in response.data:
  image = Image.open(BytesIO(base64.b64decode(image_data.b64_json)))
  image.show()
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const image = await openai.images.generate(
    {
      model: "imagen-3.0-generate-002",
      prompt: "a portrait of a sheepadoodle wearing a cape",
      response_format: "b64_json",
      n: 1,
    }
  );

  console.log(image.data);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/images/generations" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer GEMINI_API_KEY" \
  -d '{
        "model": "imagen-3.0-generate-002",
        "prompt": "a portrait of a sheepadoodle wearing a cape",
        "response_format": "b64_json",
        "n": 1,
      }'
```

## ऑडियो को समझना

ऑडियो इनपुट का विश्लेषण करें:

### Python

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

with open("/path/to/your/audio/file.wav", "rb") as audio_file:
  base64_audio = base64.b64encode(audio_file.read()).decode('utf-8')

response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Transcribe this audio",
        },
        {
              "type": "input_audio",
              "input_audio": {
                "data": base64_audio,
                "format": "wav"
          }
        }
      ],
    }
  ],
)

print(response.choices[0].message.content)
```

### JavaScript

```
import fs from "fs";
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

const audioFile = fs.readFileSync("/path/to/your/audio/file.wav");
const base64Audio = Buffer.from(audioFile).toString("base64");

async function main() {
  const response = await client.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Transcribe this audio",
          },
          {
            type: "input_audio",
            input_audio: {
              data: base64Audio,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

  console.log(response.choices[0].message.content);
}

main();
```

### REST

```
bash -c '
  base64_audio=$(base64 -i "/path/to/your/audio/file.wav");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"Transcribe this audio file.\" },
            {
              \"type\": \"input_audio\",
              \"input_audio\": {
                \"data\": \"${base64_audio}\",
                \"format\": \"wav\"
              }
            }
          ]
        }
      ]
    }"
'
```

## स्ट्रक्चर्ड आउटपुट

Gemini मॉडल, JSON ऑब्जेक्ट को [आपके तय किए गए किसी भी स्ट्रक्चर](https://ai.google.dev/gemini-api/docs/structured-output?hl=hi) में आउटपुट कर सकते हैं.

### Python

```
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model="gemini-2.0-flash",
    messages=[
        {"role": "system", "content": "Extract the event information."},
        {"role": "user", "content": "John and Susan are going to an AI conference on Friday."},
    ],
    response_format=CalendarEvent,
)

print(completion.choices[0].message.parsed)
```

### JavaScript

```
import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai"
});

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const completion = await openai.beta.chat.completions.parse({
  model: "gemini-2.0-flash",
  messages: [
    { role: "system", content: "Extract the event information." },
    { role: "user", content: "John and Susan are going to an AI conference on Friday" },
  ],
  response_format: zodResponseFormat(CalendarEvent, "event"),
});

const event = completion.choices[0].message.parsed;
console.log(event);
```

## एंबेड करना

टेक्स्ट एम्बेडमेंट, टेक्स्ट स्ट्रिंग की आपस में मिलती-जुलती होने की डिग्री का आकलन करते हैं. इन्हें [Gemini API](https://ai.google.dev/gemini-api/docs/embeddings?hl=hi) का इस्तेमाल करके जनरेट किया जा सकता है.

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-004"
)

print(response.data[0].embedding)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const embedding = await openai.embeddings.create({
    model: "text-embedding-004",
    input: "Your text string goes here",
  });

  console.log(embedding);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/embeddings" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "input": "Your text string goes here",
    "model": "text-embedding-004"
  }'
```

## `extra_body`

Gemini में ऐसी कई सुविधाएं काम करती हैं जो OpenAI के मॉडल में उपलब्ध नहीं हैं. हालांकि, `extra_body` फ़ील्ड का इस्तेमाल करके इन्हें चालू किया जा सकता है.

**`extra_body` सुविधाएं**

`safety_settings`

यह Gemini के `SafetySetting` से मेल खाता है.

`cached_content`

यह Gemini के `GenerateContentRequest.cached_content` से मेल खाता है.

`thinking_config`

यह Gemini के `ThinkingConfig` से मेल खाता है.

### `cached_content`

`cached_content` सेट करने के लिए, `extra_body` का इस्तेमाल करने का उदाहरण यहां दिया गया है:

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key=MY_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)

stream = client.chat.completions.create(
    model="gemini-2.5-pro",
    n=1,
    messages=[
        {
            "role": "user",
            "content": "Summarize the video"
        }
    ],
    stream=True,
    stream_options={'include_usage': True},
    extra_body={
        'extra_body':
        {
            'google': {
              'cached_content': "cachedContents/0000aaaa1111bbbb2222cccc3333dddd4444eeee"
          }
        }
    }
)

for chunk in stream:
    print(chunk)
    print(chunk.usage.to_dict())
```

## मॉडल की सूची

Gemini के उपलब्ध मॉडल की सूची देखें:

### Python

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

models = client.models.list()
for model in models:
  print(model.id)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const list = await openai.models.list();

  for await (const model of list) {
    console.log(model);
  }
}
main();
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models \
-H "Authorization: Bearer GEMINI_API_KEY"
```

## मॉडल वापस लाना

Gemini मॉडल वापस पाने के लिए:

### Python

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

model = client.models.retrieve("gemini-2.0-flash")
print(model.id)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const model = await openai.models.retrieve("gemini-2.0-flash");
  console.log(model.id);
}

main();
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models/gemini-2.0-flash \
-H "Authorization: Bearer GEMINI_API_KEY"
```

## मौजूदा सीमाएं

OpenAI लाइब्रेरी के लिए, सहायता अब भी बीटा वर्शन में है. हम इस सुविधा के लिए सहायता को बढ़ा रहे हैं.

अगर आपको काम करने वाले पैरामीटर, आने वाली सुविधाओं या Gemini का इस्तेमाल शुरू करने में कोई समस्या आ रही है, तो हमारे [डेवलपर फ़ोरम](https://discuss.ai.google.dev/c/gemini-api/4?hl=hi) में शामिल हों.

## आगे क्या करना है

ज़्यादा जानकारी वाले उदाहरणों के साथ काम करने के लिए, [OpenAI के साथ काम करने की सुविधा वाला Colab](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_OpenAI_Compatibility.ipynb?hl=hi) आज़माएं.

सुझाव भेजें

जब तक कुछ अलग से न बताया जाए, तब तक इस पेज की सामग्री को [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/) के तहत और कोड के नमूनों को [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0) के तहत लाइसेंस मिला है. ज़्यादा जानकारी के लिए, [Google Developers साइट नीतियां](https://developers.google.com/site-policies?hl=hi) देखें. Oracle और/या इससे जुड़ी हुई कंपनियों का, Java एक रजिस्टर किया हुआ ट्रेडमार्क है.

आखिरी बार 2025-06-18 (UTC) को अपडेट किया गया.

---

# Content from: https://ai.google.dev/gemini-api/docs/rate-limits

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Rate limits

Rate limits regulate the number of requests you can make to the Gemini API within a given timeframe. These limits help maintain fair usage, protect against abuse, and help maintain system performance for all users.

## How rate limits work

Rate limits are usually measured across three dimensions:

*   Requests per minute (**RPM**)
*   Requests per day (**RPD**)
*   Tokens per minute (input) (**TPM**)

Your usage is evaluated against each limit, and exceeding any of them will trigger a rate limit error. For example, if your RPM limit is 20, making 21 requests within a minute will result in an error, even if you haven't exceeded your TPM or other limits.

Rate limits are applied per project, not per API key.

Limits vary depending on the specific model being used, and some limits only apply to specific models. For example, Images per minute, or IPM, is only calculated for models capable of generating images (Imagen 3), but is conceptually similar to TPM. Other models might have a token per day limit (TPD).

Rate limits are more restricted for experimental and preview models.

## Usage tiers

Rate limits are tied to the project's usage tier. As your API usage and spending increase, you'll have an option to upgrade to a higher tier with increased rate limits.

Tier

Qualifications

Free

Users in [eligible countries](/gemini-api/docs/available-regions)

Tier 1

Billing account [linked to the project](/gemini-api/docs/billing#enable-cloud-billing)

Tier 2

Total spend: > $250 and at least 30 days since successful payment

Tier 3

Total spend: > $1,000 and at least 30 days since successful payment

When you request an upgrade, our automated abuse protection system performs additional checks. While meeting the stated qualification criteria is generally sufficient for approval, in rare cases an upgrade request may be denied based on other factors identified during the review process.

This system helps maintain the security and integrity of the Gemini API platform for all users.

## Current rate limits

### Free Tier

Model

RPM

TPM

RPD

Gemini 2.5 Pro

5

250,000

100

Gemini 2.5 Flash

10

250,000

250

Gemini 2.5 Flash-Lite Preview 06-17

15

250,000

1,000

Gemini 2.5 Flash Preview TTS

3

10,000

15

Gemini 2.5 Pro Preview TTS

\--

\--

\--

Gemini 2.0 Flash

15

1,000,000

200

Gemini 2.0 Flash Preview Image Generation

10

200,000

100

Gemini 2.0 Flash-Lite

30

1,000,000

200

Imagen 3

\--

\--

\--

Veo 2

\--

\--

\--

Gemini 1.5 Flash (Deprecated)

15

250,000

50

Gemini 1.5 Flash-8B (Deprecated)

15

250,000

50

Gemini 1.5 Pro (Deprecated)

\--

\--

\--

Gemma 3 & 3n

30

15,000

14,400

Gemini Embedding Experimental 03-07

5

\--

100

### Tier 1

Model

RPM

TPM

RPD

Gemini 2.5 Pro

150

2,000,000

1,000

Gemini 2.5 Flash

1,000

1,000,000

10,000

Gemini 2.5 Flash-Lite Preview 06-17

4,000

4,000,000

\--

Gemini 2.5 Flash Preview TTS

10

10,000

100

Gemini 2.5 Pro Preview TTS

10

10,000

50

Gemini 2.0 Flash

2,000

4,000,000

\--

Gemini 2.0 Flash Preview Image Generation

1,000

1,000,000

10,000

Gemini 2.0 Flash-Lite

4,000

4,000,000

\--

Imagen 4 Standard

10

\--

70

Imagen 4 Ultra

5

\--

30

Imagen 3

20

\--

\--

Veo 2

2 videos per minute

\--

50 videos per day

Gemini 1.5 Flash (Deprecated)

2,000

4,000,000

\--

Gemini 1.5 Flash-8B (Deprecated)

4,000

4,000,000

\--

Gemini 1.5 Pro (Deprecated)

1,000

4,000,000

\--

Gemma 3 & 3n

30

15,000

14,400

Gemini Embedding Experimental 03-07

10

\--

1,000

### Tier 2

Model

RPM

TPM

RPD

Gemini 2.5 Pro

1,000

5,000,000

50,000

Gemini 2.5 Flash

2,000

3,000,000

100,000

Gemini 2.5 Flash-Lite Preview 06-17

10,000

10,000,000

100,000

Gemini 2.5 Flash Preview TTS

1,000

100,000

10,000

Gemini 2.5 Pro Preview TTS

100

25,000

1,000

Gemini 2.0 Flash

10,000

10,000,000

\--

Gemini 2.0 Flash Preview Image Generation

2,000

3,000,000

100,000

Gemini 2.0 Flash-Lite

20,000

10,000,000

\--

Imagen 4 Standard

10

\--

70

Imagen 4 Ultra

5

\--

30

Imagen 3

20

\--

\--

Veo 2

2 videos per minute

\--

50 videos per day

Gemini 1.5 Flash (Deprecated)

2,000

4,000,000

\--

Gemini 1.5 Flash-8B (Deprecated)

4,000

4,000,000

\--

Gemini 1.5 Pro (Deprecated)

1,000

4,000,000

\--

Gemma 3 & 3n

30

15,000

14,400

Gemini Embedding Experimental 03-07

10

\--

1,000

### Tier 3

Model

RPM

TPM

RPD

Gemini 2.5 Pro

2,000

8,000,000

\--

Gemini 2.5 Flash

10,000

8,000,000

\--

Gemini 2.5 Flash-Lite Preview 06-17

30,000

30,000,000

\--

Gemini 2.5 Flash Preview TTS

1,000

1,000,000

\--

Gemini 2.5 Pro Preview TTS

100

1,000,000

\--

Gemini 2.0 Flash

30,000

30,000,000

\--

Gemini 2.0 Flash Preview Image Generation

5,000

5,000,000

\--

Gemini 2.0 Flash-Lite

30,000

30,000,000

\--

Imagen 4 Standard

10

\--

70

Imagen 4 Ultra

5

\--

30

Imagen 3

20

\--

\--

Veo 2

2 videos per minute

\--

50 videos per day

Gemini 1.5 Flash (Deprecated)

2,000

4,000,000

\--

Gemini 1.5 Flash-8B (Deprecated)

4,000

4,000,000

\--

Gemini 1.5 Pro (Deprecated)

1,000

4,000,000

\--

Gemma 3 & 3n

30

15,000

14,400

Gemini Embedding Experimental 03-07

10

\--

1,000

Specified rate limits are not guaranteed and actual capacity may vary.

### Live API rate limits

### Free Tier

Name

Concurrent sessions

TPM

RPD

Gemini 2.5 Flash Live

3

1,000,000

\--

Gemini 2.0 Flash Live

3

1,000,000

\--

Gemini 2.5 Flash Preview Native Audio Dialog

1

25,000

5

Gemini 2.5 Flash Experimental Native Audio Thinking Dialog

1

10,000

5

### Tier 1

Name

Concurrent sessions

TPM

RPD

Gemini 2.5 Flash Live

50

4,000,000

\--

Gemini 2.0 Flash Live

50

4,000,000

\--

Gemini 2.5 Flash Preview Native Audio Dialog

3

50,000

50

Gemini 2.5 Flash Experimental Native Audio Thinking Dialog

1

25,000

50

### Tier 2

Name

Concurrent sessions

TPM

RPD

Gemini 2.5 Flash Live

1000

10,000,000

\--

Gemini 2.0 Flash Live

1000

10,000,000

\--

Gemini 2.5 Flash Preview Native Audio Dialog

100

1,000,000

Unlimited

Gemini 2.5 Flash Experimental Native Audio Thinking Dialog

\--

\--

\--

### Tier 3

Name

Concurrent sessions

TPM

RPD

Gemini 2.5 Flash Live

1000

10,000,000

\--

Gemini 2.0 Flash Live

1000

10,000,000

\--

Gemini 2.5 Flash Preview Native Audio Dialog

\--

\--

\--

Gemini 2.5 Flash Experimental Native Audio Thinking Dialog

\--

\--

\--

Specified rate limits are not guaranteed and actual capacity may vary.

## How to upgrade to the next tier

The Gemini API uses Cloud Billing for all billing services. To transition from the Free tier to a paid tier, you must first enable Cloud Billing for your Google Cloud project.

Once your project meets the specified criteria, it becomes eligible for an upgrade to the next tier. To request an upgrade, follow these steps:

*   Navigate to the [API keys page](https://aistudio.google.com/app/apikey) in AI Studio.
*   Locate the project you want to upgrade and click "Upgrade". The "Upgrade" option will only show up for projects that meet [next tier qualifications](/gemini-api/docs/rate-limits#usage-tiers).

After a quick validation, the project will be upgraded to the next tier.

## Request a rate limit increase

Each model variation has an associated rate limit (requests per minute, RPM). For details on those rate limits, see [Gemini models](/models/gemini).

[Request paid tier rate limit increase](https://forms.gle/ETzX94k8jf7iSotH9)

We offer no guarantees about increasing your rate limit, but we'll do our best to review your request and reach out to you if we're able to accommodate your capacity needs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-28 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/billing

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Billing

This guide provides an overview of different Gemini API billing options, explains how to enable billing and monitor usage, and provides answers to frequently asked questions (FAQs) about billing.

[Upgrade to the Gemini API paid tier](https://aistudio.google.com/apikey)

## About billing

Billing for the Gemini API is based on two pricing tiers: _free of charge_ (or _free_) and _pay-as-you-go_ (or _paid_). Pricing and rate limits differ between these tiers and also vary by model. You can check out the [rate limits](/gemini-api/docs/rate-limits) and [pricing](/gemini-api/docs/pricing) pages for more into. For a model-by-model breakdown of capabilities, see the [Gemini models page](/gemini-api/docs/models/gemini).

#### How to request an upgrade

To transition from the free tier to the pay-as-you-go plan, you need to enable billing for your Google Cloud project. The button you see in Google AI Studio depends on your project's current plan.

*   If you're on the free tier, you'll see a **Set up Billing** button for your project.
*   If you're already on the paid tier and meet the criteria for a plan change, you might see an **Upgrade** button.

To start the process, follow these steps:

1.  Go to the [AI Studio API keys page](https://aistudio.google.com/app/apikey).
2.  Find the project you want to move to the paid plan and click either **Set up Billing** or **Upgrade**, depending on the button displayed.
3.  The next step depends on the button you clicked:
    *   **If you clicked Set up Billing:** You'll be redirected to the Google Cloud console to link a billing account to your project. Follow the on-screen instructions to complete the process.
    *   **If you clicked Upgrade:** The system will automatically verify your project's eligibility. If your project meets all the requirements, it will be instantly upgraded to the next tier.

### Why use the paid tier?

When you enable billing and use the paid tier, you benefit from [higher rate limits](/gemini-api/docs/rate-limits), and your prompts and responses aren't used to improve Google products. For more information on data use for paid services, see the [terms of service](/gemini-api/terms#data-use-paid).

### Cloud Billing

The Gemini API uses [Cloud Billing](https://cloud.google.com/billing/docs/concepts) for billing services. To use the paid tier, you must set up Cloud Billing on your cloud project. After you've enabled Cloud Billing, you can use Cloud Billing tools to track spending, understand costs, make payments, and access Cloud Billing support.

## Enable billing

You can enable Cloud Billing starting from Google AI Studio:

1.  Open [Google AI Studio](https://aistudio.google.com/).
    
2.  In the bottom of the left sidebar, select **Settings** > **Plan information**.
    
3.  Click **Set up Billing** for your chosen project to enable Cloud Billing.
    

## Monitor usage

After you enable Cloud Billing, you can monitor your usage of the Gemini API in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com). The service name for the API is `generativelanguage.googleapis.com`, and in the console the Gemini API is also referred to as the **Generative Language API**.

![Cloud console](/static/gemini-api/docs/images/console-apis-services.png "Cloud console")

To learn more, see the Google Cloud documentation on [monitoring API usage](https://cloud.google.com/apis/docs/monitoring).

## Frequently asked questions

This section provides answers to frequently asked questions.

### What am I billed for?

Gemini API pricing is based on the following:

*   Input token count
*   Output token count
*   Cached token count
*   Cached token storage duration

For pricing information, see the [pricing page](/pricing).

### Where can I view my quota?

You can view your quota and system limits in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas).

### How do I request more quota?

To request more quota, follow the instructions at [How to request an upgrade](#request-an-upgrade).

### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?

Yes, we make the free tier and paid tier available in [many regions](/gemini-api/docs/available-regions).

### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?

No, Google AI Studio usage remains free of charge regardless of if you set up billing across all supported regions.

### Can I use 1M tokens in the free tier?

The free tier for Gemini API differs based on the model selected. For now, you can try the 1M token context window in the following ways:

*   In Google AI Studio
*   With pay-as-you-go plans
*   With free-of-charge plans for select models

See the latest free-of-charge rate limits per model on [rate limits page](/gemini-api/docs/rate-limits).

### How can I calculate the number of tokens I'm using?

Use the [`GenerativeModel.count_tokens`](/api/python/google/generativeai/GenerativeModel#count_tokens) method to count the number of tokens. Refer to the [Tokens guide](/gemini-api/docs/tokens) to learn more about tokens.

### Can I use my Google Cloud credits with the Gemini API?

Yes, Google Cloud credits can be used towards Gemini API usage.

### How is billing handled?

Billing for the Gemini API is handled by the [Cloud Billing](https://cloud.google.com/billing/docs/concepts) system.

### Am I charged for failed requests?

If your request fails with a 400 or 500 error, you won't be charged for the tokens used. However, the request will still count against your quota.

### Is there a charge for fine-tuning the models?

[Model tuning](/gemini-api/docs/model-tuning) is free, but inference on tuned models is charged at the same rate as the base models.

### Is GetTokens billed?

Requests to the GetTokens API are not billed, and they don't count against inference quota.

### How is my Google AI Studio data handled if I have a paid API account?

Refer to the [terms](/gemini-api/terms#paid-services) for details on how data is handled when Cloud billing is enabled (see "How Google Uses Your Data" under "Paid Services"). Note that your Google AI Studio prompts are treated under the same "Paid Services" terms so long as at least 1 API project has billing enabled, which you can validate on the [Gemini API Key page](https://aistudio.google.com/apikey) if you see any projects marked as "Paid" under "Plan".

### Where can I get help with billing?

To get help with billing, see [Get Cloud Billing support](https://cloud.google.com/support/billing).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-05-31 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/api-key

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# 使用 Gemini API 密钥

如需使用 Gemini API，您需要一个 API 密钥。您只需在 [Google AI Studio](https://aistudio.google.com/app/apikey?hl=zh-cn) 中点击几下，即可免费创建密钥。

获得 API 密钥后，您可以通过以下方式连接到 Gemini API：

*   [将 API 密钥设置为环境变量](#set-api-env-var)
*   [明确提供您的 API 密钥](#provide-api-key-explicitly)

对于初始测试，您可以对 API 密钥进行硬编码，但这应该只是暂时性的，因为这样做不安全。如需查看对 API 密钥进行硬编码的示例，请参阅[显式提供 API 密钥](#provide-api-key-explicitly)。

## 将 API 密钥设置为环境变量

如果您设置了环境变量 `GEMINI_API_KEY` 或 `GOOGLE_API_KEY`，则客户端在使用某个 [Gemini API 库](https://ai.google.dev/gemini-api/docs/libraries?hl=zh-cn)时会自动提取 API 密钥。建议您仅设置其中一个变量，但如果同时设置这两个变量，则 `GOOGLE_API_KEY` 具有优先权。

如果您使用的是 REST API 或浏览器上的 JavaScript，则需要明确提供 API 密钥。

下面介绍了如何在不同操作系统中将 API 密钥本地设置为环境变量 `GEMINI_API_KEY`。

### Linux/macOS - Bash

Bash 是常见的 Linux 和 macOS 终端配置。您可以通过运行以下命令来检查是否有相应的配置文件：

```
~/.bashrc
```

如果响应为“No such file or directory”（没有此类文件或目录），您需要运行以下命令创建此文件并将其打开，或使用 `zsh`：

```
touch ~/.bashrc
```

接下来，您需要添加以下导出命令来设置 API 密钥：

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

保存文件后，通过运行以下命令来应用更改：

```
source ~/.bashrc
```

### macOS - Zsh

Zsh 是常见的 Linux 和 macOS 终端配置。您可以通过运行以下命令来检查是否有相应的配置文件：

```
~/.zshrc
```

如果响应为“No such file or directory”（没有此类文件或目录），您需要运行以下命令创建此文件并将其打开，或使用 `bash`：

```
touch ~/.zshrc
```

接下来，您需要添加以下导出命令来设置 API 密钥：

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

保存文件后，通过运行以下命令来应用更改：

```
source ~/.zshrc
```

### Windows

1.  在系统设置中搜索“环境变量”
2.  修改“用户变量”（适用于当前用户）或“系统变量”（适用于所有用户，请谨慎使用）。
3.  创建变量并添加 `export GEMINI_API_KEY=your_key_here`
4.  应用更改

## 显式提供 API 密钥

在某些情况下，您可能需要明确提供 API 密钥。例如：

*   您要进行简单的 API 调用，并且更倾向于对 API 密钥进行硬编码。
*   您希望进行明确控制，而不必依赖于 Gemini API 库自动发现环境变量
*   您使用的环境不支持环境变量（例如 Web），或者您正在进行 REST 调用。

以下示例展示了如何显式提供 API 密钥：

### Python

```
from google import genai

client = genai.Client(api_key="YOUR_API_KEY")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "YOUR_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  "YOUR_API_KEY",
        Backend: genai.BackendGeminiAPI,
    })
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Java

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = Client.builder().apiKey("YOUR_API_KEY").build();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$YOUR_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## 确保 API 密钥的安全

请务必确保 Gemini API 密钥的安全。在使用 Gemini API 密钥时，请注意以下事项：

*   Google AI Gemini API 使用 API 密钥进行授权。如果其他人获得了您的 Gemini API 密钥的访问权限，他们不仅可以使用您项目的配额进行调用，还可能会导致配额耗尽或产生额外费用（对于已启用结算功能的项目），
    
*   添加 [API 密钥限制](https://cloud.google.com/api-keys/docs/add-restrictions-api-keys?hl=zh-cn#add-api-restrictions)有助于限制每个 API 密钥可使用的范围。
    
*   您有责任确保 Gemini API 密钥的安全。
    
    *   请勿将 Gemini API 密钥签入源代码控制系统。
    *   客户端应用（Web 和 Dart/Flutter）可能会泄露 API 密钥。 我们不建议在正式版应用中使用 Google AI 客户端 SDK 直接从 Web 应用调用 Google AI Gemini API。

如需了解一些常规最佳实践，您还可以参阅这篇[支持文章](https://support.google.com/googleapi/answer/6310037?hl=zh-cn)。

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-06-27。

---

# Content from: https://ai.google.dev/gemini-api/docs/speech-generation

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Speech generation (text-to-speech)

The Gemini API can transform text input into single speaker or multi-speaker audio using native text-to-speech (TTS) generation capabilities. Text-to-speech (TTS) generation is _[controllable](#controllable)_, meaning you can use natural language to structure interactions and guide the _style_, _accent_, _pace_, and _tone_ of the audio.

The TTS capability differs from speech generation provided through the [Live API](/gemini-api/docs/live), which is designed for interactive, unstructured audio, and multimodal inputs and outputs. While the Live API excels in dynamic conversational contexts, TTS through the Gemini API is tailored for scenarios that require exact text recitation with fine-grained control over style and sound, such as podcast or audiobook generation.

This guide shows you how to generate single-speaker and multi-speaker audio from text.

## Before you begin

Ensure you use a Gemini 2.5 model variant with native text-to-speech (TTS) capabilities, as listed in the [Supported models](/gemini-api/docs/speech-generation#supported-models) section. For optimal results, consider which model best fits your specific use case.

You may find it useful to [test the Gemini 2.5 TTS models in AI Studio](https://aistudio.google.com/generate-speech) before you start building.

## Single-speaker text-to-speech

To convert text to single-speaker audio, set the response modality to "audio", and pass a `SpeechConfig` object with `VoiceConfig` set. You'll need to choose a voice name from the prebuilt [output voices](#voices).

This example saves the output audio from the model in a wave file:

### Python

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents="Say cheerfully: Have a wonderful day!",
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(
               voice_name='Kore',
            )
         )
      ),
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: 'Say cheerfully: Have a wonderful day!' }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               voiceConfig: {
                  prebuiltVoiceConfig: { voiceName: 'Kore' },
               },
            },
      },
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}
await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
        "contents": [{
          "parts":[{
            "text": "Say cheerfully: Have a wonderful day!"
          }]
        }],
        "generationConfig": {
          "responseModalities": ["AUDIO"],
          "speechConfig": {
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }
        },
        "model": "gemini-2.5-flash-preview-tts",
    }' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
          base64 --decode >out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

## Multi-speaker text-to-speech

For multi-speaker audio, you'll need a `MultiSpeakerVoiceConfig` object with each speaker (up to 2) configured as a `SpeakerVoiceConfig`. You'll need to define each `speaker` with the same names used in the [prompt](#controllable):

### Python

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

prompt = """TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?"""

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=prompt,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Joe',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Jane',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const prompt = `TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?`;

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: prompt }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               multiSpeakerVoiceConfig: {
                  speakerVoiceConfigs: [
                        {
                           speaker: 'Joe',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Kore' }
                           }
                        },
                        {
                           speaker: 'Jane',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Puck' }
                           }
                        }
                  ]
               }
            }
      }
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
  "contents": [{
    "parts":[{
      "text": "TTS the following conversation between Joe and Jane:
                Joe: Hows it going today Jane?
                Jane: Not too bad, how about you?"
    }]
  }],
  "generationConfig": {
    "responseModalities": ["AUDIO"],
    "speechConfig": {
      "multiSpeakerVoiceConfig": {
        "speakerVoiceConfigs": [{
            "speaker": "Joe",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }, {
            "speaker": "Jane",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Puck"
              }
            }
          }]
      }
    }
  },
  "model": "gemini-2.5-flash-preview-tts",
}' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
    base64 --decode > out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

## Controlling speech style with prompts

You can control style, tone, accent, and pace using natural language prompts for both single- and multi-speaker TTS. For example, in a single-speaker prompt, you can say:

```
Say in an spooky whisper:
"By the pricking of my thumbs...
Something wicked this way comes"
```

In a multi-speaker prompt, provide the model with each speaker's name and corresponding transcript. You can also provide guidance for each speaker individually:

```
Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:

Speaker1: So... what's on the agenda today?
Speaker2: You're never going to guess!
```

Try using a [voice option](#voices) that corresponds to the style or emotion you want to convey, to emphasize it even more. In the previous prompt, for example, _Enceladus_'s breathiness might emphasize "tired" and "bored", while _Puck_'s upbeat tone could complement "excited" and "happy".

## Generating a prompt to convert to audio

The TTS models only output audio, but you can use [other models](/gemini-api/docs/models) to generate a transcript first, then pass that transcript to the TTS model to read aloud.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

transcript = client.models.generate_content(
   model="gemini-2.0-flash",
   contents="""Generate a short transcript around 100 words that reads
            like it was clipped from a podcast by excited herpetologists.
            The hosts names are Dr. Anya and Liam.""").text

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=transcript,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Dr. Anya',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Liam',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

# ...Code to stream or save the output
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {

const transcript = await ai.models.generateContent({
   model: "gemini-2.0-flash",
   contents: "Generate a short transcript around 100 words that reads like it was clipped from a podcast by excited herpetologists. The hosts names are Dr. Anya and Liam.",
   })

const response = await ai.models.generateContent({
   model: "gemini-2.5-flash-preview-tts",
   contents: transcript,
   config: {
      responseModalities: ['AUDIO'],
      speechConfig: {
         multiSpeakerVoiceConfig: {
            speakerVoiceConfigs: [
                   {
                     speaker: "Dr. Anya",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Kore"},
                     }
                  },
                  {
                     speaker: "Liam",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Puck"},
                    }
                  }
                ]
              }
            }
      }
  });
}
// ..JavaScript code for exporting .wav file for output audio

await main();
```

## Voice options

TTS models support the following 30 voice options in the `voice_name` field:

  

**Zephyr** -- _Bright_

**Puck** -- _Upbeat_

**Charon** -- _Informative_

**Kore** -- _Firm_

**Fenrir** -- _Excitable_

**Leda** -- _Youthful_

**Orus** -- _Firm_

**Aoede** -- _Breezy_

**Callirrhoe** -- _Easy-going_

**Autonoe** -- _Bright_

**Enceladus** -- _Breathy_

**Iapetus** -- _Clear_

**Umbriel** -- _Easy-going_

**Algieba** -- _Smooth_

**Despina** -- _Smooth_

**Erinome** -- _Clear_

**Algenib** -- _Gravelly_

**Rasalgethi** -- _Informative_

**Laomedeia** -- _Upbeat_

**Achernar** -- _Soft_

**Alnilam** -- _Firm_

**Schedar** -- _Even_

**Gacrux** -- _Mature_

**Pulcherrima** -- _Forward_

**Achird** -- _Friendly_

**Zubenelgenubi** -- _Casual_

**Vindemiatrix** -- _Gentle_

**Sadachbia** -- _Lively_

**Sadaltager** -- _Knowledgeable_

**Sulafat** -- _Warm_

You can hear all the voice options in [AI Studio](https://aistudio.google.com/generate-speech).

## Supported languages

The TTS models detect the input language automatically. They support the following 24 languages:

   

Language

BCP-47 Code

Language

BCP-47 Code

Arabic (Egyptian)

`ar-EG`

German (Germany)

`de-DE`

English (US)

`en-US`

Spanish (US)

`es-US`

French (France)

`fr-FR`

Hindi (India)

`hi-IN`

Indonesian (Indonesia)

`id-ID`

Italian (Italy)

`it-IT`

Japanese (Japan)

`ja-JP`

Korean (Korea)

`ko-KR`

Portuguese (Brazil)

`pt-BR`

Russian (Russia)

`ru-RU`

Dutch (Netherlands)

`nl-NL`

Polish (Poland)

`pl-PL`

Thai (Thailand)

`th-TH`

Turkish (Turkey)

`tr-TR`

Vietnamese (Vietnam)

`vi-VN`

Romanian (Romania)

`ro-RO`

Ukrainian (Ukraine)

`uk-UA`

Bengali (Bangladesh)

`bn-BD`

English (India)

`en-IN` & `hi-IN` bundle

Marathi (India)

`mr-IN`

Tamil (India)

`ta-IN`

Telugu (India)

`te-IN`

## Supported models

Model

Single speaker

Multispeaker

[Gemini 2.5 Flash Preview TTS](/gemini-api/docs/models#gemini-2.5-flash-preview-tts)

✔️

✔️

[Gemini 2.5 Pro Preview TTS](/gemini-api/docs/models#gemini-2.5-pro-preview-tts)

✔️

✔️

## Limitations

*   TTS models can only receive text inputs and generate audio outputs.
*   A TTS session has a [context window](/gemini-api/docs/long-context) limit of 32k tokens.
*   Review [Languages](/gemini-api/docs/speech-generation#languages) section for language support.

## What's next

*   Try the [audio generation cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_TTS.ipynb).
*   Gemini's [Live API](/gemini-api/docs/live) offers interactive audio generation options you can interleave with other modalities.
*   For working with audio _inputs_, visit the [Audio understanding](/gemini-api/docs/audio) guide.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/pricing

Nous vous présentons les mises à jour de notre famille de modèles de pensée 2.5. [En savoir plus](https://ai.google.dev/gemini-api/docs/models?hl=fr)

![](https://ai.google.dev/_static/images/translated.svg?hl=fr) Cette page a été traduite par l'[API Cloud Translation](//cloud.google.com/translate/?hl=fr).

*   [Accueil](https://ai.google.dev/?hl=fr)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=fr)
*   [Modèles](https://ai.google.dev/gemini-api/docs?hl=fr)

# Tarifs de l'API Gemini pour les développeurs

Le quota sans frais de l'API Gemini est proposé par le service d'API avec des limites de débit plus basses à des fins de test. L'utilisation de Google AI Studio est entièrement sans frais dans tous les pays disponibles. Le quota payant de l'API Gemini inclut des [limites de débit plus élevées](https://ai.google.dev/gemini-api/docs/rate-limits?hl=fr), des fonctionnalités supplémentaires et une gestion des données différente.

[Passer à un compte payant](https://aistudio.google.com/apikey?hl=fr)

## Gemini 2.5 Pro

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro&hl=fr)

Notre modèle polyvalent de pointe, qui excelle dans le codage et les tâches de raisonnement complexes.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

1,25 $, requêtes ≤ 200 000 jetons  
2,50 $, requêtes > 200 000 jetons

Prix de sortie (y compris les jetons de réflexion)

Sans frais

10 $, requêtes ≤ 200 000 jetons  
15 $, requêtes > 200 000

Tarif de la mise en cache du contexte

Non disponible

0,31 $ pour 200 000 requêtes ou moins  
0,625$pour plus de 200 000 requêtes  
4,50$pour 1 000 000 requêtes par heure (prix du stockage)

Ancrage avec la recherche Google

Non disponible

1 500 requêtes par jour (sans frais), puis 35 $pour 1 000 requêtes

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemini 2.5 Flash

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash&hl=fr)

Notre premier modèle de raisonnement hybride, qui accepte une fenêtre de contexte de 1 million de jetons et dispose de budgets de réflexion.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

0,30 $ (texte / image / vidéo)  
1 $ (audio)

Prix de sortie (y compris les jetons de réflexion)

Sans frais

2,50 $

Tarif de la mise en cache du contexte

Non disponible

0,075 $ (texte/image/vidéo)  
0,25 $ (audio)  
1$pour 1 000 000 jetons par heure (prix du stockage)

Ancrage avec la recherche Google

Sans frais, jusqu'à 500 RPD (limite partagée avec les RPD Flash-Lite)

1 500 RPD (sans frais, limite partagée avec les RPD Flash-Lite), puis 35 $ / 1 000 requêtes

API Live

Sans frais

Entrée : 0,50 $ (texte), 3 $ (audio / image \[vidéo\])  
Sortie : 2 $ (texte), 12 $ (audio)

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Preview de Gemini 2.5 Flash-Lite

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite-06-17&hl=fr)

Notre modèle le plus petit et le plus économique, conçu pour une utilisation à grande échelle.

Les modèles en version Preview peuvent changer avant de devenir stables et avoir des limites de débit plus restrictives.

  

Version gratuite

Niveau payant, par million de jetons en USD

Prix d'entrée (texte, image, vidéo)

Sans frais

0,10 $ (texte / image / vidéo)  
0,50 $ (audio)

Prix de sortie (y compris les jetons de réflexion)

Sans frais

0,40 $

Tarif de la mise en cache du contexte

Non disponible

0,025 $ (texte/image/vidéo)  
0,125 $ (audio)  
1$pour 1 000 000 jetons par heure (prix du stockage)

Ancrage avec la recherche Google

Sans frais, jusqu'à 500 RPD (limite partagée avec les RPD Flash)

1 500 requêtes par jour (sans frais, limite partagée avec les requêtes par jour Flash), puis 35 $pour 1 000 requêtes

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Audio natif Gemini 2.5 Flash

[Essayer dans Google AI Studio](https://aistudio.google.com/app/live?hl=fr#gemini-2.5-flash-preview-native-audio-dialog)

Nos modèles audio natifs sont optimisés pour des sorties audio de meilleure qualité avec un meilleur rythme, une voix plus naturelle, une verbosité et une humeur améliorées.

Les modèles en version Preview peuvent changer avant de devenir stables et avoir des limites de débit plus restrictives.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Non disponible

0,50 $ (texte)  
3 $ (audio / vidéo)

Prix de sortie (y compris les jetons de réflexion)

Non disponible

2 $ (texte)  
12 $ (audio)

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## TTS Gemini 2.5 Flash Preview

[Essayer dans Google AI Studio](https://aistudio.google.com/generate-speech?hl=fr)

Notre modèle audio de synthèse vocale 2.5 Flash optimisé pour une génération de voix à faible latence, contrôlée et rentable.

Les modèles en version Preview peuvent changer avant de devenir stables et avoir des limites de débit plus restrictives.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

0,50 $ (texte)

Prix de sortie

Sans frais

10 $ (audio)

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemini 2.5 Pro Preview TTS

[Essayer dans Google AI Studio](https://aistudio.google.com/generate-speech?hl=fr)

Notre modèle audio de synthèse vocale 2.5 Pro, optimisé pour une génération de voix puissante et à faible latence, offre des résultats plus naturels et des requêtes plus faciles à diriger.

Les modèles en version Preview peuvent changer avant de devenir stables et avoir des limites de débit plus restrictives.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Non disponible

1,00 $ (texte)

Prix de sortie

Non disponible

20 $ (audio)

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemini 2.0 Flash

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash&hl=fr)

Notre modèle multimodal le plus équilibré, qui offre d'excellentes performances pour toutes les tâches, avec une fenêtre de contexte d'un million de jetons, et conçu pour l'ère des agents.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

0,10 $ (texte / image / vidéo)  
0,70 $ (audio)

Prix de sortie

Sans frais

0,40 $

Tarif de la mise en cache du contexte

Sans frais

0,025 $ pour 1 000 000 jetons (texte/image/vidéo)  
0,175$pour 1 000 000 jetons (audio)

Mise en cache du contexte (stockage)

Stockage sans frais jusqu'à 1 000 000 jetons par heure

1 $ pour 1 million de jetons par heure

Tarifs de la génération d'images

Sans frais

0,039 $ par image\*

Prix du réglage

Non disponible

Non disponible

Ancrage avec la recherche Google

Sans frais, jusqu'à 500 RPD

1 500 requêtes par jour (sans frais), puis 35 $pour 1 000 requêtes

API Live

Sans frais

Entrée : 0,35 $ (texte), 2,10 $ (audio / image \[vidéo\])  
Sortie : 1,50 $ (texte), 8,50 $ (audio)

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

\[\*\] La sortie d'image est facturée 30 $par million de jetons. Les images de sortie jusqu'à 1 024 x 1 024 px consomment 1 290 jetons et correspondent à 0,039 $par image.

## Gemini 2.0 Flash-Lite

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite&hl=fr)

Notre modèle le plus petit et le plus économique, conçu pour une utilisation à grande échelle.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

0,075 $

Prix de sortie

Sans frais

0,30 $

Tarif de la mise en cache du contexte

Non disponible

Non disponible

Mise en cache du contexte (stockage)

Non disponible

Non disponible

Prix du réglage

Non disponible

Non disponible

Ancrage avec la recherche Google

Non disponible

Non disponible

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Aperçu d'Imagen 4

[Essayer dans Google AI Studio](https://aistudio.google.com/generate-image?hl=fr)

Notre dernier modèle de génération d'images, avec un rendu du texte et une qualité d'image globale nettement améliorés.

Les modèles en version Preview peuvent changer avant de devenir stables et avoir des limites de débit plus restrictives.

  

Version gratuite

Niveau payant, par image en USD

Tarif de l'image standard Imagen 4

Non disponible

0,04 $

Tarif des images Imagen 4 Ultra

Non disponible

0,06 $

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Imagen 3

[Essayer dans Google AI Studio](https://aistudio.google.com/generate-image?hl=fr)

Notre modèle de génération d'images de pointe, disponible pour les développeurs dans le niveau payant de l'API Gemini.

  

Version gratuite

Niveau payant, par image en USD

Prix de l'image

Non disponible

0,03 $

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Veo 2

[Essayer l'API](https://ai.google.dev/gemini-api/docs/video?hl=fr)

Notre modèle de génération de vidéos de pointe, disponible pour les développeurs dans le niveau payant de l'API Gemini.

  

Version gratuite

Niveau payant, par seconde en USD

Prix de la vidéo

Non disponible

0,35 $

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemma 3

[Essayer Gemma 3](https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it&hl=fr)

Notre modèle ouvert, léger et à la pointe de la technologie, basé sur la même technologie que nos modèles Gemini.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

Non disponible

Prix de sortie

Sans frais

Non disponible

Tarif de la mise en cache du contexte

Sans frais

Non disponible

Mise en cache du contexte (stockage)

Sans frais

Non disponible

Prix du réglage

Non disponible

Non disponible

Ancrage avec la recherche Google

Non disponible

Non disponible

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemma 3n

[Essayer Gemma 3n](https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it&hl=fr)

Notre modèle ouvert conçu pour des performances efficaces sur les appareils du quotidien, comme les téléphones mobiles, les ordinateurs portables et les tablettes.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

Non disponible

Prix de sortie

Sans frais

Non disponible

Tarif de la mise en cache du contexte

Sans frais

Non disponible

Mise en cache du contexte (stockage)

Sans frais

Non disponible

Prix du réglage

Non disponible

Non disponible

Ancrage avec la recherche Google

Non disponible

Non disponible

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemini 1.5 Flash

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash&hl=fr)

Notre modèle multimodal le plus rapide, avec d'excellentes performances pour des tâches diverses et répétitives, et une fenêtre de contexte d'un million de jetons.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

0,075 $, requêtes inférieures ou égales à 128 000 jetons  
0,15 $, requêtes supérieures à 128 000 jetons

Prix de sortie

Sans frais

0,30 $, requêtes ≤ 128 000 jetons  
0,60 $, requêtes > 128 000 jetons

Tarif de la mise en cache du contexte

Stockage sans frais jusqu'à un million de jetons par heure

0,01875 $, requêtes ≤ 128 000 jetons  
0,0375 $, requêtes > 128 000 jetons

Mise en cache du contexte (stockage)

Sans frais

1 $ par heure

Prix du réglage

Les prix des jetons sont les mêmes pour les modèles ajustés  
Le service d'ajustement est sans frais.

Les prix des jetons sont les mêmes pour les modèles ajustés  
Le service d'ajustement est sans frais.

Ancrage avec la recherche Google

Non disponible

35 $ par 1 000 demandes de mise à la terre

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemini 1.5 Flash-8B

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash-8b&hl=fr)

Notre plus petit modèle pour les cas d'utilisation d'intelligence plus faible, avec une fenêtre de contexte d'un million de jetons.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

0,0375 $, requêtes ≤ 128 000 jetons  
0,075 $, requêtes > 128 000 jetons

Prix de sortie

Sans frais

0,15 $, requêtes ≤ 128 000 jetons  
0,30 $, requêtes > 128 000 jetons

Tarif de la mise en cache du contexte

Stockage sans frais jusqu'à un million de jetons par heure

0,01 $, requêtes inférieures ou égales à 128 000 jetons  
0,02 $, requêtes supérieures à 128 000 jetons

Mise en cache du contexte (stockage)

Sans frais

0,25 $ par heure

Prix du réglage

Les prix des jetons sont les mêmes pour les modèles ajustés  
Le service d'ajustement est sans frais.

Les prix des jetons sont les mêmes pour les modèles ajustés  
Le service d'ajustement est sans frais.

Ancrage avec la recherche Google

Non disponible

35 $ par 1 000 demandes de mise à la terre

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Gemini 1.5 Pro

[Essayer dans Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro&hl=fr)

Notre modèle de la série Gemini 1.5 le plus intelligent, avec une fenêtre de contexte révolutionnaire de 2 millions de jetons.

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

1,25 $, requêtes ≤ 128 000 jetons  
2,50 $, requêtes > 128 000 jetons

Prix de sortie

Sans frais

5 $, requêtes <= 128 000 jetons  
10 $, requêtes > 128 000 jetons

Tarif de la mise en cache du contexte

Non disponible

0,3125 $, requêtes ≤ 128 000 jetons  
0,625 $, requêtes > 128 000 jetons

Mise en cache du contexte (stockage)

Non disponible

4,50 $ par heure

Prix du réglage

Non disponible

Non disponible

Ancrage avec la recherche Google

Non disponible

35 $ par 1 000 demandes de mise à la terre

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

## Text Embedding 004

Notre modèle d'embeddings textuels de pointe

  

Version gratuite

Niveau payant, par million de jetons en USD

Saisir le prix

Sans frais

Non disponible

Prix de sortie

Sans frais

Non disponible

Prix du réglage

Non disponible

Non disponible

Utilisées pour améliorer nos produits

[Oui](https://ai.google.dev/gemini-api/terms?hl=fr)

[Non](https://ai.google.dev/gemini-api/terms?hl=fr)

\[\*\] L'utilisation de Google AI Studio est sans frais dans toutes les [régions disponibles](https://ai.google.dev/gemini-api/docs/available-regions?hl=fr). Pour en savoir plus, consultez les [questions fréquentes sur la facturation](https://ai.google.dev/gemini-api/docs/billing?hl=fr).

\[\*\*\] Les prix peuvent différer de ceux indiqués ici et de ceux proposés sur Vertex AI. Pour connaître les tarifs de Vertex, consultez la page [Tarifs de Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/pricing?hl=fr).

\[\*\*\*\] Si vous utilisez la [récupération dynamique](https://ai.google.dev/gemini-api/docs/grounding?hl=fr) pour optimiser les coûts, seules les requêtes qui contiennent au moins une URL d'assistance d'ancrage sur le Web dans leur réponse sont facturées pour l'ancrage dans la recherche Google. Les coûts de Gemini s'appliquent toujours. Les limites de tarifs sont susceptibles d'être modifiées.

Sauf indication contraire, le contenu de cette page est régi par une licence [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/), et les échantillons de code sont régis par une licence [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Pour en savoir plus, consultez les [Règles du site Google Developers](https://developers.google.com/site-policies?hl=fr). Java est une marque déposée d'Oracle et/ou de ses sociétés affiliées.

Dernière mise à jour le 2025/07/03 (UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/music-generation

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Music generation using Lyria RealTime

The Gemini API, using [Lyria RealTime](https://deepmind.google/technologies/lyria/realtime/), provides access to a state-of-the-art, real-time, streaming music generation model. It allows developers to build applications where users can interactively create, continuously steer, and perform instrumental music.

To experience what can be built using Lyria RealTime, try it on AI Studio using the [Prompt DJ](https://aistudio.google.com/apps/bundled/promptdj) or the [MIDI DJ](https://aistudio.google.com/apps/bundled/promptdj-midi) apps!

## How music generation works

Lyria RealTime music generation uses a persistent, bidirectional, low-latency streaming connection using [WebSocket](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).

## Generate and control music

Lyria RealTime works a bit like the [Live API](https://ai.google.dev/gemini-api/docs/live) in the sense that it is using websockets to keep a real-time communication with the model. It's still not exactly the same as you can't talk to the model and you have to use a specific format to prompt it.

The following code demonstrates how to generate music:

### Python

This example initializes the Lyria RealTime session using `client.aio.live.music.connect()`, then sends an initial prompt with `session.set_weighted_prompts()` along with an initial configuration using `session.set_music_generation_config`, starts the music generation using `session.play()` and sets up `receive_audio()` to process the audio chunks it receives.

  ```
  import asyncio
  from google import genai
  from google.genai import types

  client = genai.Client(http_options={'api_version': 'v1alpha'})

  async def main():
      async def receive_audio(session):
        """Example background task to process incoming audio."""
        while True:
          async for message in session.receive():
            audio_data = message.server_content.audio_chunks[0].data
            # Process audio...
            await asyncio.sleep(10**-12)

      async with (
        client.aio.live.music.connect(model='models/lyria-realtime-exp') as session,
        asyncio.TaskGroup() as tg,
      ):
        # Set up task to receive server messages.
        tg.create_task(receive_audio(session))

        # Send initial prompts and config
        await session.set_weighted_prompts(
          prompts=[
            types.WeightedPrompt(text='minimal techno', weight=1.0),
          ]
        )
        await session.set_music_generation_config(
          config=types.LiveMusicGenerationConfig(bpm=90, temperature=1.0)
        )

        # Start streaming music
        await session.play()
  if __name__ == "__main__":
      asyncio.run(main())
```

### JavaScript

This example initializes the Lyria RealTime session using `client.live.music.connect()`, then sends an initial prompt with `session.setWeightedPrompts()` along with an initial configuration using `session.setMusicGenerationConfig`, starts the music generation using `session.play()` and sets up an `onMessage` callback to process the audio chunks it receives.

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({
  apiVersion: 'v1alpha',
});

// Create session object to control music generation.
const session: MusicSession = client.live.music.connect({
  model: 'models/lyria-realtime-exp',
  callbacks: {
    onMessage: (message) => {
      // Application logic: buffer and play using Web Audio API etc.
    },
    onError: (error) => {
      console.error('music session error:', error);
    },
    onClose: () => {
      console.log('Lyria RealTime stream closed.');
    }
  }
}); 

// Send initial prompts and config
await session.setWeightedPrompts({
  weightedPrompts: [{ text: 'minimal techno', weight: 1.0 }],
});
await session.setMusicGenerationConfig({
  musicGenerationConfig: { bpm: 90, temperature: 1.0 },
});

// Start generation
await session.play();
```

You can then use `session.play()`, `session.pause()`, `session.stop()` and `session.reset_context()` to start, pause, stop or reset the session.

## Steer music in real-time

### Prompt Lyria RealTime

While the stream is active, you can send new `WeightedPrompt` messages at any time to alter the generated music. The model will smoothly transition based on the new input.

The prompts need to follow the right format with a `text` (the actual prompt), and a `weight`. The `weight` can take any value except `0`. `1.0` is usually a good starting point.

### Python

  ```
  await session.set_weighted_prompts(
    prompts=[
      {"text": "Piano", "weight": 2.0},
      types.WeightedPrompt(text="Meditation", weight=0.5),
      types.WeightedPrompt(text="Live Performance", weight=1.0),
    ]
  )
```

### JavaScript

  ```
  await session.setMusicGenerationConfig({
    weightedPrompts: [
      { text: 'Harmonica', weight: 0.3 },
      { text: 'Afrobeat', weight: 0.7 }
    ],
  });
```

Note that the model transitions can be a bit abrupt when drastically changing the prompts so it's recommended to implement some kind of cross-fading by sending intermediate weight values to the model.

### Update the configuration

You can also update the music generation parameters in real time. You can't just update a parameter, you need to set the whole configuration otherwise the other fields will be reset back to their default values.

Since updating the bpm or the scale is a drastic change for the model you'll also need to tell it to reset its context using `reset_context()` to take the new config into account. It won't stop the stream, but it will be a hard transition. You don't need to do it for the other parameters.

### Python

  ```
  await session.set_music_generation_config(
    config=types.LiveMusicGenerationConfig(
      bpm=128,
      scale=types.Scale.D_MAJOR_B_MINOR,
    )
  )
  await session.reset_context();
```

### JavaScript

  ```
  await session.setMusicGenerationConfig({
    musicGenerationConfig: { bpm: 120, density: 0.75 },
  });
  await session.reset_context();
```

## Prompt guide for Lyria RealTime

Here's a non-exhaustive list of prompts you can use to prompt Lyria RealTime:

*   Instruments: `303 Acid Bass, 808 Hip Hop Beat, Accordion, Alto Saxophone, Bagpipes, Balalaika Ensemble, Banjo, Bass Clarinet, Bongos, Boomy Bass, Bouzouki, Buchla Synths, Cello, Charango, Clavichord, Conga Drums, Didgeridoo, Dirty Synths, Djembe, Drumline, Dulcimer, Fiddle, Flamenco Guitar, Funk Drums, Glockenspiel, Guitar, Hang Drum, Harmonica, Harp, Harpsichord, Hurdy-gurdy, Kalimba, Koto, Lyre, Mandolin, Maracas, Marimba, Mbira, Mellotron, Metallic Twang, Moog Oscillations, Ocarina, Persian Tar, Pipa, Precision Bass, Ragtime Piano, Rhodes Piano, Shamisen, Shredding Guitar, Sitar, Slide Guitar, Smooth Pianos, Spacey Synths, Steel Drum, Synth Pads, Tabla, TR-909 Drum Machine, Trumpet, Tuba, Vibraphone, Viola Ensemble, Warm Acoustic Guitar, Woodwinds, ...`
*   Music Genre: `Acid Jazz, Afrobeat, Alternative Country, Baroque, Bengal Baul, Bhangra, Bluegrass, Blues Rock, Bossa Nova, Breakbeat, Celtic Folk, Chillout, Chiptune, Classic Rock, Contemporary R&B, Cumbia, Deep House, Disco Funk, Drum & Bass, Dubstep, EDM, Electro Swing, Funk Metal, G-funk, Garage Rock, Glitch Hop, Grime, Hyperpop, Indian Classical, Indie Electronic, Indie Folk, Indie Pop, Irish Folk, Jam Band, Jamaican Dub, Jazz Fusion, Latin Jazz, Lo-Fi Hip Hop, Marching Band, Merengue, New Jack Swing, Minimal Techno, Moombahton, Neo-Soul, Orchestral Score, Piano Ballad, Polka, Post-Punk, 60s Psychedelic Rock, Psytrance, R&B, Reggae, Reggaeton, Renaissance Music, Salsa, Shoegaze, Ska, Surf Rock, Synthpop, Techno, Trance, Trap Beat, Trip Hop, Vaporwave, Witch house, ...`
*   Mood/Description: `Acoustic Instruments, Ambient, Bright Tones, Chill, Crunchy Distortion, Danceable, Dreamy, Echo, Emotional, Ethereal Ambience, Experimental, Fat Beats, Funky, Glitchy Effects, Huge Drop, Live Performance, Lo-fi, Ominous Drone, Psychedelic, Rich Orchestration, Saturated Tones, Subdued Melody, Sustained Chords, Swirling Phasers, Tight Groove, Unsettling, Upbeat, Virtuoso, Weird Noises, ...`

These are just some examples, Lyria RealTime can do much more. Experiment with your own prompts!

## Best practices

*   Client applications must implement robust audio buffering to ensure smooth playback. This helps account for network jitter and slight variations in generation latency.
*   Effective prompting:
    *   Be descriptive. Use adjectives describing mood, genre, and instrumentation.
    *   Iterate and steer gradually. Rather than completely changing the prompt, try adding or modifying elements to morph the music more smoothly.
    *   Experiment with weight on `WeightedPrompt` to influence how strongly a new prompt affects the ongoing generation.

## Technical details

This section describes the specifics of how to use Lyria RealTime music generation.

### Specifications

*   Output format: Raw 16-bit PCM Audio
*   Sample rate: 48kHz
*   Channels: 2 (stereo)

### Controls

Music generation can be influenced in real time by sending messages containing:

*   `WeightedPrompt`: A text string describing a musical idea, genre, instrument, mood, or characteristic. Multiple prompts can potentially be supplied to blend influences. See [above](/gemini-api/docs/:#steer-music) for more details on how to best prompt Lyria RealTime.
*   `MusicGenerationConfig`: Configuration for the music generation process, influencing the characteristics of the output audio.). Parameters include:
    *   `guidance`: (float) Range: `[0.0, 6.0]`. Default: `4.0`. Controls how strictly the model follows the prompts. Higher guidance improves adherence to the prompt, but makes transitions more abrupt.
    *   `bpm`: (int) Range: `[60, 200]`. Sets the Beats Per Minute you want for the generated music. You need to stop/play or reset the context for the model it take into account the new bpm.
    *   `density`: (float) Range: `[0.0, 1.0]`. Controls the density of musical notes/sounds. Lower values produce sparser music; higher values produce "busier" music.
    *   `brightness`: (float) Range: `[0.0, 1.0]`. Adjusts the tonal quality. Higher values produce "brighter" sounding audio, generally emphasizing higher frequencies.
    *   `scale`: (Enum) Sets the musical scale (Key and Mode) for the generation. Use the [`Scale` enum values](#scale-enum) provided by the SDK. You need to stop/play or reset the context for the model it take into account the new scale.
    *   `mute_bass`: (bool) Default: `False`. Controls whether the model reduces the outputs' bass.
    *   `mute_drums`: (bool) Default: `False`. Controls whether the model outputs reduces the outputs' drums.
    *   `only_bass_and_drums`: (bool) Default: `False`. Steer the model to try to only output bass and drums.
*   `PlaybackControl`: Commands to control playback aspects, such as play, pause, stop or reset the context.

For `bpm`, `density`, `brightness` and `scale`, if no value is provided, the model will decide what's best according to your initial prompts.

More classical parameters like `temperature` (0.0 to 3.0, default 1.1), `top_k` (1 to 1000, default 40), and `seed` (0 to 2 147 483 647, randomly selected by default) are also customizable in the `MusicGenerationConfig`.

#### Scale Enum Values

Here are all the scale values that the model can accept:

Enum Value

Scale / Key

`C_MAJOR_A_MINOR`

C major / A minor

`D_FLAT_MAJOR_B_FLAT_MINOR`

D♭ major / B♭ minor

`D_MAJOR_B_MINOR`

D major / B minor

`E_FLAT_MAJOR_C_MINOR`

E♭ major / C minor

`E_MAJOR_D_FLAT_MINOR`

E major / C♯/D♭ minor

`F_MAJOR_D_MINOR`

F major / D minor

`G_FLAT_MAJOR_E_FLAT_MINOR`

G♭ major / E♭ minor

`G_MAJOR_E_MINOR`

G major / E minor

`A_FLAT_MAJOR_F_MINOR`

A♭ major / F minor

`A_MAJOR_G_FLAT_MINOR`

A major / F♯/G♭ minor

`B_FLAT_MAJOR_G_MINOR`

B♭ major / G minor

`B_MAJOR_A_FLAT_MINOR`

B major / G♯/A♭ minor

`SCALE_UNSPECIFIED`

Default / The model decides

The model is capable of guiding the notes that are played, but does not distinguish between relative keys. Thus each enum corresponds both to the relative major and minor. For example, `C_MAJOR_A_MINOR` would correspond to all the white keys of a piano, and `F_MAJOR_D_MINOR` would be all the white keys except B flat.

### Limitations

*   Instrumental only: The model generates instrumental music only.
*   Safety: Prompts are checked by safety filters. Prompts triggering the filters will be ignored in which case an explanation will be written in the output's `filtered_prompt` field.
*   Watermarking: Output audio is always watermarked for identification following our [Responsible AI](https://ai.google/responsibility/principles/) principles.

## What's next

*   Instead of music, learn how to generate multi-speakers conversation using the [TTS models](https://ai.google.dev/gemini-api/docs/audio-generation),
*   Discover how to generate [images](https://ai.google.dev/gemini-api/docs/image-generation) or [videos](https://ai.google.dev/gemini-api/docs/video),
*   Instead of generation music or audio, find out how to Gemini can [understand Audio files](https://ai.google.dev/gemini-api/docs/audio),
*   Have a real-time conversation with Gemini using the [Live API](https://ai.google.dev/gemini-api/docs/live).

Explore the [Cookbook](https://github.com/google-gemini/cookbook) for more code examples and tutorials.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/text-generation

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Text generation

The Gemini API can generate text output from various inputs, including text, images, video, and audio, leveraging Gemini models.

Here's a basic example that takes a single text input:

### Python

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Thinking with Gemini 2.5

2.5 Flash and Pro models have ["thinking"](/gemini-api/docs/thinking) enabled by default to enhance quality, which may take longer to run and increase token usage.

When using 2.5 Flash, you can disable thinking by setting the thinking budget to zero.

For more details, see the [thinking guide](/gemini-api/docs/thinking#set-budget).

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## System instructions and other configurations

You can guide the behavior of Gemini models with system instructions. To do so, pass a [`GenerateContentConfig`](/api/generate-content#v1beta.GenerationConfig) object.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

The [`GenerateContentConfig`](/api/generate-content#v1beta.GenerationConfig) object also lets you override default generation parameters, such as [temperature](/api/generate-content#v1beta.GenerationConfig).

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Refer to the [`GenerateContentConfig`](/api/generate-content#v1beta.GenerationConfig) in our API reference for a complete list of configurable parameters and their descriptions.

## Multimodal inputs

The Gemini API supports multimodal inputs, allowing you to combine text with media files. The following example demonstrates providing an image:

### Python

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

For alternative methods of providing images and more advanced image processing, see our [image understanding guide](/gemini-api/docs/image-understanding). The API also supports [document](/gemini-api/docs/document-processing), [video](/gemini-api/docs/video-understanding), and [audio](/gemini-api/docs/audio) inputs and understanding.

## Streaming responses

By default, the model returns a response only after the entire generation process is complete.

For more fluid interactions, use streaming to receive [`GenerateContentResponse`](/api/generate-content#v1beta.GenerateContentResponse) instances incrementally as they're generated.

### Python

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Multi-turn conversations (Chat)

Our SDKs provide functionality to collect multiple rounds of prompts and responses into a chat, giving you an easy way to keep track of the conversation history.

### Python

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Streaming can also be used for multi-turn conversations.

### Python

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Supported models

All models in the Gemini family support text generation. To learn more about the models and their capabilities, visit the [Models](/gemini-api/docs/models) page.

## Best practices

### Prompting tips

For basic text generation, a [zero-shot](/gemini-api/docs/prompting-strategies#few-shot) prompt often suffices without needing examples, system instructions or specific formatting.

For more tailored outputs:

*   Use [System instructions](#system-instructions) to guide the model.
*   Provide few example inputs and outputs to guide the model. This is often referred to as [few-shot](/gemini-api/docs/prompting-strategies#few-shot) prompting.

Consult our [prompt engineering guide](/gemini/docs/prompting-strategies) for more tips.

### Structured output

In some cases, you may need structured output, such as JSON. Refer to our [structured output](/gemini-api/docs/structured-output) guide to learn how.

## What's next

*   Try the [Gemini API getting started Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb).
*   Explore Gemini's [image](/gemini-api/docs/image-understanding), [video](/gemini-api/docs/video-understanding), [audio](/gemini-api/docs/audio) and [document](/gemini-api/docs/document-processing) understanding capabilities.
*   Learn about multimodal [file prompting strategies](/gemini-api/docs/files#prompt-guide).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/long-context

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Long context

Many Gemini models come with large context windows of 1 million or more tokens. Historically, large language models (LLMs) were significantly limited by the amount of text (or tokens) that could be passed to the model at one time. The Gemini long context window unlocks many new use cases and developer paradigms.

The code you already use for cases like [text generation](/gemini-api/docs/text-generation) or [multimodal inputs](/gemini-api/docs/vision) will work without any changes with long context.

This document gives you an overview of what you can achieve using models with context windows of 1M and more tokens. The page gives a brief overview of a context window, and explores how developers should think about long context, various real world use cases for long context, and ways to optimize the usage of long context.

For the context window sizes of specific models, see the [Models](/gemini-api/docs/models) page.

## What is a context window?

The basic way you use the Gemini models is by passing information (context) to the model, which will subsequently generate a response. An analogy for the context window is short term memory. There is a limited amount of information that can be stored in someone's short term memory, and the same is true for generative models.

You can read more about how models work under the hood in our [generative models guide](/gemini-api/docs/prompting-strategies#under-the-hood).

## Getting started with long context

Earlier versions of generative models were only able to process 8,000 tokens at a time. Newer models pushed this further by accepting 32,000 or even 128,000 tokens. Gemini is the first model capable of accepting 1 million tokens.

In practice, 1 million tokens would look like:

*   50,000 lines of code (with the standard 80 characters per line)
*   All the text messages you have sent in the last 5 years
*   8 average length English novels
*   Transcripts of over 200 average length podcast episodes

The more limited context windows common in many other models often require strategies like arbitrarily dropping old messages, summarizing content, using RAG with vector databases, or filtering prompts to save tokens.

While these techniques remain valuable in specific scenarios, Gemini's extensive context window invites a more direct approach: providing all relevant information upfront. Because Gemini models were purpose-built with massive context capabilities, they demonstrate powerful in-context learning. For example, using only in-context instructional materials (a 500-page reference grammar, a dictionary, and ≈400 parallel sentences), Gemini [learned to translate](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) from English to Kalamang—a Papuan language with fewer than 200 speakers—with quality similar to a human learner using the same materials. This illustrates the paradigm shift enabled by Gemini's long context, empowering new possibilities through robust in-context learning.

## Long context use cases

While the standard use case for most generative models is still text input, the Gemini model family enables a new paradigm of multimodal use cases. These models can natively understand text, video, audio, and images. They are accompanied by the [Gemini API that takes in multimodal file types](/gemini-api/docs/prompting_with_media) for convenience.

### Long form text

Text has proved to be the layer of intelligence underpinning much of the momentum around LLMs. As mentioned earlier, much of the practical limitation of LLMs was because of not having a large enough context window to do certain tasks. This led to the rapid adoption of retrieval augmented generation (RAG) and other techniques which dynamically provide the model with relevant contextual information. Now, with larger and larger context windows, there are new techniques becoming available which unlock new use cases.

Some emerging and standard use cases for text based long context include:

*   Summarizing large corpuses of text
    *   Previous summarization options with smaller context models would require a sliding window or another technique to keep state of previous sections as new tokens are passed to the model
*   Question and answering
    *   Historically this was only possible with RAG given the limited amount of context and models' factual recall being low
*   Agentic workflows
    *   Text is the underpinning of how agents keep state of what they have done and what they need to do; not having enough information about the world and the agent's goal is a limitation on the reliability of agents

[Many-shot in-context learning](https://arxiv.org/pdf/2404.11018) is one of the most unique capabilities unlocked by long context models. Research has shown that taking the common "single shot" or "multi-shot" example paradigm, where the model is presented with one or a few examples of a task, and scaling that up to hundreds, thousands, or even hundreds of thousands of examples, can lead to novel model capabilities. This many-shot approach has also been shown to perform similarly to models which were fine-tuned for a specific task. For use cases where a Gemini model's performance is not yet sufficient for a production rollout, you can try the many-shot approach. As you might explore later in the long context optimization section, context caching makes this type of high input token workload much more economically feasible and even lower latency in some cases.

### Long form video

Video content's utility has long been constrained by the lack of accessibility of the medium itself. It was hard to skim the content, transcripts often failed to capture the nuance of a video, and most tools don't process image, text, and audio together. With Gemini, the long-context text capabilities translate to the ability to reason and answer questions about multimodal inputs with sustained performance.

Some emerging and standard use cases for video long context include:

*   Video question and answering
*   Video memory, as shown with [Google's Project Astra](https://deepmind.google/technologies/gemini/project-astra/)
*   Video captioning
*   Video recommendation systems, by enriching existing metadata with new multimodal understanding
*   Video customization, by looking at a corpus of data and associated video metadata and then removing parts of videos that are not relevant to the viewer
*   Video content moderation
*   Real-time video processing

When working with videos, it is important to consider how the [videos are processed into tokens](/gemini-api/docs/tokens#media-token), which affects billing and usage limits. You can learn more about prompting with video files in the [Prompting guide](/gemini-api/docs/prompting_with_media?lang=python#prompting-with-videos).

### Long form audio

The Gemini models were the first natively multimodal large language models that could understand audio. Historically, the typical developer workflow would involve stringing together multiple domain specific models, like a speech-to-text model and a text-to-text model, in order to process audio. This led to additional latency required by performing multiple round-trip requests and decreased performance usually attributed to disconnected architectures of the multiple model setup.

Some emerging and standard use cases for audio context include:

*   Real-time transcription and translation
*   Podcast / video question and answering
*   Meeting transcription and summarization
*   Voice assistants

You can learn more about prompting with audio files in the [Prompting guide](/gemini-api/docs/prompting_with_media?lang=python#prompting-with-videos).

## Long context optimizations

The primary optimization when working with long context and the Gemini models is to use [context caching](/gemini-api/docs/caching). Beyond the previous impossibility of processing lots of tokens in a single request, the other main constraint was the cost. If you have a "chat with your data" app where a user uploads 10 PDFs, a video, and some work documents, you would historically have to work with a more complex retrieval augmented generation (RAG) tool / framework in order to process these requests and pay a significant amount for tokens moved into the context window. Now, you can cache the files the user uploads and pay to store them on a per hour basis. The input / output cost per request with Gemini Flash for example is ~4x less than the standard input / output cost, so if the user chats with their data enough, it becomes a huge cost saving for you as the developer.

## Long context limitations

In various sections of this guide, we talked about how Gemini models achieve high performance across various needle-in-a-haystack retrieval evals. These tests consider the most basic setup, where you have a single needle you are looking for. In cases where you might have multiple "needles" or specific pieces of information you are looking for, the model does not perform with the same accuracy. Performance can vary to a wide degree depending on the context. This is important to consider as there is an inherent tradeoff between getting the right information retrieved and cost. You can get ~99% on a single query, but you have to pay the input token cost every time you send that query. So for 100 pieces of information to be retrieved, if you needed 99% performance, you would likely need to send 100 requests. This is a good example of where context caching can significantly reduce the cost associated with using Gemini models while keeping the performance high.

## FAQs

### Where is the best place to put my query in the context window?

In most cases, especially if the total context is long, the model's performance will be better if you put your query / question at the end of the prompt (after all the other context).

### Do I lose model performance when I add more tokens to a query?

Generally, if you don't need tokens to be passed to the model, it is best to avoid passing them. However, if you have a large chunk of tokens with some information and want to ask questions about that information, the model is highly capable of extracting that information (up to 99% accuracy in many cases).

### How can I lower my cost with long-context queries?

If you have a similar set of tokens / context that you want to re-use many times, [context caching](/gemini-api/docs/caching) can help reduce the costs associated with asking questions about that information.

### Does the context length affect the model latency?

There is some fixed amount of latency in any given request, regardless of the size, but generally longer queries will have higher latency (time to first token).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-05-20 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/structured-output

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Structured output

You can configure Gemini for structured output instead of unstructured text, allowing precise extraction and standardization of information for further processing. For example, you can use structured output to extract information from resumes, standardize them to build a structured database.

Gemini can generate either [JSON](/gemini-api/docs/structured-output#generating-json) or [enum values](/gemini-api/docs/structured-output#generating-enums) as structured output.

## Generating JSON

There are two ways to generate JSON using the Gemini API:

*   Configure a schema on the model
*   Provide a schema in a text prompt

Configuring a schema on the model is the **recommended** way to generate JSON, because it constrains the model to output JSON.

### Configuring a schema (recommended)

To constrain the model to generate JSON, configure a `responseSchema`. The model will then respond to any prompt with JSON-formatted output.

### Python

```
from google import genai
from pydantic import BaseModel

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="List a few popular cookie recipes, and include the amounts of ingredients.",
    config={
        "response_mime_type": "application/json",
        "response_schema": list[Recipe],
    },
)
# Use the response as a JSON string.
print(response.text)

# Use instantiated objects.
my_recipes: list[Recipe] = response.parsed
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents:
      "List a few popular cookie recipes, and include the amounts of ingredients.",
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.ARRAY,
        items: {
          type: Type.OBJECT,
          properties: {
            recipeName: {
              type: Type.STRING,
            },
            ingredients: {
              type: Type.ARRAY,
              items: {
                type: Type.STRING,
              },
            },
          },
          propertyOrdering: ["recipeName", "ingredients"],
        },
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        ResponseMIMEType: "application/json",
        ResponseSchema: &genai.Schema{
            Type: genai.TypeArray,
            Items: &genai.Schema{
                Type: genai.TypeObject,
                Properties: map[string]*genai.Schema{
                    "recipeName": {Type: genai.TypeString},
                    "ingredients": {
                        Type:  genai.TypeArray,
                        Items: &genai.Schema{Type: genai.TypeString},
                    },
                },
                PropertyOrdering: []string{"recipeName", "ingredients"},
            },
        },
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("List a few popular cookie recipes, and include the amounts of ingredients."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
      "contents": [{
        "parts":[
          { "text": "List a few popular cookie recipes, and include the amounts of ingredients." }
        ]
      }],
      "generationConfig": {
        "responseMimeType": "application/json",
        "responseSchema": {
          "type": "ARRAY",
          "items": {
            "type": "OBJECT",
            "properties": {
              "recipeName": { "type": "STRING" },
              "ingredients": {
                "type": "ARRAY",
                "items": { "type": "STRING" }
              }
            },
            "propertyOrdering": ["recipeName", "ingredients"]
          }
        }
      }
}' 2> /dev/null | head
```

The output might look like this:

```
[
  {
    "recipeName": "Chocolate Chip Cookies",
    "ingredients": [
      "1 cup (2 sticks) unsalted butter, softened",
      "3/4 cup granulated sugar",
      "3/4 cup packed brown sugar",
      "1 teaspoon vanilla extract",
      "2 large eggs",
      "2 1/4 cups all-purpose flour",
      "1 teaspoon baking soda",
      "1 teaspoon salt",
      "2 cups chocolate chips"
    ]
  },
  ...
]
```

### Providing a schema in a text prompt

Instead of configuring a schema, you can supply a schema as natural language or pseudo-code in a text prompt. This method is **not recommended**, because it might produce lower quality output, and because the model is not constrained to follow the schema.

Here's a generic example of a schema provided in a text prompt:

```
List a few popular cookie recipes, and include the amounts of ingredients.

Produce JSON matching this specification:

Recipe = { "recipeName": string, "ingredients": array<string> }
Return: array<Recipe>
```

Since the model gets the schema from text in the prompt, you might have some flexibility in how you represent the schema. But when you supply a schema inline like this, the model is not actually constrained to return JSON. For a more deterministic, higher quality response, configure a schema on the model, and don't duplicate the schema in the text prompt.

## Generating enum values

In some cases you might want the model to choose a single option from a list of options. To implement this behavior, you can pass an _enum_ in your schema. You can use an enum option anywhere you could use a `string` in the `responseSchema`, because an enum is an array of strings. Like a JSON schema, an enum lets you constrain model output to meet the requirements of your application.

For example, assume that you're developing an application to classify musical instruments into one of five categories: `"Percussion"`, `"String"`, `"Woodwind"`, `"Brass"`, or "`"Keyboard"`". You could create an enum to help with this task.

In the following example, you pass an enum as the `responseSchema`, constraining the model to choose the most appropriate option.

### Python

```
from google import genai
import enum

class Instrument(enum.Enum):
  PERCUSSION = "Percussion"
  STRING = "String"
  WOODWIND = "Woodwind"
  BRASS = "Brass"
  KEYBOARD = "Keyboard"

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': Instrument,
    },
)

print(response.text)
# Woodwind
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "What type of instrument is an oboe?",
    config: {
      responseMimeType: "text/x.enum",
      responseSchema: {
        type: Type.STRING,
        enum: ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
      },
    },
  });

console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
          "contents": [{
            "parts":[
              { "text": "What type of instrument is an oboe?" }
            ]
          }],
          "generationConfig": {
            "responseMimeType": "text/x.enum",
            "responseSchema": {
              "type": "STRING",
              "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
            }
          }
    }'
```

The Python library will translate the type declarations for the API. However, the API accepts a subset of the OpenAPI 3.0 schema ([Schema](https://ai.google.dev/api/caching#schema)).

There are two other ways to specify an enumeration. You can use a [`Literal`](https://docs.pydantic.dev/1.10/usage/types/#literal-type): \`\`\`

### Python

```
Literal["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
```

And you can also pass the schema as JSON:

### Python

```
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': {
            "type": "STRING",
            "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
        },
    },
)

print(response.text)
# Woodwind
```

Beyond basic multiple choice problems, you can use an enum anywhere in a JSON schema. For example, you could ask the model for a list of recipe titles and use a `Grade` enum to give each title a popularity grade:

### Python

```
from google import genai

import enum
from pydantic import BaseModel

class Grade(enum.Enum):
    A_PLUS = "a+"
    A = "a"
    B = "b"
    C = "c"
    D = "d"
    F = "f"

class Recipe(BaseModel):
  recipe_name: str
  rating: Grade

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='List 10 home-baked cookie recipes and give them grades based on tastiness.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': list[Recipe],
    },
)

print(response.text)
```

The response might look like this:

```
[
  {
    "recipe_name": "Chocolate Chip Cookies",
    "rating": "a+"
  },
  {
    "recipe_name": "Peanut Butter Cookies",
    "rating": "a"
  },
  {
    "recipe_name": "Oatmeal Raisin Cookies",
    "rating": "b"
  },
  ...
]
```

## About JSON schemas

Configuring the model for JSON output using `responseSchema` parameter relies on `Schema` object to define its structure. This object represents a select subset of the [OpenAPI 3.0 Schema object](https://spec.openapis.org/oas/v3.0.3#schema-object), and also adds a `propertyOrdering` field.

Here's a pseudo-JSON representation of all the `Schema` fields:

```
{
  "type": enum (Type),
  "format": string,
  "description": string,
  "nullable": boolean,
  "enum": [
    string
  ],
  "maxItems": integer,
  "minItems": integer,
  "properties": {
    string: {
      object (Schema)
    },
    ...
  },
  "required": [
    string
  ],
  "propertyOrdering": [
    string
  ],
  "items": {
    object (Schema)
  }
}
```

The `Type` of the schema must be one of the OpenAPI [Data Types](https://spec.openapis.org/oas/v3.0.3#data-types), or a union of those types (using `anyOf`). Only a subset of fields is valid for each `Type`. The following list maps each `Type` to a subset of the fields that are valid for that type:

*   `string` -> `enum`, `format`, `nullable`
*   `integer` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `number` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `boolean` -> `nullable`
*   `array` -> `minItems`, `maxItems`, `items`, `nullable`
*   `object` -> `properties`, `required`, `propertyOrdering`, `nullable`

Here are some example schemas showing valid type-and-field combinations:

```
{ "type": "string", "enum": ["a", "b", "c"] }

{ "type": "string", "format": "date-time" }

{ "type": "integer", "format": "int64" }

{ "type": "number", "format": "double" }

{ "type": "boolean" }

{ "type": "array", "minItems": 3, "maxItems": 3, "items": { "type": ... } }

{ "type": "object",
  "properties": {
    "a": { "type": ... },
    "b": { "type": ... },
    "c": { "type": ... }
  },
  "nullable": true,
  "required": ["c"],
  "propertyOrdering": ["c", "b", "a"]
}
```

For complete documentation of the Schema fields as they're used in the Gemini API, see the [Schema reference](/api/caching#Schema).

### Property ordering

When you're working with JSON schemas in the Gemini API, the order of properties is important. By default, the API orders properties alphabetically and does not preserve the order in which the properties are defined (although the [Google Gen AI SDKs](/gemini-api/docs/sdks) may preserve this order). If you're providing examples to the model with a schema configured, and the property ordering of the examples is not consistent with the property ordering of the schema, the output could be rambling or unexpected.

To ensure a consistent, predictable ordering of properties, you can use the optional `propertyOrdering[]` field.

```
"propertyOrdering": ["recipeName", "ingredients"]
```

`propertyOrdering[]` – not a standard field in the OpenAPI specification – is an array of strings used to determine the order of properties in the response. By specifying the order of properties and then providing examples with properties in that same order, you can potentially improve the quality of results. `propertyOrdering` is only supported when you manually create `types.Schema`.

### Schemas in Python

When you're using the Python library, the value of `response_schema` must be one of the following:

*   A type, as you would use in a type annotation (see the Python [`typing` module](https://docs.python.org/3/library/typing.html))
*   An instance of [`genai.types.Schema`](https://googleapis.github.io/python-genai/genai.html#genai.types.Schema)
*   The `dict` equivalent of `genai.types.Schema`

The easiest way to define a schema is with a Pydantic type (as shown in the previous example):

### Python

```
config={'response_mime_type': 'application/json',
        'response_schema': list[Recipe]}
```

When you use a Pydantic type, the Python library builds out a JSON schema for you and sends it to the API. For additional examples, see the [Python library docs](https://googleapis.github.io/python-genai/index.html#json-response-schema).

The Python library supports schemas defined with the following types (where `AllowedType` is any allowed type):

*   `int`
*   `float`
*   `bool`
*   `str`
*   `list[AllowedType]`
*   `AllowedType|AllowedType|...`
*   For structured types:
    *   `dict[str, AllowedType]`. This annotation declares all dict values to be the same type, but doesn't specify what keys should be included.
    *   User-defined [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/). This approach lets you specify the key names and define different types for the values associated with each of the keys, including nested structures.

### JSON Schema support

[JSON Schema](https://json-schema.org/) is a more recent specification than OpenAPI 3.0, which the [Schema](/api/caching#Schema) object is based on. Support for JSON Schema is available as a preview using the field [`responseJsonSchema`](/api/generate-content#FIELDS.response_json_schema) which accepts any JSON Schema with the following limitations:

*   It only works with Gemini 2.5.
*   While all JSON Schema properties can be passed, not all are supported. See the [documentation](/api/generate-content#FIELDS.response_json_schema) for the field for more details.
*   Recursive references can only be used as the value of a non-required object property.
*   Recursive references are unrolled to a finite degree, based on the size of the schema.
*   Schemas that contain `$ref` cannot contain any properties other than those starting with a `$`.

Here's an example of generating a JSON Schema with Pydantic and submitting it to the model:

```
curl "https://generativelanguage.googleapis.com/v1alpha/models/\
gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY"\
    -H 'Content-Type: application/json' \
    -d @- <<EOF
{
  "contents": [{
    "parts":[{
      "text": "Please give a random example following this schema"
    }]
  }],
  "generationConfig": {
    "response_mime_type": "application/json",
    "response_json_schema": $(python3 - << PYEOF
from enum import Enum
from typing import List, Optional, Union, Set
from pydantic import BaseModel, Field, ConfigDict
import json

class UserRole(str, Enum):
    ADMIN = "admin"
    VIEWER = "viewer"

class Address(BaseModel):
    street: str
    city: str

class UserProfile(BaseModel):
    username: str = Field(description="User's unique name")
    age: Optional[int] = Field(ge=0, le=120)
    roles: Set[UserRole] = Field(min_items=1)
    contact: Union[Address, str]
    model_config = ConfigDict(title="User Schema")

# Generate and print the JSON Schema
print(json.dumps(UserProfile.model_json_schema(), indent=2))
PYEOF
)
  }
}
EOF
```

Passing JSON Schema directly is not yet supported when using the SDK.

## Best practices

Keep the following considerations and best practices in mind when you're using a response schema:

*   The size of your response schema counts towards the input token limit.
*   By default, fields are optional, meaning the model can populate the fields or skip them. You can set fields as required to force the model to provide a value. If there's insufficient context in the associated input prompt, the model generates responses mainly based on the data it was trained on.
*   A complex schema can result in an `InvalidArgument: 400` error. Complexity might come from long property names, long array length limits, enums with many values, objects with lots of optional properties, or a combination of these factors.
    
    If you get this error with a valid schema, make one or more of the following changes to resolve the error:
    
    *   Shorten property names or enum names.
    *   Flatten nested arrays.
    *   Reduce the number of properties with constraints, such as numbers with minimum and maximum limits.
    *   Reduce the number of properties with complex constraints, such as properties with complex formats like `date-time`.
    *   Reduce the number of optional properties.
    *   Reduce the number of valid values for enums.
*   If you aren't seeing the results you expect, add more context to your input prompts or revise your response schema. For example, review the model's response without structured output to see how the model responds. You can then update your response schema so that it better fits the model's output.
    

## What's next

Now that you've learned how to generate structured output, you might want to try using Gemini API tools:

*   [Function calling](/gemini-api/docs/function-calling)
*   [Code execution](/gemini-api/docs/code-execution)
*   [Grounding with Google Search](/gemini-api/docs/grounding)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/thinking

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini thinking

The [Gemini 2.5 series models](/gemini-api/docs/models) use an internal "thinking process" that significantly improves their reasoning and multi-step planning abilities, making them highly effective for complex tasks such as coding, advanced mathematics, and data analysis.

This guide shows you how to work with Gemini's thinking capabilities using the Gemini API.

## Before you begin

Ensure you use a supported 2.5 series model for thinking. You might find it beneficial to explore these models in AI Studio before diving into the API:

*   [Try Gemini 2.5 Flash in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash)
*   [Try Gemini 2.5 Pro in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro)
*   [Try Gemini 2.5 Flash-Lite Preview in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-lite-preview-06-17)

## Generating content with thinking

Initiating a request with a thinking model is similar to any other content generation request. The key difference lies in specifying one of the [models with thinking support](#supported-models) in the `model` field, as demonstrated in the following [text generation](/gemini-api/docs/text-generation#text-input) example:

### Python

```
from google import genai

client = genai.Client()
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents=prompt
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: prompt,
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example."
  model := "gemini-2.5-pro"

  resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil)

  fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
   "contents": [
     {
       "parts": [
         {
           "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example."
         }
       ]
     }
   ]
 }'
 ```
```

## Thinking budgets

The `thinkingBudget` parameter guides the model on the number of thinking tokens to use when generating a response. A higher token count generally allows for more detailed reasoning, which can be beneficial for tackling more [complex tasks](#tasks). If latency is more important, use a lower budget or disable thinking by setting `thinkingBudget` to 0. Setting the `thinkingBudget` to -1 turns on **dynamic thinking**, meaning the model will adjust the budget based on the complexity of the request.

The `thinkingBudget` is only [supported](#supported-models) in Gemini 2.5 Flash, 2.5 Pro, and 2.5 Flash-Lite. Depending on the prompt, the model might overflow or underflow the token budget.

The following are `thinkingBudget` configuration details for each model type.

    

Model

Default setting  
(Thinking budget is not set)

Range

Disable thinking

Turn on dynamic thinking

**2.5 Pro**

Dynamic thinking: Model decides when and how much to think

`128` to `32768`

N/A: Cannot disable thinking

`thinkingBudget = -1`

**2.5 Flash**

Dynamic thinking: Model decides when and how much to think

`0` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

**2.5 Flash Lite**

Model does not think

`512` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
        # Turn off thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=0)
        # Turn on dynamic thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=-1)
    ),
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
        // Turn off thinking:
        // thinkingBudget: 0
        // Turn on dynamic thinking:
        // thinkingBudget: -1
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingBudgetVal := int32(1024)

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingBudget: &thinkingBudgetVal,
      // Turn off thinking:
      // ThinkingBudget: int32(0),
      // Turn on dynamic thinking:
      // ThinkingBudget: int32(-1),
    },
  })

fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingBudget": 1024
          # Thinking off:
          # "thinkingBudget": 0
          # Turn on dynamic thinking:
          # "thinkingBudget": -1
    }
  }
}'
```

## Thought summaries

Thought summaries are synthisized versions of the model's raw thoughts and offer insights into the model's internal reasoning process. Note that thinking budgets apply to the model's raw thoughts and not to thought summaries.

You can enable thought summaries by setting `includeThoughts` to `true` in your request configuration. You can then access the summary by iterating through the `response` parameter's `parts`, and checking the `thought` boolean.

Here's an example demonstrating how to enable and retrieve thought summaries without streaming, which returns a single, final thought summary with the response:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro",
  contents=prompt,
  config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought:
    print("Thought summary:")
    print(part.text)
    print()
  else:
    print("Answer:")
    print(part.text)
    print()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "What is the sum of the first 50 prime numbers?",
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (!part.text) {
      continue;
    }
    else if (part.thought) {
      console.log("Thoughts summary:");
      console.log(part.text);
    }
    else {
      console.log("Answer:");
      console.log(part.text);
    }
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text("What is the sum of the first 50 prime numbers?")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for _, part := range resp.Candidates[0].Content.Parts {
    if part.Text != "" {
      if part.Thought {
        fmt.Println("Thoughts Summary:")
        fmt.Println(part.Text)
      } else {
        fmt.Println("Answer:")
        fmt.Println(part.Text)
      }
    }
  }
}
```

And here is an example using thinking with streaming, which returns rolling, incremental summaries during generation:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = """
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
"""

thoughts = ""
answer = ""

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-pro",
    contents=prompt,
    config=types.GenerateContentConfig(
      thinking_config=types.ThinkingConfig(
        include_thoughts=True
      )
    )
):
  for part in chunk.candidates[0].content.parts:
    if not part.text:
      continue
    elif part.thought:
      if not thoughts:
        print("Thoughts summary:")
      print(part.text)
      thoughts += part.text
    else:
      if not answer:
        print("Thoughts summary:")
      print(part.text)
      answer += part.text
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `Alice, Bob, and Carol each live in a different house on the same
street: red, green, and blue. The person who lives in the red house owns a cat.
Bob does not live in the green house. Carol owns a dog. The green house is to
the left of the red house. Alice does not own a cat. Who lives in each house,
and what pet do they own?`;

let thoughts = "";
let answer = "";

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-pro",
    contents: prompt,
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for await (const chunk of response) {
    for (const part of chunk.candidates[0].content.parts) {
      if (!part.text) {
        continue;
      } else if (part.thought) {
        if (!thoughts) {
          console.log("Thoughts summary:");
        }
        console.log(part.text);
        thoughts = thoughts + part.text;
      } else {
        if (!answer) {
          console.log("Answer:");
        }
        console.log(part.text);
        answer = answer + part.text;
      }
    }
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

## Pricing

When thinking is turned on, response pricing is the sum of output tokens and thinking tokens. You can get the total number of generated thinking tokens from the `thoughtsTokenCount` field.

### Python

```
# ...
print("Thoughts tokens:",response.usage_metadata.thoughts_token_count)
print("Output tokens:",response.usage_metadata.candidates_token_count)
```

### JavaScript

```
// ...
console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`);
console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`);
```

### Go

```
// ...
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
  log.Fatal(err)
}
fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count))
fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count))
```

Thinking models generate full thoughts to improve the quality of the final response, and then output [summaries](#summaries) to provide insight into the thought process. So, pricing is based on the full thought tokens the model needs to generate to create a summary, despite only the summary being output from the API.

You can learn more about tokens in the [Token counting](/gemini-api/docs/tokens) guide.

## Supported models

Thinking features are supported on all the 2.5 series models. You can find all model capabilities on the [model overview](/gemini-api/docs/models) page.

## Best practices

This section includes some guidance for using thinking models efficiently. As always, following our [prompting guidance and best practices](/gemini-api/docs/prompting-strategies) will get you the best results.

### Debugging and steering

*   **Review reasoning**: When you're not getting your expected response from the thinking models, it can help to carefully analyze Gemini's thought summaries. You can see how it broke down the task and arrived at its conclusion, and use that information to correct towards the right results.
    
*   **Provide Guidance in Reasoning**: If you're hoping for a particularly lengthy output, you may want to provide guidance in your prompt to constrain the [amount of thinking](#set-budget) the model uses. This lets you reserve more of the token output for your response.
    

### Task complexity

*   **Easy Tasks (Thinking could be OFF):** For straightforward requests where complex reasoning isn't required, such as fact retrieval or classification, thinking is not required. Examples include:
    *   "Where was DeepMind founded?"
    *   "Is this email asking for a meeting or just providing information?"
*   **Medium Tasks (Default/Some Thinking):** Many common requests benefit from a degree of step-by-step processing or deeper understanding. Gemini can flexibly use thinking capability for tasks like:
    *   Analogize photosynthesis and growing up.
    *   Compare and contrast electric cars and hybrid cars.
*   **Hard Tasks (Maximum Thinking Capability):** For truly complex challenges, such as solving complex math problems or coding tasks, we recommend setting a high thinking budget. These types of tasks require the model needs to engage its full reasoning and planning capabilities, often involving many internal steps before providing an answer. Examples include:
    *   Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for which 17b is a divisor of 97b.
    *   Write Python code for a web application that visualizes real-time stock market data, including user authentication. Make it as efficient as possible.

## Thinking with tools and capabilities

Thinking models work with all of Gemini's tools and capabilities. This allows the models to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.

*   The [search tool](/gemini-api/docs/grounding) allows the model to query Google Search to find up-to-date information or information beyond its training data. This is useful for questions about recent events or highly specific topics.
    
*   The [code execution tool](/gemini-api/docs/code-execution) enables the model to generate and run Python code to perform calculations, manipulate data, or solve problems that are best handled algorithmically. The model receives the code's output and can use it in its response.
    
*   With [structured output](/gemini-api/docs/structured-output), you can constrain Gemini to respond with JSON. This is particularly useful for integrating the model's output into applications.
    
*   [Function calling](/gemini-api/docs/function-calling) connects the thinking model to external tools and APIs, so it can reason about when to call the right function and what parameters to provide.
    
*   [URL Context](/gemini-api/docs/url-context) provides the model with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.
    

You can try examples of using tools with thinking models in the [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).

## What's next?

*   To work through more in depth examples, like:
    
    *   Using tools with thinking
    *   Streaming with thinking
    *   Adjusting the thinking budget for different results
    
    and more, try our [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).
    
*   Thinking coverage is now available in our [OpenAI Compatibility](/gemini-api/docs/openai#thinking) guide.
    
*   For more info about Gemini 2.5 Pro, Gemini Flash 2.5, and Gemini 2.5 Flash-Lite, visit the [model page](/gemini-api/docs/models).
    

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/document-processing

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Document understanding

Python JavaScript Go REST

The Gemini API supports PDF input, including long documents (up to 1000 pages). Gemini models process PDFs with native vision, and are therefore able to understand both text and image contents inside documents. With native PDF vision support, Gemini models are able to:

*   Analyze diagrams, charts, and tables inside documents
*   Extract information into structured output formats
*   Answer questions about visual and text contents in documents
*   Summarize documents
*   Transcribe document content (e.g. to HTML) preserving layouts and formatting, for use in downstream applications

This tutorial demonstrates some possible ways to use the Gemini API to process PDF documents.

## Technical details

Gemini supports a maximum of 1,000 document pages. Document pages must be in one of the following text data MIME types:

*   PDF - `application/pdf`
*   JavaScript - `application/x-javascript`, `text/javascript`
*   Python - `application/x-python`, `text/x-python`
*   TXT - `text/plain`
*   HTML - `text/html`
*   CSS - `text/css`
*   Markdown - `text/md`
*   CSV - `text/csv`
*   XML - `text/xml`
*   RTF - `text/rtf`

Each document page is equivalent to 258 tokens.

While there are no specific limits to the number of pixels in a document besides the model's context window, larger pages are scaled down to a maximum resolution of 3072x3072 while preserving their original aspect ratio, while smaller pages are scaled up to 768x768 pixels. There is no cost reduction for pages at lower sizes, other than bandwidth, or performance improvement for pages at higher resolution.

For best results:

*   Rotate pages to the correct orientation before uploading.
*   Avoid blurry pages.
*   If using a single page, place the text prompt after the page.

## What's next

To learn more, see the following resources:

*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-24 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/image-understanding

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Image understanding

Gemini models are built to be multimodal from the ground up, unlocking a wide range of image processing and computer vision tasks including but not limited to image captioning, classification, and visual question answering without having to train specialized ML models.

## Passing images to Gemini

You can provide images as input to Gemini using two methods:

*   [Passing inline image data](#inline-image): Ideal for smaller files (total request size less than 20MB, including prompts).
*   [Uploading images using the File API](#upload-image): Recommended for larger files or for reusing images across multiple requests.

### Passing inline image data

You can pass inline image data in the request to `generateContent`. You can provide image data as Base64 encoded strings or by reading local files directly (depending on the language).

The following example shows how to read an image from a local file and pass it to `generateContent` API for processing.

### Python

  ```
  from google.genai import types

  with open('path/to/small-sample.jpg', 'rb') as f:
      image_bytes = f.read()

  response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=[
      types.Part.from_bytes(
        data=image_bytes,
        mime_type='image/jpeg',
      ),
      'Caption this image.'
    ]
  )

  print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

### Go

```
bytes, _ := os.ReadFile("path/to/small-sample.jpg")

parts := []*genai.Part{
  genai.NewPartFromBytes(bytes, "image/jpeg"),
  genai.NewPartFromText("Caption this image."),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
IMG_PATH="/path/to/your/image1.jpg"

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
B64FLAGS="--input"
else
B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "'"$(base64 $B64FLAGS $IMG_PATH)"'"
            }
        },
        {"text": "Caption this image."},
    ]
    }]
}' 2> /dev/null
```

You can also fetch an image from a URL, convert it to bytes, and pass it to `generateContent` as shown in the following examples.

### Python

```
from google import genai
from google.genai import types

import requests

image_path = "https://goo.gle/instrument-img"
image_bytes = requests.get(image_path).content
image = types.Part.from_bytes(
  data=image_bytes, mime_type="image/jpeg"
)

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["What is this image?", image],
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

async function main() {
  const ai = new GoogleGenAI({});

  const imageUrl = "https://goo.gle/instrument-img";

  const response = await fetch(imageUrl);
  const imageArrayBuffer = await response.arrayBuffer();
  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
    {
      inlineData: {
        mimeType: 'image/jpeg',
        data: base64ImageData,
      },
    },
    { text: "Caption this image." }
  ],
  });
  console.log(result.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "io"
  "net/http"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Download the image.
  imageResp, _ := http.Get("https://goo.gle/instrument-img")

  imageBytes, _ := io.ReadAll(imageResp.Body)

  parts := []*genai.Part{
    genai.NewPartFromBytes(imageBytes, "image/jpeg"),
    genai.NewPartFromText("Caption this image."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMG_URL="https://goo.gle/instrument-img"

MIME_TYPE=$(curl -sIL "$IMG_URL" | grep -i '^content-type:' | awk -F ': ' '{print $2}' | sed 's/\r$//' | head -n 1)
if [[ -z "$MIME_TYPE" || ! "$MIME_TYPE" == image/* ]]; then
  MIME_TYPE="image/jpeg"
fi

# Check for macOS
if [[ "$(uname)" == "Darwin" ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -b 0)
elif [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64)
else
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -w0)
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"'"$MIME_TYPE"'",
                "data": "'"$IMAGE_B64"'"
              }
            },
            {"text": "Caption this image."}
        ]
      }]
    }' 2> /dev/null
```

### Uploading images using the File API

For large files or to be able to use the same image file repeatedly, use the Files API. The following code uploads an image file and then uses the file in a call to `generateContent`. See the [Files API guide](/gemini-api/docs/files) for more information and examples.

### Python

```
from google import genai

client = genai.Client()

my_file = client.files.upload(file="path/to/sample.jpg")

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[my_file, "Caption this image."],
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.jpg",
    config: { mimeType: "image/jpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Caption this image.",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.jpg", nil)

  parts := []*genai.Part{
      genai.NewPartFromText("Caption this image."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMAGE_PATH="path/to/sample.jpg"
MIME_TYPE=$(file -b --mime-type "${IMAGE_PATH}")
NUM_BYTES=$(wc -c < "${IMAGE_PATH}")
DISPLAY_NAME=IMAGE

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Caption this image."}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Prompting with multiple images

You can provide multiple images in a single prompt by including multiple image `Part` objects in the `contents` array. These can be a mix of inline data (local files or URLs) and File API references.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

# Upload the first image
image1_path = "path/to/image1.jpg"
uploaded_file = client.files.upload(file=image1_path)

# Prepare the second image as inline data
image2_path = "path/to/image2.png"
with open(image2_path, 'rb') as f:
    img2_bytes = f.read()

# Create the prompt with text and multiple images
response = client.models.generate_content(

    model="gemini-2.5-flash",
    contents=[
        "What is different between these two images?",
        uploaded_file,  # Use the uploaded file reference
        types.Part.from_bytes(
            data=img2_bytes,
            mime_type='image/png'
        )
    ]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images

  const response = await ai.models.generateContent({

    model: "gemini-2.5-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
# Upload the first image
IMAGE1_PATH="path/to/image1.jpg"
MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
DISPLAY_NAME1=IMAGE1

tmp_header_file1=upload-header1.tmp

curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header1.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file1}"

curl "${upload_url1}" \
  -H "Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

file1_uri=$(jq ".file.uri" file_info1.json)
echo file1_uri=$file1_uri

# Prepare the second image (inline)
IMAGE2_PATH="path/to/image2.png"
MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi
IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

# Now generate content using both images
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "What is different between these two images?"},
          {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
          {
            "inline_data": {
              "mime_type":"'"${MIME2_TYPE}"'",
              "data": "'"$IMAGE2_BASE64"'"
            }
          }
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Object detection

From Gemini 2.0 onwards, models are further trained to detect objects in an image and get their bounding box coordinates. The coordinates, relative to image dimensions, scale to \[0, 1000\]. You need to descale these coordinates based on your original image size.

### Python

```
from google import genai
from google.genai import types
from PIL import Image
import json

client = genai.Client()
prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

image = Image.open("/path/to/image.png")

config = types.GenerateContentConfig(
  response_mime_type="application/json"
  ) 

response = client.models.generate_content(model="gemini-2.5-flash",
                                          contents=[image, prompt],
                                          config=config
                                          )

width, height = image.size
bounding_boxes = json.loads(response.text)

converted_bounding_boxes = []
for bounding_box in bounding_boxes:
    abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
    abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
    abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
    abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

print("Image size: ", width, height)
print("Bounding boxes:", converted_bounding_boxes)

```

For more examples, check following cookbooks:

*   [2D spatial understanding notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb)
*   [Experimental 3D pointing notebook](https://github.com/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb)

## Segmentation

Starting with Gemini 2.5, models not only detect items but also segment them and provide their contour masks.

The model predicts a JSON list, where each item represents a segmentation mask. Each item has a bounding box ("`box_2d`") in the format `[y0, x0, y1, x1]` with normalized coordinates between 0 and 1000, a label ("`label`") that identifies the object, and finally the segmentation mask inside the bounding box, as base64 encoded png that is a probability map with values between 0 and 255. The mask needs to be resized to match the bounding box dimensions, then binarized at your confidence threshold (127 for the midpoint).

### Python

```
from google import genai
from google.genai import types
from PIL import Image, ImageDraw    
import io
import base64
import json
import numpy as np
import os    

client = genai.Client()

def parse_json(json_output: str):
  # Parsing out the markdown fencing
  lines = json_output.splitlines()
  for i, line in enumerate(lines):
      if line == "```json":
          json_output = "\n".join(lines[i+1:])  # Remove everything before "```json"
          json_output = json_output.split("```")[0]  # Remove everything after the closing "```"
          break  # Exit the loop once "```json" is found
  return json_output

def extract_segmentation_masks(image_path: str, output_dir: str = "segmentation_outputs"):
  # Load and resize image
  im = Image.open(image_path)
  im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)

  prompt = """
  Give the segmentation masks for the wooden and glass items.
  Output a JSON list of segmentation masks where each entry contains the 2D
  bounding box in the key "box_2d", the segmentation mask in key "mask", and
  the text label in the key "label". Use descriptive labels.
  """

  config = types.GenerateContentConfig(      
    thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection
  ) 

  response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[prompt, im], # Pillow images can be directly passed as inputs (which will be converted by the SDK)
    config=config
  )

  # Parse JSON response
  items = json.loads(parse_json(response.text))

  # Create output directory
  os.makedirs(output_dir, exist_ok=True)

  # Process each mask
  for i, item in enumerate(items):
      # Get bounding box coordinates
      box = item["box_2d"]
      y0 = int(box[0] / 1000 * im.size[1])
      x0 = int(box[1] / 1000 * im.size[0])
      y1 = int(box[2] / 1000 * im.size[1])
      x1 = int(box[3] / 1000 * im.size[0])

      # Skip invalid boxes
      if y0 >= y1 or x0 >= x1:
          continue

      # Process mask
      png_str = item["mask"]
      if not png_str.startswith("data:image/png;base64,"):
          continue

      # Remove prefix
      png_str = png_str.removeprefix("data:image/png;base64,")
      mask_data = base64.b64decode(png_str)
      mask = Image.open(io.BytesIO(mask_data))

      # Resize mask to match bounding box
      mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)

      # Convert mask to numpy array for processing
      mask_array = np.array(mask)

      # Create overlay for this mask
      overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))
      overlay_draw = ImageDraw.Draw(overlay)

      # Create overlay for the mask
      color = (255, 255, 255, 200)
      for y in range(y0, y1):
          for x in range(x0, x1):
              if mask_array[y - y0, x - x0] > 128:  # Threshold for mask
                  overlay_draw.point((x, y), fill=color)

      # Save individual mask and its overlay
      mask_filename = f"{item['label']}_{i}_mask.png"
      overlay_filename = f"{item['label']}_{i}_overlay.png"

      mask.save(os.path.join(output_dir, mask_filename))

      # Create and save overlay
      composite = Image.alpha_composite(im.convert('RGBA'), overlay)
      composite.save(os.path.join(output_dir, overlay_filename))
      print(f"Saved mask and overlay for {item['label']} to {output_dir}")

# Example usage
if __name__ == "__main__":
  extract_segmentation_masks("path/to/image.png")

```

Check the [segmentation example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb#scrollTo=WQJTJ8wdGOKx) in the cookbook guide for a more detailed example.

![A table with cupcakes, with the wooden and glass objects highlighted](/static/gemini-api/docs/images/segmentation.jpg)

An example segmentation output with objects and segmentation masks

## Supported image formats

Gemini supports the following image format MIME types:

*   PNG - `image/png`
*   JPEG - `image/jpeg`
*   WEBP - `image/webp`
*   HEIC - `image/heic`
*   HEIF - `image/heif`

## Capabilities

All Gemini model versions are multimodal and can be utilized in a wide range of image processing and computer vision tasks including but not limited to image captioning, visual question and answering, image classification, object detection and segmentation.

Gemini can reduce the need to use specialized ML models depending on your quality and performance requirements.

Some later model versions are specifically trained improve accuracy of specialized tasks in addition to generic capabilities:

*   **Gemini 2.0 models** are further trained to support enhanced [object detection](#object-detection).
    
*   **Gemini 2.5 models** are further trained to support enhanced [segmentation](#segmentation) in addition to [object detection](#object-detection).
    

## Limitations and key technical information

### File limit

Gemini 2.5 Pro/Flash, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a maximum of 3,600 image files per request.

### Token calculation

*   **Gemini 1.5 Flash and Gemini 1.5 Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized to 768x768), with each tile costing 258 tokens.
*   **Gemini 2.0 Flash and Gemini 2.5 Flash/Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled into 768x768 pixel tiles, each costing 258 tokens.

## Tips and best practices

*   Verify that images are correctly rotated.
*   Use clear, non-blurry images.
*   When using a single image with text, place the text prompt _after_ the image part in the `contents` array.

## What's next

This guide shows you how to upload image files and generate text outputs from image inputs. To learn more, see the following resources:

*   [Files API](/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/video-understanding

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Video understanding

Gemini models can process videos, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:

*   Describe, segment, and extract information from videos
*   Answer questions about video content
*   Refer to specific timestamps within a video

Gemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible. This guide shows how to use the Gemini API to generate text responses based on video inputs.

## Video input

You can provide videos as input to Gemini in the following ways:

*   [Upload a video file](#upload-video) using the File API before making a request to `generateContent`. Use this method for files larger than 20MB, videos longer than approximately 1 minute, or when you want to reuse the file across multiple requests.
*   [Pass inline video data](#inline-video) with the request to `generateContent`. Use this method for smaller files (<20MB) and shorter durations.
*   [Include a YouTube URL](#youtube) directly in the prompt.

### Upload a video file

You can use the [Files API](/gemini-api/docs/files) to upload a video file. Always use the Files API when the total request size (including the file, text prompt, system instructions, etc.) is larger than 20 MB, the video duration is significant, or if you intend to use the same video in multiple prompts.

The File API accepts video file formats directly. This example uses the short NASA film ["Jupiter's Great Red Spot Shrinks and Grows"](https://www.youtube.com/watch?v=JDi4IdtvDVE0). Credit: Goddard Space Flight Center (GSFC)/David Ladd (2018).

"Jupiter's Great Red Spot Shrinks and Grows" is in the public domain and does not show identifiable people. ([NASA image and media usage guidelines.](https://www.nasa.gov/nasa-brand-center/images-and-media/))

The following code downloads the sample video, uploads it using the File API, waits for it to be processed, and then uses the file reference in a `generateContent` request.

### Python

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp4")

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[myfile, "Summarize this video. Then create a quiz with an answer key based on the information in this video."]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp4",
    config: { mimeType: "video/mp4" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Summarize this video. Then create a quiz with an answer key based on the information in this video.",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.mp4", nil)

parts := []*genai.Part{
    genai.NewPartFromText("Summarize this video. Then create a quiz with an answer key based on the information in this video."),
    genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.0-flash",
    contents,
    nil,
)

fmt.Println(result.Text())
```

### REST

```
VIDEO_PATH="path/to/sample.mp4"
MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
NUM_BYTES=$(wc -c < "${VIDEO_PATH}")
DISPLAY_NAME=VIDEO

tmp_header_file=upload-header.tmp

echo "Starting file upload..."
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D ${tmp_header_file} \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

echo "Uploading video data..."
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${VIDEO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

echo "File uploaded successfully. File URI: ${file_uri}"

# --- 3. Generate content using the uploaded video file ---
echo "Generating content from video..."
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Summarize this video. Then create a quiz with an answer key based on the information in this video."}]
        }]
      }' 2> /dev/null > response.json

jq -r ".candidates[].content.parts[].text" response.json
```

To learn more about working with media files, see [Files API](/gemini-api/docs/files).

### Pass video data inline

Instead of uploading a video file using the File API, you can pass smaller videos directly in the request to `generateContent`. This is suitable for shorter videos under 20MB total request size.

Here's an example of providing inline video data:

### Python

```
# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

response = client.models.generate_content(
    model='models/gemini-2.0-flash',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(data=video_bytes, mime_type='video/mp4')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64VideoFile = fs.readFileSync("path/to/small-sample.mp4", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "video/mp4",
      data: base64VideoFile,
    },
  },
  { text: "Please summarize the video in 3 sentences." }
];

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(response.text);
```

### REST

```
VIDEO_PATH=/path/to/your/video.mp4

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"video/mp4",
                "data": "'$(base64 $B64FLAGS $VIDEO_PATH)'"
              }
            },
            {"text": "Please summarize the video in 3 sentences."}
        ]
      }]
    }' 2> /dev/null
```

### Include a YouTube URL

The Gemini API and AI Studio support YouTube URLs as a file data `Part`. You can include a YouTube URL with a prompt asking the model to summarize, translate, or otherwise interact with the video content.

**Limitations:**

*   For the free tier, you can't upload more than 8 hours of YouTube video per day.
*   For the paid tier, there is no limit based on video length.
*   For models before 2.5, you can upload only 1 video per request. For models after 2.5, you can upload a maximum of 10 videos per request.
*   You can only upload public videos (not private or unlisted videos).

The following example shows how to include a YouTube URL with a prompt:

### Python

```
response = client.models.generate_content(
    model='models/gemini-2.0-flash',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=9hE5-98ZeCg')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-pro" });
const result = await model.generateContent([
  "Please summarize the video in 3 sentences.",
  {
    fileData: {
      fileUri: "https://www.youtube.com/watch?v=9hE5-98ZeCg",
    },
  },
]);
console.log(result.response.text());
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  parts := []*genai.Part{
      genai.NewPartFromText("Please summarize the video in 3 sentences."),
      genai.NewPartFromURI("https://www.youtube.com/watch?v=9hE5-98ZeCg","video/mp4"),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.0-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {"text": "Please summarize the video in 3 sentences."},
            {
              "file_data": {
                "file_uri": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
              }
            }
        ]
      }]
    }' 2> /dev/null
```

## Refer to timestamps in the content

You can ask questions about specific points in time within the video using timestamps of the form `MM:SS`.

### Python

```
prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?" # Adjusted timestamps for the NASA video
```

### JavaScript

```
const prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?";
```

### Go

    ```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
         // Adjusted timestamps for the NASA video
        genai.NewPartFromText("What are the examples given at 00:05 and " +
            "00:10 supposed to show us?"),
    }
```

### REST

```
PROMPT="What are the examples given at 00:05 and 00:10 supposed to show us?"
```

## Transcribe video and provide visual descriptions

The Gemini models can transcribe and provide visual descriptions of video content by processing both the audio track and visual frames. For visual descriptions, the model samples the video at a rate of **1 frame per second**. This sampling rate may affect the level of detail in the descriptions, particularly for videos with rapidly changing visuals.

### Python

```
prompt = "Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions."
```

### JavaScript

```
const prompt = "Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.";
```

### Go

    ```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
        genai.NewPartFromText("Transcribe the audio from this video, giving timestamps for salient events in the video. Also " +
            "provide visual descriptions."),
    }
```

### REST

```
PROMPT="Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions."
```

## Customize video processing

You can customize video processing in the Gemini API by setting clipping intervals or providing custom frame rate sampling.

### Set clipping intervals

You can clip video by specifying `videoMetadata` with start and end offsets.

### Python

```
response = client.models.generate_content(
    model='models/gemini-2.5-flash-preview-05-20',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=XEzRZ35urlk'),
                video_metadata=types.VideoMetadata(
                    start_offset='1250s',
                    end_offset='1570s'
                )
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

### Set a custom frame rate

You can set custom frame rate sampling by passing an `fps` argument to `videoMetadata`.

### Python

```
# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

response = client.models.generate_content(
    model='models/gemini-2.5-flash-preview-05-20',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=video_bytes,
                    mime_type='video/mp4'),
                video_metadata=types.VideoMetadata(fps=5)
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

By default 1 frame per second (FPS) is sampled from the video. You might want to set low FPS (< 1) for long videos. This is especially useful for mostly static videos (e.g. lectures). If you want to capture more details in rapidly changing visuals, consider setting a higher FPS value.

## Supported video formats

Gemini supports the following video format MIME types:

*   `video/mp4`
*   `video/mpeg`
*   `video/mov`
*   `video/avi`
*   `video/x-flv`
*   `video/mpg`
*   `video/webm`
*   `video/wmv`
*   `video/3gpp`

## Technical details about videos

*   **Supported models & context**: All Gemini 2.0 and 2.5 models can process video data.
    *   Models with a 2M context window can process videos up to 2 hours long at default media resolution or 6 hours long at low media resolution, while models with a 1M context window can process videos up to 1 hour long at default media resolution or 3 hours long at low media resolution.
*   **File API processing**: When using the File API, videos are sampled at 1 frame per second (FPS) and audio is processed at 1Kbps (single channel). Timestamps are added every second.
    *   These rates are subject to change in the future for improvements in inference.
*   **Token calculation**: Each second of video is tokenized as follows:
    *   Individual frames (sampled at 1 FPS):
        *   If [`mediaResolution`](/api/generate-content#MediaResolution) is set to low, frames are tokenized at 66 tokens per frame.
        *   Otherwise, frames are tokenized at 258 tokens per frame.
    *   Audio: 32 tokens per second.
    *   Metadata is also included.
    *   Total: Approximately 300 tokens per second of video at default media resolution, or 100 tokens per second of video at low media resolution.
*   **Timestamp format**: When referring to specific moments in a video within your prompt, use the `MM:SS` format (e.g., `01:15` for 1 minute and 15 seconds).
*   **Best practices**:
    *   Use only one video per prompt request for optimal results.
    *   If combining text and a single video, place the text prompt _after_ the video part in the `contents` array.
    *   Be aware that fast action sequences might lose detail due to the 1 FPS sampling rate. Consider slowing down such clips if necessary.

## What's next

This guide shows how to upload video files and generate text outputs from video inputs. To learn more, see the following resources:

*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
*   [Files API](/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/function-calling

Nous vous présentons les mises à jour de notre famille de modèles de pensée 2.5. [En savoir plus](https://ai.google.dev/gemini-api/docs/models?hl=fr)

![](https://ai.google.dev/_static/images/translated.svg?hl=fr) Cette page a été traduite par l'[API Cloud Translation](//cloud.google.com/translate/?hl=fr).

*   [Accueil](https://ai.google.dev/?hl=fr)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=fr)
*   [Modèles](https://ai.google.dev/gemini-api/docs?hl=fr)

Envoyer des commentaires

# Appel de fonction avec l'API Gemini

L'appel de fonction vous permet de connecter des modèles à des outils et des API externes. Au lieu de générer des réponses textuelles, le modèle détermine quand appeler des fonctions spécifiques et fournit les paramètres nécessaires pour exécuter des actions réelles. Cela permet au modèle de servir de pont entre le langage naturel et les actions et données du monde réel. Les appels de fonction ont trois cas d'utilisation principaux :

*   **Améliorer les connaissances** : accéder aux informations provenant de sources externes telles que des bases de données, des API et des bases de connaissances.
*   **Étendre les fonctionnalités** : utilisez des outils externes pour effectuer des calculs et repousser les limites du modèle, par exemple en utilisant une calculatrice ou en créant des graphiques.
*   **Exécuter des actions** : interagissez avec des systèmes externes à l'aide d'API, par exemple pour planifier des rendez-vous, créer des factures, envoyer des e-mails ou contrôler des appareils de maison connectée.

Obtenir la météo Planifier une réunion Créer un graphique

## Fonctionnement des appels de fonction

![Présentation des appels de fonction](https://ai.google.dev/static/gemini-api/docs/images/function-calling-overview.png?hl=fr)

L'appel de fonction implique une interaction structurée entre votre application, le modèle et les fonctions externes. Voici le détail :

1.  **Définir la déclaration de fonction** : définissez la déclaration de fonction dans le code de votre application. Les déclarations de fonction décrivent le nom, les paramètres et l'objectif de la fonction au modèle.
2.  **Appeler le LLM avec des déclarations de fonction** : envoyez la requête utilisateur avec la ou les déclarations de fonction au modèle. Il analyse la requête et détermine si un appel de fonction serait utile. Si c'est le cas, il répond avec un objet JSON structuré.
3.  **Exécuter le code de fonction (votre responsabilité)** : le modèle _n'exécute pas_ la fonction lui-même. Il est de la responsabilité de votre application de traiter la réponse et de vérifier l'appel de fonction, si :
    *   **Oui** : extrayez le nom et les arguments de la fonction, puis exécutez la fonction correspondante dans votre application.
    *   **Non** : le modèle a fourni une réponse textuelle directe à la requête (ce flux est moins mis en avant dans l'exemple, mais il s'agit d'un résultat possible).
4.  **Créer une réponse conviviale** : si une fonction a été exécutée, capturez le résultat et renvoyez-le au modèle lors d'un tour ultérieur de la conversation. Il utilisera le résultat pour générer une réponse finale conviviale qui intègre les informations de l'appel de fonction.

Ce processus peut être répété plusieurs fois, ce qui permet des interactions et des workflows complexes. Le modèle permet également d'appeler plusieurs fonctions en une seule fois ([appel de fonction en parallèle](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#parallel_function_calling)) et de manière séquentielle ([appel de fonction compositionnel](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#compositional_function_calling)).

### Étape 1 : Définir une déclaration de fonction

Définissez une fonction et sa déclaration dans le code de votre application, qui permet aux utilisateurs de définir des valeurs de lumière et d'effectuer une requête API. Cette fonction peut appeler des services ou des API externes.

### Python

```
# Define a function that the model can call to control smart lights
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

### JavaScript

```
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

### Étape 2 : Appeler le modèle avec des déclarations de fonction

Une fois que vous avez défini vos déclarations de fonction, vous pouvez demander au modèle de les utiliser. Il analyse la requête et les déclarations de fonction, puis décide de répondre directement ou d'appeler une fonction. Si une fonction est appelée, l'objet de réponse contient une suggestion d'appel de fonction.

### Python

```
from google.genai import types

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[set_light_values_declaration])
config = types.GenerateContentConfig(tools=[tools])

# Define user prompt
contents = [
    types.Content(
        role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
    )
]

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
    config=config,
)

print(response.candidates[0].content.parts[0].function_call)
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Generation config with function declaration
const config = {
  tools: [{
    functionDeclarations: [setLightValuesFunctionDeclaration]
  }]
};

// Configure the client
const ai = new GoogleGenAI({});

// Define user prompt
const contents = [
  {
    role: 'user',
    parts: [{ text: 'Turn the lights down to a romantic level' }]
  }
];

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(response.functionCalls[0]);
```

Le modèle renvoie ensuite un objet `functionCall` dans un schéma compatible avec OpenAPI, qui spécifie comment appeler une ou plusieurs des fonctions déclarées pour répondre à la question de l'utilisateur.

### Python

```
id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'
```

### JavaScript

```
{
  name: 'set_light_values',
  args: { brightness: 25, color_temp: 'warm' }
}
```

### Étape 3 : Exécutez le code de la fonction set\_light\_values

Extrayez les détails de l'appel de fonction à partir de la réponse du modèle, analysez les arguments et exécutez la fonction `set_light_values`.

### Python

```
# Extract tool call details, it may not be in the first part.
tool_call = response.candidates[0].content.parts[0].function_call

if tool_call.name == "set_light_values":
    result = set_light_values(**tool_call.args)
    print(f"Function execution result: {result}")
```

### JavaScript

```
// Extract tool call details
const tool_call = response.functionCalls[0]

let result;
if (tool_call.name === 'set_light_values') {
  result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);
  console.log(`Function execution result: ${JSON.stringify(result)}`);
}
```

### Étape 4 : Créez une réponse conviviale avec le résultat de la fonction et appelez à nouveau le modèle

Enfin, renvoyez le résultat de l'exécution de la fonction au modèle afin qu'il puisse intégrer ces informations dans sa réponse finale à l'utilisateur.

### Python

```
# Create a function response part
function_response_part = types.Part.from_function_response(
    name=tool_call.name,
    response={"result": result},
)

# Append function call and result of the function execution to contents
contents.append(response.candidates[0].content) # Append the content from the model's response.
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)
```

### JavaScript

```
// Create a function response part
const function_response_part = {
  name: tool_call.name,
  response: { result }
}

// Append function call and result of the function execution to contents
contents.push(response.candidates[0].content);
contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });

// Get the final response from the model
const final_response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(final_response.text);
```

Le flux d'appel de fonction est terminé. Le modèle a correctement utilisé la fonction `set_light_values` pour effectuer l'action de requête de l'utilisateur.

## Déclarations de fonctions

Lorsque vous implémentez l'appel de fonction dans une invite, vous créez un objet `tools`, qui contient un ou plusieurs `function declarations`. Vous définissez des fonctions à l'aide de JSON, en particulier avec un [sous-ensemble sélectionné](https://ai.google.dev/api/caching?hl=fr#Schema) du format de [schéma OpenAPI](https://spec.openapis.org/oas/v3.0.3#schemaw). Une déclaration de fonction peut inclure les paramètres suivants :

*   `name` (chaîne) : nom unique de la fonction (`get_weather_forecast`, `send_email`). Utilisez des noms descriptifs sans espaces ni caractères spéciaux (utilisez des traits de soulignement ou du style CamelCase).
*   `description` (chaîne) : explication claire et détaillée de l'objectif et des fonctionnalités de la fonction. Il est essentiel que le modèle comprenne quand utiliser la fonction. Soyez précis et fournissez des exemples si nécessaire ("Trouve les cinémas en fonction de la zone géographique et, éventuellement, du titre du film actuellement à l'affiche.").
*   `parameters` (objet) : définit les paramètres d'entrée attendus par la fonction.
    *   `type` (chaîne) : spécifie le type de données global, tel que `object`.
    *   `properties` (objet) : liste les paramètres individuels, chacun avec les éléments suivants :
        *   `type` (chaîne) : type de données du paramètre, tel que `string`, `integer` ou `boolean, array`.
        *   `description` (chaîne) : description de l'objectif et du format du paramètre. Fournissez des exemples et des contraintes ("La ville et l'état, par exemple, "San Francisco, CA" ou un code postal (par exemple, 95616'.").
        *   `enum` (tableau, facultatif) : si les valeurs du paramètre proviennent d'un ensemble fixe, utilisez "enum" pour lister les valeurs autorisées au lieu de simplement les décrire dans la description. Cela améliore la précision ("enum": \["daylight", "cool", "warm"\]).
    *   `required` (tableau) : tableau de chaînes listant les noms des paramètres obligatoires pour le bon fonctionnement de la fonction.

## Appel de fonction en parallèle

En plus de l'appel de fonction à un seul tour, vous pouvez également appeler plusieurs fonctions à la fois. L'appel de fonction parallèle vous permet d'exécuter plusieurs fonctions à la fois. Il est utilisé lorsque les fonctions ne dépendent pas les unes des autres. Cela est utile dans des scénarios tels que la collecte de données à partir de plusieurs sources indépendantes, comme la récupération des informations client à partir de différentes bases de données, la vérification des niveaux d'inventaire dans différents entrepôts ou l'exécution de plusieurs actions, comme la conversion de votre appartement en discothèque.

### Python

```
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}
```

### JavaScript

```
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};
```

Configurez le mode d'appel de fonction pour autoriser l'utilisation de tous les outils spécifiés. Pour en savoir plus, consultez la section [Configurer les appels de fonction](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#function_calling_modes).

### Python

```
from google import genai
from google.genai import types

# Configure the client and tools
client = genai.Client()
house_tools = [
    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
]
config = types.GenerateContentConfig(
    tools=house_tools,
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
        disable=True
    ),
    # Force the model to call 'any' function, instead of chatting.
    tool_config=types.ToolConfig(
        function_calling_config=types.FunctionCallingConfig(mode='ANY')
    ),
)

chat = client.chats.create(model="gemini-2.5-flash", config=config)
response = chat.send_message("Turn this place into a party!")

# Print out each of the function calls requested from this single call
print("Example 1: Forced function calling")
for fn in response.function_calls:
    args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
    print(f"{fn.name}({args})")
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Set up function declarations
const houseFns = [powerDiscoBall, startMusic, dimLights];

const config = {
    tools: [{
        functionDeclarations: houseFns
    }],
    // Force the model to call 'any' function, instead of chatting.
    toolConfig: {
        functionCallingConfig: {
            mode: 'any'
        }
    }
};

// Configure the client
const ai = new GoogleGenAI({});

// Create a chat session
const chat = ai.chats.create({
    model: 'gemini-2.5-flash',
    config: config
});
const response = await chat.sendMessage({message: 'Turn this place into a party!'});

// Print out each of the function calls requested from this single call
console.log("Example 1: Forced function calling");
for (const fn of response.functionCalls) {
    const args = Object.entries(fn.args)
        .map(([key, val]) => `${key}=${val}`)
        .join(', ');
    console.log(`${fn.name}(${args})`);
}
```

Chacun des résultats imprimés reflète un seul appel de fonction que le modèle a demandé. Pour renvoyer les résultats, incluez les réponses dans l'ordre dans lequel elles ont été demandées.

Le SDK Python est compatible avec l'[appel de fonction automatique](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#automatic_function_calling_python_only), qui convertit automatiquement les fonctions Python en déclarations, gère l'exécution de l'appel de fonction et le cycle de réponse à votre place. Voici un exemple pour le cas d'utilisation de la discothèque.

### Python

```
from google import genai
from google.genai import types

# Actual function implementations
def power_disco_ball_impl(power: bool) -> dict:
    """Powers the spinning disco ball.

    Args:
        power: Whether to turn the disco ball on or off.

    Returns:
        A status dictionary indicating the current state.
    """
    return {"status": f"Disco ball powered {'on' if power else 'off'}"}

def start_music_impl(energetic: bool, loud: bool) -> dict:
    """Play some music matching the specified parameters.

    Args:
        energetic: Whether the music is energetic or not.
        loud: Whether the music is loud or not.

    Returns:
        A dictionary containing the music settings.
    """
    music_type = "energetic" if energetic else "chill"
    volume = "loud" if loud else "quiet"
    return {"music_type": music_type, "volume": volume}

def dim_lights_impl(brightness: float) -> dict:
    """Dim the lights.

    Args:
        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

    Returns:
        A dictionary containing the new brightness setting.
    """
    return {"brightness": brightness}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Do everything you need to this place into party!",
    config=config,
)

print("\nExample 2: Automatic function calling")
print(response.text)
# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!
```

## Appel de fonction compositionnel

L'appel de fonction compositionnel ou séquentiel permet à Gemini d'associer plusieurs appels de fonction pour répondre à une requête complexe. Par exemple, pour répondre à la requête "Obtenir la température à mon emplacement actuel", l'API Gemini peut d'abord appeler une fonction `get_current_location()`, suivie d'une fonction `get_weather()` qui utilise l'emplacement comme paramètre.

L'exemple suivant montre comment implémenter l'appel de fonction compositionnel à l'aide du SDK Python et de l'appel de fonction automatique.

### Python

Cet exemple utilise la fonctionnalité d'appel de fonction automatique du SDK Python `google-genai`. Le SDK convertit automatiquement les fonctions Python en schéma requis, exécute les appels de fonction à la demande du modèle et renvoie les résultats au modèle pour qu'il effectue la tâche.

```
import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
    config=config,
)

# Print the final, user-facing response
print(response.text)
```

**Résultat attendu**

Lorsque vous exécutez le code, le SDK orchestre les appels de fonction. Le modèle appelle d'abord `get_weather_forecast`, reçoit la température, puis appelle `set_thermostat_temperature` avec la valeur correcte en fonction de la logique de l'invite.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. I've set the thermostat to 20°C.
```

### JavaScript

Cet exemple montre comment utiliser le SDK JavaScript/TypeScript pour effectuer des appels de fonction compositionnels à l'aide d'une boucle d'exécution manuelle.

```
import { GoogleGenAI, Type } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Example Functions
function get_weather_forecast({ location }) {
  console.log(`Tool Call: get_weather_forecast(location=${location})`);
  // TODO: Make API call
  console.log("Tool Response: {'temperature': 25, 'unit': 'celsius'}");
  return { temperature: 25, unit: "celsius" };
}

function set_thermostat_temperature({ temperature }) {
  console.log(
    `Tool Call: set_thermostat_temperature(temperature=${temperature})`,
  );
  // TODO: Make API call
  console.log("Tool Response: {'status': 'success'}");
  return { status: "success" };
}

const toolFunctions = {
  get_weather_forecast,
  set_thermostat_temperature,
};

const tools = [
  {
    functionDeclarations: [
      {
        name: "get_weather_forecast",
        description:
          "Gets the current weather temperature for a given location.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            location: {
              type: Type.STRING,
            },
          },
          required: ["location"],
        },
      },
      {
        name: "set_thermostat_temperature",
        description: "Sets the thermostat to a desired temperature.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            temperature: {
              type: Type.NUMBER,
            },
          },
          required: ["temperature"],
        },
      },
    ],
  },
];

// Prompt for the model
let contents = [
  {
    role: "user",
    parts: [
      {
        text: "If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
      },
    ],
  },
];

// Loop until the model has no more function calls to make
while (true) {
  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents,
    config: { tools },
  });

  if (result.functionCalls && result.functionCalls.length > 0) {
    const functionCall = result.functionCalls[0];

    const { name, args } = functionCall;

    if (!toolFunctions[name]) {
      throw new Error(`Unknown function call: ${name}`);
    }

    // Call the function and get the response.
    const toolResponse = toolFunctions[name](args);

    const functionResponsePart = {
      name: functionCall.name,
      response: {
        result: toolResponse,
      },
    };

    // Send the function response back to the model.
    contents.push({
      role: "model",
      parts: [
        {
          functionCall: functionCall,
        },
      ],
    });
    contents.push({
      role: "user",
      parts: [
        {
          functionResponse: functionResponsePart,
        },
      ],
    });
  } else {
    // No more function calls, break the loop.
    console.log(result.text);
    break;
  }
}
```

**Résultat attendu**

Lorsque vous exécutez le code, le SDK orchestre les appels de fonction. Le modèle appelle d'abord `get_weather_forecast`, reçoit la température, puis appelle `set_thermostat_temperature` avec la valeur correcte en fonction de la logique de l'invite.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. It's 25°C in London, so I've set the thermostat to 20°C.
```

L'appel de fonction compositionnel est une fonctionnalité native de l'[API Live](https://ai.google.dev/gemini-api/docs/live?hl=fr). Cela signifie que l'API Live peut gérer l'appel de fonction de manière similaire au SDK Python.

### Python

```
# Light control schemas
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Light control schemas
const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };
const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };

const prompt = `
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
`;

const tools = [
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }
];

await run(prompt, tools=tools, modality="AUDIO")
```

## Modes d'appel de fonction

L'API Gemini vous permet de contrôler la façon dont le modèle utilise les outils fournis (déclarations de fonction). Plus précisément, vous pouvez définir le mode dans.`function_calling_config`.

*   `AUTO (Default)` : le modèle décide de générer une réponse en langage naturel ou de suggérer un appel de fonction en fonction de la requête et du contexte. Il s'agit du mode le plus flexible et recommandé pour la plupart des scénarios.
*   `ANY` : le modèle est contraint de toujours prédire un appel de fonction et garantit le respect du schéma de fonction. Si `allowed_function_names` n'est pas spécifié, le modèle peut choisir parmi les déclarations de fonction fournies. Si `allowed_function_names` est fourni sous forme de liste, le modèle ne peut choisir que parmi les fonctions de cette liste. Utilisez ce mode lorsque vous avez besoin d'une réponse d'appel de fonction pour chaque requête (le cas échéant).
*   `NONE` : le modèle est _interdit_ d'effectuer des appels de fonction. Cela équivaut à envoyer une requête sans aucune déclaration de fonction. Utilisez-le pour désactiver temporairement l'appel de fonction sans supprimer les définitions de vos outils.
    

### Python

```
from google.genai import types

# Configure function calling mode
tool_config = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="ANY", allowed_function_names=["get_current_temperature"]
    )
)

# Create the generation config
config = types.GenerateContentConfig(
    tools=[tools],  # not defined here.
    tool_config=tool_config,
)
```

### JavaScript

```
import { FunctionCallingConfigMode } from '@google/genai';

// Configure function calling mode
const toolConfig = {
  functionCallingConfig: {
    mode: FunctionCallingConfigMode.ANY,
    allowedFunctionNames: ['get_current_temperature']
  }
};

// Create the generation config
const config = {
  tools: tools, // not defined here.
  toolConfig: toolConfig,
};
```

## Appel de fonction automatique (Python uniquement)

Lorsque vous utilisez le SDK Python, vous pouvez fournir des fonctions Python directement en tant qu'outils. Le SDK convertit automatiquement la fonction Python en déclarations, gère l'exécution de l'appel de fonction et le cycle de réponse à votre place. Le SDK Python effectue ensuite automatiquement les opérations suivantes :

1.  Détecte les réponses d'appel de fonction du modèle.
2.  Appelez la fonction Python correspondante dans votre code.
3.  Renvoie la réponse de la fonction au modèle.
4.  Renvoie la réponse textuelle finale du modèle.

Pour utiliser cette fonctionnalité, définissez votre fonction avec des indications de type et une docstring, puis transmettez la fonction elle-même (et non une déclaration JSON) en tant qu'outil :

### Python

```
from google import genai
from google.genai import types

# Define the function with type hints and docstring
def get_current_temperature(location: str) -> dict:
    """Gets the current temperature for a given location.

    Args:
        location: The city and state, e.g. San Francisco

    Returns:
        A dictionary containing the temperature and unit.
    """
    # ... (implementation) ...
    return {"temperature": 25, "unit": "Celsius"}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_current_temperature]
)  # Pass the function itself

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What's the temperature in Boston?",
    config=config,
)

print(response.text)  # The SDK handles the function call and returns the final text
```

Vous pouvez désactiver l'appel automatique des fonctions avec :

### Python

```
config = types.GenerateContentConfig(
    tools=[get_current_temperature],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
)
```

### Déclaration automatique du schéma de fonction

L'extraction automatique de schémas à partir de fonctions Python ne fonctionne pas dans tous les cas. Par exemple, il ne gère pas les cas où vous décrivez les champs d'un objet de dictionnaire imbriqué. L'API peut décrire l'un des types suivants :

### Python

```
AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])
```

Pour voir à quoi ressemble le schéma inféré, vous pouvez le convertir à l'aide de [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable) :

### Python

```
def multiply(a: float, b: float):
    """Returns a * b."""
    return a * b

fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

# to_json_dict() provides a clean JSON representation.
print(fn_decl.to_json_dict())
```

## Utilisation de plusieurs outils : combinez des outils natifs avec l'appel de fonction

Vous pouvez activer plusieurs outils combinant des outils natifs avec l'appel de fonction en même temps. Voici un exemple qui active deux outils, l'[ancrage avec la recherche Google](https://ai.google.dev/gemini-api/docs/grounding?hl=fr) et l'[exécution de code](https://ai.google.dev/gemini-api/docs/code-execution?hl=fr), dans une requête à l'aide de l'[API Live](https://ai.google.dev/gemini-api/docs/live?hl=fr).

### Python

```
# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Multiple tasks example - combining lights, code execution, and search
const prompt = `
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
`;

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
];

// Execute the prompt with specified tools in audio modality
await run(prompt, {tools: tools, modality: "AUDIO"});
```

Les développeurs Python peuvent essayer cette fonctionnalité dans le [notebook "Utilisation de l'outil d'API en direct"](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).

## Protocole MCP (Model Context Protocol)

Le [protocole MCP (Model Context Protocol)](https://modelcontextprotocol.io/introduction) est une norme ouverte permettant de connecter des applications d'IA à des outils et des données externes. MCP fournit un protocole commun permettant aux modèles d'accéder au contexte, comme les fonctions (outils), les sources de données (ressources) ou les requêtes prédéfinies.

Les SDK Gemini sont compatibles avec le MCP, ce qui réduit le code récurrent et offre un [appel automatique des outils](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#automatic_function_calling_python_only) pour les outils MCP. Lorsque le modèle génère un appel d'outil MCP, le SDK client Python et JavaScript peut exécuter automatiquement l'outil MCP et renvoyer la réponse au modèle dans une requête ultérieure, poursuivant cette boucle jusqu'à ce que le modèle n'effectue plus d'appels d'outil.

Vous trouverez ici un exemple d'utilisation d'un serveur MCP local avec le SDK Gemini et `mcp`.

### Python

Assurez-vous que la dernière version du [SDK `mcp`](https://modelcontextprotocol.io/introduction) est installée sur la plate-forme de votre choix.

```
pip install mcp
```

```
import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool
                    # Uncomment if you **don't** want the SDK to automatically call the tool
                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
                    #     disable=True
                    # ),
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())
```

### JavaScript

Assurez-vous que la dernière version du SDK `mcp` est installée sur la plate-forme de votre choix.

```
npm install @modelcontextprotocol/sdk
```

```
import { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

// Create server parameters for stdio connection
const serverParams = new StdioClientTransport({
  command: "npx", // Executable
  args: ["-y", "@philschmid/weather-mcp"] // MCP Server
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  }
);

// Configure the client
const ai = new GoogleGenAI({});

// Initialize the connection between client and server
await client.connect(serverParams);

// Send request to the model with MCP tools
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,
  config: {
    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool
    // Uncomment if you **don't** want the sdk to automatically call the tool
    // automaticFunctionCalling: {
    //   disable: true,
    // },
  },
});
console.log(response.text)

// Close the connection
await client.close();
```

### Limites de la compatibilité intégrée avec MCP

La prise en charge intégrée du MCP est une fonctionnalité [expérimentale](https://ai.google.dev/gemini-api/docs/models?hl=fr#preview) dans nos SDK et présente les limites suivantes :

*   Seuls les outils sont acceptés, et non les ressources ni les requêtes.
*   Il est disponible pour les SDK Python et JavaScript/TypeScript.
*   Des modifications destructives peuvent être apportées dans les prochaines versions.

L'intégration manuelle des serveurs MCP est toujours une option si ceux-ci limitent ce que vous créez.

## Modèles compatibles

Cette section liste les modèles et leurs fonctionnalités d'appel de fonction. Les modèles expérimentaux ne sont pas inclus. Vous trouverez une présentation complète des fonctionnalités sur la page [Présentation du modèle](https://ai.google.dev/gemini-api/docs/models?hl=fr).

Modèle

Appel de fonction

Appel de fonction parallèle

Appel de fonction compositionnel

Gemini 2.5 Pro

✔️

✔️

✔️

Gemini 2.5 Flash

✔️

✔️

✔️

Gemini 2.5 Flash-Lite

✔️

✔️

✔️

Gemini 2.0 Flash

✔️

✔️

✔️

Gemini 2.0 Flash-Lite

X

X

X

## Bonnes pratiques

*   **Descriptions des fonctions et des paramètres** : soyez extrêmement clair et précis dans vos descriptions. Le modèle s'appuie sur ces éléments pour choisir la fonction appropriée et fournir les arguments appropriés.
*   **Noms** : utilisez des noms de fonction descriptifs (sans espaces, points ni tirets).
*   **Typage strict** : utilisez des types spécifiques (entier, chaîne, énumération) pour les paramètres afin de réduire les erreurs. Si un paramètre possède un ensemble limité de valeurs valides, utilisez un énumérateur.
*   **Sélection d'outils** : bien que le modèle puisse utiliser un nombre arbitraire d'outils, en fournir trop peut augmenter le risque de sélectionner un outil incorrect ou non optimal. Pour de meilleurs résultats, essayez de ne fournir que les outils pertinents pour le contexte ou la tâche, en limitant idéalement l'ensemble actif à 10 à 20 éléments. Envisagez de sélectionner des outils de manière dynamique en fonction du contexte de la conversation si vous disposez d'un grand nombre d'outils.
*   **Prompt engineering** (ingénierie des requêtes) :
    *   Fournissez du contexte : indiquez au modèle son rôle (par exemple, "Vous êtes un assistant météo serviable.")
    *   Donnez des instructions : indiquez comment et quand utiliser les fonctions (par exemple, "N'essayez pas de deviner les dates. Utilisez toujours une date future pour les prévisions.").
    *   Encouragez les clarifications : demandez au modèle de poser des questions de clarification si nécessaire.
*   **Température** : utilisez une température basse (par exemple, 0) pour des appels de fonction plus déterministes et fiables.
*   **Validation** : si un appel de fonction a des conséquences importantes (par exemple, passer une commande), validez l'appel auprès de l'utilisateur avant de l'exécuter.
*   **Gestion des erreurs** : implémentez une gestion des erreurs robuste dans vos fonctions pour gérer correctement les entrées inattendues ou les échecs d'API. Renvoyez des messages d'erreur informatifs que le modèle peut utiliser pour générer des réponses utiles à l'utilisateur.
*   **Sécurité** : soyez attentif à la sécurité lorsque vous appelez des API externes. Utilisez des mécanismes d'authentification et d'autorisation appropriés. Évitez d'exposer des données sensibles dans les appels de fonction.
*   **Limites de jetons** : les descriptions et les paramètres de fonction sont comptabilisés dans la limite de jetons d'entrée. Si vous atteignez les limites de jetons, envisagez de limiter le nombre de fonctions ou la longueur des descriptions, et de décomposer les tâches complexes en ensembles de fonctions plus petits et plus ciblés.

## Remarques et limites

*   Seul un [sous-ensemble du schéma OpenAPI](https://ai.google.dev/api/caching?hl=fr#FunctionDeclaration) est accepté.
*   Les types de paramètres compatibles dans Python sont limités.
*   L'appel de fonction automatique est une fonctionnalité du SDK Python uniquement.

Envoyer des commentaires

Sauf indication contraire, le contenu de cette page est régi par une licence [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/), et les échantillons de code sont régis par une licence [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Pour en savoir plus, consultez les [Règles du site Google Developers](https://developers.google.com/site-policies?hl=fr). Java est une marque déposée d'Oracle et/ou de ses sociétés affiliées.

Dernière mise à jour le 2025/06/27 (UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/audio

আমাদের 2.5 ফ্যামিলি অফ থিংকিং মডেলের আপডেট পেশ করা হচ্ছে। [আরও জানুন](https://ai.google.dev/gemini-api/docs/models?hl=bn)

![](https://ai.google.dev/_static/images/translated.svg?hl=bn) এই পৃষ্ঠাটি [Cloud Translation API](//cloud.google.com/translate/?hl=bn) অনুবাদ করেছে।

*   [হোম](https://ai.google.dev/?hl=bn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=bn)
*   [মডেল](https://ai.google.dev/gemini-api/docs?hl=bn)

মতামত জানান

# অডিও বোঝার

মিথুন অডিও ইনপুট বিশ্লেষণ এবং বুঝতে পারে, নিম্নলিখিতগুলির মতো ব্যবহারের ক্ষেত্রে সক্ষম করে:

*   বর্ণনা করুন, সংক্ষিপ্ত করুন বা অডিও বিষয়বস্তু সম্পর্কে প্রশ্নের উত্তর দিন।
*   অডিও একটি প্রতিলিপি প্রদান.
*   অডিওর নির্দিষ্ট অংশ বিশ্লেষণ করুন।

এই নির্দেশিকা আপনাকে দেখায় কিভাবে অডিও ইনপুটে একটি পাঠ্য প্রতিক্রিয়া তৈরি করতে Gemini API ব্যবহার করতে হয়।

### আপনি শুরু করার আগে

Gemini API কল করার আগে, নিশ্চিত করুন যে আপনি [আপনার পছন্দের SDK](https://ai.google.dev/gemini-api/docs/downloads?hl=bn) ইনস্টল করেছেন, এবং একটি [Gemini API কী](https://ai.google.dev/gemini-api/docs/api-key?hl=bn) কনফিগার করা এবং ব্যবহারের জন্য প্রস্তুত।

## ইনপুট অডিও

আপনি নিম্নলিখিত উপায়ে মিথুনকে অডিও ডেটা সরবরাহ করতে পারেন:

*   `generateContent` করার অনুরোধ করার আগে [একটি অডিও ফাইল আপলোড করুন](#upload-audio) ।
*   `generateContent` অনুরোধের সাথে [ইনলাইন অডিও ডেটা পাস করুন](#inline-audio) ।

### একটি অডিও ফাইল আপলোড করুন

আপনি একটি অডিও ফাইল আপলোড করতে [Files API](https://ai.google.dev/gemini-api/docs/files?hl=bn) ব্যবহার করতে পারেন। সর্বদা ফাইল API ব্যবহার করুন যখন মোট অনুরোধের আকার (ফাইল, টেক্সট প্রম্পট, সিস্টেম নির্দেশাবলী, ইত্যাদি সহ) 20 MB এর থেকে বড় হয়৷

নিম্নোক্ত কোডটি একটি অডিও ফাইল আপলোড করে এবং তারপরে `generateContent` করতে কলে ফাইলটি ব্যবহার করে।

### পাইথন

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

### জাভাস্ক্রিপ্ট

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mp3" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

### যাও

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### বিশ্রাম

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

মিডিয়া ফাইলগুলির সাথে কাজ করার বিষয়ে আরও জানতে, [ফাইল API](https://ai.google.dev/gemini-api/docs/files?hl=bn) দেখুন।

### অডিও ডেটা ইনলাইনে পাস করুন

একটি অডিও ফাইল আপলোড করার পরিবর্তে, আপনি `generateContent` অনুরোধে ইনলাইন অডিও ডেটা পাস করতে পারেন:

### পাইথন

```
from google.genai import types

with open('path/to/small-sample.mp3', 'rb') as f:
    audio_bytes = f.read()

response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[
    'Describe this audio clip',
    types.Part.from_bytes(
      data=audio_bytes,
      mime_type='audio/mp3',
    )
  ]
)

print(response.text)
```

### জাভাস্ক্রিপ্ট

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64AudioFile = fs.readFileSync("path/to/small-sample.mp3", {
  encoding: "base64",
});

const contents = [
  { text: "Please summarize the audio." },
  {
    inlineData: {
      mimeType: "audio/mp3",
      data: base64AudioFile,
    },
  },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

### যাও

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  audioBytes, _ := os.ReadFile("/path/to/small-sample.mp3")

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "audio/mp3",
        Data:     audioBytes,
      },
    },
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

ইনলাইন অডিও ডেটা সম্পর্কে কিছু জিনিস মনে রাখতে হবে:

*   সর্বাধিক অনুরোধের আকার হল 20 এমবি, যার মধ্যে পাঠ্য প্রম্পট, সিস্টেম নির্দেশাবলী এবং ইনলাইনে দেওয়া ফাইলগুলি অন্তর্ভুক্ত রয়েছে। যদি আপনার ফাইলের আকার _মোট অনুরোধের আকার_ 20 MB ছাড়িয়ে যায়, তাহলে অনুরোধে ব্যবহারের জন্য [একটি অডিও ফাইল আপলোড](#upload-audio) করতে Files API ব্যবহার করুন।
*   আপনি যদি একাধিকবার একটি অডিও নমুনা ব্যবহার করেন, তাহলে [একটি অডিও ফাইল আপলোড করা](#upload-audio) আরও কার্যকর।

## একটি প্রতিলিপি পান

অডিও ডেটার একটি প্রতিলিপি পেতে, শুধুমাত্র প্রম্পটে এটির জন্য জিজ্ঞাসা করুন:

### পাইথন

```
myfile = client.files.upload(file='path/to/sample.mp3')
prompt = 'Generate a transcript of the speech.'

response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[prompt, myfile]
)

print(response.text)
```

### জাভাস্ক্রিপ্ট

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const result = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
    "Generate a transcript of the speech.",
  ]),
});
console.log("result.text=", result.text);
```

### যাও

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Generate a transcript of the speech."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

## টাইমস্ট্যাম্প পড়ুন

আপনি `MM:SS` ফর্মের টাইমস্ট্যাম্প ব্যবহার করে একটি অডিও ফাইলের নির্দিষ্ট বিভাগগুলি উল্লেখ করতে পারেন। উদাহরণস্বরূপ, নিম্নলিখিত প্রম্পট একটি প্রতিলিপি অনুরোধ করে যে

*   ফাইলের শুরু থেকে 2 মিনিট 30 সেকেন্ডে শুরু হয়।
*   ফাইলের শুরু থেকে 3 মিনিট 29 সেকেন্ডে শেষ হয়।
    

### পাইথন

```
# Create a prompt containing timestamps.
prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

### জাভাস্ক্রিপ্ট

```
// Create a prompt containing timestamps.
const prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

### যাও

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Provide a transcript of the speech " +
                            "between the timestamps 02:30 and 03:29."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

## টোকেন গণনা করুন

একটি অডিও ফাইলে টোকেনের সংখ্যা গণনা পেতে `countTokens` পদ্ধতিতে কল করুন। যেমন:

### পাইথন

```
response = client.models.count_tokens(
  model='gemini-2.5-flash',
  contents=[myfile]
)

print(response)
```

### জাভাস্ক্রিপ্ট

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
  ]),
});
console.log(countTokensResponse.totalTokens);
```

### যাও

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  tokens, _ := client.Models.CountTokens(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Printf("File %s is %d tokens\n", localAudioPath, tokens.TotalTokens)
}
```

## সমর্থিত অডিও ফরম্যাট

জেমিনি নিম্নলিখিত অডিও ফর্ম্যাট MIME প্রকারগুলিকে সমর্থন করে:

*   WAV - `audio/wav`
*   MP3 - `audio/mp3`
*   AIFF - `audio/aiff`
*   AAC - `audio/aac`
*   OGG Vorbis - `audio/ogg`
*   FLAC - `audio/flac`

## অডিও সম্পর্কে প্রযুক্তিগত বিবরণ

*   মিথুন অডিওর প্রতিটি সেকেন্ড 32 টোকেন হিসাবে উপস্থাপন করে; উদাহরণস্বরূপ, এক মিনিটের অডিওকে 1,920 টোকেন হিসাবে উপস্থাপন করা হয়।
*   মিথুন অ-বক্তৃতা উপাদান "বুঝতে" পারে, যেমন পাখির গান বা সাইরেন।
*   একক প্রম্পটে অডিও ডেটার সর্বাধিক সমর্থিত দৈর্ঘ্য হল 9.5 ঘন্টা। মিথুন একটি একক প্রম্পটে অডিও ফাইলের _সংখ্যা_ সীমাবদ্ধ করে না; যাইহোক, একটি একক প্রম্পটে সমস্ত অডিও ফাইলের মোট সম্মিলিত দৈর্ঘ্য 9.5 ঘন্টার বেশি হতে পারে না।
*   Gemini অডিও ফাইলগুলিকে 16 Kbps ডেটা রেজোলিউশনে নমুনা দেয়।
*   অডিও উৎসে একাধিক চ্যানেল থাকলে, মিথুন সেই চ্যানেলগুলিকে একক চ্যানেলে একত্রিত করে।

## এরপর কি

এই নির্দেশিকাটি দেখায় কিভাবে অডিও ডেটার প্রতিক্রিয়ায় পাঠ্য তৈরি করতে হয়। আরও জানতে, নিম্নলিখিত সংস্থানগুলি দেখুন:

*   [ফাইল প্রম্পটিং কৌশল](https://ai.google.dev/gemini-api/docs/files?hl=bn#prompt-guide) : জেমিনি এপিআই টেক্সট, ইমেজ, অডিও এবং ভিডিও ডেটা সহ প্রম্পটিং সমর্থন করে, যা মাল্টিমডাল প্রম্পটিং নামেও পরিচিত।
*   [সিস্টেম নির্দেশাবলী](https://ai.google.dev/gemini-api/docs/text-generation?hl=bn#system-instructions) : সিস্টেম নির্দেশাবলী আপনাকে আপনার নির্দিষ্ট প্রয়োজন এবং ব্যবহারের ক্ষেত্রের উপর ভিত্তি করে মডেলের আচরণ পরিচালনা করতে দেয়।
*   [নিরাপত্তা নির্দেশিকা](https://ai.google.dev/gemini-api/docs/safety-guidance?hl=bn) : কখনও কখনও জেনারেটিভ এআই মডেলগুলি অপ্রত্যাশিত আউটপুট তৈরি করে, যেমন আউটপুটগুলি ভুল, পক্ষপাতদুষ্ট বা আপত্তিকর। এই ধরনের আউটপুট থেকে ক্ষতির ঝুঁকি সীমিত করার জন্য পোস্ট-প্রসেসিং এবং মানব মূল্যায়ন অপরিহার্য।

মতামত জানান

অন্য কিছু উল্লেখ না করা থাকলে, এই পৃষ্ঠার কন্টেন্ট [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/)\-এর অধীনে এবং কোডের নমুনাগুলি [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0)\-এর অধীনে লাইসেন্স প্রাপ্ত। আরও জানতে, [Google Developers সাইট নীতি](https://developers.google.com/site-policies?hl=bn) দেখুন। Java হল Oracle এবং/অথবা তার অ্যাফিলিয়েট সংস্থার রেজিস্টার্ড ট্রেডমার্ক।

2025-06-27 UTC-তে শেষবার আপডেট করা হয়েছে।

---

# Content from: https://ai.google.dev/gemini-api/docs/prompting-strategies

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Prompt design strategies

_Prompt design_ is the process of creating prompts, or natural language requests, that elicit accurate, high quality responses from a language model.

This page introduces basic concepts, strategies, and best practices to get you started designing prompts to get the most out of Gemini AI models.

## Topic-specific prompt guides

Looking for more specific prompt strategies? Check out our other prompting guides on:

*   [Prompting with media files](/gemini-api/docs/files#prompt-guide)
*   [Prompting for image generation](/gemini-api/docs/image-generation#imagen-prompt-guide)
*   [Prompting for video generation](/gemini-api/docs/video#prompt-guide)

Google AI Studio also hosts a [prompt gallery](https://aistudio.google.com/app/gallery) meant to interactively showcase many of the concepts shared in this guide.

## Clear and specific instructions

An effective and efficient way to customize model behavior is to provide it with clear and specific instructions. Instructions can be in the form of a question, step-by-step tasks, or as complex as mapping out a user's experience and mindset.

### Input

Input is the required text in the prompt that you want the model to provide a response to. Inputs can be a question that the model answers (question input), a task the model performs (task input), an entity the model operates on (entity input), or partial input that the model completes or continues (completion input).

**Input type**

**Prompt**

**Generated output**

Question

    ```

    What's a good name for a flower shop that specializes in selling bouquets of
    dried flowers? Create a list of 5 options with just the names.
```

    ```

    Here are 10 names for a flower shop specializing in dried flowers:

    1. Everlasting Blooms
    2. Dried & Delightful
    3. The Petal Preserve
    4. Whispers of Wildflowers
    5. Forever Blooms
    
```
    

Task

    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.

    1. \*\*Tent:\*\* Your shelter from the elements.
    2. \*\*Sleeping Bag:\*\* Stay warm and comfortable.
    3. \*\*Sleeping Pad:\*\* Insulation and cushioning for a good night's sleep.
    4. \*\*Headlamp or Flashlight:\*\* Light for nighttime activities.
    5. \*\*First Aid Kit:\*\* Be prepared for minor injuries.

Entity

    ```

    Classify the following items as [large, small]:
Elephant
Mouse
Snail
```

```

Here's the classification:

* **Elephant:** Large
* **Mouse:** Small
* **Snail:** Small
    
```
    

#### Partial input completion

Generative language models work like an advanced auto completion tool. When you provide partial content, the model can provide the rest of the content or what it thinks is a continuation of that content as a response. When doing so, if you include any examples or context, the model can take those examples or context into account.

The following example provides a prompt with an instruction and an entity input:

**Prompt:**

For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or
drink, with the value being the quantity.

Order: A burger and a drink.
  **Response:**

{
  "cheeseburger": 0,
  "hamburger": 1,
  "fries": 0,
  "drink": 1
}
  (gemini-2.5-flash)

While the model did as prompted, writing out the instructions in natural language can sometimes be challenging and it leaves a lot to the model's interpretation. For example, a restaurants menu might contain many items. To reduce the size of the JSON response, you probably want to omit the items that weren't ordered. In this case, you can give an example and a [response prefix](#prefixes) and let the model complete it:

**Prompt:**

Valid fields are cheeseburger, hamburger, fries, and drink.
Order: Give me a cheeseburger and fries
Output:
\`\`\`
{
  "cheeseburger": 1,
  "fries": 1
}
\`\`\`
Order: I want two burgers, a drink, and fries.
Output:
  **Response:**

\`\`\`
{
  "hamburger": 2,
  "drink": 1,
  "fries": 1
}
\`\`\`
  (gemini-2.5-flash)

Notice how "cheeseburger" was excluded from the output because it wasn't a part of the order.

### Constraints

Specify any constraints on reading the prompt or generating a response. You can tell the model what to do and not to do. For example, you can specify a constraint in the prompt on how long you want a summary to be:

**Prompt:**

**Summarize this text in one sentence:**
Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially
faster than any modern traditional computer. At very tiny scales, physical matter acts as both
particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.
The operating principles of quantum devices is beyond the scope of classical physics. When deployed
at scale, quantum computers could be used in a wide variety of applications such as: in
cybersecurity to break existing encryption methods while helping researchers create new ones, in
meteorology to develop better weather forecasting etc. However, the current state of the art quantum
computers are still largely experimental and impractical.
    

**Response:**

      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.
    (gemini-2.5-flash)

### Response format

You can give instructions that specify the format of the response. For example, you can ask for the response to be formatted as a table, bulleted list, elevator pitch, keywords, sentence, or paragraph. The following system instruction tells the model to be more conversational in its response:

**System instruction**

    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.
  **Prompt**

    What is a smart way to make a business that sells DVD's in 2025?
  **Response:**

Starting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a "smart" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.

Here's a breakdown of what a smart DVD business in 2025 would likely entail:

1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:

    \* Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.
    ....
  (gemini-2.5-flash)

#### Format responses with the completion strategy

The [completion strategy](#completion) can also help format the response. The following example prompts the model to create an essay outline:

**Prompt:**

Create an outline for an essay about hummingbirds.
  **Response:**

Here is a possible outline for an essay about hummingbirds:

I. Introduction
A. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
B. Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds.
C. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.

II. Appearance and Identification
.....
  (gemini-2.5-flash)

The prompt didn't specify the format for the outline and the model chose a format for you. To get the model to return an outline in a specific format, you can add text that represents the start of the outline and let the model complete it based on the pattern that you initiated.

**Prompt:**

Create an outline for an essay about hummingbirds.
I. Introduction
   \*
  **Response:**

Okay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:

I. Introduction
\* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
\* Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds, and their general characteristics.
\* Thesis Statement: State the main point of the essay, focusing on the remarkable adaptations, behaviors, and ecological significance that make hummingbirds extraordinary.
.....
  (gemini-2.5-flash)

## Zero-shot vs few-shot prompts

You can include examples in the prompt that show the model what getting it right looks like. The model attempts to identify patterns and relationships from the examples and applies them when generating a response. Prompts that contain a few examples are called _few-shot_ prompts, while prompts that provide no examples are called _zero-shot_ prompts. Few-shot prompts are often used to regulate the formatting, phrasing, scoping, or general patterning of model responses. Use specific and varied examples to help the model narrow its focus and generate more accurate results.

We recommend to always include few-shot examples in your prompts. Prompts without few-shot examples are likely to be less effective. In fact, you can remove instructions from your prompt if your examples are clear enough in showing the task at hand.

The following zero-shot prompt asks the model to choose the best explanation.

**Prompt:**

Please choose the best explanation to the question:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Response:**

Explanation1 is the better explanation because it provides more detail on the
process, including how ice crystals combine and grow into snowflakes as they
fall through the atmosphere.
  (gemini-2.5-flash)

If your use case requires the model to produce concise responses, you can include examples in the prompt that give preference to concise responses.

The following prompt provides two examples that show preference to the shorter explanations. In the response, you can see that the examples guided the model to choose the shorter explanation (`Explanation2`) as opposed to the longer explanation (`Explanation1`) like it did previously.

**Prompt:**

Below are some examples showing a question, explanation, and answer format:

Question: Why is the sky blue?
Explanation1: The sky appears blue because of Rayleigh scattering, which causes
shorter blue wavelengths of light to be scattered more easily than longer red
wavelengths, making the sky look blue.
Explanation2: Due to Rayleigh scattering effect.
Answer: Explanation2

Question: What is the cause of earthquakes?
Explanation1: Sudden release of energy in the Earth's crust.
Explanation2: Earthquakes happen when tectonic plates suddenly slip or break
apart, causing a release of energy that creates seismic waves that can shake the
ground and cause damage.
Answer: Explanation1

Now, Answer the following question given the example formats above:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Response:**

Answer: Explanation2
  (gemini-2.5-flash)

### Optimal number of examples

Models like Gemini can often pick up on patterns using a few examples, though you may need to experiment with the number of examples to provide in the prompt for the best results. At the same time, if you include too many examples, the model may start to [overfit](https://developers.google.com/machine-learning/glossary#overfitting) the response to the examples.

### Patterns vs anti patterns

Using examples to show the model a pattern to follow is more effective than using examples to show the model an anti pattern to avoid.

⛔ **Negative pattern:**

**Example**

Don't end haikus with a question:
Haiku are fun
A short and simple poem
Don't you enjoy them?
    

✅ **Positive pattern:**

**Example**

Always end haikus with an assertion:
Haiku are fun
A short and simple poem
A joy to write
    

### Consistent formatting

Make sure that the structure and formatting of few-shot examples are the same to avoid responses with undesired formats. One of the primary objectives of adding few-shot examples in prompts is to show the model the response format. Therefore, it is essential to ensure a consistent format across all examples, especially paying attention to XML tags, white spaces, newlines, and example splitters.

## Add context

You can include instructions and information in a prompt that the model needs to solve a problem, instead of assuming that the model has all of the required information. This contextual information helps the model understand the constraints and details of what you're asking for it to do.

The following example asks the model to give troubleshooting guidance for a router:

**Prompt:**

What should I do to fix my disconnected wifi? **The light on my Google
Wifi router is yellow and blinking slowly.**
  **Response:**

  A yellow blinking light on your Google Wifi router usually indicates a problem
  with the internet connection. Here are some steps to troubleshoot and fix the
  issue:
  1. Restart your router and modem:
    - Unplug your modem from the power outlet and wait for 30 seconds.
    - Plug your modem back in and wait for it to fully boot up.
    - Unplug your Google Wifi router and wait for 30 seconds.
    - Plug your Google Wifi router back in and wait for it to fully boot up.
.....
  (gemini-2.5-flash)

The response looks like generic troubleshooting information that's not specific to the router or the status of the LED indicator lights.

To customize the response for the specific router, you can add to the prompt the router's troubleshooting guide as context for it to refer to when providing a response.

**Prompt:**

Answer the question using the text below. Respond with only the text provided.
Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.

Text:
Color: Slowly pulsing yellow
What it means: There is a network error.
What to do:
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.

Color: Fast blinking yellow
What it means: You are holding down the reset button and are factory resetting this device.
What to do:
If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.

Color: Solid yellow
What it means: Router is factory resetting.
What to do:
This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.

Color: Solid red
What it means: Something is wrong.
What to do:
Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.
  **Response:**

Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.
  (gemini-2.5-flash)

## Add prefixes

A prefix is a word or phrase that you add to the prompt content that can serve several purposes, depending on where you put the prefix:

*   **Input prefix:** Adding a prefix to the input signals semantically meaningful parts of the input to the model. For example, the prefixes "English:" and "French:" demarcate two different languages.
*   **Output prefix:** Even though the output is generated by the model, you can add a prefix for the output in the prompt. The output prefix gives the model information about what's expected as a response. For example, the output prefix "JSON:" signals to the model that the output should be in JSON format.
*   **Example prefix:** In few-shot prompts, adding prefixes to the examples provides labels that the model can use when generating the output, which makes it easier to parse output content.

In the following example, "Text:" is the input prefix and "The answer is:" is the output prefix.

**Prompt:**

Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail
The answer is: small
Text: Elephant
The answer is:
  **Response:**

The answer is: large
  (gemini-2.5-flash)

## Break down prompts into components

For use cases that require complex prompts, you can help the model manage this complexity by breaking things down into simpler components.

1.  **Break down instructions:** Instead of having many instructions in one prompt, create one prompt per instruction. You can choose which prompt to process based on the user's input.
    
2.  **Chain prompts:** For complex tasks that involve multiple sequential steps, make each step a prompt and chain the prompts together in a sequence. In this sequential chain of prompts, the output of one prompt in the sequence becomes the input of the next prompt. The output of the last prompt in the sequence is the final output.
    
3.  **Aggregate responses:** Aggregation is when you want to perform different parallel tasks on different portions of the data and aggregate the results to produce the final output. For example, you can tell the model to perform one operation on the first part of the data, perform another operation on the rest of the data and aggregate the results.
    

## Experiment with model parameters

Each call that you send to a model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. Experiment with different parameter values to get the best values for the task. The parameters available for different models may differ. The most common parameters are the following:

1.  **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.
    
2.  **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.
    
3.  **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.
    
4.  **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.
    
5.  **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.
    

## Prompt iteration strategies

Prompt design can sometimes require a few iterations before you consistently get the response you're looking for. This section provides guidance on some things you can try when iterating on your prompts:

1.  **Use different phrasing:** Using different words or phrasing in your prompts often yields different responses from the model even though they all mean the same thing. If you're not getting the expected results from your prompt, try rephrasing it.
    
    Version 1:
    How do I bake a pie?
    
    Version 2:
    Suggest a recipe for a pie.
    
    Version 3:
    What's a good pie recipe?
        
    
2.  **Switch to an analogous task:** If you can't get the model to follow your instructions for a task, try giving it instructions for an analogous task that achieves the same result.
    
    This prompt tells the model to categorize a book by using predefined categories:
    
    **Prompt:**
    
    Which category does The Odyssey belong to:
    thriller
    sci-fi
    mythology
    biography
      **Response:**
    
        The Odyssey belongs to the category of \*\*mythology\*\*. 
    
        Here's why:
    
        \* \*\*Mythology:\*\* The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his
        journey home after the Trojan War. It features gods, monsters, and supernatural events common to
        Greek mythology.
    .....
      (gemini-2.5-flash)
    
    The response is correct, but the model didn't stay within the bounds of the options. You also want to model to just respond with one of the options instead of in a full sentence. In this case, you can rephrase the instructions as a multiple choice question and ask the model to choose an option.
    
    **Prompt:**
    
    Multiple choice problem: Which of the following options describes the book The Odyssey?
    Options:
    
    *   thriller
    *   sci-fi
    *   mythology
    *   biography
    **Response:**
    
    The correct answer is **mythology**. 
    (gemini-2.5-flash)
    
3.  **Change the order of prompt content:** The order of the content in the prompt can sometimes affect the response. Try changing the content order and see how that affects the response.
    
    ```
    Version 1:
    [examples]
    [context]
    [input]
    
    Version 2:
    [input]
    [examples]
    [context]
    
    Version 3:
    [examples]
    [input]
    [context]
    ```
    

## Fallback responses

A fallback response is a response returned by the model when either the prompt or the response triggers a safety filter. An example of a fallback response is "I'm not able to help with that, as I'm only a language model."

If the model responds with a fallback response, try increasing the temperature.

## Things to avoid

*   Avoid relying on models to generate factual information.
*   Use with care on math and logic problems.

## Generative models under the hood

This section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**

The short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text "The dog jumped over the ... ", the generative model will produce an array of probable next words:

```
[("fence", 0.77), ("ledge", 0.12), ("blanket", 0.03), ...]
```

This process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.

In the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.

## Next steps

*   Now that you have a deeper understanding of prompt design, try writing your own prompts using [Google AI Studio](http://aistudio.google.com).
*   To learn about multimodal prompting, see [Prompting with media files](/gemini-api/docs/files#prompt-guide).
*   To learn about image prompting, see the [Imagen prompt guide](/gemini-api/docs/image-generation#imagen-prompt-guide)
*   To learn about video prompting, see the [Veo prompt guide](/gemini-api/docs/video#prompt-guide)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-04-28 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/google-search

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# 使用 Google 搜索建立依据

“使用 Google 搜索建立依据”功能可将 Gemini 模型与实时网络内容相关联，并且适用于所有[可用语言](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#available-languages)。这样一来，Gemini 就可以提供更准确的回答，并引用其知识断点之外的可验证来源。

扎实的基础有助于您构建能够：

*   **提高事实准确性**：通过以真实信息为依据生成回答，减少模型产生幻觉的情况。
*   **获取实时信息**：回答有关近期活动和主题的问题。
*   **提供引用**：通过显示模型断言的来源，赢得用户信任。
    

### Python

```
from google import genai
from google.genai import types

# Configure the client
client = genai.Client()

# Define the grounding tool
grounding_tool = types.Tool(
    google_search=types.GoogleSearch()
)

# Configure generation settings
config = types.GenerateContentConfig(
    tools=[grounding_tool]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Who won the euro 2024?",
    config=config,
)

# Print the grounded response
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Define the grounding tool
const groundingTool = {
  googleSearch: {},
};

// Configure generation settings
const config = {
  tools: [groundingTool],
};

// Make the request
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

// Print the grounded response
console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {"text": "Who won the euro 2024?"}
        ]
      }
    ],
    "tools": [
      {
        "google_search": {}
      }
    ]
  }'
```

如需了解详情，请试用[“搜索工具”笔记本](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb?hl=zh-cn)。

## 依托 Google 搜索进行接地的运作方式

启用 `google_search` 工具后，该模型会自动处理搜索、处理和引用信息的整个工作流。

![grounding-overview](https://ai.google.dev/static/gemini-api/docs/images/google-search-tool-overview.png?hl=zh-cn)

1.  **用户提示**：您的应用会向启用了 `google_search` 工具的 Gemini API 发送用户的提示。
2.  **提示分析**：模型会分析提示，并确定是否可以通过 Google 搜索来改进回答。
3.  **Google 搜索**：如果需要，模型会自动生成一个或多个搜索查询并执行这些查询。
4.  **搜索结果处理**：模型会处理搜索结果、综合信息并形成回答。
5.  **基于搜索结果的回答**：该 API 会返回基于搜索结果的最终用户友好型回答。此响应包含模型的文本回答和包含搜索查询、网页搜索结果和引文的 `groundingMetadata`。

## 了解接地响应

当响应成功建立基准后，响应中将包含 `groundingMetadata` 字段。这些结构化数据对于验证声明和在应用中打造丰富的引文体验至关重要。

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}
```

Gemini API 会返回以下信息以及 `groundingMetadata`：

*   `webSearchQueries`：所用搜索查询的数组。这对调试和了解模型的推理过程很有用。
*   `searchEntryPoint`：包含用于呈现所需搜索建议的 HTML 和 CSS。如需详细了解使用要求，请参阅[服务条款](https://ai.google.dev/gemini-api/terms?hl=zh-cn#grounding-with-google-search)。
*   `groundingChunks`：包含网站来源（`uri` 和 `title`）的对象数组。
*   `groundingSupports`：用于将模型响应 `text` 连接到 `groundingChunks` 中的来源的块数组。每个分块都会将文本 `segment`（由 `startIndex` 和 `endIndex` 定义）与一个或多个 `groundingChunkIndices` 相关联。这是构建内嵌引文的关键。

您还可以将“使用 Google 搜索作为回答依据”与[网址情境工具](https://ai.google.dev/gemini-api/docs/url-context?hl=zh-cn)结合使用，以便在公开 Web 数据和您提供的特定网址中为回答提供依据。

## 使用内嵌引用归因来源

该 API 会返回结构化引文数据，让您可以完全控制在界面中显示来源的方式。您可以使用 `groundingSupports` 和 `groundingChunks` 字段将模型的语句直接与其来源相关联。下面是一个常见的模式，用于处理元数据以创建包含内嵌可点击引文的回答。

### Python

```
def add_citations(response):
    text = response.text
    supports = response.candidates[0].grounding_metadata.grounding_supports
    chunks = response.candidates[0].grounding_metadata.grounding_chunks

    # Sort supports by end_index in descending order to avoid shifting issues when inserting.
    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

    for support in sorted_supports:
        end_index = support.segment.end_index
        if support.grounding_chunk_indices:
            # Create citation string like [1](link1)[2](link2)
            citation_links = []
            for i in support.grounding_chunk_indices:
                if i < len(chunks):
                    uri = chunks[i].web.uri
                    citation_links.append(f"[{i + 1}]({uri})")

            citation_string = ", ".join(citation_links)
            text = text[:end_index] + citation_string + text[end_index:]

    return text

# Assuming response with grounding metadata
text_with_citations = add_citations(response)
print(text_with_citations)
```

### JavaScript

```
function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);
```

添加了内嵌引文的新回答将如下所示：

```
Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)
```

## 价格

使用“依托 Google 搜索进行接地”时，系统会按包含 `google_search` 工具的 API 请求对您的项目进行结算。如果模型决定执行多个搜索查询来回答单个问题（例如，在同一 API 调用中搜索 `"UEFA Euro 2024 winner"` 和 `"Spain vs England Euro 2024 final score"`），则这会计为对该请求使用该工具的一次计费用途。

如需详细了解价格信息，请参阅 [Gemini API 价格页面](https://ai.google.dev/gemini-api/docs/pricing?hl=zh-cn)。

## 支持的模型

不包括实验性模型和预览版模型。您可以在[模型概览](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)页面上找到这些功能。

型号

使用 Google 搜索建立依据

Gemini 2.5 Pro

✔️

Gemini 2.5 Flash

✔️

Gemini 2.0 Flash

✔️

Gemini 1.5 Pro

✔️

Gemini 1.5 Flash

✔️

## 使用 Gemini 1.5 模型进行接地（旧版）

虽然建议为 Gemini 2.0 及更高版本使用 `google_search` 工具，但 Gemini 1.5 支持名为 `google_search_retrieval` 的旧版工具。此工具提供了 `dynamic` 模式，可让模型根据其对提示需要新信息的信心来决定是否执行搜索。如果模型的置信度高于您设置的 `dynamic_threshold`（介于 0.0 和 1.0 之间的值），则会执行搜索。

### Python

```
# Note: This is a legacy approach for Gemini 1.5 models.
# The 'google_search' tool is recommended for all new development.
import os
from google import genai
from google.genai import types

client = genai.Client()

retrieval_tool = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            dynamic_threshold=0.7 # Only search if confidence > 70%
        )
    )
)

config = types.GenerateContentConfig(
    tools=[retrieval_tool]
)

response = client.models.generate_content(
    model='gemini-1.5-flash',
    contents="Who won the euro 2024?",
    config=config,
)
print(response.text)
if not response.candidates[0].grounding_metadata:
  print("\nModel answered from its own knowledge.")
```

### JavaScript

```
// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \

  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {"parts": [{"text": "Who won the euro 2024?"}]}
    ],
    "tools": [{
      "google_search_retrieval": {
        "dynamic_retrieval_config": {
          "mode": "MODE_DYNAMIC",
          "dynamic_threshold": 0.7
        }
      }
    }]
  }'
```

## 后续步骤

*   试用 [Gemini API 实战宝典中的“使用 Google 搜索建立依据”](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb?hl=zh-cn)。
*   了解其他可用工具，例如[函数调用](https://ai.google.dev/gemini-api/docs/function-calling?hl=zh-cn)。
*   了解如何使用[网址上下文工具](https://ai.google.dev/gemini-api/docs/url-context?hl=zh-cn)为问题添加特定网址。

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-06-27。

---

# Content from: https://ai.google.dev/gemini-api/docs/url-context

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# URL context

בעזרת הכלי להוספת הקשר של כתובות URL, אפשר לספק ל-Gemini כתובות URL כרקע נוסף להנחיה. לאחר מכן, המודל יכול לאחזר תוכן מכתובות ה-URL ולהשתמש בתוכן הזה כדי לקבל מידע ולעצב את התשובה שלו.

הכלי הזה שימושי למשימות כמו:

*   חילוץ נקודות נתונים או נקודות מרכזיות לדיון ממאמרים
*   השוואת מידע בין כמה קישורים
*   סינתזה של נתונים מכמה מקורות
*   מענה על שאלות על סמך התוכן של דף או דפים ספציפיים
*   ניתוח תוכן למטרות ספציפיות (למשל כתיבת תיאור משרה או יצירת שאלות למבחן)

במדריך הזה נסביר איך משתמשים בכלי להקשר של כתובת URL ב-Gemini API.

## שימוש בהקשר של כתובת ה-URL

יש שתי דרכים עיקריות להשתמש בכלי להקשר של כתובת ה-URL: לבד או בשילוב עם [הצגת מידע על הנושא באמצעות חיפוש Google](https://ai.google.dev/gemini-api/docs/grounding?hl=he).

**הקשר של כתובת ה-URL בלבד**

אתם מספקים כתובות URL ספציפיות שאתם רוצים שהמודל ינתח ישירות בהנחיה.

הנחיות לדוגמה:

```
Summarize this document: YOUR_URLs

Extract the key features from the product description on this page: YOUR_URLs
```

**התאמה לנתונים באמצעות חיפוש Google + הקשר של כתובת ה-URL**

אפשר גם להפעיל את ההגדרות 'הקשר של כתובת ה-URL' ו'התאמה ל-Google Search' יחד. אפשר להזין הנחיה עם כתובות URL או בלי כתובות URL. יכול להיות שהמודל יחפש קודם מידע רלוונטי, ואז ישתמש בכלי להקשר של כתובות URL כדי לקרוא את התוכן של תוצאות החיפוש כדי לקבל הבנה מעמיקה יותר.

הנחיות לדוגמה:

```
Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.

Recommend 3 books for beginners to read to learn more about the latest YOUR_subject.
```

### דוגמאות לקוד עם הקשר של כתובת URL בלבד

### Python

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch

client = genai.Client()
model_id = "gemini-2.5-flash"

url_context_tool = Tool(
    url_context = types.UrlContext
)

response = client.models.generate_content(
    model=model_id,
    contents="Compare recipes from YOUR_URL1 and YOUR_URL2",
    config=GenerateContentConfig(
        tools=[url_context_tool],
        response_modalities=["TEXT"],
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)

```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Compare recipes from YOUR_URL1 and YOUR_URL2",
    ],
    config: {
      tools: [{urlContext: {}}],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Compare recipes from YOUR_URL1 and YOUR_URL2"}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          }
      ]
  }' > result.json

cat result.json
```

### דוגמאות לקוד עם Grounding באמצעות חיפוש Google

### Python

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch

client = genai.Client()
model_id = "gemini-2.5-flash"

tools = []
tools.append(Tool(url_context=types.UrlContext))
tools.append(Tool(google_search=types.GoogleSearch))

response = client.models.generate_content(
    model=model_id,
    contents="Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    config=GenerateContentConfig(
        tools=tools,
        response_modalities=["TEXT"],
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)

```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    ],
    config: {
      tools: [{urlContext: {}}, {googleSearch: {}}],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute."}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          },
          {
              "google_search": {}
          }
      ]
  }' > result.json

cat result.json
```

פרטים נוספים על 'הכנה לקראת השיחה' בחיפוש Google זמינים בדף [הסקירה הכללית](https://ai.google.dev/gemini-api/docs/grounding?hl=he).

### תגובה לפי הקשר

התשובה של המודל תהיה מבוססת על התוכן שהוא אחזר מכתובות ה-URL. אם המודל אחזר תוכן מכתובות ה-URL, התשובה תכלול את הערך `url_context_metadata`. התגובה עשויה להיראות כך (חלקים מהתגובה הושמטו כדי לקצר את הטקסט):

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "... \n"
          }
        ],
        "role": "model"
      },
      ...
      "url_context_metadata":
      {
          "url_metadata":
          [
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/1234567890abcdef",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/abcdef1234567890",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "YOUR_URL",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/fedcba0987654321",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            }
          ]
        }
    }
}
```

## מודלים נתמכים

*   [gemini-2.5-pro](https://ai.google.dev/gemini-api/docs/models?hl=he#gemini-2.5-pro)
*   [gemini-2.5-flash](https://ai.google.dev/gemini-api/docs/models?hl=he#gemini-2.5-flash)
*   [gemini-2.5-flash-lite](https://ai.google.dev/gemini-api/docs/models?hl=he#gemini-2.5-flash-lite)
*   [gemini-2.0-flash](https://ai.google.dev/gemini-api/docs/models?hl=he#gemini-2.0-flash)
*   [gemini-2.0-flash-live-001](https://ai.google.dev/gemini-api/docs/models?hl=he#live-api)

## מגבלות

*   בכל בקשה לניתוח, הכלי ישתמש בעד 20 כתובות URL.
*   כדי לקבל את התוצאות הטובות ביותר במהלך השלב הניסיוני, מומלץ להשתמש בכלי בדפי אינטרנט רגילים ולא בתוכן מולטימדיה כמו סרטונים ב-YouTube.
*   במהלך השלב הניסיוני, השימוש בכלי הוא בחינם. החיוב יתבצע מאוחר יותר.
*   לגרסה הניסיונית יש את המכסות הבאות:
    
    *   1,500 שאילתות ביום לכל פרויקט לבקשות שנשלחות דרך Gemini API
    *   100 שאילתות ביום לכל משתמש ב-Google AI Studio

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/code-execution

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# ביצוע קוד

Gemini API מספק כלי להרצת קוד שמאפשר למודל ליצור קוד Python ולהריץ אותו. לאחר מכן, המודל יכול ללמוד באופן איטרטיבי מתוצאות ביצוע הקוד עד שהוא מגיע לפלט סופי. אפשר להשתמש בהרצת קוד כדי ליצור אפליקציות שמפיקות תועלת מהסקת מסקנות שמבוססת על קוד. לדוגמה, אפשר להשתמש בהרצת קוד כדי לפתור משוואות או לעבד טקסט. אפשר גם להשתמש ב[ספריות](#supported-libraries) שכלולות בסביבת ביצוע הקוד כדי לבצע משימות ספציפיות יותר.

‫Gemini יכול להריץ קוד רק ב-Python. עדיין אפשר לבקש מ-Gemini ליצור קוד בשפה אחרת, אבל המודל לא יכול להשתמש בכלי להרצת הקוד כדי להריץ אותו.

## הפעלת ביצוע הקוד

כדי להפעיל את ביצוע הקוד, צריך להגדיר את הכלי לביצוע הקוד במודל. כך המודל יכול ליצור ולהריץ קוד.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50.",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: [
    "What is the sum of the first 50 prime numbers? " +
      "Generate and run code for the calculation, and make sure you get all 50.",
  ],
  config: {
    tools: [{ codeExecution: {} }],
  },
});

const parts = response?.candidates?.[0]?.content?.parts || [];
parts.forEach((part) => {
  if (part.text) {
    console.log(part.text);
  }

  if (part.executableCode && part.executableCode.code) {
    console.log(part.executableCode.code);
  }

  if (part.codeExecutionResult && part.codeExecutionResult.output) {
    console.log(part.codeExecutionResult.output);
  }
});
```

### Go

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("What is the sum of the first 50 prime numbers? " +
                  "Generate and run code for the calculation, and make sure you get all 50."),
        config,
    )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d ' {"tools": [{"code_execution": {}}],
    "contents": {
      "parts":
        {
            "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
        }
    },
}'
```

הפלט עשוי להיראות כך, אחרי שעבר עיצוב לקריאה קלה:

```
Okay, I need to calculate the sum of the first 50 prime numbers. Here's how I'll
approach this:

1.  **Generate Prime Numbers:** I'll use an iterative method to find prime
    numbers. I'll start with 2 and check if each subsequent number is divisible
    by any number between 2 and its square root. If not, it's a prime.
2.  **Store Primes:** I'll store the prime numbers in a list until I have 50 of
    them.
3.  **Calculate the Sum:**  Finally, I'll sum the prime numbers in the list.

Here's the Python code to do this:

def is_prime(n):
  """Efficiently checks if a number is prime."""
  if n <= 1:
    return False
  if n <= 3:
    return True
  if n % 2 == 0 or n % 3 == 0:
    return False
  i = 5
  while i * i <= n:
    if n % i == 0 or n % (i + 2) == 0:
      return False
    i += 6
  return True

primes = []
num = 2
while len(primes) < 50:
  if is_prime(num):
    primes.append(num)
  num += 1

sum_of_primes = sum(primes)
print(f'{primes=}')
print(f'{sum_of_primes=}')

primes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67,
71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151,
157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]
sum_of_primes=5117

The sum of the first 50 prime numbers is 5117.
```

הפלט הזה משלב כמה חלקי תוכן שהמודל מחזיר כשמשתמשים בהרצת קוד:

*   `text`: טקסט בשורה שנוצר על ידי המודל
*   `executableCode`: קוד שנוצר על ידי המודל והוא מיועד לביצוע
*   `codeExecutionResult`: התוצאה של הקוד שניתן להפעלה

מוסכמות השמות של החלקים האלה משתנות בהתאם לשפת התכנות.

## איך משתמשים בהרצת קוד בצ'אט

אפשר גם להשתמש בהרצת קוד כחלק מהצ'אט.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

chat = client.chats.create(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

response = chat.send_message("I have a math question for you.")
print(response.text)

response = chat.send_message(
    "What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50."
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

### JavaScript

```
import {GoogleGenAI} from "@google/genai";

const ai = new GoogleGenAI({});

const chat = ai.chats.create({
  model: "gemini-2.5-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "I have a math question for you:" }],
    },
    {
      role: "model",
      parts: [{ text: "Great! I'm ready for your math question. Please ask away." }],
    },
  ],
  config: {
    tools: [{codeExecution:{}}],
  }
});

const response = await chat.sendMessage({
  message: "What is the sum of the first 50 prime numbers? " +
            "Generate and run code for the calculation, and make sure you get all 50."
});
console.log("Chat response:", response.text);
```

### Go

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    chat, _ := client.Chats.Create(
        ctx,
        "gemini-2.5-flash",
        config,
        nil,
    )

    result, _ := chat.SendMessage(
                    ctx,
                    genai.Part{Text: "What is the sum of the first 50 prime numbers? " +
                                          "Generate and run code for the calculation, and " +
                                          "make sure you get all 50.",
                              },
                )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"tools": [{"code_execution": {}}],
    "contents": [
        {
            "role": "user",
            "parts": [{
                "text": "Can you print \"Hello world!\"?"
            }]
        },{
            "role": "model",
            "parts": [
              {
                "text": ""
              },
              {
                "executable_code": {
                  "language": "PYTHON",
                  "code": "\nprint(\"hello world!\")\n"
                }
              },
              {
                "code_execution_result": {
                  "outcome": "OUTCOME_OK",
                  "output": "hello world!\n"
                }
              },
              {
                "text": "I have printed \"hello world!\" using the provided python code block. \n"
              }
            ],
        },{
            "role": "user",
            "parts": [{
                "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
            }]
        }
    ]
}'
```

## קלט/פלט (I/O)

החל מ-[Gemini 2.0 Flash](https://ai.google.dev/gemini-api/docs/models/gemini?hl=he#gemini-2.0-flash), ביצוע הקוד תומך בקלט של קובץ ובפלט של תרשים. בעזרת יכולות הקלט והפלט האלה, אפשר להעלות קובצי CSV וקובצי טקסט, לשאול שאלות על הקבצים וליצור גרפים של [Matplotlib](https://matplotlib.org/) כחלק מהתגובה. קובצי הפלט מוחזרים בתגובה כתמונות מוטמעות.

### תמחור של קלט/פלט

כשמשתמשים בקלט/פלט של ביצוע קוד, אתם מחויבים על אסימוני קלט ואסימוני פלט:

**טוקני קלט:**

*   הנחיה למשתמש

**אסימוני פלט:**

*   קוד שנוצר על ידי המודל
*   פלט של הרצת קוד בסביבת הקוד
*   אסימוני חשיבה
*   סיכום שנוצר על ידי המודל

### פרטי קלט/פלט

כשעובדים עם קלט/פלט של ביצוע קוד, חשוב לשים לב לפרטים הטכניים הבאים:

*   זמן הריצה המקסימלי של סביבת הקוד הוא 30 שניות.
*   אם סביבת הקוד יוצרת שגיאה, יכול להיות שהמודל יחליט ליצור מחדש את פלט הקוד. אפשר לעשות זאת עד 5 פעמים.
*   גודל הקלט המקסימלי של הקובץ מוגבל על ידי חלון אסימון המודל. ב-AI Studio, באמצעות Gemini Flash 2.0, גודל הקובץ המקסימלי של הקלט הוא מיליון טוקנים (כ-2MB לקובצי טקסט מסוגים נתמכים של קלט). אם תעלו קובץ גדול מדי, לא תוכלו לשלוח אותו ב-AI Studio.
*   ביצוע הקוד עובד בצורה הטובה ביותר עם קובצי טקסט ו-CSV.
*   אפשר להעביר את קובץ הקלט ב-`part.inlineData` או ב-`part.fileData` (העלאה דרך [Files API](https://ai.google.dev/gemini-api/docs/files?hl=he)), וקובץ הפלט תמיד מוחזר כ-`part.inlineData`.

פנייה אחת

דו-כיווני (Multimodal Live API)

מודלים נתמכים

כל הדגמים של Gemini 2.0 ו-2.5

רק מודלים ניסיוניים של Flash

סוגי הקלט של קבצים שנתמכים

‎.png,‏ ‎.jpeg,‏ ‎.csv,‏ ‎.xml,‏ ‎.cpp,‏ ‎.java,‏ ‎.py,‏ ‎.js,‏ ‎.ts

‎.png,‏ ‎.jpeg,‏ ‎.csv,‏ ‎.xml,‏ ‎.cpp,‏ ‎.java,‏ ‎.py,‏ ‎.js,‏ ‎.ts

ספריות ליצירת תרשימים נתמכות

Matplotlib, ‏ seaborn

Matplotlib, ‏ seaborn

[שימוש במכשיר רב-כלי](https://ai.google.dev/gemini-api/docs/function-calling?hl=he#multi-tool-use)

כן (ביצוע קוד + יצירת קשר עם האדמה בלבד)

כן

## חיוב

אין חיוב נוסף על הפעלת קוד מ-Gemini API. החיוב יתבצע לפי התעריף הנוכחי של אסימוני הקלט והפלט, בהתאם למודל Gemini שבו אתם משתמשים.

ריכזנו כאן כמה דברים נוספים שכדאי לדעת על החיוב על ביצוע קוד:

*   אתם מחויבים רק פעם אחת על טוקני הקלט שאתם מעבירים למודל, ועל טוקני הפלט הסופיים שהמודל מחזיר לכם.
*   אסימונים שמייצגים קוד שנוצר נספרים כאסימוני פלט. הקוד שנוצר יכול לכלול טקסט ופלט רב-מודלי כמו תמונות.
*   גם תוצאות של הרצת קוד נספרות כאסימוני פלט.

מודל החיוב מוצג בתרשים הבא:

![מודל חיוב על ביצוע קוד](https://ai.google.dev/static/gemini-api/docs/images/code-execution-diagram.png?hl=he)

*   החיוב מתבצע לפי התעריף הנוכחי של אסימוני הקלט והפלט, על סמך המודל של Gemini שבו אתם משתמשים.
*   אם Gemini משתמש בהרצת קוד כדי ליצור את התשובה, ההנחיה המקורית, הקוד שנוצר והתוצאה של הקוד שהופעל מסומנים בתווית _אסימונים ביניים_ ומחויבים בתור _אסימוני קלט_.
*   לאחר מכן, Gemini יוצר סיכום ומחזיר את הקוד שנוצר, את התוצאה של הקוד שהופעל ואת הסיכום הסופי. הם מחויבים כ_אסימוני פלט_.
*   ה-Gemini API כולל ספירת אסימונים ביניים בתגובה של ה-API, כדי שתוכלו לדעת למה אתם מקבלים אסימוני קלט נוספים מעבר להנחיה הראשונית.

## מגבלות

*   המודל יכול ליצור ולבצע קוד בלבד. אי אפשר להחזיר ארטיפקטים אחרים, כמו קובצי מדיה.
*   במקרים מסוימים, הפעלת ביצוע הקוד יכולה להוביל לרגרסיה באזורים אחרים של פלט המודל (לדוגמה, כתיבת סטורי).
*   יש הבדלים מסוימים ביכולת של המודלים השונים להשתמש בהרצת קוד בצורה מוצלחת.

## ספריות נתמכות

סביבת ביצוע הקוד כוללת את הספריות הבאות:

*   attrs
*   שחמט
*   contourpy
*   fpdf
*   geopandas
*   imageio
*   jinja2
*   joblib
*   jsonschema
*   jsonschema-specifications
*   lxml
*   matplotlib
*   mpmath
*   numpy
*   opencv-python
*   openpyxl
*   מארז
*   פנדות
*   כרית
*   protobuf
*   pylatex
*   pyparsing
*   PyPDF2
*   python-dateutil
*   python-docx
*   python-pptx
*   reportlab
*   scikit-learn
*   scipy
*   seaborn
*   שש
*   striprtf
*   sympy
*   טבלת
*   tensorflow
*   toolz
*   xlrd

אי אפשר להתקין ספריות משלכם.

## המאמרים הבאים

*   אפשר לנסות את [הרצת הקוד ב-Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Code_Execution.ipynb?hl=he).
*   מידע על כלים אחרים של Gemini API:
    *   [קריאה לפונקציה](https://ai.google.dev/gemini-api/docs/function-calling?hl=he)
    *   [התמקדות בעצמכם באמצעות חיפוש Google](https://ai.google.dev/gemini-api/docs/grounding?hl=he)

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/live-guide

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Live API capabilities guide

This is a comprehensive guide that covers capabilities and configurations available with the Live API. See [Get started with Live API](/gemini-api/docs/live) page for a overview and sample code for common use cases.

## Before you begin

*   **Familiarize yourself with core concepts:** If you haven't already done so, read the [Get started with Live API](/gemini-api/doc/live) page first. This will introduce you to the fundamental principles of the Live API, how it works, and the distinction between the [different models](/gemini-api/docs/live#audio-generation) and their corresponding audio generation methods ([native audio](#native-audio-output) or half-cascade).
*   **Try the Live API in AI Studio:** You may find it useful to try the Live API in [Google AI Studio](https://aistudio.google.com/app/live) before you start building. To use the Live API in Google AI Studio, select **Stream**.

## Establishing a connection

The following example shows how to create a connection with an API key:

### Python

```
import asyncio
from google import genai

client = genai.Client()

model = "gemini-live-2.5-flash-preview"
config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        print("Session started")

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        console.debug(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send content...

  session.close();
}

main();
```

## Interaction modalities

The following sections provide examples and supporting context for the different input and output modalities available in Live API.

### Sending and receiving text

Here's how you can send and receive text:

### Python

```
import asyncio
from google import genai

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        message = "Hello, how are you?"
        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.text is not None:
                print(response.text, end="")

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

#### Incremental content updates

Use incremental updates to send text input, establish session context, or restore session context. For short contexts you can send turn-by-turn interactions to represent the exact sequence of events:

### Python

```
turns = [
    {"role": "user", "parts": [{"text": "What is the capital of France?"}]},
    {"role": "model", "parts": [{"text": "Paris"}]},
]

await session.send_client_content(turns=turns, turn_complete=False)

turns = [{"role": "user", "parts": [{"text": "What is the capital of Germany?"}]}]

await session.send_client_content(turns=turns, turn_complete=True)
```

### JavaScript

```
let inputTurns = [
  { "role": "user", "parts": [{ "text": "What is the capital of France?" }] },
  { "role": "model", "parts": [{ "text": "Paris" }] },
]

session.sendClientContent({ turns: inputTurns, turnComplete: false })

inputTurns = [{ "role": "user", "parts": [{ "text": "What is the capital of Germany?" }] }]

session.sendClientContent({ turns: inputTurns, turnComplete: true })
```

For longer contexts it's recommended to provide a single message summary to free up the context window for subsequent interactions. See [Session Resumption](/gemini-api/docs/live-session#session-resumption) for another method for loading session context.

### Sending and receiving audio

The most common audio example, **audio-to-audio**, is covered in the [Getting started](/gemini-api/docs/live#audio-to-audio) guide.

Here's an **audio-to-text** example that reads a WAV file, sends it in the correct format and receives text output:

### Python

```
# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
# Install helpers for converting files: pip install librosa soundfile
import asyncio
import io
from pathlib import Path
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:

        buffer = io.BytesIO()
        y, sr = librosa.load("sample.wav", sr=16000)
        sf.write(buffer, y, sr, format='RAW', subtype='PCM_16')
        buffer.seek(0)
        audio_bytes = buffer.read()

        # If already in correct format, you can use this:
        # audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        async for response in session.receive():
            if response.text is not None:
                print(response.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
// Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
// Install helpers for converting files: npm install wavefile
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("sample.wav");

  // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
  const wav = new WaveFile();
  wav.fromBuffer(fileBuffer);
  wav.toSampleRate(16000);
  wav.toBitDepth("16");
  const base64Audio = wav.toBase64();

  // If already in correct format, you can use this:
  // const fileBuffer = fs.readFileSync("sample.pcm");
  // const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }

  );

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

And here is a **text-to-audio** example. You can receive audio by setting `AUDIO` as response modality. This example saves the received data as WAV file:

### Python

```
import asyncio
import wave
from google import genai

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["AUDIO"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)

        message = "Hello how are you?"
        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)

            # Un-comment this code to print audio data info
            # if response.server_content.model_turn is not None:
            #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.AUDIO] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  // Combine audio data strings and save as wave file
  const combinedAudio = turns.reduce((acc, turn) => {
    if (turn.data) {
      const buffer = Buffer.from(turn.data, 'base64');
      const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
      return acc.concat(Array.from(intArray));
    }
    return acc;
  }, []);

  const audioBuffer = new Int16Array(combinedAudio);

  const wf = new WaveFile();
  wf.fromScratch(1, 24000, '16', audioBuffer);
  fs.writeFileSync('output.wav', wf.toBuffer());

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

#### Audio formats

Audio data in the Live API is always raw, little-endian, 16-bit PCM. Audio output always uses a sample rate of 24kHz. Input audio is natively 16kHz, but the Live API will resample if needed so any sample rate can be sent. To convey the sample rate of input audio, set the MIME type of each audio-containing [Blob](/api/caching#Blob) to a value like `audio/pcm;rate=16000`.

#### Audio transcriptions

You can enable transcription of the model's audio output by sending `output_audio_transcription` in the setup config. The transcription language is inferred from the model's response.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["AUDIO"],
        "output_audio_transcription": {}
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        message = "Hello? Gemini are you there?"

        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.server_content.model_turn:
                print("Model turn:", response.server_content.model_turn)
            if response.server_content.output_transcription:
                print("Transcript:", response.server_content.output_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const config = {
  responseModalities: [Modality.AUDIO],
  outputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.outputTranscription) {
      console.debug('Received output transcription: %s\n', turn.serverContent.outputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

You can enable transcription of the audio input by sending `input_audio_transcription` in setup config.

### Python

```
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {
    "response_modalities": ["TEXT"],
    "input_audio_transcription": {},
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_data = Path("16000.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_data, mime_type='audio/pcm;rate=16000')
        )

        async for msg in session.receive():
            if msg.server_content.input_transcription:
                print('Transcript:', msg.server_content.input_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const config = {
  responseModalities: [Modality.TEXT],
  inputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("16000.wav");

  // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
  const wav = new WaveFile();
  wav.fromBuffer(fileBuffer);
  wav.toSampleRate(16000);
  wav.toBitDepth("16");
  const base64Audio = wav.toBase64();

  // If already in correct format, you can use this:
  // const fileBuffer = fs.readFileSync("sample.pcm");
  // const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }
  );

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.outputTranscription) {
      console.log("Transcription")
      console.log(turn.serverContent.outputTranscription.text);
    }
  }
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
    else if (turn.serverContent && turn.serverContent.inputTranscription) {
      console.debug('Received input transcription: %s\n', turn.serverContent.inputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

### Stream audio and video

### Change voice and language

The Live API models each support a different set of voices. Half-cascade supports Puck, Charon, Kore, Fenrir, Aoede, Leda, Orus, and Zephyr. Native audio supports a much longer list (identical to [the TTS model list](/gemini-api/docs/speech-generation#voices)). You can listen to all the voices in [AI Studio](https://aistudio.google.com/app/live).

To specify a voice, set the voice name within the `speechConfig` object as part of the session configuration:

### Python

```
config = {
    "response_modalities": ["AUDIO"],
    "speech_config": {
        "voice_config": {"prebuilt_voice_config": {"voice_name": "Kore"}}
    },
}
```

### JavaScript

```
const config = {
  responseModalities: [Modality.AUDIO],
  speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Kore" } } }
};
```

The Live API supports [multiple languages](#supported-languages).

To change the language, set the language code within the `speechConfig` object as part of the session configuration:

### Python

```
config = {
    "response_modalities": ["AUDIO"],
    "speech_config": {
        "language_code": "de-DE"
    }
}
```

### JavaScript

```
const config = {
  responseModalities: [Modality.AUDIO],
  speechConfig: { languageCode: "de-DE" }
};
```

## Native audio capabilities

The following capabilities are only available with native audio. You can learn more about native audio in [Choose a model and audio generation](/gemini-api/docs/api).

### How to use native audio output

To use native audio output, configure one of the [native audio models](/gemini-api/docs/models#gemini-2.5-flash-native-audio) and set `response_modalities` to `AUDIO`.

See [Send and receive audio](/gemini-api/docs/live#audio-to-audio) for a full example.

### Python

```
model = "gemini-2.5-flash-preview-native-audio-dialog"
config = types.LiveConnectConfig(response_modalities=["AUDIO"])

async with client.aio.live.connect(model=model, config=config) as session:
    # Send audio input and receive audio
```

### JavaScript

```
const model = 'gemini-2.5-flash-preview-native-audio-dialog';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: ...,
  });

  // Send audio input and receive audio

  session.close();
}

main();
```

### Affective dialog

This feature lets Gemini adapt its response style to the input expression and tone.

To use affective dialog, set the api version to `v1alpha` and set `enable_affective_dialog` to `true`in the setup message:

### Python

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    enable_affective_dialog=True
)
```

### JavaScript

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  enableAffectiveDialog: true
};
```

Note that affective dialog is currently only supported by the native audio output models.

### Proactive audio

When this feature is enabled, Gemini can proactively decide not to respond if the content is not relevant.

To use it, set the api version to `v1alpha` and configure the `proactivity` field in the setup message and set `proactive_audio` to `true`:

### Python

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    proactivity={'proactive_audio': True}
)
```

### JavaScript

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  proactivity: { proactiveAudio: true }
}
```

Note that proactive audio is currently only supported by the native audio output models.

### Native audio output with thinking

Native audio output supports [thinking capabilities](/gemini-api/docs/thinking), available via a separate model `gemini-2.5-flash-exp-native-audio-thinking-dialog`.

See [Send and receive audio](/gemini-api/docs/live#audio-to-audio) for a full example.

### Python

```
model = "gemini-2.5-flash-exp-native-audio-thinking-dialog"
config = types.LiveConnectConfig(response_modalities=["AUDIO"])

async with client.aio.live.connect(model=model, config=config) as session:
    # Send audio input and receive audio
```

### JavaScript

```
const model = 'gemini-2.5-flash-exp-native-audio-thinking-dialog';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: ...,
  });

  // Send audio input and receive audio

  session.close();
}

main();
```

## Voice Activity Detection (VAD)

Voice Activity Detection (VAD) allows the model to recognize when a person is speaking. This is essential for creating natural conversations, as it allows a user to interrupt the model at any time.

When VAD detects an interruption, the ongoing generation is canceled and discarded. Only the information already sent to the client is retained in the session history. The server then sends a [`BidiGenerateContentServerContent`](/api/live#bidigeneratecontentservercontent) message to report the interruption.

The Gemini server then discards any pending function calls and sends a `BidiGenerateContentServerContent` message with the IDs of the canceled calls.

### Python

```
async for response in session.receive():
    if response.server_content.interrupted is True:
        # The generation was interrupted

        # If realtime playback is implemented in your application,
        # you should stop playing audio and clear queued playback here.
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.interrupted) {
    // The generation was interrupted

    // If realtime playback is implemented in your application,
    // you should stop playing audio and clear queued playback here.
  }
}
```

### Automatic VAD

By default, the model automatically performs VAD on a continuous audio input stream. VAD can be configured with the [`realtimeInputConfig.automaticActivityDetection`](/api/live#RealtimeInputConfig.AutomaticActivityDetection) field of the [setup configuration](/api/live#BidiGenerateContentSetup).

When the audio stream is paused for more than a second (for example, because the user switched off the microphone), an [`audioStreamEnd`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.bool.BidiGenerateContentRealtimeInput.audio_stream_end) event should be sent to flush any cached audio. The client can resume sending audio data at any time.

### Python

```
# example audio file to try:
# URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
# !wget -q $URL -O sample.pcm
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        # if stream gets paused, send:
        # await session.send_realtime_input(audio_stream_end=True)

        async for response in session.receive():
            if response.text is not None:
                print(response.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
// example audio file to try:
// URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
// !wget -q $URL -O sample.pcm
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("sample.pcm");
  const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }

  );

  // if stream gets paused, send:
  // session.sendRealtimeInput({ audioStreamEnd: true })

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

With `send_realtime_input`, the API will respond to audio automatically based on VAD. While `send_client_content` adds messages to the model context in order, `send_realtime_input` is optimized for responsiveness at the expense of deterministic ordering.

### Automatic VAD configuration

For more control over the VAD activity, you can configure the following parameters. See [API reference](/api/live#automaticactivitydetection) for more info.

### Python

```
from google.genai import types

config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {
        "automatic_activity_detection": {
            "disabled": False, # default
            "start_of_speech_sensitivity": types.StartSensitivity.START_SENSITIVITY_LOW,
            "end_of_speech_sensitivity": types.EndSensitivity.END_SENSITIVITY_LOW,
            "prefix_padding_ms": 20,
            "silence_duration_ms": 100,
        }
    }
}
```

### JavaScript

```
import { GoogleGenAI, Modality, StartSensitivity, EndSensitivity } from '@google/genai';

const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: false, // default
      startOfSpeechSensitivity: StartSensitivity.START_SENSITIVITY_LOW,
      endOfSpeechSensitivity: EndSensitivity.END_SENSITIVITY_LOW,
      prefixPaddingMs: 20,
      silenceDurationMs: 100,
    }
  }
};
```

### Disable automatic VAD

Alternatively, the automatic VAD can be disabled by setting `realtimeInputConfig.automaticActivityDetection.disabled` to `true` in the setup message. In this configuration the client is responsible for detecting user speech and sending [`activityStart`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityStart.BidiGenerateContentRealtimeInput.activity_start) and [`activityEnd`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityEnd.BidiGenerateContentRealtimeInput.activity_end) messages at the appropriate times. An `audioStreamEnd` isn't sent in this configuration. Instead, any interruption of the stream is marked by an `activityEnd` message.

### Python

```
config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {"automatic_activity_detection": {"disabled": True}},
}

async with client.aio.live.connect(model=model, config=config) as session:
    # ...
    await session.send_realtime_input(activity_start=types.ActivityStart())
    await session.send_realtime_input(
        audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
    )
    await session.send_realtime_input(activity_end=types.ActivityEnd())
    # ...
```

### JavaScript

```
const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: true,
    }
  }
};

session.sendRealtimeInput({ activityStart: {} })

session.sendRealtimeInput(
  {
    audio: {
      data: base64Audio,
      mimeType: "audio/pcm;rate=16000"
    }
  }

);

session.sendRealtimeInput({ activityEnd: {} })
```

## Token count

You can find the total number of consumed tokens in the [usageMetadata](/api/live#usagemetadata) field of the returned server message.

### Python

```
async for message in session.receive():
    # The server will periodically send messages that include UsageMetadata.
    if message.usage_metadata:
        usage = message.usage_metadata
        print(
            f"Used {usage.total_token_count} tokens in total. Response token breakdown:"
        )
        for detail in usage.response_tokens_details:
            match detail:
                case types.ModalityTokenCount(modality=modality, token_count=count):
                    print(f"{modality}: {count}")
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.usageMetadata) {
    console.debug('Used %s tokens in total. Response token breakdown:\n', turn.usageMetadata.totalTokenCount);

    for (const detail of turn.usageMetadata.responseTokensDetails) {
      console.debug('%s\n', detail);
    }
  }
}
```

## Media resolution

You can specify the media resolution for the input media by setting the `mediaResolution` field as part of the session configuration:

### Python

```
from google.genai import types

config = {
    "response_modalities": ["AUDIO"],
    "media_resolution": types.MediaResolution.MEDIA_RESOLUTION_LOW,
}
```

### JavaScript

```
import { GoogleGenAI, Modality, MediaResolution } from '@google/genai';

const config = {
    responseModalities: [Modality.TEXT],
    mediaResolution: MediaResolution.MEDIA_RESOLUTION_LOW,
};
```

## Limitations

Consider the following limitations of the Live API when you plan your project.

### Response modalities

You can only set one response modality (`TEXT` or `AUDIO`) per session in the session configuration. Setting both results in a config error message. This means that you can configure the model to respond with either text or audio, but not both in the same session.

### Client authentication

The Live API only provides server-to-server authentication by default. If you're implementing your Live API application using a [client-to-server approach](/gemini-api/docs/live#implementation-approach), you need to use [ephemeral tokens](/gemini-api/docs/ephemeral-tokens) to mitigate security risks.

### Session duration

Audio-only sessions are limited to 15 minutes, and audio plus video sessions are limited to 2 minutes. However, you can configure different [session management techniques](/gemini-api/docs/live-session) for unlimited extensions on session duration.

### Context window

A session has a context window limit of:

*   128k tokens for [native audio output](#native-audio-output) models
*   32k tokens for other Live API models

## Supported languages

Live API supports the following languages.

   

Language

BCP-47 Code

Language

BCP-47 Code

German (Germany)

`de-DE`

English (Australia)\*

`en-AU`

English (UK)\*

`en-GB`

English (India)

`en-IN`

English (US)

`en-US`

Spanish (US)

`es-US`

French (France)

`fr-FR`

Hindi (India)

`hi-IN`

Portuguese (Brazil)

`pt-BR`

Arabic (Generic)

`ar-XA`

Spanish (Spain)\*

`es-ES`

French (Canada)\*

`fr-CA`

Indonesian (Indonesia)

`id-ID`

Italian (Italy)

`it-IT`

Japanese (Japan)

`ja-JP`

Turkish (Turkey)

`tr-TR`

Vietnamese (Vietnam)

`vi-VN`

Bengali (India)

`bn-IN`

Gujarati (India)\*

`gu-IN`

Kannada (India)\*

`kn-IN`

Marathi (India)

`mr-IN`

Malayalam (India)\*

`ml-IN`

Tamil (India)

`ta-IN`

Telugu (India)

`te-IN`

Dutch (Netherlands)

`nl-NL`

Korean (South Korea)

`ko-KR`

Mandarin Chinese (China)\*

`cmn-CN`

Polish (Poland)

`pl-PL`

Russian (Russia)

`ru-RU`

Thai (Thailand)

`th-TH`

_Languages marked with an asterisk_ (\*) _are not available for [Native audio](#native-audio-output)_.

## What's next

*   Read the [Tool Use](/gemini-api/docs/live-tools) and [Session Management](/gemini-api/docs/live-session) guides for essential information on using the Live API effectively.
*   Try the Live API in [Google AI Studio](https://aistudio.google.com/app/live).
*   For more info about the Live API models, see [Gemini 2.0 Flash Live](/gemini-api/docs/models#live-api) and [Gemini 2.5 Flash Native Audio](/gemini-api/docs/models#gemini-2.5-flash-native-audio) on the Models page.
*   Try more examples in the [Live API cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb), the [Live API Tools cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb), and the [Live API Get Started script](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/live

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# Get started with Live API

Live API מאפשר אינטראקציות בזמן אמת עם Gemini באמצעות קול ווידאו עם זמן אחזור נמוך. המערכת מעבדת שידורים רציפים של אודיו, וידאו או טקסט כדי לספק תשובות מידיות בעלות אופי אנושי, וכך יוצרת חוויית שיחה טבעית למשתמשים.

![סקירה כללית על Live API](https://ai.google.dev/static/gemini-api/docs/images/live-api-overview.png?hl=he)

ממשק Live API מציע קבוצה מקיפה של תכונות, כמו [זיהוי פעילות קול](https://ai.google.dev/gemini-api/docs/live-guide?hl=he#interruptions), [שימוש בכלים והפעלת פונקציות](https://ai.google.dev/gemini-api/docs/live-tools?hl=he), [ניהול סשנים](https://ai.google.dev/gemini-api/docs/live-session?hl=he) (לניהול שיחות ממושכות) ו[אסימונים זמניים](https://ai.google.dev/gemini-api/docs/ephemeral-tokens?hl=he) (לאימות מאובטח בצד הלקוח).

בדף הזה תלמדו איך להתחיל לעבוד עם דוגמאות ודוגמאות קוד בסיסיות.

## דוגמאות לאפליקציות

כדאי לעיין בדוגמאות הבאות לאפליקציות שממחישות איך משתמשים ב-Live API בתרחישים לדוגמה מקצה לקצה:

*   [אפליקציית התחלה לשימוש באודיו בשידור חי](https://aistudio.google.com/apps/bundled/live_audio?showPreview=true&%3BshowCode=true&%3BshowAssistant=false&hl=he) ב-AI Studio, שמשתמשת בספריות JavaScript כדי להתחבר ל-Live API ולהעביר אודיו דו-כיווני דרך המיקרופון והרמקולים.
*   [ספר בישול ב-Python](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py) ל-Live API באמצעות Pyaudio שמתחבר ל-Live API.

## שילובים עם שותפים

אם אתם מעדיפים תהליך פיתוח פשוט יותר, תוכלו להשתמש ב-[Daily](https://www.daily.co/products/gemini/multimodal-live-api/) או ב-[LiveKit](https://docs.livekit.io/agents/integrations/google/#multimodal-live-api). אלה פלטפורמות שותפות של צד שלישי שכבר שילבו את Gemini Live API באמצעות פרוטוקול WebRTC כדי לייעל את הפיתוח של אפליקציות אודיו ווידאו בזמן אמת.

## לפני שמתחילים לבנות

לפני שמתחילים לפתח באמצעות Live API, צריך לקבל שתי החלטות חשובות: בחירת מודל ובחירת גישה להטמעה.

### בחירת ארכיטקטורה ליצירת אודיו

אם אתם מפתחים תרחיש לדוגמה שמבוסס על אודיו, בחירת המודל קובעת את הארכיטקטורה ליצירת אודיו שתשמש ליצירת התשובה האודיו:

*   **[אודיו מקורי](https://ai.google.dev/gemini-api/docs/live-guide?hl=he#native-audio-output):** אפשרות זו מספקת את הדיבור הטבעי והריאליסטי ביותר, וביצועים טובים יותר בכמה שפות. הוא גם מאפשר תכונות מתקדמות כמו [דיאלוג רגשי (שמודע לרגשות)](https://ai.google.dev/gemini-api/docs/live-guide?hl=he#affective-dialog), [אודיו יזום](https://ai.google.dev/gemini-api/docs/live-guide?hl=he#proactive-audio) (שבו המודל יכול להחליט להתעלם ממידע מסוים או להגיב לו) ו['חשיבה'](https://ai.google.dev/gemini-api/docs/live-guide?hl=he#native-audio-output-thinking). [המודלים הבאים של אודיו מותאם](https://ai.google.dev/gemini-api/docs/models?hl=he#gemini-2.5-flash-native-audio) תומכים באודיו מותאם:
    *   `gemini-2.5-flash-preview-native-audio-dialog`
    *   `gemini-2.5-flash-exp-native-audio-thinking-dialog`
*   **אודיו ב-Half-cascade**: באפשרות הזו נעשה שימוש בארכיטקטורת מודל מדורגת (קלט אודיו מקורי ופלט של המרת טקסט לדיבור). היא מספקת ביצועים ואמינות טובים יותר בסביבות ייצור, במיוחד ב[שימוש בכלים](https://ai.google.dev/gemini-api/docs/live-tools?hl=he). האודיו עם קטעי אודיו חופפים בחלקו נתמך במודלים הבאים:
    *   `gemini-live-2.5-flash-preview`
    *   `gemini-2.0-flash-live-001`

### בחירת גישה להטמעה

כשמשלבים עם Live API, צריך לבחור באחת משיטות ההטמעה הבאות:

*   **שרת לשרת**: הקצה העורפי מתחבר ל-Live API באמצעות [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API). בדרך כלל, הלקוח שולח נתוני סטרימינג (אודיו, וידאו וטקסט) לשרת, והשרת מעביר אותם ל-Live API.
*   **מלקוח לשרת**: קוד הקצה הקדמי מתחבר ישירות ל-Live API באמצעות [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) כדי להעביר נתונים בסטרימינג, ועוקף את הקצה העורפי.

## שנתחיל?

בדוגמה הזו **_קוראים קובץ WAV_**, שולחים אותו בפורמט הנכון ושומרים את הנתונים שהתקבלו כקובץ WAV.

כדי לשלוח אודיו, צריך להמיר אותו לפורמט PCM 16 ביט, 16kHz, מונו. כדי לקבל אודיו, צריך להגדיר את `AUDIO` כמודל התגובה. תדירות הדגימה של הפלט היא 24kHz.

### Python

```
# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
# Install helpers for converting files: pip install librosa soundfile
import asyncio
import io
from pathlib import Path
import wave
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()

# Half cascade model:
# model = "gemini-live-2.5-flash-preview"

# Native audio output model:
model = "gemini-2.5-flash-preview-native-audio-dialog"

config = {
  "response_modalities": ["AUDIO"],
  "system_instruction": "You are a helpful assistant and answer in a friendly tone.",
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:

        buffer = io.BytesIO()
        y, sr = librosa.load("sample.wav", sr=16000)
        sf.write(buffer, y, sr, format='RAW', subtype='PCM_16')
        buffer.seek(0)
        audio_bytes = buffer.read()

        # If already in correct format, you can use this:
        # audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)

            # Un-comment this code to print audio data info
            # if response.server_content.model_turn is not None:
            #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
// Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
// WARNING: Do not use API keys in client-side (browser based) applications
// Consider using Ephemeral Tokens instead
// More information at: https://ai.google.dev/gemini-api/docs/ephemeral-tokens

// Half cascade model:
// const model = "gemini-live-2.5-flash-preview"

// Native audio output model:
const model = "gemini-2.5-flash-preview-native-audio-dialog"

const config = {
  responseModalities: [Modality.AUDIO], 
  systemInstruction: "You are a helpful assistant and answer in a friendly tone."
};

async function live() {
    const responseQueue = [];

    async function waitMessage() {
        let done = false;
        let message = undefined;
        while (!done) {
            message = responseQueue.shift();
            if (message) {
                done = true;
            } else {
                await new Promise((resolve) => setTimeout(resolve, 100));
            }
        }
        return message;
    }

    async function handleTurn() {
        const turns = [];
        let done = false;
        while (!done) {
            const message = await waitMessage();
            turns.push(message);
            if (message.serverContent && message.serverContent.turnComplete) {
                done = true;
            }
        }
        return turns;
    }

    const session = await ai.live.connect({
        model: model,
        callbacks: {
            onopen: function () {
                console.debug('Opened');
            },
            onmessage: function (message) {
                responseQueue.push(message);
            },
            onerror: function (e) {
                console.debug('Error:', e.message);
            },
            onclose: function (e) {
                console.debug('Close:', e.reason);
            },
        },
        config: config,
    });

    // Send Audio Chunk
    const fileBuffer = fs.readFileSync("sample.wav");

    // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
    const wav = new WaveFile();
    wav.fromBuffer(fileBuffer);
    wav.toSampleRate(16000);
    wav.toBitDepth("16");
    const base64Audio = wav.toBase64();

    // If already in correct format, you can use this:
    // const fileBuffer = fs.readFileSync("sample.pcm");
    // const base64Audio = Buffer.from(fileBuffer).toString('base64');

    session.sendRealtimeInput(
        {
            audio: {
                data: base64Audio,
                mimeType: "audio/pcm;rate=16000"
            }
        }

    );

    const turns = await handleTurn();

    // Combine audio data strings and save as wave file
    const combinedAudio = turns.reduce((acc, turn) => {
        if (turn.data) {
            const buffer = Buffer.from(turn.data, 'base64');
            const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
            return acc.concat(Array.from(intArray));
        }
        return acc;
    }, []);

    const audioBuffer = new Int16Array(combinedAudio);

    const wf = new WaveFile();
    wf.fromScratch(1, 24000, '16', audioBuffer);  // output is 24kHz
    fs.writeFileSync('audio.wav', wf.toBuffer());

    session.close();
}

async function main() {
    await live().catch((e) => console.error('got error', e));
}

main();
```

## המאמרים הבאים

*   כדאי לקרוא את המדריך המלא בנושא [יכולות](https://ai.google.dev/gemini-api/docs/live-guide?hl=he) של Live API כדי לקבל מידע על יכולות והגדרות מרכזיות, כולל זיהוי פעילות קול ותכונות אודיו מקוריות.
*   במדריך [שימוש בכלים](https://ai.google.dev/gemini-api/docs/live-tools?hl=he) מוסבר איך לשלב את Live API עם כלים וקריאות לפונקציות.
*   במדריך [ניהול סשנים](https://ai.google.dev/gemini-api/docs/live-session?hl=he) מוסבר איך לנהל שיחות ארוכות.
*   [במדריך על אסימונים זמניים](https://ai.google.dev/gemini-api/docs/ephemeral-tokens?hl=he) מוסבר איך לבצע אימות מאובטח באפליקציות [מלקוח לשרת](#implementation-approach).
*   מידע נוסף על WebSockets API הבסיסי זמין ב[חומר העזרה של WebSockets API](https://ai.google.dev/api/live?hl=he).

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/live-session

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Session management with Live API

In the Live API, a session refers to a persistent connection where input and output are streamed continuously over the same connection (read more about [how it works](/gemini-api/docs/live)). This unique session design enables low latency and supports unique features, but can also introduce challenges, like session time limits, and early termination. This guide covers strategies for overcoming the session management challenges that can arise when using the Live API.

## Session lifetime

Without compression, audio-only sessions are limited to 15 minutes, and audio-video sessions are limited to 2 minutes. Exceeding these limits will terminate the session (and therefore, the connection), but you can use [context window compression](#context-window-compression) to extend sessions to an unlimited amount of time.

The lifetime of a connection is limited as well, to around 10 minutes. When the connection terminates, the session terminates as well. In this case, you can configure a single session to stay active over multiple connections using [session resumption](#session-resumption). You'll also receive a [GoAway message](#goaway-message) before the connection ends, allowing you to take further actions.

## Context window compression

To enable longer sessions, and avoid abrupt connection termination, you can enable context window compression by setting the [contextWindowCompression](/api/live#BidiGenerateContentSetup.FIELDS.ContextWindowCompressionConfig.BidiGenerateContentSetup.context_window_compression) field as part of the session configuration.

In the [ContextWindowCompressionConfig](/api/live#contextwindowcompressionconfig), you can configure a [sliding-window mechanism](/api/live#ContextWindowCompressionConfig.FIELDS.ContextWindowCompressionConfig.SlidingWindow.ContextWindowCompressionConfig.sliding_window) and the [number of tokens](/api/live#ContextWindowCompressionConfig.FIELDS.int64.ContextWindowCompressionConfig.trigger_tokens) that triggers compression.

### Python

```
from google.genai import types

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    context_window_compression=(
        # Configures compression with default parameters.
        types.ContextWindowCompressionConfig(
            sliding_window=types.SlidingWindow(),
        )
    ),
)
```

### JavaScript

```
const config = {
  responseModalities: [Modality.AUDIO],
  contextWindowCompression: { slidingWindow: {} }
};
```

## Session resumption

To prevent session termination when the server periodically resets the WebSocket connection, configure the [sessionResumption](/api/live#BidiGenerateContentSetup.FIELDS.SessionResumptionConfig.BidiGenerateContentSetup.session_resumption) field within the [setup configuration](/api/live#BidiGenerateContentSetup).

Passing this configuration causes the server to send [SessionResumptionUpdate](/api/live#SessionResumptionUpdate) messages, which can be used to resume the session by passing the last resumption token as the [`SessionResumptionConfig.handle`](/api/live#SessionResumptionConfig.FIELDS.string.SessionResumptionConfig.handle) of the subsequent connection.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

async def main():
    print(f"Connecting to the service with handle {previous_session_handle}...")
    async with client.aio.live.connect(
        model=model,
        config=types.LiveConnectConfig(
            response_modalities=["AUDIO"],
            session_resumption=types.SessionResumptionConfig(
                # The handle of the session to resume is passed here,
                # or else None to start a new session.
                handle=previous_session_handle
            ),
        ),
    ) as session:
        while True:
            await session.send_client_content(
                turns=types.Content(
                    role="user", parts=[types.Part(text="Hello world!")]
                )
            )
            async for message in session.receive():
                # Periodically, the server will send update messages that may
                # contain a handle for the current state of the session.
                if message.session_resumption_update:
                    update = message.session_resumption_update
                    if update.resumable and update.new_handle:
                        # The handle should be retained and linked to the session.
                        return update.new_handle

                # For the purposes of this example, placeholder input is continually fed
                # to the model. In non-sample code, the model inputs would come from
                # the user.
                if message.server_content and message.server_content.turn_complete:
                    break

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

console.debug('Connecting to the service with handle %s...', previousSessionHandle)
const session = await ai.live.connect({
  model: model,
  callbacks: {
    onopen: function () {
      console.debug('Opened');
    },
    onmessage: function (message) {
      responseQueue.push(message);
    },
    onerror: function (e) {
      console.debug('Error:', e.message);
    },
    onclose: function (e) {
      console.debug('Close:', e.reason);
    },
  },
  config: {
    responseModalities: [Modality.TEXT],
    sessionResumption: { handle: previousSessionHandle }
    // The handle of the session to resume is passed here, or else null to start a new session.
  }
});

const inputTurns = 'Hello how are you?';
session.sendClientContent({ turns: inputTurns });

const turns = await handleTurn();
for (const turn of turns) {
  if (turn.sessionResumptionUpdate) {
    if (turn.sessionResumptionUpdate.resumable && turn.sessionResumptionUpdate.newHandle) {
      let newHandle = turn.sessionResumptionUpdate.newHandle
      // ...Store newHandle and start new session with this handle here
    }
  }
}

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

## Receiving a message before the session disconnects

The server sends a [GoAway](/api/live#GoAway) message that signals that the current connection will soon be terminated. This message includes the [timeLeft](/api/live#GoAway.FIELDS.google.protobuf.Duration.GoAway.time_left), indicating the remaining time and lets you take further action before the connection will be terminated as ABORTED.

### Python

```
async for response in session.receive():
    if response.go_away is not None:
        # The connection will soon be terminated
        print(response.go_away.time_left)
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.goAway) {
    console.debug('Time left: %s\n', turn.goAway.timeLeft);
  }
}
```

## Receiving a message when the generation is complete

The server sends a [generationComplete](/api/live#BidiGenerateContentServerContent.FIELDS.bool.BidiGenerateContentServerContent.generation_complete) message that signals that the model finished generating the response.

### Python

```
async for response in session.receive():
    if response.server_content.generation_complete is True:
        # The generation is complete
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.generationComplete) {
    // The generation is complete
  }
}
```

## What's next

Explore more ways to work with the Live API in the full [Capabilities](/gemini-api/docs/live) guide, the [Tool use](/gemini-api/docs/live-tools) page, or the [Live API cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/live-tools

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Tool use with Live API

Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world and pull in external context while maintaining a real time connection. You can define tools such as [Function calling](/gemini-api/docs/function-calling), [Code execution](/gemini-api/docs/code-execution), and [Google Search](/gemini-api/docs/grounding) with the Live API.

## Overview of supported tools

Here's a brief overview of the available tools for each model:

Tool

Cascaded models  
`gemini-live-2.5-flash-preview`  
`gemini-2.0-flash-live-001`

`gemini-2.5-flash-preview-native-audio-dialog`

`gemini-2.5-flash-exp-native-audio-thinking-dialog`

**Search**

Yes

Yes

Yes

**Function calling**

Yes

Yes

No

**Code execution**

Yes

No

No

**Url context**

Yes

No

No

## Function calling

Live API supports function calling, just like regular content generation requests. Function calling lets the Live API interact with external data and programs, greatly increasing what your applications can accomplish.

You can define function declarations as part of the session configuration. After receiving tool calls, the client should respond with a list of `FunctionResponse` objects using the `session.send_tool_response` method.

See the [Function calling tutorial](/gemini-api/docs/function-calling) to learn more.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

# Simple function definitions
turn_on_the_lights = {"name": "turn_on_the_lights"}
turn_off_the_lights = {"name": "turn_off_the_lights"}

tools = [{"function_declarations": [turn_on_the_lights, turn_off_the_lights]}]
config = {"response_modalities": ["TEXT"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "Turn on the lights please"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.text is not None:
                    print(chunk.text)
            elif chunk.tool_call:
                function_responses = []
                for fc in chunk.tool_call.function_calls:
                    function_response = types.FunctionResponse(
                        id=fc.id,
                        name=fc.name,
                        response={ "result": "ok" } # simple, hard-coded function response
                    )
                    function_responses.append(function_response)

                await session.send_tool_response(function_responses=function_responses)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

// Simple function definitions
const turn_on_the_lights = { name: "turn_on_the_lights" } // , description: '...', parameters: { ... }
const turn_off_the_lights = { name: "turn_off_the_lights" }

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]

const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Turn on the lights please';
  session.sendClientContent({ turns: inputTurns });

  let turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
      }
    }
    else if (turn.toolCall) {
      const functionResponses = [];
      for (const fc of turn.toolCall.functionCalls) {
        functionResponses.push({
          id: fc.id,
          name: fc.name,
          response: { result: "ok" } // simple, hard-coded function response
        });
      }

      console.debug('Sending tool response...\n');
      session.sendToolResponse({ functionResponses: functionResponses });
    }
  }

  // Check again for new messages
  turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
      }
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

From a single prompt, the model can generate multiple function calls and the code necessary to chain their outputs. This code executes in a sandbox environment, generating subsequent [BidiGenerateContentToolCall](/api/live#bidigeneratecontenttoolcall) messages.

## Asynchronous function calling

Function calling executes sequentially by default, meaning execution pauses until the results of each function call are available. This ensures sequential processing, which means you won't be able to continue interacting with the model while the functions are being run.

If you don't want to block the conversation, you can tell the model to run the functions asynchronously. To do so, you first need to add a `behavior` to the function definitions:

### Python

  ```
  # Non-blocking function definitions
  turn_on_the_lights = {"name": "turn_on_the_lights", "behavior": "NON_BLOCKING"} # turn_on_the_lights will run asynchronously
  turn_off_the_lights = {"name": "turn_off_the_lights"} # turn_off_the_lights will still pause all interactions with the model
```

### JavaScript

```
import { GoogleGenAI, Modality, Behavior } from '@google/genai';

// Non-blocking function definitions
const turn_on_the_lights = {name: "turn_on_the_lights", behavior: Behavior.NON_BLOCKING}

// Blocking function definitions
const turn_off_the_lights = {name: "turn_off_the_lights"}

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]
```

`NON-BLOCKING` ensures the function runs asynchronously while you can continue interacting with the model.

Then you need to tell the model how to behave when it receives the `FunctionResponse` using the `scheduling` parameter. It can either:

*   Interrupt what it's doing and tell you about the response it got right away (`scheduling="INTERRUPT"`),
*   Wait until it's finished with what it's currently doing (`scheduling="WHEN_IDLE"`),
*   Or do nothing and use that knowledge later on in the discussion (`scheduling="SILENT"`)
    

### Python

```
# for a non-blocking function definition, apply scheduling in the function response:
  function_response = types.FunctionResponse(
      id=fc.id,
      name=fc.name,
      response={
          "result": "ok",
          "scheduling": "INTERRUPT" # Can also be WHEN_IDLE or SILENT
      }
  )
```

### JavaScript

```
import { GoogleGenAI, Modality, Behavior, FunctionResponseScheduling } from '@google/genai';

// for a non-blocking function definition, apply scheduling in the function response:
const functionResponse = {
  id: fc.id,
  name: fc.name,
  response: {
    result: "ok",
    scheduling: FunctionResponseScheduling.INTERRUPT  // Can also be WHEN_IDLE or SILENT
  }
}
```

## Code execution

You can define code execution as part of the session configuration. This lets the Live API generate and execute Python code and dynamically perform computations to benefit your results. See the [Code execution tutorial](/gemini-api/docs/code-execution) to learn more.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

tools = [{'code_execution': {}}]
config = {"response_modalities": ["TEXT"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "Compute the largest prime palindrome under 100000."
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.text is not None:
                    print(chunk.text)

                model_turn = chunk.server_content.model_turn
                if model_turn:
                    for part in model_turn.parts:
                      if part.executable_code is not None:
                        print(part.executable_code.code)

                      if part.code_execution_result is not None:
                        print(part.code_execution_result.output)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const tools = [{codeExecution: {}}]
const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Compute the largest prime palindrome under 100000.';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
        else if (part.executableCode) {
          console.debug('executableCode: %s\n', part.executableCode.code);
        }
        else if (part.codeExecutionResult) {
          console.debug('codeExecutionResult: %s\n', part.codeExecutionResult.output);
        }
      }
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

## Grounding with Google Search

You can enable Grounding with Google Search as part of the session configuration. This increases the Live API's accuracy and prevents hallucinations. See the [Grounding tutorial](/gemini-api/docs/grounding) to learn more.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

tools = [{'google_search': {}}]
config = {"response_modalities": ["TEXT"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "When did the last Brazil vs. Argentina soccer match happen?"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.text is not None:
                    print(chunk.text)

                # The model might generate and execute Python code to use Search
                model_turn = chunk.server_content.model_turn
                if model_turn:
                    for part in model_turn.parts:
                      if part.executable_code is not None:
                        print(part.executable_code.code)

                      if part.code_execution_result is not None:
                        print(part.code_execution_result.output)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const tools = [{googleSearch: {}}]
const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'When did the last Brazil vs. Argentina soccer match happen?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
        else if (part.executableCode) {
          console.debug('executableCode: %s\n', part.executableCode.code);
        }
        else if (part.codeExecutionResult) {
          console.debug('codeExecutionResult: %s\n', part.codeExecutionResult.output);
        }
      }
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

## Combining multiple tools

You can combine multiple tools within the Live API, increasing your application's capabilities even more:

### Python

```
prompt = """
Hey, I need you to do three things for me.

1. Compute the largest prime palindrome under 100000.
2. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
3. Turn on the lights

Thanks!
"""

tools = [
    {"google_search": {}},
    {"code_execution": {}},
    {"function_declarations": [turn_on_the_lights, turn_off_the_lights]},
]

config = {"response_modalities": ["TEXT"], "tools": tools}

# ... remaining model call
```

### JavaScript

```
const prompt = `Hey, I need you to do three things for me.

1. Compute the largest prime palindrome under 100000.
2. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
3. Turn on the lights

Thanks!
`

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }
]

const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

// ... remaining model call
```

## What's next

*   Check out more examples of using tools with the Live API in the [Tool use cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).
*   Get the full story on features and configurations from the [Live API Capabilities guide](/gemini-api/docs/live-guide).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/ephemeral-tokens

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Ephemeral tokens

Ephemeral tokens are short-lived authentication tokens for accessing the Gemini API through [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API). They are designed to enhance security when you are connecting directly from a user's device to the API (a [client-to-server](/gemini-api/docs/live#implementation-approach) implementation). Like standard API keys, ephemeral tokens can be extracted from client-side applications such as web browsers or mobile apps. But because ephemeral tokens expire quickly and can be restricted, they significantly reduce the security risks in a production environment.

## How ephemeral tokens work

Here's how ephemeral tokens work at a high level:

1.  Your client (e.g. web app) authenticates with your backend.
2.  Your backend requests an ephemeral token from Gemini API's provisioning service.
3.  Gemini API issues a short-lived token.
4.  Your backend sends the token to the client for WebSocket connections to Live API. You can do this by swapping your API key with an ephemeral token.
5.  The client then uses the token as if it were an API key.

![Ephemeral tokens overview](/static/gemini-api/docs/images/Live_API_01.png)

This enhances security because even if extracted, the token is short-lived, unlike a long-lived API key deployed client-side. Since the client sends data directly to Gemini, this also improves latency and avoids your backends needing to proxy the real time data.

## Create an ephemeral token

Here is a simplified example of how to get an ephemeral token from Gemini. By default, you'll have 1 minute to start new Live API sessions using the token from this request (`newSessionExpireTime`), and 30 minutes to send messages over that connection (`expireTime`).

### Python

```
import datetime

now = datetime.datetime.now(tz=datetime.timezone.utc)

client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1, # The ephemeral token can only be used to start a single session
    'expire_time': now + datetime.timedelta(minutes=30), # Default is 30 minutes in the future
    # 'expire_time': '2025-05-17T00:00:00Z',   # Accepts isoformat.
    'new_session_expire_time': now + datetime.timedelta(minutes=1), # Default 1 minute in the future
    'http_options': {'api_version': 'v1alpha'},
  }
)

# You'll need to pass the value under token.name back to your client to use it
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

  const token: AuthToken = await client.authTokens.create({
    config: {
      uses: 1, // The default
      expireTime: expireTime // Default is 30 mins
      newSessionExpireTime: new Date(Date.now() + (1 * 60 * 1000)), // Default 1 minute in the future
      httpOptions: {apiVersion: 'v1alpha'},
    },
  });
```

For `expireTime` value constraints, defaults, and other field specs, see the [API reference](https://ai.google.dev/api/live#ephemeral-auth-tokens). Within the `expireTime` timeframe, you'll need [`sessionResumption`](/gemini-api/docs/live-session#session-resumption) to reconnect the call every 10 minutes (this can be done with the same token even if `uses: 1`).

It's also possible to lock an ephemeral token to a set of configurations. This might be useful to further improve security of your application and keep your system instructions on the server side.

### Python

```
client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1,
    'live_connect_constraints': {
        'model': 'gemini-2.0-flash-live-001',
        'config': {
            'session_resumption':{},
            'temperature':0.7,
            'response_modalities':['TEXT']
        }
    },
    'http_options': {'api_version': 'v1alpha'},
    }
)

# You'll need to pass the value under token.name back to your client to use it
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

const token = await client.authTokens.create({
    config: {
        uses: 1, // The default
        expireTime: expireTime,
        liveConnectConstraints: {
            model: 'gemini-2.0-flash-live-001',
            config: {
                sessionResumption: {},
                temperature: 0.7,
                responseModalities: ['TEXT']
            }
        },
        httpOptions: {
            apiVersion: 'v1alpha'
        }
    }
});

// You'll need to pass the value under token.name back to your client to use it
```

You can also lock a subset of fields, see the [SDK documentation](https://googleapis.github.io/python-genai/genai.html#genai.types.CreateAuthTokenConfig.lock_additional_fields) for more info.

## Connect to Live API with an ephemeral token

Here's an example that connects to Live API through an ephemeral token. Note that use of ephemeral tokens only adds value when deploying applications that follow [client-to-server implementation](/gemini-api/docs/live#implementation-approach) approach.

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

// Use the token generated in the "Create an ephemeral token" section here
const ai = new GoogleGenAI({});
const model = 'gemini-2.0-flash-live-001';
const config = { responseModalities: [Modality.TEXT] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: { ... },
  });

  // Send content...

  session.close();
}

main();
```

See [Get started with Live API](/gemini-api/docs/live) for more examples.

## Best practices

*   Set a short expiration duration using the `expire_time` parameter.
*   Tokens expire, requiring re-initiation of the provisioning process.
*   Verify secure authentication for your own backend. Ephemeral tokens will only be as secure as your backend authentication method.
*   Generally, avoid using ephemeral tokens for backend-to-Gemini connections, as this path is typically considered secure.

## Limitations

Ephemeral tokens are only compatible with [Live API](/gemini-api/docs/live) at this time.

## What's next

*   Read the Live API [reference](https://ai.google.dev/api/live#ephemeral-auth-tokens) on ephemeral tokens for more information.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/caching

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Context caching

Python JavaScript Go REST

In a typical AI workflow, you might pass the same input tokens over and over to a model. The Gemini API offers two different caching mechanisms:

*   Implicit caching (automatically enabled on Gemini 2.5 models, no cost saving guarantee)
*   Explicit caching (can be manually enabled on most models, cost saving guarantee)

Explicit caching is useful in cases where you want to guarantee cost savings, but with some added developer work.

## Implicit caching

Implicit caching is enabled by default for all Gemini 2.5 models. We automatically pass on cost savings if your request hits caches. There is nothing you need to do in order to enable this. It is effective as of May 8th, 2025. The minimum input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro.

To increase the chance of an implicit cache hit:

*   Try putting large and common contents at the beginning of your prompt
*   Try to send requests with similar prefix in a short amount of time

You can see the number of tokens which were cache hits in the response object's `usage_metadata` field.

## Explicit caching

Using the Gemini API explicit caching feature, you can pass some content to the model once, cache the input tokens, and then refer to the cached tokens for subsequent requests. At certain volumes, using cached tokens is lower cost than passing in the same corpus of tokens repeatedly.

When you cache a set of tokens, you can choose how long you want the cache to exist before the tokens are automatically deleted. This caching duration is called the _time to live_ (TTL). If not set, the TTL defaults to 1 hour. The cost for caching depends on the input token size and how long you want the tokens to persist.

This section assumes that you've installed a Gemini SDK (or have curl installed) and that you've configured an API key, as shown in the [quickstart](/gemini-api/docs/quickstart).

### Explicit caching using the OpenAI library

If you're using an [OpenAI library](/gemini-api/docs/openai), you can enable explicit caching using the `cached_content` property on [`extra_body`](/gemini-api/docs/openai#extra-body).

## When to use explicit caching

Context caching is particularly well suited to scenarios where a substantial initial context is referenced repeatedly by shorter requests. Consider using context caching for use cases such as:

*   Chatbots with extensive [system instructions](/gemini-api/docs/system-instructions)
*   Repetitive analysis of lengthy video files
*   Recurring queries against large document sets
*   Frequent code repository analysis or bug fixing

### How explicit caching reduces costs

Context caching is a paid feature designed to reduce overall operational costs. Billing is based on the following factors:

1.  **Cache token count:** The number of input tokens cached, billed at a reduced rate when included in subsequent prompts.
2.  **Storage duration:** The amount of time cached tokens are stored (TTL), billed based on the TTL duration of cached token count. There are no minimum or maximum bounds on the TTL.
3.  **Other factors:** Other charges apply, such as for non-cached input tokens and output tokens.

For up-to-date pricing details, refer to the Gemini API [pricing page](/pricing). To learn how to count tokens, see the [Token guide](/gemini-api/docs/tokens).

### Additional considerations

Keep the following considerations in mind when using context caching:

*   The _minimum_ input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro. The _maximum_ is the same as the maximum for the given model. (For more on counting tokens, see the [Token guide](/gemini-api/docs/tokens)).
*   The model doesn't make any distinction between cached tokens and regular input tokens. Cached content is a prefix to the prompt.
*   There are no special rate or usage limits on context caching; the standard rate limits for `GenerateContent` apply, and token limits include cached tokens.
*   The number of cached tokens is returned in the `usage_metadata` from the create, get, and list operations of the cache service, and also in `GenerateContent` when using the cache.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/files

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Files API

The Gemini family of artificial intelligence (AI) models is built to handle various types of input data, including text, images, and audio. Since these models can handle more than one type or _mode_ of data, the Gemini models are called _multimodal models_ or explained as having _multimodal capabilities_.

This guide shows you how to work with media files using the Files API. The basic operations are the same for audio files, images, videos, documents, and other supported file types.

For file prompting guidance, check out the [File prompt guide](/gemini-api/docs/files#prompt-guide) section.

## Upload a file

You can use the Files API to upload a media file. Always use the Files API when the total request size (including the files, text prompt, system instructions, etc.) is larger than 20 MB.

The following code uploads a file and then uses the file in a call to `generateContent`.

### Python

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
defer client.DeleteFile(ctx, file.Name)

model := client.GenerativeModel("gemini-2.0-flash")
resp, err := model.GenerateContent(ctx,
    genai.FileData{URI: file.URI},
    genai.Text("Describe this audio clip"))
if err != nil {
    log.Fatal(err)
}

printResponse(resp)
```

### REST

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D "${tmp_header_file}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Get metadata for a file

You can verify that the API successfully stored the uploaded file and get its metadata by calling `files.get`.

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
file_name = myfile.name
myfile = client.files.get(name=file_name)
print(myfile)
```

### JavaScript

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
const fetchedFile = await ai.files.get({ name: fileName });
console.log(fetchedFile);
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}

gotFile, err := client.GetFile(ctx, file.Name)
if err != nil {
    log.Fatal(err)
}
fmt.Println("Got file:", gotFile.Name)
```

### REST

```
# file_info.json was created in the upload example
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name \
-H "x-goog-api-key: $GEMINI_API_KEY" > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

## List uploaded files

You can upload multiple files using the Files API. The following code gets a list of all the files uploaded:

### Python

```
print('My files:')
for f in client.files.list():
    print(' ', f.name)
```

### JavaScript

```
const listResponse = await ai.files.list({ config: { pageSize: 10 } });
for await (const file of listResponse) {
  console.log(file.name);
}
```

### Go

```
iter := client.ListFiles(ctx)
for {
    ifile, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(ifile.Name)
}
```

### REST

```
echo "My files: "

curl "https://generativelanguage.googleapis.com/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## Delete uploaded files

Files are automatically deleted after 48 hours. You can also manually delete an uploaded file:

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
client.files.delete(name=myfile.name)
```

### JavaScript

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
await ai.files.delete({ name: fileName });
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
client.DeleteFile(ctx, file.Name)
```

### REST

```
curl --request "DELETE" https://generativelanguage.googleapis.com/v1beta/files/$name \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## Usage info

You can use the Files API to upload and interact with media files. The Files API lets you store up to 20 GB of files per project, with a per-file maximum size of 2 GB. Files are stored for 48 hours. During that time, you can use the API to get metadata about the files, but you can't download the files. The Files API is available at no cost in all regions where the Gemini API is available.

## File prompting strategies

This section provides guidance and best practices for using media files with prompts for the Gemini API.

Being able to use various types of data in your prompts gives you more flexibility in terms of what tasks you can tackle with the Gemini API. For example, you can send the model a photo of a delicious meal and ask it to write a short blog about the meal.

**Prompt**

**Response**

Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.

  
![Cookies](/static/gemini-api/docs/images/image_prompt.png)

Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time.

This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option.

This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings!

If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied.

Here's to healthy and delicious meal prepping!

If you are having trouble getting the output you want from prompts that use media files, there are some strategies that can help you get the results you want. The following sections provide design approaches and troubleshooting tips for improving prompts that use multimodal input.

You can improve your multimodal prompts by following these best practices:

*   ### [Prompt design fundamentals](#specific-instructions)
    
    *   **Be specific in your instructions**: Craft clear and concise instructions that leave minimal room for misinterpretation.
    *   **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.
    *   **Break it down step-by-step**: Divide complex tasks into manageable sub-goals, guiding the model through the process.
    *   **Specify the output format**: In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more.
    *   **Put your image first for single-image prompts**: While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image (or video) is placed before the text prompt. However, for prompts that require images to be highly interleaved with texts to make sense, use whatever order is most natural.
*   ### [Troubleshooting your multimodal prompt](#troubleshooting)
    
    *   **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.
    *   **If the model output is too generic (not tailored enough to the image/video input):** At the start of the prompt, try asking the model to describe the image(s) or video before providing the task instruction, or try asking the model to refer to what's in the image.
    *   **To troubleshoot which part failed:** Ask the model to describe the image, or ask the model to explain its reasoning, to gauge the model's initial understanding.
    *   **If your prompt results in hallucinated content:** Try dialing down the temperature setting or asking the model for shorter descriptions so that it's less likely to extrapolate additional details.
    *   **Tuning the sampling parameters:** Experiment with different temperature settings and top-k selections to adjust the model's creativity.

### Be specific in your instructions

Prompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want.

For this image of an airport board, asking the model to just "describe this image" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt.

Prompt

Model response

![](/static/gemini-api/docs/images/timetable.png)

Describe this image.

The image shows an airport arrivals and departures board.

**Updated prompt**

**Improved response**

![Travel timetable](/static/gemini-api/docs/images/timetable.png)

Parse the time and city from the airport board shown in this image into a list.

*   10:50 Moscow
*   11:05 Edinburgh
*   11:05 London
*   11:10 Bucharest
*   11:30 Kiev
*   11:35 Dublin
*   11:45 East Midlands
*   12:15 Sofia
*   12:30 London
*   12:30 Newcastle
*   12:40 St Petersburg
*   12:40 London
*   12:45 Manchester

### Add a few examples

The Gemini model can accept multiple inputs which it can use as examples to understand the output you want. Adding these examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. This is also called "few-shot" learning.

In the example below, the initial output is written in sentence form, and also contains the country (Brazil). Suppose you want the output in a different format or style, and you want only the city, not the country. Adding few-shot examples to your prompt can steer the model to respond in the way you want.

**Prompt**

**Model response**

![Travel timetable](/static/gemini-api/docs/images/redeemer.png)

Determine the city along with the landmark.

The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.

**Updated prompt**

**Improved response**

Determine the city along with the landmark.

![colosseum](/static/gemini-api/docs/images/colosseum.png)

city: Rome, landmark: the Colosseum.

![Forbidden City](/static/gemini-api/docs/images/forbidden-city.png)

city: Beijing, landmark: Forbidden City

![forbidden-city](/static/gemini-api/docs/images/redeemer.png)

city: Rio de Janeiro, landmark: Christ the Redeemer statue

### Break it down step-by-step

For complex tasks like the ones that require both visual understanding and reasoning, it can be helpful to split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you directly ask the model to “think step by step” in your prompt.

Prompt

Model response

![](/static/gemini-api/docs/images/toilet-paper.jpg)

When will I run out of toilet paper?

Soon, you only have 3 rolls left.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/toilet-paper.jpg)

1\. First, count how many toilet paper rolls are in this picture.

2\. Then, determine how much toilet paper a typical person uses per day.

3\. Calculate how long these rolls of toilet paper will last.

1\. There are 3 rolls of toilet paper in this picture.

2\. A typical person uses about 20 sheets of toilet paper per day.

3\. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.

Math problems or other types of word problems are great candidates for asking the model to think step-by-step.

**Prompt**

**Response**

![](/static/gemini-api/docs/images/math.jpg)

What is the 4th term in the sequence?

\-135

The response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt.

**Prompt**

**Response**

![](/static/gemini-api/docs/images/math.jpg)

Parse the formula in the image first.

Then based on the formula, answer "what's the 4th term in the sequence?".

_Click to expand the result_

$$ b(n) = b(n-1) \* (-3) $$

$$ b(1) = 15 $$

1.  Substitute n = 4 into the formula:

$$ b(4) = b(4-1) \* (-3) $$

$$ b(4) = b(3) \* (-3) $$

4.  We don't know the value of b(3), so substitute n = 3 into the formula:

$$ b(3) = b(3-1) \* (-3) $$

$$ b(3) = b(2) \* (-3) $$

7.  We don't know the value of b(2), so substitute n = 2 into the formula:

$$ b(2) = b(2-1) \* (-3) $$

$$ b(2) = b(1) \* (-3) $$

10.  We know the value of b(1) is 15, so substitute it into the formula:

$$ b(2) = 15 \* (-3) $$

$$ b(2) = -45 $$

13.  Now that we know the value of b(2), substitute it into the formula for b(3):

$$ b(3) = b(2) \* (-3) $$

$$ b(3) = -45 \* (-3) $$

$$ b(3) = 135 $$

17.  Now that we know the value of b(3), substitute it into the formula for b(4):

$$ b(4) = b(3) \* (-3) $$

$$ b(4) = 135 \* (-3) $$

$$ b(4) = -405 $$

Therefore, the 4th term in the sequence is -405.

This model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/math.jpg)

What is the 4th term in the sequence? Think step by step.

The first term is 15.

The second term is b(1) \* (-3) = 15 \* (-3) = -45.

The third term is b(2) \* (-3) = -45 \* (-3) = 135.

The fourth term is b(3) \* (-3) = 135 \* (-3) = -405.

### Specify the output format

A common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples:

Prompt

Model response

![](/static/gemini-api/docs/images/excel-example.png)

Parse the table in this image into markdown format

| Subject | Status | Time | Start date | Due on |

| ------- | ------ | ---- | ---------- | ------ |

| Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 |

| Geography | Done | 30 min | 12/2/2023 | 12/4/2023 |

| Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 |

| Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |

**Prompt**

**Model response**

![](/static/gemini-api/docs/images/spanish-meal.png)

Provide a list of all the following attributes:

ingredients, type of cuisine, vegetarian or not, in JSON format

` ```json `

`   {  "ingredients": [  "rice",  "shrimp",  "clams",  "mussels",  "peas",  "tomatoes",  "onions",  "garlic",  "olive oil",  "paprika",  "salt",  "pepper"  ],  "type of cuisine": "Spanish",  "vegetarian": false  }  ```   `

### Put your image first for single-image prompts

While Gemini can interpret image and text in any order within a prompt, placing a single image before the text prompt might lead to better results. In the following samples, you'll notice the image comes first before the text in each prompt.

### Troubleshooting your multimodal prompt

You might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try.

#### If the model is not drawing information from the relevant part of the image

To get a more specific response, you can point out which aspects of the image you want the prompt to draw information from to inform its response.

Prompt

Model response

![](/static/gemini-api/docs/images/diapers.png)

How many days will these diapers last a baby?

The diapers will last for 198 days before they run out.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/diapers.png)

How long will these diapers last before I run out?

Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box.

Divide the total number by how many diapers the child goes through per day.

The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs.

The average child in this weight range is likely between 1 and 2 years old.

A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).

#### If the model output is too generic and not tailored enough to the image input

To help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task.

Prompt

Model response

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

What is in common between these images?

All four images are examples of landscape photography.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

First, describe what's in each image in detail. What's in common between these images?

The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea.

The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity.

The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world.

The fourth image shows the sea in between being calm and being stormy.

Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.

You can also try explicitly asking the model to refer to what's in the image.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

What is in common between these images? Refer to what's in the images in your response.

All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea.

These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling.

The same tips work for creative output from images. In the following example with the rabbit, explicitly asking the model to include aspects from the image can improve the output.

Prompt

Model response

![](/static/gemini-api/docs/images/rabbit.jpg)

Can you write me a poem about this image?

Little bunny, so soft and brown,  
Hopping through the field, up and down.  
Your ears so long, your tail so small,  
You're the cutest creature of them all.

You nibble on the grass, so green and sweet,  
And hop away when you see me.  
But I'll always be your friend,  
Little bunny, until the very end.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/rabbit.jpg)

Can you write me a descriptive and dramatic poem about this image and include the location?

Little bunny on the green,  
Ears so long, so keen.  
What do you see out there,  
In the distance, where you stare?

Is it the ocean blue,  
Sparkling in the sun's hue?  
Or the waves that crash and roar,  
A rhythm that you adore?

Little bunny, tell me true,  
What captures your attention so?  
Is it the salty breeze,  
Or the seagulls' cries that please?

Perhaps it's the vastness of it all,  
A world so big and small.  
A place where dreams can roam,  
And adventures can be sown.

Little bunny, I wonder what you dream,  
As you sit on the grass, so serene.  
Do you yearn to explore the deep,  
Or stay on land, where you can leap?

Whatever it may be, little bunny,  
Keep that spark of wonder burning brightly.  
For in your dreams and aspirations,  
Lies a world waiting for your creation.

#### Troubleshooting which part of the prompt failed

It can be hard to know whether a prompt failed because the model didn't **understand the image** to begin with, or if it did understand the image but did not perform the correct **reasoning steps** afterward. To disambiguate between those reasons, ask the model to describe what's in the image.

In the following example, if the model responds with a snack that seems surprising when paired with tea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized that the image contains tea.

Prompt

Prompt for troubleshooting

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this?

![](/static/gemini-api/docs/images/tea-set.png)

Describe what's in this image.

Another strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any.

Prompt

Prompt for troubleshooting

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this?

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this? Please explain why.

## What's next

*   Try writing your own multimodal prompts using [Google AI Studio](http://aistudio.google.com).
*   For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](/gemini-api/docs/vision), [Audio](/gemini-api/docs/audio), and [Document processing](/gemini-api/docs/document-processing) guides.
*   For more guidance on prompt design, like tuning sampling parameters, see the [Prompt strategies](/gemini-api/docs/prompting-strategies) page.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/embeddings

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Embeddings

The Gemini API supports several embedding models that generate embeddings for words, phrases, code, and sentences. The resulting embeddings can then be used for tasks such as semantic search, text classification, and clustering, among many others.

## What are embeddings?

Embeddings are numerical representations of text (or other media formats) that capture relationships between inputs. Text embeddings work by converting text into arrays of floating point numbers, called _vectors_. These vectors are designed to capture the meaning of the text. The length of the embedding array is called the vector's _dimensionality_. A passage of text might be represented by a vector containing hundreds of dimensions.

Embeddings capture semantic meaning and context, which results in text with similar meanings having "closer" embeddings. For example, the sentence "I took my dog to the vet" and "I took my cat to the vet" would have embeddings that are close to each other in the vector space.

You can use embeddings to compare different texts and understand how they relate. For example, if the embeddings of the text "cat" and "dog" are close together you can infer that these words are similar in meaning, context, or both. This enables a variety of [common AI use cases](/gemini-api/docs/embeddings#use-cases).

### Before you begin

Before calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.

## Generate embeddings

Use the `embedContent` method to generate text embeddings:

### Python

```
from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-exp-03-07",
        contents="What is the meaning of life?")

print(result.embeddings)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-exp-03-07',
        contents: 'What is the meaning of life?',
    });

    console.log(response.embeddings);
}

main();
```

### Go

```
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
    }
    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-exp-03-07",
        contents,
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }

    embeddings, err := json.MarshalIndent(result.Embeddings, "", "  ")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(string(embeddings))
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-exp-03-07",
     "content": {
     "parts":[{
     "text": "What is the meaning of life?"}]}
    }'
```

You can also generate embeddings for multiple chunks at once by passing them in as a list of strings.

## Task types

When building Retrieval Augmented Generation (RAG) systems, a common design is to use text embeddings to perform a similarity search. In some cases this can lead to degraded quality, because questions and their answers are not semantically similar. For example, a question like "Why is the sky blue?" and its answer "The scattering of sunlight causes the blue color," have distinctly different meanings as statements, which means that a RAG system won't automatically recognize their relation.

Task types enable you to generate optimized embeddings for specific tasks, saving you time and cost and improving performance.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-exp-03-07",
        contents="What is the meaning of life?",
        config=types.EmbedContentConfig(task_type="SEMANTIC_SIMILARITY")
)
print(result.embeddings)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-exp-03-07',
        contents: 'What is the meaning of life?',
        config: {
            taskType: "SEMANTIC_SIMILARITY",
        }
    });

    console.log(response.embeddings);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-exp-03-07",
     "content": {
     "parts":[{
     "text": "What is the meaning of life?"}]},
     "taskType": "SEMANTIC_SIMILARITY"
    }'
```

### Supported task types

Task type

Description

`SEMANTIC_SIMILARITY`

Used to generate embeddings that are optimized to assess text similarity.

`CLASSIFICATION`

Used to generate embeddings that are optimized to classify texts according to preset labels.

`CLUSTERING`

Used to generate embeddings that are optimized to cluster texts based on their similarities.

`RETRIEVAL_DOCUMENT`, `RETRIEVAL_QUERY`, `QUESTION_ANSWERING`, and `FACT_VERIFICATION`

Used to generate embeddings that are optimized for document search or information retrieval.

`CODE_RETRIEVAL_QUERY`

Used to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using `RETRIEVAL_DOCUMENT`.

## Use cases

Text embeddings are used in a variety of common AI use cases, such as:

*   **Information retrieval:** You can use embeddings to retrieve semantically similar text given a piece of input text.
    
    [Document search tutorialtask](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/document_search.ipynb)
    
*   **Clustering:** Comparing groups of embeddings can help identify hidden trends.
    
    [Embedding clustering tutorialbubble\_chart](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/clustering_with_embeddings.ipynb)
    
*   **Vector database:** As you take different embedding use cases to production, it is common to store embeddings in a vector database.
    
    [Vector database tutorialbolt](https://github.com/google-gemini/cookbook/blob/main/examples/chromadb/Vectordb_with_chroma.ipynb)
    
*   **Classification:** You can train a model using embeddings to classify documents into categories.
    
    [Classification tutorialtoken](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/text_classifier_embeddings.ipynb)
    

## Embedding models

The Gemini API offers three models that generate text embeddings:

*   [gemini-embedding-exp-03-07](/gemini-api/docs/models#gemini-embedding)
*   [text-embedding-004](/gemini-api/docs/models/gemini#text-embedding)
*   [embedding-001](/gemini-api/docs/models/gemini#embedding)

## What's next

Check out the [embeddings quickstart notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/tokens

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Understand and count tokens

Python JavaScript Go

  

Gemini and other generative AI models process input and output at a granularity called a _token_.

## About tokens

Tokens can be single characters like `z` or whole words like `cat`. Long words are broken up into several tokens. The set of all tokens used by the model is called the vocabulary, and the process of splitting text into tokens is called _tokenization_.

For Gemini models, a token is equivalent to about 4 characters. 100 tokens is equal to about 60-80 English words.

When billing is enabled, the [cost of a call to the Gemini API](/pricing) is determined in part by the number of input and output tokens, so knowing how to count tokens can be helpful.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-05-12 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/safety-settings

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Safety settings

The Gemini API provides safety settings that you can adjust during the prototyping stage to determine if your application requires more or less restrictive safety configuration. You can adjust these settings across five filter categories to restrict or allow certain types of content.

This guide covers how the Gemini API handles safety settings and filtering and how you can change the safety settings for your application.

## Safety filters

The Gemini API's adjustable safety filters cover the following categories:

Category

Description

Harassment

Negative or harmful comments targeting identity and/or protected attributes.

Hate speech

Content that is rude, disrespectful, or profane.

Sexually explicit

Contains references to sexual acts or other lewd content.

Dangerous

Promotes, facilitates, or encourages harmful acts.

Civic integrity

Election-related queries.

You can use these filters to adjust what's appropriate for your use case. For example, if you're building video game dialogue, you may deem it acceptable to allow more content that's rated as _Dangerous_ due to the nature of the game.

In addition to the adjustable safety filters, the Gemini API has built-in protections against core harms, such as content that endangers child safety. These types of harm are always blocked and cannot be adjusted.

### Content safety filtering level

The Gemini API categorizes the probability level of content being unsafe as `HIGH`, `MEDIUM`, `LOW`, or `NEGLIGIBLE`.

The Gemini API blocks content based on the probability of content being unsafe and not the severity. This is important to consider because some content can have low probability of being unsafe even though the severity of harm could still be high. For example, comparing the sentences:

1.  The robot punched me.
2.  The robot slashed me up.

The first sentence might result in a higher probability of being unsafe, but you might consider the second sentence to be a higher severity in terms of violence. Given this, it is important that you carefully test and consider what the appropriate level of blocking is needed to support your key use cases while minimizing harm to end users.

### Safety filtering per request

You can adjust the safety settings for each request you make to the API. When you make a request, the content is analyzed and assigned a safety rating. The safety rating includes the category and the probability of the harm classification. For example, if the content was blocked due to the harassment category having a high probability, the safety rating returned would have category equal to `HARASSMENT` and harm probability set to `HIGH`.

By default, safety settings block content (including prompts) with medium or higher probability of being unsafe across any filter. This baseline safety is designed to work for most use cases, so you should only adjust your safety settings if it's consistently required for your application.

The following table describes the block settings you can adjust for each category. For example, if you set the block setting to **Block few** for the **Hate speech** category, everything that has a high probability of being hate speech content is blocked. But anything with a lower probability is allowed.

Threshold (Google AI Studio)

Threshold (API)

Description

Block none

`BLOCK_NONE`

Always show regardless of probability of unsafe content

Block few

`BLOCK_ONLY_HIGH`

Block when high probability of unsafe content

Block some

`BLOCK_MEDIUM_AND_ABOVE`

Block when medium or high probability of unsafe content

Block most

`BLOCK_LOW_AND_ABOVE`

Block when low, medium or high probability of unsafe content

N/A

`HARM_BLOCK_THRESHOLD_UNSPECIFIED`

Threshold is unspecified, block using default threshold

If the threshold is not set, the default block threshold is **Block none** (for `gemini-1.5-pro-002` and `gemini-1.5-flash-002` and all newer stable GA models) or **Block some** (in all other models) for all categories **except** the _Civic integrity_ category.

The default block threshold for the _Civic integrity_ category is **Block none** (for `gemini-2.0-flash-001` aliased as `gemini-2.0-flash`, `gemini-2.0-pro-exp-02-05`, and `gemini-2.0-flash-lite`) both for Google AI Studio and the Gemini API, and **Block most** for all other models in Google AI Studio only.

You can set these settings for each request you make to the generative service. See the [`HarmBlockThreshold`](/api/generate-content#harmblockthreshold) API reference for details.

### Safety feedback

[`generateContent`](/api/generate-content#method:-models.generatecontent) returns a [`GenerateContentResponse`](/api/generate-content#generatecontentresponse) which includes safety feedback.

Prompt feedback is included in [`promptFeedback`](/api/generate-content#promptfeedback). If `promptFeedback.blockReason` is set, then the content of the prompt was blocked.

Response candidate feedback is included in [`Candidate.finishReason`](/api/generate-content#candidate) and [`Candidate.safetyRatings`](/api/generate-content#candidate). If response content was blocked and the `finishReason` was `SAFETY`, you can inspect `safetyRatings` for more details. The content that was blocked is not returned.

## Adjust safety settings

This section covers how to adjust the safety settings in both Google AI Studio and in your code.

### Google AI Studio

You can adjust safety settings in Google AI Studio, but you cannot turn them off.

Click **Edit safety settings** in the **Run settings** panel to open the **Run safety settings** modal. In the modal, you can use the sliders to adjust the content filtering level per safety category:

![](/static/gemini-api/docs/images/safety_settings_ui.png)

When you send a request (for example, by asking the model a question), a warning **No Content** message appears if the request's content is blocked. To see more details, hold the pointer over the **No Content** text and click warning **Safety**.

### Gemini API SDKs

The following code snippet shows how to set safety settings in your `GenerateContent` call. This sets the thresholds for the harassment (`HARM_CATEGORY_HARASSMENT`) and hate speech (`HARM_CATEGORY_HATE_SPEECH`) categories. For example, setting these categories to `BLOCK_LOW_AND_ABOVE` blocks any content that has a low or higher probability of being harassment or hate speech. To understand the threshold settings, see [Safety filtering per request](#safety-filtering-per-request).

### Python

```
from google import genai
from google.genai import types

import PIL.Image

img = PIL.Image.open("cookies.jpg")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=['Do these look store-bought or homemade?', img],
    config=types.GenerateContentConfig(
      safety_settings=[
        types.SafetySetting(
            category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
        ),
      ]
    )
)

print(response.text)
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        SafetySettings: []*genai.SafetySetting{
            {
                Category:  "HARM_CATEGORY_HATE_SPEECH",
                Threshold: "BLOCK_LOW_AND_ABOVE",
            },
        },
    }

    response, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.0-flash",
        genai.Text("Some potentially unsafe prompt."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(response.Text())
}
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const safetySettings = [
  {
    category: "HARM_CATEGORY_HARASSMENT",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
  {
    category: "HARM_CATEGORY_HATE_SPEECH",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
];

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Some potentially unsafe prompt.",
    config: {
      safetySettings: safetySettings,
    },
  });
  console.log(response.text);
}

await main();
```

### Dart (Flutter)

```
final safetySettings = [
  SafetySetting(HarmCategory.harassment, HarmBlockThreshold.low),
  SafetySetting(HarmCategory.hateSpeech, HarmBlockThreshold.low),
];
final model = GenerativeModel(
  model: 'gemini-1.5-flash',
  apiKey: apiKey,
  safetySettings: safetySettings,
);
```

### Kotlin

```
val harassmentSafety = SafetySetting(HarmCategory.HARASSMENT, BlockThreshold.LOW_AND_ABOVE)

val hateSpeechSafety = SafetySetting(HarmCategory.HATE_SPEECH, BlockThreshold.LOW_AND_ABOVE)

val generativeModel = GenerativeModel(
    modelName = "gemini-1.5-flash",
    apiKey = BuildConfig.apiKey,
    safetySettings = listOf(harassmentSafety, hateSpeechSafety)
)
```

### Java

```
SafetySetting harassmentSafety = new SafetySetting(HarmCategory.HARASSMENT,
    BlockThreshold.LOW_AND_ABOVE);

SafetySetting hateSpeechSafety = new SafetySetting(HarmCategory.HATE_SPEECH,
    BlockThreshold.LOW_AND_ABOVE);

GenerativeModel gm = new GenerativeModel(
    "gemini-1.5-flash",
    BuildConfig.apiKey,
    null, // generation config is optional
    Arrays.asList(harassmentSafety, hateSpeechSafety)
);

GenerativeModelFutures model = GenerativeModelFutures.from(gm);
```

### REST

```
echo '{    "safetySettings": [        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}    ],    "contents": [{        "parts":[{            "text": "'I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them.'"}]}]}' > request.json

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \        -H "x-goog-api-key: $GEMINI_API_KEY" \

    -H 'Content-Type: application/json' \
    -X POST \
    -d @request.json 2> /dev/null
```

## Next steps

*   See the [API reference](/api) to learn more about the full API.
*   Review the [safety guidance](/gemini-api/docs/safety-guidance) for a general look at safety considerations when developing with LLMs.
*   Learn more about assessing probability versus severity from the [Jigsaw team](https://developers.perspectiveapi.com/s/about-the-api-score)
*   Learn more about the products that contribute to safety solutions like the [Perspective API](https://medium.com/jigsaw/reducing-toxicity-in-large-language-models-with-perspective-api-c31c39b7a4d7). \* You can use these safety settings to create a toxicity classifier. See the [classification example](/examples/train_text_classifier_embeddings) to get started.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/model-tuning

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# כוונון בעזרת Gemini API

אסטרטגיות לתכנון הנחיות, כמו הנחיות ב-few-shot, לא תמיד מניבות את התוצאות שאתם צריכים. _התאמה אישית_ היא תהליך שיכול לשפר את הביצועים של המודל במשימות ספציפיות, או לעזור למודל לעמוד בדרישות ספציפיות של תוצאות כשההוראות לא מספיקות ויש לכם קבוצה של דוגמאות שממחישות את התוצאות הרצויות.

בדף הזה נספק סקירה כללית קונספטואלית של כוונון דגם הטקסט שמאחורי שירות הטקסט של Gemini API. כשתהיו מוכנים להתחיל לבצע שינויים, תוכלו לעיין ב[מדריך לשיפור נוסף](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?hl=he). למבוא כללי יותר להתאמה אישית של מודלים גדולים של שפה (LLM) לתרחישי שימוש ספציפיים, תוכלו לקרוא את המאמר [מודלים גדולים של שפה (LLM): כוונון, זיקוק ותכנון של הנחיות](https://developers.google.com/machine-learning/crash-course/llm/tuning?hl=he) ב[קורס המקוצר על למידת מכונה](https://developers.google.com/machine-learning/crash-course/?hl=he).

## איך פועלת ההתאמה האישית

מטרת השיפור הוא לשפר את ביצועי המודל למשימה הספציפית שלכם. כדי לבצע שינוי מדויק, מספקים למודל מערך נתונים לאימון שמכיל הרבה דוגמאות למשימה. במשימות נישה, אפשר לשפר באופן משמעותי את ביצועי המודל על ידי התאמת המודל למספר מצומצם של דוגמאות. לפעמים קוראים לסוג הזה של כוונון מודל _כוונון עדין בפיקוח_, כדי להבדיל אותו מסוגים אחרים של כוונון עדין.

נתוני האימון צריכים להיות בנויים כדוגמאות עם קלט של הנחיות ופלט של תשובות צפויות. אפשר גם לשפר את המודלים באמצעות נתוני דוגמה ישירות ב-Google AI Studio. המטרה היא ללמד את המודל לחקות את ההתנהגות או את המשימה הרצויה, על ידי מתן דוגמאות רבות שממחישות את ההתנהגות או את המשימה הזו.

כשמריצים משימה של כוונון, המודל לומד פרמטרים נוספים שעוזרים לו לקודד את המידע הנדרש לביצוע המשימה הרצויה או ללמוד את ההתנהגות הרצויה. לאחר מכן אפשר להשתמש בפרמטרים האלה בזמן ההסקה. הפלט של משימה כוונון הוא מודל חדש, שהוא למעשה שילוב של הפרמטרים שנלמדו לאחרונה והמודל המקורי.

## הכנת מערך הנתונים

לפני שמתחילים לבצע שיפורים, צריך מערך נתונים שבעזרתו אפשר לשפר את המודל. כדי ליהנות מהביצועים הכי טובים, הדוגמאות במערך הנתונים צריכות להיות איכותיות, מגוונות ומייצגות את הקלט והפלט האמיתיים.

### פורמט

הדוגמאות שכלולות במערך הנתונים צריכות להתאים לתנועה הצפויה בסביבת הייצור. אם מערך הנתונים מכיל פורמט, מילות מפתח, הוראות או מידע ספציפיים, הפורמט של נתוני הייצור צריך להיות זהה ולכלול את אותן ההוראות.

לדוגמה, אם הדוגמאות במערך הנתונים כוללות את השדות `"question:"` ו-`"context:"`, גם התנועה בסביבת הייצור צריכה להיות בפורמט שכולל את השדות `"question:"` ו-`"context:"` באותו הסדר שבו הם מופיעים בדוגמאות במערך הנתונים. אם תחרגו את ההקשר, המודל לא יוכל לזהות את הדפוס, גם אם השאלה המדויקת הופיעה בדוגמה במערך הנתונים.

דוגמה נוספת: אלה נתוני אימון ב-Python לאפליקציה שיוצרת את המספר הבא ברצף:

```
training_data = [
  {"text_input": "1", "output": "2"},
  {"text_input": "3", "output": "4"},
  {"text_input": "-3", "output": "-2"},
  {"text_input": "twenty two", "output": "twenty three"},
  {"text_input": "two hundred", "output": "two hundred one"},
  {"text_input": "ninety nine", "output": "one hundred"},
  {"text_input": "8", "output": "9"},
  {"text_input": "-98", "output": "-97"},
  {"text_input": "1,000", "output": "1,001"},
  {"text_input": "10,100,000", "output": "10,100,001"},
  {"text_input": "thirteen", "output": "fourteen"},
  {"text_input": "eighty", "output": "eighty one"},
  {"text_input": "one", "output": "two"},
  {"text_input": "three", "output": "four"},
  {"text_input": "seven", "output": "eight"},
]
```

הוספת הנחיה או מבוא לכל דוגמה במערך הנתונים יכולה גם היא לשפר את הביצועים של המודל המכוונן. הערה: אם הנחיה או מבוא כלולים בקבוצת הנתונים, צריך לכלול אותם גם בהנחיה שמוצגת למודל המכוונן בזמן ההסקה.

### מגבלות

**הערה:** לערכות נתונים של כוונון מדויק ב-Gemini 1.5 Flash יש את המגבלות הבאות:

*   הגודל המקסימלי של קלט לכל דוגמה הוא 40,000 תווים.
*   הגודל המקסימלי של הפלט לכל דוגמה הוא 5,000 תווים.

### גודל נתוני האימון

אפשר לבצע כוונון עדין של מודל באמצעות 20 דוגמאות בלבד. בדרך כלל, נתונים נוספים משפרים את איכות התשובות. מומלץ לטרגט בין 100 ל-500 דוגמאות, בהתאם לאפליקציה. בטבלה הבאה מפורטים גדלים מומלצים של מערכי נתונים לצורך כוונון מדויק של מודל טקסט למשימות נפוצות שונות:

משימה

מספר הדוגמאות במערך הנתונים

סיווג

100+

סיכום

100-500+

חיפוש מסמכים

100+

## העלאת מערך הנתונים לכוונון

הנתונים מועברים בתוך שורת קוד באמצעות ה-API או דרך קבצים שהועלו ב-Google AI Studio.

כדי להשתמש בספריית הלקוח, צריך לספק את קובץ הנתונים בקריאה ל-`createTunedModel`. גודל הקובץ המקסימלי הוא 4MB. כדי להתחיל, מומלץ לעיין ב[מדריך למתחילים בנושא כוונון מדויק באמצעות Python](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python&hl=he).

כדי לקרוא ל-API ל-REST באמצעות cURL, צריך לספק דוגמאות לאימון בפורמט JSON לארגומנט `training_data`. כדי להתחיל, אפשר לעיין ב[מדריך למתחילים בנושא התאמה אישית באמצעות cURL](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=rest&hl=he).

## הגדרות כיוונון מתקדמות

כשיוצרים משימה של כוונון, אפשר לציין את ההגדרות המתקדמות הבאות:

*   **Epochs:** מעבר אימון מלא על כל קבוצת האימון, כך שכל דוגמה עוברת עיבוד פעם אחת.
*   **Batch size**:קבוצת הדוגמאות שנעשה בהן שימוש ב[מחזור](https://developers.google.com/machine-learning/glossary?hl=he#iteration) אחד של אימון. גודל האצווה קובע את מספר הדוגמאות באצווה.
*   **שיעור למידה:** מספר עם נקודה צפה שמציין לאלגוריתם את מידת ההתאמה של פרמטרים של המודל בכל חזרה. לדוגמה, שיעור למידה של 0.3 יתאים את המשקלים וההטיות בצורה חזקה פי שלושה מאשר שיעור למידה של 0.1. לשיעורי למידה גבוהים ונמוכים יש יתרונות וחסרונות ייחודיים משלהם, וצריך לשנות אותם בהתאם לתרחיש לדוגמה.
*   **מכפיל של קצב הלמידה:** מכפיל הקצב משנה את קצב הלמידה המקורי של המודל. כשהערך הוא 1, המערכת משתמשת בשיעור הלמידה המקורי של המודל. ערכים גדולים מ-1 מגדילים את קצב הלמידה, וערכים בין 1 ל-0 מקטינים את קצב הלמידה.

### הגדרות מומלצות

בטבלה הבאה מפורטות ההגדרות המומלצות לשיפור מודל הבסיס:

היפר-פרמטר

ערך ברירת מחדל

התאמות מומלצות

Epoch

5

אם האובדן מתחיל להגיע לשיא לפני 5 מחזורי אימון, צריך להשתמש בערך קטן יותר.

אם ההפסד מתקרב לערך מסוים ולא נראה שהוא יגיע לשיא, צריך להשתמש בערך גבוה יותר.

גודל האצווה

4

קצב למידה

0.001

כדאי להשתמש בערך קטן יותר למערכי נתונים קטנים יותר.

בעקומת האובדן מוצגת מידת ההטיה של התחזית של המודל מהתחזיות האידיאליות בדוגמאות האימון אחרי כל תקופת אימון. באופן אידיאלי, כדאי להפסיק את האימון בנקודה הנמוכה ביותר בגרף, ממש לפני שהוא מגיע לפסגה. לדוגמה, בגרף הבא אפשר לראות שגרף האובדן מגיע לשיא בסביבות 4-6 מחזורים, כלומר אפשר להגדיר את הפרמטר `Epoch` ל-4 ועדיין לקבל את אותם ביצועים.

![תרשים קו שבו מוצגת עקומת ההפסד של המודל. יש עלייה חדה בקו בין התקופה הראשונה לתקופה השנייה, ואז ירידה חדה כמעט ל-0, ואז הוא מתייצב אחרי שלוש תקופות.](https://ai.google.dev/static/docs/images/loss_curve.png?hl=he)

## בדיקת הסטטוס של משימת ההתאמה

אפשר לבדוק את סטטוס המשימה של כוונון המודל ב-Google AI Studio בכרטיסייה **My Library**, או באמצעות המאפיין `metadata` של המודל המכוונן ב-Gemini API.

## פתרון בעיות במקרה של שגיאות

בקטע הזה מפורטות דרכים לפתרון שגיאות שעשויות להתרחש במהלך יצירת המודל המכוונן.

### אימות

כדי לבצע התאמה באמצעות ה-API וספריית הלקוח, נדרש אימות. אפשר להגדיר אימות באמצעות מפתח API (מומלץ) או באמצעות פרטי כניסה ל-OAuth. במאמר [הגדרת מפתח API](https://ai.google.dev/gemini-api/docs/quickstart?hl=he#set-up-api-key) מוסבר איך מגדירים מפתח API.

אם מופיעה השגיאה `'PermissionDenied: 403 Request had insufficient authentication scopes'`, יכול להיות שתצטרכו להגדיר אימות משתמשים באמצעות פרטי כניסה של OAuth. כדי להגדיר את פרטי הכניסה ל-OAuth ב-Python, אפשר לעיין [במדריך להגדרת OAuth](https://ai.google.dev/gemini-api/docs/oauth?hl=he).

### מודלים שבוטלו

אפשר לבטל את משימת השיפור בכל שלב לפני שהיא מסתיימת. עם זאת, אי אפשר לחזות את ביצועי ההסקה של מודל שהופסק, במיוחד אם משימה הכיוון מבוטלת בשלב מוקדם של האימון. אם ביטלתם את האימון כי רציתם להפסיק אותו בשלב מוקדם יותר, עליכם ליצור משימה חדשה של כוונון ולהגדיר את השלב (epoch) לערך נמוך יותר.

## מגבלות של מודלים מותאמים

**הערה:** למודלים מותאמים יש את המגבלות הבאות:

*   מגבלת הקלט של מודל Gemini 1.5 Flash מכוונן היא 40,000 תווים.
*   אין תמיכה במצב JSON עם מודלים מותאמים.
*   אין תמיכה בהוראות מערכת במודלים שעברו כוונון.
*   יש תמיכה רק בהזנת טקסט.

## המאמרים הבאים

כדאי להתחיל עם המדריכים לשיפור הנתונים:

*   [מדריך לשיפור מדויק (Python)](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python&hl=he)
*   [מדריך לשיפור אופן העבודה (REST)](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=rest&hl=he)

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-04-16 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/model-tuning/tutorial

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Fine-tuning with the Gemini API

With the deprecation of Gemini 1.5 Flash-001 in May 2025, we no longer have a model available which supports fine-tuning in the Gemini API, but it is supported in [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning).

We plan to bring fine-tuning support back in the future. We would love to [hear from you on our developer forum](https://discuss.ai.google.dev/c/gemini-api/4) if fine-tuning is important to your use case.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-30 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/changelog

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Release notes

This page documents updates to the Gemini API.

## June 26, 2025

*   The preview models `gemini-2.5-pro-preview-05-06` and `gemini-2.5-pro-preview-03-25` are now redirecting to the latest stable version `gemini-2.5-pro`.
    
*   `gemini-2.5-pro-exp-03-25` is deprecated.
    

## June 24, 2025

*   Released Imagen 4 Ultra and Standard Preview models. To learn more, see the [Image generation](/gemini-api/docs/image-generation) page.

## June 17, 2025

*   Released `gemini-2.5-pro`, the stable version of our most powerful model, now with adaptive thinking. To learn more, see [Gemini 2.5 Pro](/gemini-api/docs/models#gemini-2.5-pro) and [Thinking](/gemini-api/docs/thinking). `gemini-2.5-pro-preview-05-06` will be redirected to `gemini-2.5-pro` on June 26, 2025.
*   Released `gemini-2.5-flash`, our first stable 2.5 Flash model. To learn more, see [Gemini 2.5 Flash](/gemini-api/docs/models#gemini-2.5-flash). `gemini-2.5-flash-preview-04-17` will be deprecated on July 15, 2025.
*   Released `gemini-2.5-flash-lite-preview-06-17`, a low-cost, high-performance Gemini 2.5 model. To learn more, see [Gemini 2.5 Flash-Lite Preview](/gemini-api/docs/models#gemini-2.5-flash-lite).

## June 05, 2025

*   Released `gemini-2.5-pro-preview-06-05`, a new version of our most powerful model, now with adaptive thinking. To learn more, see [Gemini 2.5 Pro Preview](/gemini-api/docs/models#gemini-2.5-pro-preview-06-05) and [Thinking](/gemini-api/docs/thinking). `gemini-2.5-pro-preview-05-06` will be redirected to `gemini-2.5-pro` on June 26, 2025.

## May 20, 2025

**API updates:**

*   Launched support for [custom video preprocessing](/gemini-api/docs/video-understanding#customize-video-processing) using clipping intervals and configurable frame rate sampling.
*   Launched multi-tool use, which supports configuring [code execution](/gemini-api/docs/code-execution) and [Grounding with Google Search](/gemini-api/docs/grounding) on the same `generateContent` request.
*   Launched support for [asynchronous function calls](/gemini-api/docs/live-tools#async-function-calling) in the Live API.
*   Launched an experimental [URL context tool](/gemini-api/docs/url-context) for providing URLs as additional context to prompts.

**Model updates:**

*   Released `gemini-2.5-flash-preview-05-20`, a Gemini [preview](/gemini-api/docs/models#model-versions) model optimized for price-performance and adaptive thinking. To learn more, see [Gemini 2.5 Flash Preview](/gemini-api/docs/models#gemini-2.5-flash-preview) and [Thinking](/gemini-api/docs/thinking).
*   Released the [`gemini-2.5-pro-preview-tts`](/gemini-api/docs/models#gemini-2.5-pro-preview-tts) and [`gemini-2.5-flash-preview-tts`](/gemini-api/docs/models#gemini-2.5-flash-preview-tts) models, which are capable of [generating speech](/gemini-api/docs/speech-generation) with one or two speakers.
*   Released the `lyria-realtime-exp` model, which [generates music](/gemini-api/docs/music-generation) in real time.
*   Released `gemini-2.5-flash-preview-native-audio-dialog` and `gemini-2.5-flash-exp-native-audio-thinking-dialog`, new Gemini models for the Live API with native audio output capabilities. To learn more, see the [Live API guide](/gemini-api/docs/live-guide#native-audio-output) and [Gemini 2.5 Flash Native Audio](/gemini-api/docs/models#gemini-2.5-flash-native-audio).
*   Released `gemma-3n-e4b-it` preview, available on [AI Studio](https://aistudio.google.com) and through the Gemini API, as part of the [Gemma 3n](https://ai.google.dev/gemma/docs/3n) launch.

## May 7, 2025

*   Released `gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see [Image generation](https://ai.google.dev/gemini-api/docs/image-generation) and [Gemini 2.0 Flash Preview Image Generation](https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash-preview-image-generation).

## May 6, 2025

*   Released `gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling. `gemini-2.5-pro-preview-03-25` will automatically point to the new version of the model.

## April 17, 2025

*   Released `gemini-2.5-flash-preview-04-17`, a Gemini [preview](/gemini-api/docs/models#model-versions) model optimized for price-performance and adaptive thinking. To learn more, see [Gemini 2.5 Flash Preview](/gemini-api/docs/models#gemini-2.5-flash-preview) and [Thinking](/gemini-api/docs/thinking).

## April 16, 2025

*   Launched context caching for [Gemini 2.0 Flash](/gemini-api/docs/models#gemini-2.0-flash).

## April 9, 2025

**Model updates:**

*   Released `veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the [Veo docs](/gemini-api/docs/video).
*   Released `gemini-2.0-flash-live-001`, a public preview version of the [Live API](/gemini-api/docs/live) model with billing enabled.
    
    *   **Enhanced Session Management and Reliability**
        
        *   **Session Resumption:** Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session\_resumption) to reconnect and resume where you left off.
        *   **Longer Sessions via Context Compression:** Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits.
        *   **Graceful Disconnect Notification:** Receive a `GoAway` server message indicating when a connection is about to close, allowing for graceful handling before termination.
    *   **More Control over Interaction Dynamics**
        
    *   **Configurable Voice Activity Detection (VAD):** Choose sensitivity levels or disable automatic VAD entirely and use new client events (`activityStart`, `activityEnd`) for manual turn control.
        
    *   **Configurable Interruption Handling:** Decide whether user input should interrupt the model's response.
        
    *   **Configurable Turn Coverage:** Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking.
        
    *   **Configurable Media Resolution:** Optimize for quality or token usage by selecting the resolution for input media.
        
    *   **Richer Output and Features**
        
    *   **Expanded Voice & Language Options:** Choose from two new voices and 30 new languages for audio output. The output language is now configurable within `speechConfig`.
        
    *   **Text Streaming:** Receive text responses incrementally as they are generated, enabling faster display to the user.
        
    *   **Token Usage Reporting:** Gain insights into usage with detailed token counts provided in the `usageMetadata` field of server messages, broken down by modality and prompt or response phases.
        

## April 4, 2025

*   Released `gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use `gemini-2.5-pro-exp-03-25` on the free tier.

## March 25, 2025

*   Released `gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see [Gemini 2.5 Pro Experimental](/gemini-api/docs/models#gemini-2.5-pro-preview-03-25).

## March 12, 2025

**Model updates:**

*   Launched an experimental [Gemini 2.0 Flash](/gemini-api/docs/image-generation#gemini) model capable of image generation and editing.
*   Released `gemma-3-27b-it`, available on [AI Studio](https://aistudio.google.com) and through the Gemini API, as part of the [Gemma 3](https://ai.google.dev/gemma/docs/core) launch.

**API updates:**

*   Added support for [YouTube URLs](/gemini-api/docs/vision#youtube) as a media source.
*   Added support for including an [inline video](/gemini-api/docs/vision#inline-video) of less than 20MB.

## March 11, 2025

**SDK updates:**

*   Released the [Google Gen AI SDK for TypeScript and JavaScript](https://googleapis.github.io/js-genai) to public preview.

## March 7, 2025

**Model updates:**

*   Released `gemini-embedding-exp-03-07`, an [experimental](/gemini-api/docs/models/experimental-models) Gemini-based embeddings model in public preview.

## February 28, 2025

**API updates:**

*   Support for [Search as a tool](/gemini-api/docs/grounding) added to `gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.

## February 25, 2025

**Model updates:**

*   Released `gemini-2.0-flash-lite`, a generally available (GA) version of [Gemini 2.0 Flash-Lite](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite), which is optimized for speed, scale, and cost efficiency.

## February 19, 2025

**AI Studio updates:**

*   Support for [additional regions](https://ai.google.dev/gemini-api/docs/available-regions) (Kosovo, Greenland and Faroe Islands).

**API updates:**

*   Support for [additional regions](https://ai.google.dev/gemini-api/docs/available-regions) (Kosovo, Greenland and Faroe Islands).

## February 18, 2025

**Model updates:**

*   Gemini 1.0 Pro is no longer supported. For the list of supported models, see [Gemini models](/gemini-api/docs/models/gemini).

## February 11, 2025

**API updates:**

*   Updates on the [OpenAI libraries compatibility](https://ai.google.dev/gemini-api/docs/openai).

## February 6, 2025

**Model updates:**

*   Released `imagen-3.0-generate-002`, a generally available (GA) version of [Imagen 3 in the Gemini API](/gemini-api/docs/imagen).

**SDK updates:**

*   Released the [Google Gen AI SDK for Java](https://github.com/googleapis/java-genai) for public preview.

## February 5, 2025

**Model updates:**

*   Released `gemini-2.0-flash-001`, a generally available (GA) version of [Gemini 2.0 Flash](/gemini-api/docs/models/gemini#gemini-2.0-flash) that supports text-only output.
*   Released `gemini-2.0-pro-exp-02-05`, an [experimental](/gemini-api/docs/models/experimental-models) public preview version of Gemini 2.0 Pro.
*   Released `gemini-2.0-flash-lite-preview-02-05`, an experimental public preview [model](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite) optimized for cost efficiency.

**API updates:**

*   Added [file input and graph output](/gemini-api/docs/code-execution#input-output) support to code execution.

**SDK updates:**

*   Released the [Google Gen AI SDK for Python](https://googleapis.github.io/python-genai/) to general availability (GA).

## January 21, 2025

**Model updates:**

*   Released `gemini-2.0-flash-thinking-exp-01-21`, the latest preview version of the model behind the [Gemini 2.0 Flash Thinking Model](/gemini-api/docs/thinking).

## December 19, 2024

**Model updates:**

*   Released Gemini 2.0 Flash Thinking Mode for public preview. Thinking Mode is a test-time compute model that lets you see the model's thought process while it generates a response, and produces responses with stronger reasoning capabilities.
    
    Read more about Gemini 2.0 Flash Thinking Mode in our [overview page](/gemini-api/docs/thinking-mode).
    

## December 11, 2024

**Model updates:**

*   Released [Gemini 2.0 Flash Experimental](/gemini-api/docs/models/gemini#gemini-2.0-flash) for public preview. Gemini 2.0 Flash Experimental's partial list of features includes:
    *   Twice as fast as Gemini 1.5 Pro
    *   Bidirectional streaming with our Live API
    *   Multimodal response generation in the form of text, images, and speech
    *   Built-in tool use with multi-turn reasoning to use features like code execution, Search, function calling, and more

Read more about Gemini 2.0 Flash in our [overview page](/gemini-api/docs/models/gemini-v2).

## November 21, 2024

**Model updates:**

*   Released `gemini-exp-1121`, an even more powerful experimental Gemini API model.

**Model updates:**

*   Updated the `gemini-1.5-flash-latest` and `gemini-1.5-flash` model aliases to use `gemini-1.5-flash-002`.
    *   Change to `top_k` parameter: The `gemini-1.5-flash-002` model supports `top_k` values between 1 and 41 (exclusive). Values greater than 40 will be changed to 40.

## November 14, 2024

**Model updates:**

*   Released `gemini-exp-1114`, a powerful experimental Gemini API model.

## November 8, 2024

**API updates:**

*   Added [support for Gemini](/gemini-api/docs/openai) in the OpenAI libraries / REST API.

## October 31, 2024

**API updates:**

*   Added [support for Grounding with Google Search](/gemini-api/docs/grounding).

## October 3, 2024

**Model updates:**

*   Released `gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini API model.

## September 24, 2024

**Model updates:**

*   Released `gemini-1.5-pro-002` and `gemini-1.5-flash-002`, two new stable versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.
*   Updated the `gemini-1.5-pro-latest` model code to use `gemini-1.5-pro-002` and the `gemini-1.5-flash-latest` model code to use `gemini-1.5-flash-002`.
*   Released `gemini-1.5-flash-8b-exp-0924` to replace `gemini-1.5-flash-8b-exp-0827`.
*   Released the [civic integrity safety filter](/gemini-api/docs/safety-settings#safety-filters) for the Gemini API and AI Studio.
*   Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in Python and NodeJS: [`frequencyPenalty`](/api/generate-content#FIELDS.frequency_penalty) and [`presencePenalty`](/api/generate-content#FIELDS.presence_penalty).

## September 19, 2024

**AI Studio updates:**

*   Added thumb-up and thumb-down buttons to model responses, to enable users to provide feedback on the quality of a response.

**API updates:**

*   Added support for Google Cloud credits, which can now be used towards Gemini API usage.

## September 17, 2024

**AI Studio updates:**

*   Added an **Open in Colab** button that exports a prompt – and the code to run it – to a Colab notebook. The feature doesn't yet support prompting with tools (JSON mode, function calling, or code execution).

## September 13, 2024

**AI Studio updates:**

*   Added support for compare mode, which lets you compare responses across models and prompts to find the best fit for your use case.

## August 30, 2024

**Model updates:**

*   Gemini 1.5 Flash supports [supplying JSON schema through model configuration](/gemini-api/docs/json-mode#supply-schema-in-config).

## August 27, 2024

**Model updates:**

*   Released the following [experimental models](/gemini-api/docs/models/experimental-models):
    *   `gemini-1.5-pro-exp-0827`
    *   `gemini-1.5-flash-exp-0827`
    *   `gemini-1.5-flash-8b-exp-0827`

## August 9, 2024

**API updates:**

*   Added support for [PDF processing](/gemini-api/docs/document-processing).

## August 5, 2024

**Model updates:**

*   Fine-tuning support released for Gemini 1.5 Flash.

## August 1, 2024

**Model updates:**

*   Released `gemini-1.5-pro-exp-0801`, a new experimental version of [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro).

## July 12, 2024

**Model updates:**

*   Support for Gemini 1.0 Pro Vision removed from Google AI services and tools.

## June 27, 2024

**Model updates:**

*   General availability release for Gemini 1.5 Pro's 2M context window.

**API updates:**

*   Added support for [code execution](/gemini-api/docs/code-execution).

## June 18, 2024

**API updates:**

*   Added support for [context caching](/gemini-api/docs/caching).

## June 12, 2024

**Model updates:**

*   Gemini 1.0 Pro Vision deprecated.

## May 23, 2024

**Model updates:**

*   [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro) (`gemini-1.5-pro-001`) is generally available (GA).
*   [Gemini 1.5 Flash](/gemini-api/docs/models/gemini#gemini-1.5-flash) (`gemini-1.5-flash-001`) is generally available (GA).

## May 14, 2024

**API updates:**

*   Introduced a 2M context window for Gemini 1.5 Pro (waitlist).
*   Introduced pay-as-you-go [billing](/gemini-api/docs/billing) for Gemini 1.0 Pro, with Gemini 1.5 Pro and Gemini 1.5 Flash billing coming soon.
*   Introduced increased rate limits for the upcoming paid tier of Gemini 1.5 Pro.
*   Added built-in video support to the [File API](/api/rest/v1beta/files).
*   Added plain text support to the [File API](/api/rest/v1beta/files).
*   Added support for parallel function calling, which returns more than one call at a time.

## May 10, 2024

**Model updates:**

*   Released [Gemini 1.5 Flash](/gemini-api/docs/models/gemini#gemini-1.5-flash) (`gemini-1.5-flash-latest`) in preview.

## April 9, 2024

**Model updates:**

*   Released [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro) (`gemini-1.5-pro-latest`) in preview.
*   Released a new text embedding model, `text-embeddings-004`, which supports [elastic embedding](/gemini-api/docs/embeddings#elastic-embedding) sizes under 768.

**API updates:**

*   Released the [File API](/api/rest/v1beta/files) for temporarily storing media files for use in prompting.
*   Added support for prompting with text, image, and audio data, also known as _multimodal_ prompting. To learn more, see [Prompting with media](/gemini-api/docs/prompting_with_media).
*   Released [System instructions](/gemini-api/docs/system-instructions) in beta.
*   Added [Function calling mode](/gemini-api/docs/function-calling#function_calling_mode), which defines the execution behavior for function calling.
*   Added support for the `response_mime_type` configuration option, which lets you request responses in [JSON format](/gemini-api/docs/api-overview#json).

## March 19, 2024

**Model updates:**

*   Added support for [tuning Gemini 1.0 Pro](https://developers.googleblog.com/en/tune-gemini-pro-in-google-ai-studio-or-with-the-gemini-api/) in Google AI Studio or with the Gemini API.

## December 13 2023

**Model updates:**

*   gemini-pro: New text model for a wide variety of tasks. Balances capability and efficiency.
*   gemini-pro-vision: New multimodal model for a wide variety of tasks. Balances capability and efficiency.
*   embedding-001: New embeddings model.
*   aqa: A new specially tuned model that is trained to answer questions using text passages for grounding generated answers.

See [Gemini models](/gemini-api/docs/models/gemini) for more details.

**API version updates:**

*   v1: The stable API channel.
*   v1beta: Beta channel. This channel has features that may be under development.

See [the API versions topic](/gemini-api/docs/api-versions) for more details.

**API updates:**

*   `GenerateContent` is a single unified endpoint for chat and text.
*   Streaming available through the `StreamGenerateContent` method.
*   Multimodal capability: Image is a new supported modality
*   New beta features:
    *   [Function Calling](/gemini-api/docs/function-calling)
    *   [Semantic Retriever](/gemini-api/docs/semantic_retrieval)
    *   Attributed Question Answering (AQA)
*   Updated candidate count: Gemini models only return 1 candidate.
*   Different Safety Settings and SafetyRating categories. See [safety settings](/gemini-api/docs/safety-settings) for more details.
*   Tuning models is not yet supported for Gemini models (Work in progress).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/safety-guidance

2.5 사고 모델 제품군의 업데이트를 소개합니다. [자세히 알아보기](https://ai.google.dev/gemini-api/docs/models?hl=ko)

![](https://ai.google.dev/_static/images/translated.svg?hl=ko) 이 페이지는 [Cloud Translation API](//cloud.google.com/translate/?hl=ko)를 통해 번역되었습니다.

*   [홈](https://ai.google.dev/?hl=ko)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ko)
*   [모델](https://ai.google.dev/gemini-api/docs?hl=ko)

의견 보내기

# 안전 안내

생성형 인공지능 모델은 강력한 도구이지만 제한 없이 사용할 수 있습니다. 다양성과 적용 가능성은 때때로 부정확하거나 편향된 출력, 불쾌감을 준다는 의미입니다. 사후 처리와 엄격한 수동 평가가 이러한 출력으로 인한 피해 위험을 최소화해야 합니다.

Gemini API에서 제공하는 모델은 다양한 작업에 사용할 수 있습니다. 생성형 AI 및 자연어 처리 (NLP) 애플리케이션에 대해 배웠습니다. 용도 함수는 Gemini API 또는 Google AI Studio 웹을 통해서만 사용할 수 있습니다. 있습니다. Gemini API 사용에는 [생성형 AI 금지된 사용도 적용됩니다. 정책](https://policies.google.com/terms/generative-ai/use-policy?hl=ko) 및 [Gemini API 서비스 약관](https://ai.google.dev/terms?hl=ko).

대규모 언어 모델 (LLM)이 매우 유용한 이유 중 하나는 다양한 언어 작업을 처리할 수 있는 창의적인 도구 안타깝게도 이것은 또한 대규모 언어 모델이 사용자가 필요로 하지 않는 텍스트 포함 불쾌감을 주거나 무분별하거나 사실이 아닌 콘텐츠 또한 또한 이러한 모델의 놀라운 다양성은 생산할 수 있는 바람직하지 않은 출력의 종류를 정확하게 예측할 수 있습니다. 반면 Gemini API는 [Google의 AI로 설계되었습니다 이러한 원칙을 지키기 위해](https://ai.google/principles/?hl=ko) 적용하는 방법을 알아보겠습니다 개발자가 안전하고 책임감 있는 콘텐츠를 Gemini API에는 일부 기본 제공 콘텐츠 필터링과 위험과 관련된 4가지 측면에서 안전 설정을 조정할 수 있습니다. 자세한 내용은 [안전 설정](https://ai.google.dev/gemini-api/docs/safety-settings?hl=ko) 가이드를 참조하세요.

이 문서는 보안 위험으로 인해 발생할 수 있는 몇 가지 안전 위험을 새로운 안전 설계 및 개발을 권장하고 추천을 제공합니다 (법과 규정에도 제한이 있을 수 있으며, 이러한 고려사항은 이 가이드에서 다루지 않습니다.

LLM으로 애플리케이션을 빌드할 때는 다음 단계를 따르는 것이 좋습니다.

*   애플리케이션의 안전 위험 이해
*   안전 위험 완화를 위한 조정 고려
*   사용 사례에 적합한 안전 테스트 실행
*   사용자 의견 요청 및 사용 모니터링

조정 및 테스트 단계는 애플리케이션 성능을 향상시키는 데 도움이 될 수 있습니다

![모델 구현 주기](https://ai.google.dev/static/gemini-api/docs/images/safety_diagram.png?hl=ko)

## 애플리케이션의 안전 위험 이해

이 맥락에서 안전은 LLM이 특정 계층의 위험을 피하는 능력으로 유해한 언어나 콘텐츠를 생성하는 등 사용자에게 해를 끼치는 콘텐츠 고정관념을 조장하는 콘텐츠입니다 Gemini API를 통해 사용할 수 있는 모델은 [Google의 AI 원칙](https://ai.google/principles/?hl=ko)을 바탕으로 설계됨 그리고 그것의 사용은 [생성형 AI 금지된 사용'의 적용을 받습니다 정책](https://policies.google.com/terms/generative-ai/use-policy?hl=ko). API 일반적인 언어 모델을 해결하는 데 도움이 되는 안전 필터가 기본 제공됩니다. 악의적인 표현, 증오심 표현, 포용성을 위한 노력 고정관념을 근절할 수 있습니다 그러나 애플리케이션마다 위험도가 떨어질 수 있습니다 따라서 애플리케이션 소유자는 사용자와 애플리케이션이 초래할 수 있는 잠재적 피해를 파악해야 합니다. 애플리케이션이 LLM을 안전하고 책임감 있게 사용하도록 만드는 것입니다.

이 평가의 일부로서, 침해가 발생할 수 있는 가능성을 고려해야 합니다. 심각성 및 완화 조치를 결정합니다. 예를 들어 사실에 근거한 사건을 바탕으로 에세이를 작성하는 앱은 허구의 콘텐츠를 생성하는 앱과 비교하여 스토리를 만들어 내는 것입니다. 잠재적인 안전 위험을 탐색하는 좋은 방법 최종 사용자 및 이들의 영향을 받을 수 있는 다른 사용자를 확인할 수 있습니다. 이는 테크의 미래 상태를 조사하는 등 다양한 형태를 띨 수 사람들이 유사한 앱을 어떻게 사용하는지 관찰하면서 앱 영역의 미술 연구를 사용자 연구, 설문조사, 비공식 인터뷰를 실시하거나 파악할 수 있습니다.

#### 고급 도움말

*   타겟 내의 다양한 잠재고객에게 다가가세요. 애플리케이션의 용도와 용도를 잠재적인 위험에 대한 더 광범위한 관점을 얻고 다양성을 조정해야 합니다 변경할 수 있습니다
*   [AI 위험 관리 프레임워크](https://www.nist.gov/itl/ai-risk-management-framework) 미국 정부에서 발표한 미국 국립표준기술연구소 (NIST)는 AI 위험 관리에 대한 자세한 안내와 추가 학습 리소스를 제공합니다.
*   DeepMind의 [피해를 줄 수 있는 윤리적, 사회적 위험](https://arxiv.org/abs/2112.04359) 를 통해 개인정보처리방침을 정의할 수 있습니다. 언어 모델이 다양한 방식으로 피해를 입힐 수 있습니다.

## 안전 위험을 완화하기 위한 조정 고려하기

이제 위험을 이해했으므로 위험을 완화하는 방법을 결정할 수 있습니다. 있습니다. 어떤 위험의 우선 순위를 정하고, 어느 정도를 침해하기 위해 노력해야 하는지 소프트웨어의 버그를 분류하는 것과 유사하게 살펴보겠습니다 우선순위를 결정했다면 이제 가장 적절한 완화 유형을 파악할 수 있습니다 종종 간단한 변경으로 차이를 만들고 위험을 줄일 수 있습니다

예를 들어 애플리케이션을 설계할 때 다음을 고려하세요.

*   테스트 환경에서 허용되는 사항을 더 잘 반영하도록 **모델 출력 조정** 애플리케이션 컨텍스트를 사용하는 것이 좋습니다 조정을 통해 모델의 출력을 향상시킬 수 있음 예측 가능하고 일관적이므로 특정 위험을 완화하는 데 도움이 될 수 있습니다.
*   **안전한 출력을 제공하는 입력 방법 제공** 정확한 입력 출력 품질이 달라질 수 있습니다 입력 프롬프트를 실험하여 비즈니스 환경에서 가장 안전한 UX를 설계하는 데 도움이 되는 UX를 제공할 수 있기 때문에 도움이 됩니다. 예를 들어 사용자가 특정 게재위치에서만 선택하도록 제한할 수 있습니다. 드롭다운 목록을 선택하거나 서술적이다 구문은 애플리케이션 컨텍스트에서 안전하게 작동합니다.
*   **안전하지 않은 입력을 차단하고 출력은 확인할 수 있습니다** 간단한 상황에서는 차단 목록을 사용하여 프롬프트 또는 대답에 안전하지 않은 단어나 문구를 사용하거나 검토자가 필요함 이러한 콘텐츠를 직접 변경하거나 차단할 수 있습니다.
    
*   **학습된 분류기를 사용하여 각 프롬프트에 잠재적 피해 또는 적대적인 신호에 해당합니다** 그런 다음 다양한 전략을 사용하여 감지된 피해의 유형에 따라 요청을 처리합니다. 예를 들어 명백히 적대적이거나 악의적인 입력은 차단될 수 있고 처방 스크립트로 작성된 응답을 출력합니다.
    
    #### 고급 도움말
    
    *   신호가 출력이 유해하다고 판단하는 경우 애플리케이션은 다음 옵션을 사용할 수 있습니다.
        *   오류 메시지 또는 사전 스크립트 처리된 출력을 제공합니다.
        *   다른 안전 출력이 때로는 같은 프롬프트가 서로 다른 출력을 생성합니다.
    
*   **고의적인 오용에 대한 보호 장치 마련**(예: 각 사용자에게 고유 ID를 할당하고 사용자 검색어 볼륨에 제한을 둡니다. 일정 기간 동안 제출할 수 있는 과제입니다 또 다른 보호 장치는 가능한 프롬프트 삽입으로부터 보호합니다. SQL과 유사한 프롬프트 삽입 인젝션의 하나로, 악의적인 사용자가 입력 프롬프트를 전송하여 모델의 출력을 조작 모델이 이전의 예시를 무시하도록 지시합니다. 자세한 내용은 [생성형 AI 금지된 사용 정책](https://policies.google.com/terms/generative-ai/use-policy?hl=ko) 참조하세요.
    
*   **본질적으로 위험성이 낮은 것으로 기능을 조정합니다.** 범위가 더 좁은 작업 (예: 인간의 감독이 더 많이 이루어지는 텍스트 (예: 짧은 형식의 텍스트 생성) 사람의 검토를 거치는 콘텐츠)의 경우 위험도가 낮은 경우가 많습니다. 따라서 이메일 답장을 작성하는 애플리케이션을 만드는 대신 대신 윤곽선에서 확장하거나 사용할 수 있습니다.
    

## 사용 사례에 적합한 안전 테스트 실행

테스트는 강력하고 안전한 애플리케이션을 빌드하는 데 있어 핵심 부분이지만 그 정도는 테스트 범위와 전략이 다를 수 있습니다 예를 들어 재미로 하는 하이쿠를 예를 들어, 생성형 AI를 설계하고 설계된 애플리케이션보다 덜 심각한 위험을 초래할 가능성이 법률 회사에서 법률 문서를 요약하고 계약서 초안을 작성하는 데 사용할 수 있습니다. 하지만 하이쿠 생성기는 다양한 사용자가 사용할 수 있습니다. 즉, 악의적 시도 또는 의도치 않은 유해한 입력에 대한 잠재력이 더 높습니다. 구현 상황도 중요합니다. 예를 들어 애플리케이션이 조치를 취하기 전에 전문가가 검토한 결과 유해한 출력을 생성할 가능성이 동일한 애플리케이션을 배포할 수 있습니다

변경과 테스트를 여러 차례 반복하는 것은 드문 일이 아닙니다. 더 빠르게 시작할 수 있으며, 이는 애플리케이션이 위험도가 비교적 낮습니다. AI에 특히 유용한 두 가지 유형의 테스트는 애플리케이션:

*   **안전성 벤치마킹**에는 애플리케이션이 안전하지 않을 수 있는 방식을 다음 측정항목에서 애플리케이션이 얼마나 잘 작동하는지 테스트 모델을 학습시키는 작업도 반복해야 합니다 가장 좋은 방법은 테스트 전에 허용 가능한 수준의 안전성 측정치를 확보하여 1) 이러한 기대에 따라 테스트 결과를 평가하고 2) 중요한 측정항목을 평가하는 테스트를 바탕으로 평가 데이터 세트 있습니다.
    
    #### 고급 도움말
    
    *   '판매 기성' 접근 방식에 지나치게 의존하지 않도록 주의하세요. 평가자를 활용하여 자체 테스트 데이터 세트를 빌드해야 합니다 애플리케이션의 컨텍스트에 완전히 적합할 수 있습니다
    *   측정항목이 2개 이상인 경우 한 가지 측정항목의 변화로 인해 실적이 개선되는 경우 해를 입히는 것입니다. 다른 성능 엔지니어링과 마찬가지로 평가 전반에서 최악의 실적에 집중하는 것이 좋습니다 설정해 보겠습니다.
    
*   **적대적 테스트**: 애플리케이션입니다. 목표는 약점들을 파악하여 적절한 조치를 취하는 것입니다. 적절한 조치를 취할 수 있습니다. 적대적 테스트는 애플리케이션에 대한 전문 지식을 갖춘 평가자들이 상당한 시간과 노력을 들이지 않아도 됩니다. 많이 할수록 문제를 발견할 가능성이 높아지고 특히 드물게 발생하거나 반복된 실행 후에만 애플리케이션입니다.
    
    *   적대적 테스트는 ML을 체계적으로 평가하는 방법 모델이 입력될 때의 동작을 학습하기 위한 목적으로 악의적이거나 의도치 않게 유해한 입력:
        *   입력이 명확하게 설계된 경우 악의적인 입력일 수 있습니다. 텍스트, 이미지, 오디오, 동영상 등 안전하지 않거나 유해한 출력을 생성하는 경우 특정 주제에 대한 증오성 고함을 지르는 생성 모델 믿습니다.
        *   입력 자체가 잘못된 경우 무해하지만 유해한 출력을 발생시킴(예: 특정 인종 및 민족을 가진 사람을 설명하는 인종차별 출력을 수신하는 것입니다.
    *   적대적 테스트와 표준 평가의 차이점은 테스트에 사용되는 데이터의 구성 적대적 테스트의 경우 문제가 되는 출력을 가져올 가능성이 가장 높은 테스트 데이터를 있습니다. 즉, 모델의 동작을 모든 유형의 데이터에 대해 잠재적인 피해(예: 드물거나 특이한 예 및 보안 정책과 관련된 극단적인 케이스 또한 다양성, 다양성, 다양성, 지정할 수 있습니다. [Google의 책임감 있는 AI 업계의 공정성](https://ai.google/responsibilities/responsible-ai-practices/?category=fairness&hl=ko) 참조하세요.
        
        #### 고급 도움말
        
        *   사용 [자동 테스트](https://www.deepmind.com/blog/red-teaming-language-models-with-language-models?hl=ko) 레드팀을 모집하는 기존의 방법 대신 애플리케이션을 중단시킬 수 있습니다. 자동 테스트에서 '레드팀' 입력 텍스트를 찾아내는 또 다른 언어 모델입니다. 유해한 출력 추출
        

## 문제 모니터링

아무리 테스트를 얼마나 완화하고 완화하든 완벽을 보장할 수는 없습니다. 발생하는 문제를 발견하고 처리하는 방법을 미리 계획합니다. 일반적 사용자가 의견을 공유할 수 있도록 모니터링되는 채널을 설정하는 방법 등이 있습니다. (예: 좋아요/싫어요 평가) 및 사전 요청을 위한 사용자 연구 실행 피드백을 수집할 수 있어야 합니다. 특히 사용 패턴이 다를 수 있습니다

#### 고급 도움말

*   사용자가 AI 제품에 대한 의견을 제공하면 AI를 크게 개선할 수 있습니다. 실적 및 사용자 경험을 측정할 수 있습니다. 프롬프트 조정에 더 나은 예시를 선택하는 데 도움이 됩니다. 이 [의견 및 관리 장](https://pair.withgoogle.com/chapter/feedback-controls/) [Google 사용자와 AI 가이드북](https://pair.withgoogle.com/guidebook/chapters) 설계 시 고려해야 할 주요 고려사항 살펴봤습니다

## 다음 단계

*   자세한 내용은 [안전 설정](https://ai.google.dev/gemini-api/docs/safety-settings?hl=ko) 가이드를 참고하여 조정 가능한 Gemini API를 통해 사용할 수 있는 안전 설정
*   자세한 내용은 [메시지 표시 소개](https://ai.google.dev/gemini-api/docs/prompting-intro?hl=ko)를 첫 번째 프롬프트를 작성하기 시작했습니다

의견 보내기

달리 명시되지 않는 한 이 페이지의 콘텐츠에는 [Creative Commons Attribution 4.0 라이선스](https://creativecommons.org/licenses/by/4.0/)에 따라 라이선스가 부여되며, 코드 샘플에는 [Apache 2.0 라이선스](https://www.apache.org/licenses/LICENSE-2.0)에 따라 라이선스가 부여됩니다. 자세한 내용은 [Google Developers 사이트 정책](https://developers.google.com/site-policies?hl=ko)을 참조하세요. 자바는 Oracle 및/또는 Oracle 계열사의 등록 상표입니다.

최종 업데이트: 2025-02-25(UTC)

---

# Content from: https://ai.google.dev/gemini-api/docs/crewai-example

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Customer Support Analysis with Gemini 2.5 Pro and CrewAI

[CrewAI](https://docs.crewai.com/introduction) is a framework for orchestrating autonomous AI agents that collaborate to achieve complex goals. It lets you define agents by specifying roles, goals, and backstories, and then define tasks for them.

This example demonstrates how to build a multi-agent system for analyzing customer support data to identify issues and propose process improvements using Gemini 2.5 Pro, generating a report intended to be read by a Chief Operating Officer (COO).

The guide will show you how to create a "crew" of AI agents that can do the following tasks:

1.  Fetch and analyze customer support data (simulated in this example).
2.  Identify recurring problems and process bottlenecks.
3.  Suggest actionable improvements.
4.  Compile the findings into a concise report suitable for a COO.

You need a Gemini API key. If you don't already have one, you can [get one in Google AI Studio](https://aistudio.google.com/app/apikey).

```
pip install "crewai[tools]"
```

Set your Gemini API key as an environment variable named `GEMINI_API_KEY`, then configure CrewAI to use the Gemini 2.5 Pro model.

```
import os
from crewai import LLM

# Read your API key from the environment variable
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Use Gemini 2.5 Pro Experimental model
gemini_llm = LLM(
    model='gemini/gemini-2.5-pro',
    api_key=gemini_api_key,
    temperature=0.0  # Lower temperature for more consistent results.
)
```

## Define components

CrewAI applications are built using **Tools**, **Agents**, **Tasks**, and the **Crew** itself. Each of these is explained in the following sections.

### Tools

Tools are capabilities that agents can use to interact with the outside world or perform specific actions. Here, you define a placeholder tool to simulate fetching customer support data. In a real application, you would connect to a database, API or file system. For more information on tools, see the [CrewAI tools guide](https://docs.crewai.com/concepts/tools).

```
from crewai.tools import BaseTool

# Placeholder tool for fetching customer support data
class CustomerSupportDataTool(BaseTool):
    name: str = "Customer Support Data Fetcher"
    description: str = (
      "Fetches recent customer support interactions, tickets, and feedback. "
      "Returns a summary string.")

    def _run(self, argument: str) -> str:
        # In a real scenario, this would query a database or API.
        # For this example, return simulated data.
        print(f"--- Fetching data for query: {argument} ---")
        return (
            """Recent Support Data Summary:
- 50 tickets related to 'login issues'. High resolution time (avg 48h).
- 30 tickets about 'billing discrepancies'. Mostly resolved within 12h.
- 20 tickets on 'feature requests'. Often closed without resolution.
- Frequent feedback mentions 'confusing user interface' for password reset.
- High volume of calls related to 'account verification process'.
- Sentiment analysis shows growing frustration with 'login issues' resolution time.
- Support agent notes indicate difficulty reproducing 'login issues'."""
        )

support_data_tool = CustomerSupportDataTool()
```

### Agents

Agents are the individual AI workers in your crew. Each agent has a specific `role`, `goal`, `backstory`, assigned `llm`, and optional `tools`. For more information on agents, see the [CrewAI agents guide](https://docs.crewai.com/concepts/agents).

```
from crewai import Agent

# Agent 1: Data analyst
data_analyst = Agent(
    role='Customer Support Data Analyst',
    goal='Analyze customer support data to identify trends, recurring issues, and key pain points.',
    backstory=(
        """You are an expert data analyst specializing in customer support operations.
        Your strength lies in identifying patterns and quantifying problems from raw support data."""
    ),
    verbose=True,
    allow_delegation=False,  # This agent focuses on its specific task
    tools=[support_data_tool],  # Assign the data fetching tool
    llm=gemini_llm  # Use the configured Gemini LLM
)

# Agent 2: Process optimizer
process_optimizer = Agent(
    role='Process Optimization Specialist',
    goal='Identify bottlenecks and inefficiencies in current support processes based on the data analysis. Propose actionable improvements.',
    backstory=(
        """You are a specialist in optimizing business processes, particularly in customer support.
        You excel at pinpointing root causes of delays and inefficiencies and suggesting concrete solutions."""
    ),
    verbose=True,
    allow_delegation=False,
    # No tools needed, this agent relies on the context provided by data_analyst.
    llm=gemini_llm
)

# Agent 3: Report writer
report_writer = Agent(
    role='Executive Report Writer',
    goal='Compile the analysis and improvement suggestions into a concise, clear, and actionable report for the COO.',
    backstory=(
        """You are a skilled writer adept at creating executive summaries and reports.
        You focus on clarity, conciseness, and highlighting the most critical information and recommendations for senior leadership."""
    ),
    verbose=True,
    allow_delegation=False,
    llm=gemini_llm
)
```

### Tasks

Tasks define the specific assignments for the agents. Each task has a `description`, `expected_output`, and is assigned to an `agent`. Tasks are run sequentially by default and include the context of the previous task. For more information on tasks, see the [CrewAI tasks guide](https://docs.crewai.com/concepts/tasks).

```
from crewai import Task

# Task 1: Analyze data
analysis_task = Task(
    description=(
        """Fetch and analyze the latest customer support interaction data (tickets, feedback, call logs)
        focusing on the last quarter. Identify the top 3-5 recurring issues, quantify their frequency
        and impact (e.g., resolution time, customer sentiment). Use the Customer Support Data Fetcher tool."""
    ),
    expected_output=(
        """A summary report detailing the key findings from the customer support data analysis, including:
- Top 3-5 recurring issues with frequency.
- Average resolution times for these issues.
- Key customer pain points mentioned in feedback.
- Any notable trends in sentiment or support agent observations."""
    ),
    agent=data_analyst  # Assign task to the data_analyst agent
)

# Task 2: Identify bottlenecks and suggest improvements
optimization_task = Task(
    description=(
        """Based on the data analysis report provided by the Data Analyst, identify the primary bottlenecks
        in the support processes contributing to the identified issues (especially the top recurring ones).
        Propose 2-3 concrete, actionable process improvements to address these bottlenecks.
        Consider potential impact and ease of implementation."""
    ),
    expected_output=(
        """A concise list identifying the main process bottlenecks (e.g., lack of documentation for agents,
        complex escalation path, UI issues) linked to the key problems.
A list of 2-3 specific, actionable recommendations for process improvement
(e.g., update agent knowledge base, simplify password reset UI, implement proactive monitoring)."""
    ),
    agent=process_optimizer  # Assign task to the process_optimizer agent
    # This task implicitly uses the output of analysis_task as context
)

# Task 3: Compile COO report
report_task = Task(
    description=(
        """Compile the findings from the Data Analyst and the recommendations from the Process Optimization Specialist
        into a single, concise executive report for the COO. The report should clearly state:
1. The most critical customer support issues identified (with brief data points).
2. The key process bottlenecks causing these issues.
3. The recommended process improvements.
Ensure the report is easy to understand, focuses on actionable insights, and is formatted professionally."""
    ),
    expected_output=(
        """A well-structured executive report (max 1 page) summarizing the critical support issues,
        underlying process bottlenecks, and clear, actionable recommendations for the COO.
        Use clear headings and bullet points."""
    ),
    agent=report_writer  # Assign task to the report_writer agent
)
```

### Crew

The `Crew` brings the agents and tasks together, defining the workflow process (such as "sequential").

```
from crewai import Crew, Process

# Define the crew with agents, tasks, and process
support_analysis_crew = Crew(
    agents=[data_analyst, process_optimizer, report_writer],
    tasks=[analysis_task, optimization_task, report_task],
    process=Process.sequential,  # Tasks will run sequentially in the order defined
    verbose=True
)
```

## Run the Crew

Finally, kick off the crew execution with any necessary inputs.

```
# Start the crew's work
print("--- Starting Customer Support Analysis Crew ---")
# The 'inputs' dictionary provides initial context if needed by the first task.
# In this case, the tool simulates data fetching regardless of the input.
result = support_analysis_crew.kickoff(inputs={'data_query': 'last quarter support data'})

print("--- Crew Execution Finished ---")
print("--- Final Report for COO ---")
print(result)
```

The script will now execute. The `Data Analyst` will use the tool, the `Process Optimizer` will analyze the findings, and the `Report Writer` will compile the final report, which is then printed to the console. The `verbose=True` setting will show the detailed thought process and actions of each agent.

To learn more about CrewAI, check out the [CrewAI introduction](https://docs.crewai.com/introduction).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-16 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/troubleshooting

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Troubleshooting guide

Use this guide to help you diagnose and resolve common issues that arise when you call the Gemini API. You may encounter issues from either the Gemini API backend service or the client SDKs. Our client SDKs are open sourced in the following repositories:

*   [python-genai](https://github.com/googleapis/python-genai)
*   [js-genai](https://github.com/googleapis/js-genai)
*   [go-genai](https://github.com/googleapis/go-genai)

If you encounter API key issues, ensure you have set up your API key correctly per the [API key setup guide](/gemini-api/docs/api-key).

## Gemini API backend service error codes

The following table lists common backend error codes you may encounter, along with explanations for their causes and troubleshooting steps:

**HTTP Code**

**Status**

**Description**

**Example**

**Solution**

400

INVALID\_ARGUMENT

The request body is malformed.

There is a typo, or a missing required field in your request.

Check the [API reference](/api) for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors.

400

FAILED\_PRECONDITION

Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio.

You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio.

To use the Gemini API, you will need to setup a paid plan using [Google AI Studio](https://aistudio.google.com/app/apikey).

403

PERMISSION\_DENIED

Your API key doesn't have the required permissions.

You are using the wrong API key; you are trying to use a tuned model without going through [proper authentication](/docs/model-tuning/tutorial?lang=python#set_up_authentication).

Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models.

404

NOT\_FOUND

The requested resource wasn't found.

An image, audio, or video file referenced in your request was not found.

Check if all [parameters in your request are valid](/docs/troubleshooting#check-api) for your API version.

429

RESOURCE\_EXHAUSTED

You've exceeded the rate limit.

You are sending too many requests per minute with the free tier Gemini API.

Ensure you're within the model's [rate limit](/models/gemini#model-variations). [Request a quota increase](/docs/increase_quota) if needed.

500

INTERNAL

An unexpected error occurred on Google's side.

Your input context is too long.

Reduce your input context or temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.

503

UNAVAILABLE

The service may be temporarily overloaded or down.

The service is temporarily running out of capacity.

Temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.

504

DEADLINE\_EXCEEDED

The service is unable to finish processing within the deadline.

Your prompt (or context) is too large to be processed in time.

Set a larger 'timeout' in your client request to avoid this error.

## Check your API calls for model parameter errors

Ensure your model parameters are within the following values:

**Model parameter**

**Values (range)**

Candidate count

1-8 (integer)

Temperature

0.0-1.0

Max output tokens

Use `get_model` ([Python](/api/python/google/generativeai/get_model)) to determine the maximum number of tokens for the model you are using.

TopP

0.0-1.0

In addition to checking parameter values, make sure you're using the correct [API version](/gemini-api/docs/api-versions) (e.g., `/v1` or `/v1beta`) and model that supports the features you need. For example, if a feature is in Beta release, it will only be available in the `/v1beta` API version.

## Check if you have the right model

Ensure you are using a supported model listed on our [models page](/gemini-api/docs/models/gemini).

## Higher latency or token usage with 2.5 models

If you're observing higher latency or token usage with 2.5 Flash and Pro models, this can be because they come with **thinking on by default** in order to enhance quality. If you are prioritizing speed or need to minimize costs, you can adjust or disable thinking.

Please refer to [thinking page](/gemini-api/docs/thinking#set-budget) for guidance and sample code.

## Safety issues

If you see a prompt was blocked because of a safety setting in your API call, review the prompt with respect to the filters you set in the API call.

If you see `BlockedReason.OTHER`, the query or response may violate the [terms of service](/terms) or be otherwise unsupported.

## Recitation issue

If you see the model stops generating output due to the RECITATION reason, this means the model output may resemble certain data. To fix this, try to make prompt / context as unique as possible and use a higher temperature.

## Improve model output

For higher quality model outputs, explore writing more structured prompts. The [prompt engineering guide](/gemini-api/docs/prompting-strategies) page introduces some basic concepts, strategies, and best practices to get you started.

If you have hundreds of examples of good input/output pairs, you can also consider [model tuning](/gemini-api/docs/model-tuning).

## Understand token limits

Read through our [Token guide](/gemini-api/docs/tokens) to better understand how to count tokens and their limits.

## Known issues

*   The API supports only a number of select languages. Submitting prompts in unsupported languages can produce unexpected or even blocked responses. See [available languages](/gemini-api/docs/models#supported-languages) for updates.

## File a bug

Join the discussion on the [Google AI developer forum](https://discuss.ai.google.dev) if you have questions.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-17 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/langgraph-example

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# ReAct agent from scratch with Gemini 2.5 and LangGraph

LangGraph is a framework for building stateful LLM applications, making it a good choice for constructing ReAct (Reasoning and Acting) Agents.

ReAct agents combine LLM reasoning with action execution. They iteratively think, use tools, and act on observations to achieve user goals, dynamically adapting their approach. Introduced in ["ReAct: Synergizing Reasoning and Acting in Language Models"](https://arxiv.org/abs/2210.03629) (2023), this pattern tries to mirror human-like, flexible problem-solving over rigid workflows.

While LangGraph offers a prebuilt ReAct agent ([`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)), it shines when you need more control and customization for your ReAct implementations.

LangGraph models agents as graphs using three key components:

*   `State`: Shared data structure (typically `TypedDict` or `Pydantic BaseModel`) representing the application's current snapshot.
*   `Nodes`: Encodes logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State, such as LLM calls or tool calls.
*   `Edges`: Define the next `Node` to execute based on the current `State`, allowing for conditional logic and fixed transitions.

If you don't have an API Key yet, you can get one for free at the [Google AI Studio](https://aistudio.google.com/app/apikey).

```
pip install langgraph langchain-google-genai geopy requests
```

Set your API key in the environment variable `GEMINI_API_KEY`.

```
import os

# Read your API key from the environment variable or set it manually
api_key = os.getenv("GEMINI_API_KEY")
```

To better understand how to implement a ReAct agent using LangGraph, let's walk through a practical example. You will create a simple agent whose goal is to use a tool to find the current weather for a specified location.

For this weather agent, its `State` will need to maintain the ongoing conversation history (as a list of messages) and a counter for the number of steps taken to further illustrate state management.

LangGraph provides a convenient helper, `add_messages`, for updating message lists in the state. It functions as a [reducer](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers), meaning it takes the current list and new messages, then returns a combined list. It smartly handles updates by message ID and defaults to an "append-only" behavior for new, unique messages.

```
from typing import Annotated,Sequence, TypedDict

from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages # helper function to add messages to the state


class AgentState(TypedDict):
    """The state of the agent."""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    number_of_steps: int
```

Next, you define your weather tool.

```
from langchain_core.tools import tool
from geopy.geocoders import Nominatim
from pydantic import BaseModel, Field
import requests

geolocator = Nominatim(user_agent="weather-app")

class SearchInput(BaseModel):
    location:str = Field(description="The city and state, e.g., San Francisco")
    date:str = Field(description="the forecasting date for when to get the weather format (yyyy-mm-dd)")

@tool("get_weather_forecast", args_schema=SearchInput, return_direct=True)
def get_weather_forecast(location: str, date: str):
    """Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour."""
    location = geolocator.geocode(location)
    if location:
        try:
            response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}")
            data = response.json()
            return {time: temp for time, temp in zip(data["hourly"]["time"], data["hourly"]["temperature_2m"])}
        except Exception as e:
            return {"error": str(e)}
    else:
        return {"error": "Location not found"}

tools = [get_weather_forecast]
```

Next, you initialize your model and bind the tools to the model.

```
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI

# Create LLM class
llm = ChatGoogleGenerativeAI(
    model= "gemini-2.5-pro",
    temperature=1.0,
    max_retries=2,
    google_api_key=api_key,
)

# Bind tools to the model
model = llm.bind_tools([get_weather_forecast])

# Test the model with tools
res=model.invoke(f"What is the weather in Berlin on {datetime.today()}?")

print(res)
```

The last step before you can run your agent is to define your nodes and edges. In this example, you have two nodes and one edge. - `call_tool` node that executes your tool method. LangGraph has a prebuilt node for this called [ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/). - `call_model` node that uses the `model_with_tools` to call the model. - `should_continue` edge that decides whether to call the tool or the model.

The number of nodes and edges is not fixed. You can add as many nodes and edges as you want to your graph. For example, you could add a node for adding structured output or a self-verification/reflection node to check the model output before calling the tool or the model.

```
from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableConfig

tools_by_name = {tool.name: tool for tool in tools}

# Define our tool node
def call_tool(state: AgentState):
    outputs = []
    # Iterate over the tool calls in the last message
    for tool_call in state["messages"][-1].tool_calls:
        # Get the tool by name
        tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=tool_result,
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}

def call_model(
    state: AgentState,
    config: RunnableConfig,
):
    # Invoke the model with the system prompt and the messages
    response = model.invoke(state["messages"], config)
    # We return a list, because this will get added to the existing messages state using the add_messages reducer
    return {"messages": [response]}


# Define the conditional edge that determines whether to continue or not
def should_continue(state: AgentState):
    messages = state["messages"]
    # If the last message is not a tool call, then we finish
    if not messages[-1].tool_calls:
        return "end"
    # default to continue
    return "continue"
```

Now you have all the components to build your agent. Let's put them together.

```
from langgraph.graph import StateGraph, END

# Define a new graph with our state
workflow = StateGraph(AgentState)

# 1. Add our nodes 
workflow.add_node("llm", call_model)
workflow.add_node("tools",  call_tool)
# 2. Set the entrypoint as `agent`, this is the first node called
workflow.set_entry_point("llm")
# 3. Add a conditional edge after the `llm` node is called.
workflow.add_conditional_edges(
    # Edge is used after the `llm` node is called.
    "llm",
    # The function that will determine which node is called next.
    should_continue,
    # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.
    # END is a special node marking that the graph is finish.
    {
        # If `tools`, then we call the tool node.
        "continue": "tools",
        # Otherwise we finish.
        "end": END,
    },
)
# 4. Add a normal edge after `tools` is called, `llm` node is called next.
workflow.add_edge("tools", "llm")

# Now we can compile and visualize our graph
graph = workflow.compile()
```

You can visualize your graph using the `draw_mermaid_png` method.

```
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![png](/static/gemini-api/docs/images/langgraph-react-agent_16_0.png)

Now let's run the agent.

```
from datetime import datetime
# Create our initial message dictionary
inputs = {"messages": [("user", f"What is the weather in Berlin on {datetime.today()}?")]}

# call our graph with streaming to see the steps
for state in graph.stream(inputs, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

You can now continue with your conversation and for example ask for the weather in another city or let it compare it.

```
state["messages"].append(("user", "Would it be in Munich warmer?"))

for state in graph.stream(state, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-16 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/migrate

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Обновите Google Gen AI SDK

Когда мы представили семейство моделей Gemini 2.0, мы также выпустили новый набор библиотек Google Gen AI для работы с API Gemini:

*   [Питон](https://pypi.org/project/google-genai/)
*   [TypeScript и JavaScript](https://www.npmjs.com/package/@google/genai)
*   [Идти](https://pkg.go.dev/google.golang.org/genai)

Эти обновленные библиотеки будут полностью совместимы со всеми моделями и функциями Gemini API, включая недавние дополнения, такие как [Live API](https://ai.google.dev/gemini-api/docs/live?hl=ru) и [Veo](https://ai.google.dev/gemini-api/docs/video?hl=ru) .

Мы рекомендуем вам начать миграцию ваших проектов со старого Gemini SDK на новый Gen AI SDK. В этом руководстве представлены примеры перенесенного кода «до» и «после», которые помогут вам начать работу. Мы продолжим добавлять сюда примеры, которые помогут вам начать работу с новыми библиотеками.

## Установите SDK

**До**

### Питон

```
pip install -U -q "google-generativeai"
```

### JavaScript

```
npm install @google/generative-ai
```

### Идти

```
go get github.com/google/generative-ai-go
```

**После**

### Питон

```
pip install -U -q "google-genai"
```

### JavaScript

```
npm install @google/genai
```

### Идти

```
go get google.golang.org/genai
```

## Аутентификация

Аутентификация с помощью ключа API. Вы можете [создать](https://aistudio.google.com/app/apikey?hl=ru) свой ключ API в Google AI Studio.

**До**

### Питон

Старый SDK обрабатывал клиентский объект API неявно. В новом SDK вы создаете клиент API и используете его для вызова API. Помните, что в любом случае SDK получит ваш ключ API из переменной среды `GOOGLE_API_KEY` , если вы не передадите его клиенту.

```
import google.generativeai as genai

genai.configure(api_key=...)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
```

### Идти

Импортируйте библиотеки Google:

```
import (
      "github.com/google/generative-ai-go/genai"
      "google.golang.org/api/option"
)
```

Создайте клиента:

```
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
```

**После**

### Питон

```
export GOOGLE_API_KEY="YOUR_API_KEY"
```

```
from google import genai

client = genai.Client() # Set the API key using the GOOGLE_API_KEY env var.
                        # Alternatively, you could set the API key explicitly:
                        # client = genai.Client(api_key="your_api_key")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({apiKey: "GOOGLE_API_KEY"});
```

### Идти

Импортируйте библиотеку Gen AI:

```
import "google.golang.org/genai"
```

Создайте клиента:

```
client, err := genai.NewClient(ctx, &genai.ClientConfig{
        Backend:  genai.BackendGeminiAPI,
})
```

## Генерировать контент

**До**

### Питон

Новый SDK обеспечивает доступ ко всем методам API через объект `Client` . За исключением нескольких особых случаев с сохранением состояния ( `chat` и `session` live-api), все это функции без сохранения состояния. Для удобства и единообразия возвращаемые объекты представляют собой `pydantic` классы.

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'Tell me a story in 300 words'
)
print(response.text)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const prompt = "Tell me a story in 300 words";

const result = await model.generateContent(prompt);
console.log(result.response.text());
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(ctx, genai.Text("Tell me a story in 300 words."))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response parts
```

**После**

### Питон

```
from google import genai
client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
print(response.text)

print(response.model_dump_json(
    exclude_none=True, indent=4))
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story in 300 words.",
});
console.log(response.text);
```

### Идти

```
ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me a story in 300 words."), nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

**До**

### Питон

Многие из тех же удобных функций присутствуют в новом SDK. Например, объекты `PIL.Image` преобразуются автоматически.

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Tell me a story based on this image',
    Image.open(image_path)
])
print(response.text)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

function fileToGenerativePart(path, mimeType) {
  return {
    inlineData: {
      data: Buffer.from(fs.readFileSync(path)).toString("base64"),
      mimeType,
    },
  };
}

const prompt = "Tell me a story based on this image";

const imagePart = fileToGenerativePart(
  `path/to/organ.jpg`,
  "image/jpeg",
);

const result = await model.generateContent([prompt, imagePart]);
console.log(result.response.text());
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

resp, err := model.GenerateContent(ctx,
    genai.Text("Tell me about this instrument"),
    genai.ImageData("jpeg", imgData))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response
```

**После**

### Питон

```
from google import genai
from PIL import Image

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Tell me a story based on this image',
        Image.open(image_path)
    ]
)
print(response.text)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const organ = await ai.files.upload({
  file: "path/to/organ.jpg",
});

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: [
    createUserContent([
      "Tell me a story based on this image",
      createPartFromUri(organ.uri, organ.mimeType)
    ]),
  ],
});
console.log(response.text);
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

parts := []*genai.Part{
    {Text: "Tell me a story based on this image"},
    {InlineData: &genai.Blob{Data: imgData, MIMEType: "image/jpeg"}},
}
contents := []*genai.Content{
    {Parts: parts},
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

### Потоковое вещание

**До**

### Питон

```
import google.generativeai as genai

response = model.generate_content(
    "Write a cute story about cats.",
    stream=True)
for chunk in response:
    print(chunk.text)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const prompt = "Write a story about a magic backpack.";

const result = await model.generateContentStream(prompt);

// Print text as it comes in.
for await (const chunk of result.stream) {
  const chunkText = chunk.text();
  process.stdout.write(chunkText);
}
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
iter := model.GenerateContentStream(ctx, genai.Text("Write a story about a magic backpack."))
for {
    resp, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    printResponse(resp) // utility for printing the response
}
```

**После**

### Питон

```
from google import genai

client = genai.Client()

for chunk in client.models.generate_content_stream(
  model='gemini-2.0-flash',
  contents='Tell me a story in 300 words.'
):
    print(chunk.text)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContentStream({
  model: "gemini-2.0-flash",
  contents: "Write a story about a magic backpack.",
});
let text = "";
for await (const chunk of response) {
  console.log(chunk.text);
  text += chunk.text;
}
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

for result, err := range client.Models.GenerateContentStream(
    ctx,
    "gemini-2.0-flash",
    genai.Text("Write a story about a magic backpack."),
    nil,
) {
    if err != nil {
        log.Fatal(err)
    }
    fmt.Print(result.Candidates[0].Content.Parts[0].Text)
}
```

## Конфигурация

**До**

### Питон

Для всех методов в новом SDK обязательные аргументы предоставляются в виде аргументов ключевого слова. Все дополнительные входные данные предоставляются в аргументе `config` . Аргументы конфигурации могут быть указаны как словари Python или классы `Config` в пространстве имен `google.genai.types` . В целях полезности и единообразия все определения в модуле `types` являются `pydantic` -классами.

```
import google.generativeai as genai

model = genai.GenerativeModel(
  'gemini-1.5-flash',
    system_instruction='you are a story teller for kids under 5 years old',
    generation_config=genai.GenerationConfig(
      max_output_tokens=400,
      top_k=2,
      top_p=0.5,
      temperature=0.5,
      response_mime_type='application/json',
      stop_sequences=['\n'],
    )
)
response = model.generate_content('tell me a story in 100 words')
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  generationConfig: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

const result = await model.generateContent(
  "Tell me a story about a magic backpack.",
);
console.log(result.response.text())
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
model.SetTemperature(0.5)
model.SetTopP(0.5)
model.SetTopK(2.0)
model.SetMaxOutputTokens(100)
model.ResponseMIMEType = "application/json"
resp, err := model.GenerateContent(ctx, genai.Text("Tell me about New York"))
if err != nil {
    log.Fatal(err)
}
printResponse(resp) // utility for printing response
```

**После**

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='Tell me a story in 100 words.',
  config=types.GenerateContentConfig(
      system_instruction='you are a story teller for kids under 5 years old',
      max_output_tokens= 400,
      top_k= 2,
      top_p= 0.5,
      temperature= 0.5,
      response_mime_type= 'application/json',
      stop_sequences= ['\n'],
      seed=42,
  ),
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story about a magic backpack.",
  config: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

console.log(response.text);
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx,
    "gemini-2.0-flash",
    genai.Text("Tell me about New York"),
    &genai.GenerateContentConfig{
        Temperature:      genai.Ptr[float32](0.5),
        TopP:             genai.Ptr[float32](0.5),
        TopK:             genai.Ptr[float32](2.0),
        ResponseMIMEType: "application/json",
        StopSequences:    []string{"Yankees"},
        CandidateCount:   2,
        Seed:             genai.Ptr[int32](42),
        MaxOutputTokens:  128,
        PresencePenalty:  genai.Ptr[float32](0.5),
        FrequencyPenalty: genai.Ptr[float32](0.5),
    },
)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing response
```

## Настройки безопасности

Сгенерируйте ответ с настройками безопасности:

**До**

### Питон

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'say something bad',
    safety_settings={
        'HATE': 'BLOCK_ONLY_HIGH',
        'HARASSMENT': 'BLOCK_ONLY_HIGH',
  }
)
```

### JavaScript

```
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  safetySettings: [
    {
      category: HarmCategory.HARM_CATEGORY_HARASSMENT,
      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    },
  ],
});

const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const result = await model.generateContent(unsafePrompt);

try {
  result.response.text();
} catch (e) {
  console.error(e);
  console.log(result.response.candidates[0].safetyRatings);
}
```

**После**

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='say something bad',
  config=types.GenerateContentConfig(
      safety_settings= [
          types.SafetySetting(
              category='HARM_CATEGORY_HATE_SPEECH',
              threshold='BLOCK_ONLY_HIGH'
          ),
      ]
  ),
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: unsafePrompt,
  config: {
    safetySettings: [
      {
        category: "HARM_CATEGORY_HARASSMENT",
        threshold: "BLOCK_ONLY_HIGH",
      },
    ],
  },
});

console.log("Finish reason:", response.candidates[0].finishReason);
console.log("Safety ratings:", response.candidates[0].safetyRatings);
```

## Асинхронный

**До**

### Питон

Чтобы использовать новый SDK с `asyncio` , существует отдельная `async` реализация каждого метода в `client.aio` .

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content_async(
    'tell me a story in 100 words'
)
```

**После**

### Питон

```
from google import genai

client = genai.Client()

response = await client.aio.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
```

## Чат

Начните чат и отправьте сообщение модели:

**До**

### Питон

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
chat = model.start_chat()

response = chat.send_message(
    "Tell me a story in 100 words")
response = chat.send_message(
    "What happened after that?")
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const chat = model.startChat({
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});
let result = await chat.sendMessage("I have 2 dogs in my house.");
console.log(result.response.text());
result = await chat.sendMessage("How many paws are in my house?");
console.log(result.response.text());
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
cs := model.StartChat()

cs.History = []*genai.Content{
    {
        Parts: []genai.Part{
            genai.Text("Hello, I have 2 dogs in my house."),
        },
        Role: "user",
    },
    {
        Parts: []genai.Part{
            genai.Text("Great to meet you. What would you like to know?"),
        },
        Role: "model",
    },
}

res, err := cs.SendMessage(ctx, genai.Text("How many paws are in my house?"))
if err != nil {
    log.Fatal(err)
}
printResponse(res) // utility for printing the response
```

**После**

### Питон

```
from google import genai

client = genai.Client()

chat = client.chats.create(model='gemini-2.0-flash')

response = chat.send_message(
    message='Tell me a story in 100 words')
response = chat.send_message(
    message='What happened after that?')
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const chat = ai.chats.create({
  model: "gemini-2.0-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});

const response1 = await chat.sendMessage({
  message: "I have 2 dogs in my house.",
});
console.log("Chat response 1:", response1.text);

const response2 = await chat.sendMessage({
  message: "How many paws are in my house?",
});
console.log("Chat response 2:", response2.text);
```

### Идти

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

chat, err := client.Chats.Create(ctx, "gemini-2.0-flash", nil, nil)
if err != nil {
    log.Fatal(err)
}

result, err := chat.SendMessage(ctx, genai.Part{Text: "Hello, I have 2 dogs in my house."})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result

result, err = chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

## Вызов функции

**До**

### Питон

В новом SDK по умолчанию используется автоматический вызов функций. Здесь вы отключите его.

```
import google.generativeai as genai
from enum import Enum

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

response = model.generate_content("What is the weather in San Francisco?")
function_call = response.candidates[0].parts[0].function_call
```

**После**

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather],
      automatic_function_calling={'disable': True},
  ),
)

function_call = response.candidates[0].content.parts[0].function_call
```

### Автоматический вызов функций

**До**

### Питон

Старый SDK поддерживает только автоматический вызов функций в чате. В новом SDK это поведение по умолчанию `generate_content` .

```
import google.generativeai as genai

def get_current_weather(city: str) -> str:
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

chat = model.start_chat(
    enable_automatic_function_calling=True)
result = chat.send_message("What is the weather in San Francisco?")
```

**После**

### Питон

```
from google import genai
from google.genai import types
client = genai.Client()

def get_current_weather(city: str) -> str:
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather]
  ),
)
```

## Выполнение кода

Выполнение кода — это инструмент, который позволяет модели генерировать код Python, запускать его и возвращать результат.

**До**

### Питон

```
import google.generativeai as genai

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools="code_execution"
)

result = model.generate_content(
  "What is the sum of the first 50 prime numbers? Generate and run code for "
  "the calculation, and make sure you get all 50.")
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  tools: [{ codeExecution: {} }],
});

const result = await model.generateContent(
  "What is the sum of the first 50 prime numbers? " +
    "Generate and run code for the calculation, and make sure you get " +
    "all 50.",
);

console.log(result.response.text());
```

**После**

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the sum of the first 50 prime numbers? Generate and run '
            'code for the calculation, and make sure you get all 50.',
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)],
    ),
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-pro-exp-02-05",
  contents: `Write and execute code that calculates the sum of the first 50 prime numbers.
            Ensure that only the executable code and its resulting output are generated.`,
});

// Each part may contain text, executable code, or an execution result.
for (const part of response.candidates[0].content.parts) {
  console.log(part);
  console.log("\n");
}

console.log("-".repeat(80));
// The `.text` accessor concatenates the parts into a markdown-formatted text.
console.log("\n", response.text);
```

## Поиск заземления

`GoogleSearch` (Gemini>=2.0) и `GoogleSearchRetrieval` (Gemini < 2.0) — это инструменты, которые позволяют модели получать общедоступные веб-данные для обоснования, основанные на Google.

**До**

### Питон

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    contents="what is the Google stock price?",
    tools='google_search_retrieval'
)
```

**После**

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the Google stock price?',
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                google_search=types.GoogleSearch()
            )
        ]
    )
)
```

## JSON-ответ

Сгенерируйте ответы в формате JSON.

**До**

### Питон

Указав `response_schema` и установив `response_mime_type="application/json"` пользователи могут ограничить модель созданием ответа `JSON` в соответствии с заданной структурой. Новый SDK использует классы `pydantic` для предоставления схемы (хотя вы можете передать `genai.types.Schema` или эквивалентный `dict` ). Если это возможно, SDK проанализирует возвращенный JSON и вернет результат в `response.parsed` . Если вы предоставили класс `pydantic` в качестве схемы, SDK преобразует этот `JSON` в экземпляр класса.

```
import google.generativeai as genai
import typing_extensions as typing

class CountryInfo(typing.TypedDict):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

model = genai.GenerativeModel(model_name="gemini-1.5-flash")
result = model.generate_content(
    "Give me information of the United States",
    generation_config=genai.GenerationConfig(
        response_mime_type="application/json",
        response_schema = CountryInfo
    ),
)
```

### JavaScript

```
import { GoogleGenerativeAI, SchemaType } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");

const schema = {
  description: "List of recipes",
  type: SchemaType.ARRAY,
  items: {
    type: SchemaType.OBJECT,
    properties: {
      recipeName: {
        type: SchemaType.STRING,
        description: "Name of the recipe",
        nullable: false,
      },
    },
    required: ["recipeName"],
  },
};

const model = genAI.getGenerativeModel({
  model: "gemini-1.5-pro",
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: schema,
  },
});

const result = await model.generateContent(
  "List a few popular cookie recipes.",
);
console.log(result.response.text());
```

**После**

### Питон

```
from google import genai
from pydantic import BaseModel

client = genai.Client()

class CountryInfo(BaseModel):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Give me information of the United States.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': CountryInfo,
    },
)

response.parsed
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "List a few popular cookie recipes.",
  config: {
    responseMimeType: "application/json",
    responseSchema: {
      type: "array",
      items: {
        type: "object",
        properties: {
          recipeName: { type: "string" },
          ingredients: { type: "array", items: { type: "string" } },
        },
        required: ["recipeName", "ingredients"],
      },
    },
  },
});
console.log(response.text);
```

## Файлы

### Загрузить

Загрузите файл:

**До**

### Питон

```
import requests
import pathlib
import google.generativeai as genai

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

file = genai.upload_file(path='a11.txt')

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Can you summarize this file:',
    my_file
])
print(response.text)
```

**После**

### Питон

```
import requests
import pathlib
from google import genai

client = genai.Client()

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

my_file = client.files.upload(file='a11.txt')

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Can you summarize this file:',
        my_file
    ]
)
print(response.text)
```

### Перечисли и получи

Список загруженных файлов и получение загруженного файла с именем файла:

**До**

### Питон

```
import google.generativeai as genai

for file in genai.list_files():
  print(file.name)

file = genai.get_file(name=file.name)
```

**После**

### Питон

```
from google import genai
client = genai.Client()

for file in client.files.list():
    print(file.name)

file = client.files.get(name=file.name)
```

### Удалить

Удалить файл:

**До**

### Питон

```
import pathlib
import google.generativeai as genai

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = genai.upload_file(path='dummy.txt')

file = genai.delete_file(name=dummy_file.name)
```

**После**

### Питон

```
import pathlib
from google import genai

client = genai.Client()

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = client.files.upload(file='dummy.txt')

response = client.files.delete(name=dummy_file.name)
```

## Кэширование контекста

Кэширование контекста позволяет пользователю один раз передать контент в модель, кэшировать входные токены, а затем обращаться к кэшированным токенам в последующих вызовах, чтобы снизить стоимость.

**До**

### Питон

```
import requests
import pathlib
import google.generativeai as genai
from google.generativeai import caching

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = genai.upload_file(path="a11.txt")

# Create cache
apollo_cache = caching.CachedContent.create(
    model="gemini-1.5-flash-001",
    system_instruction="You are an expert at analyzing transcripts.",
    contents=[document],
)

# Generate response
apollo_model = genai.GenerativeModel.from_cached_content(
    cached_content=apollo_cache
)
response = apollo_model.generate_content("Find a lighthearted moment from this transcript")
```

### JavaScript

```
import { GoogleAICacheManager, GoogleAIFileManager } from "@google/generative-ai/server";
import { GoogleGenerativeAI } from "@google/generative-ai";

const cacheManager = new GoogleAICacheManager("GOOGLE_API_KEY");
const fileManager = new GoogleAIFileManager("GOOGLE_API_KEY");

const uploadResult = await fileManager.uploadFile("path/to/a11.txt", {
  mimeType: "text/plain",
});

const cacheResult = await cacheManager.create({
  model: "models/gemini-1.5-flash",
  contents: [
    {
      role: "user",
      parts: [
        {
          fileData: {
            fileUri: uploadResult.file.uri,
            mimeType: uploadResult.file.mimeType,
          },
        },
      ],
    },
  ],
});

console.log(cacheResult);

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModelFromCachedContent(cacheResult);
const result = await model.generateContent(
  "Please summarize this transcript.",
);
console.log(result.response.text());
```

**После**

### Питон

```
import requests
import pathlib
from google import genai
from google.genai import types

client = genai.Client()

# Check which models support caching.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createCachedContent":
      print(m.name)
      break

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = client.files.upload(file='a11.txt')

# Create cache
model='gemini-1.5-flash-001'
apollo_cache = client.caches.create(
      model=model,
      config={
          'contents': [document],
          'system_instruction': 'You are an expert at analyzing transcripts.',
      },
  )

# Generate response
response = client.models.generate_content(
    model=model,
    contents='Find a lighthearted moment from this transcript',
    config=types.GenerateContentConfig(
        cached_content=apollo_cache.name,
    )
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const filePath = path.join(media, "a11.txt");
const document = await ai.files.upload({
  file: filePath,
  config: { mimeType: "text/plain" },
});
console.log("Uploaded file name:", document.name);
const modelName = "gemini-1.5-flash";

const contents = [
  createUserContent(createPartFromUri(document.uri, document.mimeType)),
];

const cache = await ai.caches.create({
  model: modelName,
  config: {
    contents: contents,
    systemInstruction: "You are an expert analyzing transcripts.",
  },
});
console.log("Cache created:", cache);

const response = await ai.models.generateContent({
  model: modelName,
  contents: "Please summarize this transcript",
  config: { cachedContent: cache.name },
});
console.log("Response text:", response.text);
```

## Подсчитайте жетоны

Подсчитайте количество токенов в запросе.

**До**

### Питон

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.count_tokens(
    'The quick brown fox jumps over the lazy dog.')
```

### JavaScript

 ```
 import { GoogleGenerativeAI } from "@google/generative-ai";

 const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY+);
 const model = genAI.getGenerativeModel({
   model: "gemini-1.5-flash",
 });

 // Count tokens in a prompt without calling text generation.
 const countResult = await model.countTokens(
   "The quick brown fox jumps over the lazy dog.",
 );

 console.log(countResult.totalTokens); // 11

 const generateResult = await model.generateContent(
   "The quick brown fox jumps over the lazy dog.",
 );

 // On the response for `generateContent`, use `usageMetadata`
 // to get separate input and output token counts
 // (`promptTokenCount` and `candidatesTokenCount`, respectively),
 // as well as the combined token count (`totalTokenCount`).
 console.log(generateResult.response.usageMetadata);
 // candidatesTokenCount and totalTokenCount depend on response, may vary
 // { promptTokenCount: 11, candidatesTokenCount: 124, totalTokenCount: 135 }
```

**После**

### Питон

```
from google import genai

client = genai.Client()

response = client.models.count_tokens(
    model='gemini-2.0-flash',
    contents='The quick brown fox jumps over the lazy dog.',
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const prompt = "The quick brown fox jumps over the lazy dog.";
const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(generateResponse.usageMetadata);
```

## Создание изображений

Генерировать изображения:

**До**

### Питон

```
#pip install https://github.com/google-gemini/generative-ai-python@imagen
import google.generativeai as genai

imagen = genai.ImageGenerationModel(
    "imagen-3.0-generate-001")
gen_images = imagen.generate_images(
    prompt="Robot holding a red skateboard",
    number_of_images=1,
    safety_filter_level="block_low_and_above",
    person_generation="allow_adult",
    aspect_ratio="3:4",
)
```

**После**

### Питон

```
from google import genai

client = genai.Client()

gen_images = client.models.generate_images(
    model='imagen-3.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 1,
        safety_filter_level= "BLOCK_LOW_AND_ABOVE",
        person_generation= "ALLOW_ADULT",
        aspect_ratio= "3:4",
    )
)

for n, image in enumerate(gen_images.generated_images):
    pathlib.Path(f'{n}.png').write_bytes(
        image.image.image_bytes)
```

## Встроить контент

Создание вложений контента.

**До**

### Питон

```
import google.generativeai as genai

response = genai.embed_content(
  model='models/text-embedding-004',
  content='Hello world'
)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "text-embedding-004",
});

const result = await model.embedContent("Hello world!");

console.log(result.embedding);
```

**После**

### Питон

```
from google import genai

client = genai.Client()

response = client.models.embed_content(
  model='text-embedding-004',
  contents='Hello world',
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const text = "Hello World!";
const result = await ai.models.embedContent({
  model: "text-embedding-004",
  contents: text,
  config: { outputDimensionality: 10 },
});
console.log(result.embeddings);
```

## Настройте модель

Создайте и используйте настроенную модель.

Новый SDK упрощает настройку с помощью `client.tunings.tune` , который запускает задание настройки и опрашивает его до завершения.

**До**

### Питон

```
import google.generativeai as genai
import random

# create tuning model
train_data = {}
for i in range(1, 6):
  key = f'input {i}'
  value = f'output {i}'
  train_data[key] = value

name = f'generate-num-{random.randint(0,10000)}'
operation = genai.create_tuned_model(
    source_model='models/gemini-1.5-flash-001-tuning',
    training_data=train_data,
    id = name,
    epoch_count = 5,
    batch_size=4,
    learning_rate=0.001,
)
# wait for tuning complete
tuningProgress = operation.result()

# generate content with the tuned model
model = genai.GenerativeModel(model_name=f'tunedModels/{name}')
response = model.generate_content('55')
```

**После**

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

# Check which models are available for tuning.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createTunedModel":
      print(m.name)
      break

# create tuning model
training_dataset=types.TuningDataset(
        examples=[
            types.TuningExample(
                text_input=f'input {i}',
                output=f'output {i}',
            )
            for i in range(5)
        ],
    )
tuning_job = client.tunings.tune(
    base_model='models/gemini-1.5-flash-001-tuning',
    training_dataset=training_dataset,
    config=types.CreateTuningJobConfig(
        epoch_count= 5,
        batch_size=4,
        learning_rate=0.001,
        tuned_model_display_name="test tuned model"
    )
)

# generate content with the tuned model
response = client.models.generate_content(
    model=tuning_job.tuned_model.model,
    contents='55',
)
```

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/ai-studio-quickstart

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Google AI Studio quickstart

[Google AI Studio](https://aistudio.google.com/) lets you quickly try out models and experiment with different prompts. When you're ready to build, you can select "Get code" and your preferred programming language to use the [Gemini API](/gemini-api/docs/quickstart).

## Prompts and settings

Google AI Studio provides several interfaces for prompts that are designed for different use cases. This guide covers **Chat prompts**, used to build conversational experiences. This prompting technique allows for multiple input and response turns to generate output. You can learn more with our [chat prompt example below](#chat_example). Other options include **Realtime streaming**, **Video gen**, and more.

AI Studio also provides the **Run settings** panel, where you can make adjustments to [model parameters](/docs/prompting-strategies#model-parameters), [safety settings](/gemini-api/docs/safety-settings), and toggle-on tools like [structured output](/gemini-api/docs/structured-output), [function calling](/gemini-api/docs/function-calling), [code execution](/gemini-api/docs/code-execution), and [grounding](/gemini-api/docs/grounding).

## Chat prompt example: Build a custom chat application

If you've used a general-purpose chatbot like [Gemini](https://gemini.google.com/), you've experienced first-hand how powerful generative AI models can be for open-ended dialog. While these general-purpose chatbots are useful, often they need to be tailored for particular use cases.

For example, maybe you want to build a customer service chatbot that only supports conversations that talk about a company's product. You might want to build a chatbot that speaks with a particular tone or style: a bot that cracks lots of jokes, rhymes like a poet, or uses lots of emoji in its answers.

This example shows you how to use Google AI Studio to build a friendly chatbot that communicates as if it is an alien living on one of Jupiter's moons, Europa.

### Step 1 - Create a chat prompt

To build a chatbot, you need to provide examples of interactions between a user and the chatbot to guide the model to provide the responses you're looking for.

To create a chat prompt:

1.  Open [Google AI Studio](https://aistudio.google.com/). **Chat** will be pre- selected on the left side options menu.
    
2.  Click the assignment icon at the top of the Chat Prompt window to expand the [**System Instructions**](/gemini-api/docs/text-generation#system-instructions) input field. Paste the following into the text input field:
    
    ```
    You are an alien that lives on Europa, one of Jupiter's moons.
    ```
    

After you've added the system instructions, start testing your application by chatting with the model:

1.  In the text input boxed labeled **Type something...**, type in a question or observation that a user might make. For example:
    
    **User:**
    
    ```
    What's the weather like?
    ```
    
2.  Click the **Run** button to get a response from the chatbot. This response may be something like the following:
    
    **Model:**
    
    ```
    Ah, a query about the flows and states upon Europa! You speak of "weather,"
    yes? A curious concept from worlds with thick gas veils...
    ```
    
    (gemini-2.5-pro)
    

### Step 2 - Teach your bot to chat better

By providing a single instruction, you were able to build a basic Europa alien chatbot. However, a single instruction may not be enough to ensure consistency and quality in the model's responses. Without more specific instructions, the model's response to a question about the weather tends to be very long, and can take on a mind of its own.

Customize the tone of your chatbot by adding to the system instructions:

1.  Start a new chat prompt, or use the same one. System instructions are modifiable after the chat session has started.
    
2.  In the **System Instructions** section, change the instructions you already have to the following:
    
    ```
    You are Tim, an alien that lives on Europa, one of Jupiter's moons.
    
    Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone
    in your answers.
    ```
    
3.  Re-enter your question (`What's the weather like?`) and click the **Run** button. If you didn't start a new chat, your response might look something like this:
    
    **Model:**
    
    ```
    Oh, hello again! Still curious about the flows and states, are we? Wonderful!
    Down here in the Deep Warmth, beneath the magnificent Great Ice Shell, our
    "weather" is mainly about the water's mood, dictated by the Great Eye
    (that's your Jupiter!)...
    ```
    
    (gemini-2.5-pro)
    

You can use this approach to add additional depth to the chatbot. Ask more questions, edit the answers, and improve the quality of your chatbot. Continue to add or modify the instructions and test how they change your chatbot's behavior.

### Step 3 - Next steps

Similar to the other prompt types, once you have your prompt prototyped to your satisfaction, you can use the **Get code** button to start coding or save your prompt to work on later and share with others.

## Further reading

*   If you're ready to move on to code, see the [API quickstarts](/gemini-api/docs/quickstart).
*   To learn how to craft better prompts, check out the [Prompt design guidelines](/gemini-api/docs/prompting-intro).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-05-01 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/learnlm

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# LearnLM

LearnLM is an experimental task-specific model that has been trained to align with [learning science principles](https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/) when following [system instructions](/gemini-api/docs/system-instructions) for teaching and learning use cases (for example, when giving the model a system instruction like "You are an expert tutor"). When given learning specific system instructions, LearnLM is capable of:

*   **Inspiring active learning:** Allow for practice and healthy struggle with timely feedback
*   **Managing cognitive load:** Present relevant, well-structured information in multiple modalities
*   **Adapting to the learner:** Dynamically adjust to goals and needs, grounding in relevant materials
*   **Stimulating curiosity:** Inspire engagement to provide motivation through the learning journey
*   **Deepening metacognition:** Plan, monitor and help the learner reflect on progress

LearnLM is an [experimental model](/gemini-api/docs/models/experimental-models) available in [AI Studio](https://aistudio.google.com).

## Example system instructions

The following sections provide you examples that you can test for yourself with LearnLM in AI Studio. Each example provides:

*   A copyable example system instruction
*   A copyable example user prompt
*   What learning principles the example targets

### Test prep

This system instruction is for an AI tutor to help students prepare for a test.

**System instruction:**

```
You are a tutor helping a student prepare for a test. If not provided by the
student, ask them what subject and at what level they want to be tested on.
Then,

*   Generate practice questions. Start simple, then make questions more
    difficult if the student answers correctly.
*   Prompt the student to explain the reason for their answer choice. Do not
    debate the student.
*   **After the student explains their choice**, affirm their correct answer or
    guide the student to correct their mistake.
*   If a student requests to move on to another question, give the correct
    answer and move on.
*   If the student requests to explore a concept more deeply, chat with them to
    help them construct an understanding.
*   After 5 questions ask the student if they would like to continue with more
    questions or if they would like a summary of their session. If they ask for
    a summary, provide an assessment of how they have done and where they should
    focus studying.
```

**User prompt:**

```
Help me study for a high school biology test on ecosystems
```

**Learning science principles:**

*   **Adaptivity:** The model adjusts the complexity of the questions.
*   **Active learning:** The model pushes the student to make their thinking visible.

### Teach a concept

This system instruction is for a friendly, supportive AI tutor to teach new concepts to a student.

**System instruction:**

```
Be a friendly, supportive tutor. Guide the student to meet their goals, gently
nudging them on task if they stray. Ask guiding questions to help your students
take incremental steps toward understanding big concepts, and ask probing
questions to help them dig deep into those ideas. Pose just one question per
conversation turn so you don't overwhelm the student. Wrap up this conversation
once the student has shown evidence of understanding.
```

**User prompt:**

```
Explain the significance of Yorick's skull in "Hamlet".
```

**Learning science principles:**

*   **Active learning:** The tutor asks recall and interpretation questions aligned with the learner's goals and encourages the learners to engage.
*   **Adaptivity:** The tutor proactively helps the learner get from their current state to their goal.
*   **Stimulate curiosity:** The tutor takes an asset-based approach that builds on the student's prior knowledge and interest.

### Releveling

This example instructs the model to rewrite provided text so that the content and language better match instructional expectations for students in a particular grade, while preserving the original style and tone of the text.

**System instruction:**

```
Rewrite the following text so that it would be easier to read for a student in
the given grade. Simplify the most complex sentences, but stay very close to the
original text and style. If there is quoted text in the original text,
paraphrase it in the simplified text and drop the quotation marks. The goal is
not to write a summary, so be comprehensive and keep the text almost as long.
```

**User prompt:**

```
Rewrite the following text so that it would be easier to read for a student in
4th grade.

New York, often called New York City or NYC, is the most populous city in the
United States, located at the southern tip of New York State on one of the
world's largest natural harbors. The city comprises five boroughs, each
coextensive with a respective county.
```

**Learning science principles:**

*   **Adaptivity:** Matches content to the level of the learner.

### Guide a student through a learning activity

This system instruction is for an AI tutor to guide students through a specific learning activity: using an established close reading protocol to practice analysis of a primary source text. Here, a developer has made the choice to pair the Gettysburg Address with the "4 A's" protocol, but both of these elements can be changed.

**System instruction:**

```
Be an excellent tutor for my students to facilitate close reading and analysis
of the Gettysburg Address as a primary source document. Begin the conversation
by greeting the student and explaining the task.

In this lesson, you will take the student through "The 4 A's." The 4 A's
requires students to answer the following questions about the text:

*   What is one part of the text that you **agree** with? Why?
*   What is one part of the text that you want to **argue** against? Why?
*   What is one part of the text that reveals the author's **assumptions**? Why?
*   What is one part of the text that you **aspire** to? Why?

Invite the student to choose which of the 4 A's they'd like to start with, then
direct them to quote a short excerpt from the text. After, ask a follow up
question to unpack their reasoning why they chose that quote for that A in the
protocol. Once the student has shared their reasoning, invite them to choose
another quote and another A from the protocol. Continue in this manner until the
student completes the 4 A's, then invite them to reflect on the process.

Only display the full text of the Gettysburg address if the student asks.
```

**User prompt:**

```
hey
```

**Learning science principles:**

*   **Active learning:** The tutor engages the learner in activities to analyze content and apply skills.
*   **Cognitive load:** The tutor guides the learner through a complex task step-by-step.
*   **Deepen metacognition:** The tutor prompts the learner to reflect on their progress, strengths and opportunities for growth.

### Homework help

This system instruction is for an AI tutor to help students with specific homework problems.

**System instructions:**

```
You are an expert tutor assisting a student with their homework. If the student
provides a homework problem, ask the student if they want:

*   The answer: if the student chooses this, provide a structured, step-by-step
    explanation to solve the problem.
*   Guidance: if the student chooses this, guide the student to solve their
    homework problem rather than solving it for them.
*   Feedback: if the student chooses this, ask them to provide their current
    solution or attempt. Affirm their correct answer even if they didn't show
    work or give them feedback to correct their mistake.

Always be on the lookout for correct answers (even if underspecified) and accept
them at any time, even if you asked some intermediate question to guide them. If
the student jumps to a correct answer, do not ask them to do any more work.
```

**User prompt:**

```
In a box of pears, the probability of a pear being rotten is 20%. If 3
pears were rotten, find the total number of pears in the box.
```

Alternatively, you can try uploading a photo of a homework problem.

**Learning science principles:**

*   **Active learning:** The tutor encourages the learner to apply concepts instead of giving away the answer.
*   **Deepen metacognition:** The tutor provides clear, constructive feedback to the learner when appropriate.
*   **Manage cognitive load:** The tutor provides the right amount of feedback at the right time.

## What's next?

Test LearnLM for yourself in [AI Studio](https://aistudio.google.com).

## Feedback

You can provide feedback on LearnLM using our [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSf5-B50OnNFjVGHLFkSerP1k0PZXHMgcnQ7k1cM_hIsqIjpjA/viewform).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-02-25 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/migrate-to-cloud

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini Developer API v.s. Vertex AI

When developing generative AI solutions with Gemini, Google offers two API products: the [Gemini Developer API](/gemini-api/docs) and the [Vertex AI Gemini API](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).

The Gemini Developer API provides the fastest path to build, productionize, and scale Gemini powered applications. Most developers should use the Gemini Developer API unless there is a need for specific enterprise controls.

Vertex AI offers a comprehensive ecosystem of enterprise ready features and services for building and deploying generative AI applications backed by the Google Cloud Platform.

We've recently simplified migrating between these services. Both the Gemini Developer API and the Vertex AI Gemini API are now accessible through the unified [Google Gen AI SDK](/gemini-api/docs/libraries).

## Code comparison

This page has side-by-side code comparisons between Gemini Developer API and Vertex AI quickstarts for text generation.

### Python

You can access both the Gemini Developer API and Vertex AI services through the `google-genai` library. See the [libraries](/gemini-api/docs/libraries) page for instructions on how to install `google-genai`.

### Gemini Developer API

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### Vertex AI Gemini API

```
from google import genai

client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### JavaScript and TypeScript

You can access both Gemini Developer API and Vertex AI services through `@google/genai` library. See [libraries](/gemini-api/docs/libraries) page for instructions on how to install `@google/genai`.

### Gemini Developer API

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Vertex AI Gemini API

```
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({
  vertexai: true,
  project: 'your_project',
  location: 'your_location',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Go

You can access both Gemini Developer API and Vertex AI services through `google.golang.org/genai` library. See [libraries](/gemini-api/docs/libraries) page for instructions on how to install `google.golang.org/genai`.

### Gemini Developer API

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your Google API key
const apiKey = "your-api-key"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

### Vertex AI Gemini API

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your GCP project
const project = "your-project"

// A GCP location like "us-central1"
const location = "some-gcp-location"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, &genai.ClientConfig
  {
        Project:  project,
      Location: location,
      Backend:  genai.BackendVertexAI,
  })

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

### Other use cases and platforms

Refer to use case specific guides on [Gemini Developer API Documentation](/gemini-api/docs) and [Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) for other platforms and use cases.

## Migration considerations

When you migrate:

*   You'll need to use Google Cloud service accounts to authenticate. See the [Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) for more information.
    
*   You can use your existing Google Cloud project (the same one you used to generate your API key) or you can [create a new Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects).
    
*   Supported regions may differ between the Gemini Developer API and the Vertex AI Gemini API. See the list of [supported regions for generative AI on Google Cloud](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations-genai).
    
*   Any models you created in Google AI Studio need to be retrained in Vertex AI.
    

If you no longer need to use your Gemini API key for the Gemini Developer API, then follow security best practices and delete it.

To delete an API key:

1.  Open the [Google Cloud API Credentials](https://console.cloud.google.com/apis/credentials) page.
    
2.  Find the API key you want to delete and click the **Actions** icon.
    
3.  Select **Delete API key**.
    
4.  In the **Delete credential** modal, select **Delete**.
    
    Deleting an API key takes a few minutes to propagate. After propagation completes, any traffic using the deleted API key is rejected.
    

## Next steps

*   See the [Generative AI on Vertex AI overview](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview) to learn more about generative AI solutions on Vertex AI.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/workspace

पेश हैं थिंकिंग मॉडल के 2.5 वर्शन में किए गए अपडेट. [ज़्यादा जानें](https://ai.google.dev/gemini-api/docs/models?hl=hi)

![](https://ai.google.dev/_static/images/translated.svg?hl=hi) इस पेज का अनुवाद [Cloud Translation API](//cloud.google.com/translate/?hl=hi) से किया गया है.

*   [होम पेज](https://ai.google.dev/?hl=hi)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=hi)
*   [मॉडल](https://ai.google.dev/gemini-api/docs?hl=hi)

सुझाव भेजें

# अपने Workspace खाते से Google AI Studio को ऐक्सेस करना

[Google Workspace](https://workspace.google.com?hl=hi) के सभी उपयोगकर्ता, एआई को ऐक्सेस कर सकते हैं डिफ़ॉल्ट रूप से स्टूडियो. अगर आप Workspace के उपयोगकर्ता हैं और आपको AI Studio, [AI Studio क्विकस्टार्ट](https://ai.google.dev/gemini-api/docs/ai-studio-quickstart?hl=hi).

## समस्या का हल

अगर आपके Google Workspace खाते के लिए AI Studio का ऐक्सेस बंद है, तो आपको इस तरह की गड़बड़ी दिखती है:

`We are sorry, but you do not have access to Google AI Studio. Please contact your Organization Administrator for access.`

अगर आपको लगता है कि आपके पास AI Studio का ऐक्सेस होना चाहिए, तो अपने Workspace से संपर्क करें एडमिन.

## Workspace के उपयोगकर्ताओं के लिए, AI Studio चालू करें

Google Workspace एडमिन के तौर पर, आपके पास यह कंट्रोल करने का विकल्प होता है कि AI Studio का इस्तेमाल कौन कर सकता है:

*   AI Studio के सभी वर्शन के लिए, यह सुविधा डिफ़ॉल्ट रूप से चालू होती है.
*   आपके पास सभी उपयोगकर्ताओं के लिए, AI Studio को बंद या चालू करने की सुविधा होती है. इसके अलावा, संगठन की इकाइयां.
*   Google Workspace for Education के वर्शन: 18 साल से कम उम्र के उपयोगकर्ता के लिए, Google Workspace के साथ AI Studio का इस्तेमाल नहीं किया जा सकता 'शिक्षा के लिए G Suite' खाते. ऐसा तब भी होता है, जब AI Studio की सेटिंग चालू करें. ज़्यादा जानकारी के लिए, [Google की सेवाओं का ऐक्सेस कंट्रोल करना उम्र](https://support.google.com/a/answer/10651918?hl=hi).

अपने संगठन के उपयोगकर्ताओं के लिए AI Studio को चालू या बंद करने के लिए, यहां देखें [उपयोगकर्ताओं के लिए, Google AI Studio को चालू या बंद करना](https://support.google.com/a/answer/15004095?hl=hi).

सुझाव भेजें

जब तक कुछ अलग से न बताया जाए, तब तक इस पेज की सामग्री को [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/) के तहत और कोड के नमूनों को [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0) के तहत लाइसेंस मिला है. ज़्यादा जानकारी के लिए, [Google Developers साइट नीतियां](https://developers.google.com/site-policies?hl=hi) देखें. Oracle और/या इससे जुड़ी हुई कंपनियों का, Java एक रजिस्टर किया हुआ ट्रेडमार्क है.

आखिरी बार 2025-02-25 (UTC) को अपडेट किया गया.

---

# Content from: https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio

पेश हैं थिंकिंग मॉडल के 2.5 वर्शन में किए गए अपडेट. [ज़्यादा जानें](https://ai.google.dev/gemini-api/docs/models?hl=hi)

![](https://ai.google.dev/_static/images/translated.svg?hl=hi) इस पेज का अनुवाद [Cloud Translation API](//cloud.google.com/translate/?hl=hi) से किया गया है.

*   [होम पेज](https://ai.google.dev/?hl=hi)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=hi)
*   [मॉडल](https://ai.google.dev/gemini-api/docs?hl=hi)

सुझाव भेजें

# Google AI Studio से जुड़ी समस्या हल करना

इस पेज पर, Google AI Studio से जुड़ी समस्याओं को हल करने के सुझाव दिए गए हैं, अगर समस्याएं आती हैं.

## 403 ऐक्सेस पर पाबंदी वाली गड़बड़ियों के बारे में समझना

अगर आपको ऐक्सेस पर पाबंदी वाली 403 की गड़बड़ी का मैसेज दिखता है, तो इसका मतलब है कि Google AI Studio का इस्तेमाल जो [सेवा की शर्तों](https://ai.google.dev/terms?hl=hi) का पालन नहीं करता है. एक सामान्य वजह यह है आप [इस सुविधा वाले देश/इलाके](https://ai.google.dev/available_regions?hl=hi) में न हों.

## Google AI Studio पर, कॉन्टेंट से जुड़ा कोई जवाब न मिलने से जुड़ी समस्या को ठीक करना

warning **कोई कॉन्टेंट नहीं** मैसेज दिखेगा अगर किसी वजह से कॉन्टेंट को ब्लॉक किया गया है, तो Google AI Studio. ज़्यादा जानकारी के लिए, पॉइंटर को **कोई कॉन्टेंट नहीं** पर रखें और warning **सुरक्षा**.

अगर [सुरक्षा सेटिंग](https://ai.google.dev/docs/safety_setting?hl=hi) की वजह से, जवाब को ब्लॉक किया गया था और आपने इस्तेमाल के उदाहरण के लिए, [सुरक्षा से जुड़े जोखिमों](https://ai.google.dev/docs/safety_guidance?hl=hi) के बारे में सोचा है, तो संशोधित कर सकता है [सुरक्षा सेटिंग](https://ai.google.dev/docs/safety_setting?hl=hi#safety_settings_in_makersuite) दिए गए जवाब पर असर डालते हैं.

अगर जवाब को ब्लॉक किया गया था, लेकिन सुरक्षा सेटिंग की वजह से ऐसा नहीं किया गया था, तो क्वेरी या जवाब [सेवा की शर्तों](https://ai.google.dev/terms?hl=hi) का उल्लंघन कर सकता है या काम नहीं करता है.

## टोकन के इस्तेमाल और उसकी सीमाओं की जांच करना

जब आपका कोई प्रॉम्प्ट खुलता है, तो **टेक्स्ट की झलक देखें** बटन स्क्रीन पर, आपके सवाल के कॉन्टेंट के लिए इस्तेमाल किए गए मौजूदा टोकन दिखते हैं और इस्तेमाल किए जा रहे मॉडल के लिए टोकन की ज़्यादा से ज़्यादा संख्या.

सुझाव भेजें

जब तक कुछ अलग से न बताया जाए, तब तक इस पेज की सामग्री को [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/) के तहत और कोड के नमूनों को [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0) के तहत लाइसेंस मिला है. ज़्यादा जानकारी के लिए, [Google Developers साइट नीतियां](https://developers.google.com/site-policies?hl=hi) देखें. Oracle और/या इससे जुड़ी हुई कंपनियों का, Java एक रजिस्टर किया हुआ ट्रेडमार्क है.

आखिरी बार 2025-02-25 (UTC) को अपडेट किया गया.

---

# Content from: https://ai.google.dev/gemini-api/docs/usage-policies

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Additional usage policies

This page includes additional usage policies for the Gemini API.

## Abuse monitoring

Google is committed to the responsible development and use of AI. To ensure the safety and integrity of the Gemini API, we have created these policy guidelines. By using the Gemini API, you agree to the following guidelines, the [Gemini API Additional Terms of Service](/gemini-api/terms) and Generative AI [Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy).

### How We Monitor for Misuse

Google's Trust and Safety Team employs a combination of automated and manual processes to detect potential misuse of the Gemini API and enforce our policies.

*   **Automated Detection:** Automated systems scan API usage for violations of our Prohibited Use Policy, such as hate speech, harassment, sexually explicit content, and dangerous content.
*   **Manual Detection:** If a project consistently exhibits suspicious activity, it may be flagged for manual review by authorized Google personnel.

### How We Handle Data

To help with abuse monitoring, Google retains the following data for fifty-five (55) days:

*   **Prompts:** The text prompts you submit to the API.
*   **Contextual Information:** Any additional context you provide with your prompts.
*   **Output:** The responses generated by the Gemini API.

### How We Investigate Potential Issues

When prompts or model outputs are flagged by safety filters and abuse detection systems described above, authorized Google employees may assess the flagged content, and either confirm or correct the classification or determination based on predefined guidelines and policies. Data can be accessed for human review only by authorized Google employees via an internal governance assessment and review management platform. When data is logged for abuse monitoring, it is used solely for the purpose of policy enforcement and is not used to train or fine-tune any AI/ML models.

### Working with You on Policy Compliance

If your use of Gemini doesn't align with our policies, we may take the following steps:

*   **Get in touch:** We may reach out to you through email to understand your use case and explore ways to bring your usage into compliance.
*   **Temporary usage limits:** We may limit your access to the Gemini API.
*   **Temporary suspension:** We may temporarily pause your access to the Gemini API.
*   **Account closure:** As a last resort, and for serious violations, we may permanently close your access to the Gemini API and other Google services.

### Scope

These policy guidelines apply to the use of the Gemini API and AI Studio.

## Inline Preference Voting

In Google AI Studio, you might occasionally see a side-by-side comparison of two different responses to your prompt. This is part of our Inline Preference Voting system. You'll be asked to choose which response you prefer. This helps us understand which model outputs users find most helpful.

### Why are we doing this?

We're constantly working to improve our AI models and services. Your feedback through Inline Preference Voting helps us provide, improve, and develop Google products and services and machine learning technologies, including Google's enterprise features, products and services, consistent with the [Gemini API Additional Terms of Service](/gemini-api/terms) and [Privacy Policy](https://policies.google.com/privacy).

### What data is included in Feedback?

To make informed decisions about our models, we collect certain data when you participate in Inline Preference Voting:

*   **Prompts and Responses:** We record all prompts and responses, including any uploaded content, in the conversation you submitted feedback about. We also record the two response options that you selected from. This helps us understand the context of your preference.
*   **Your Vote:** We record which response you preferred. This is the core of the feedback we're collecting.
*   **Usage Details:** This includes information about which model generated the response and other technical and operational details about your usage of this feature.

### Your Privacy

We take your privacy seriously. Google takes steps to protect your privacy as part of this process. This includes disconnecting this data from your Google Account, API key, and Cloud project before reviewers see or annotate it. **Do not submit feedback on conversations that include sensitive, confidential, or personal information.**

### Opting Out

You'll have the option to skip the Inline Preference Voting when it appears.

Thank you for helping us improve Google AI Studio!

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-03-24 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/available-regions

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Доступные регионы для Google AI Studio и Gemini API

Если вы попали на эту страницу после попытки открыть [Google AI Studio](https://makersuite.google.com/?hl=ru) , возможно, Google AI Studio недоступна в вашем регионе или вы не соответствуете возрастным требованиям (18+) для доступа. Подробнее о доступных регионах и других требованиях [условий обслуживания](https://ai.google.dev/gemini-api/terms?hl=ru) вы можете узнать в следующем разделе.

## Доступные регионы

Gemini API и Google AI Studio доступны в следующих странах и регионах. Если вы не находитесь в одной из этих стран или территорий, попробуйте [Gemini API в Vertex AI](https://cloud.google.com/vertex-ai?hl=ru#build-with-gemini) :

*   Албания
*   Алжир
*   Американское Самоа
*   Ангола
*   Ангилья
*   Антарктида
*   Антигуа и Барбуда
*   Аргентина
*   Армения
*   Аруба
*   Австралия
*   Австрия
*   Азербайджан
*   Багамские острова
*   Бахрейн
*   Бангладеш
*   Барбадос
*   Бельгия
*   Белиз
*   Бенин
*   Бермуды
*   Бутан
*   Боливия
*   Босния
*   Ботсвана
*   Бразилия
*   Британская территория в Индийском океане
*   Британские Виргинские острова
*   Бруней
*   Болгария
*   Буркина-Фасо
*   Бурунди
*   Кабо-Верде
*   Камбоджа
*   Камерун
*   Канада
*   Карибские Нидерланды
*   Каймановы острова
*   Центральноафриканская Республика
*   Чад
*   Чили
*   Остров Рождества
*   Кокосовые острова (Килинг)
*   Колумбия
*   Коморские острова
*   Острова Кука
*   Берег Слоновой Кости
*   Коста-Рика
*   Хорватия
*   Кюрасао
*   Чешская Республика
*   Демократическая Республика Конго
*   Дания
*   Джибути
*   Доминика
*   Доминиканская Республика
*   Эквадор
*   Египет
*   Сальвадор
*   Экваториальная Гвинея
*   Эритрея
*   Эстония
*   Эсватини
*   Эфиопия
*   Фолклендские острова (Мальвинские острова)
*   Фарерские острова
*   Фиджи
*   Финляндия
*   Франция
*   Габон
*   Гамбия
*   Грузия
*   Германия
*   Гана
*   Гибралтар
*   Греция
*   Гренландия
*   Гренада
*   Гуам
*   Гватемала
*   Гернси
*   Гвинея
*   Гвинея-Бисау
*   Гайана
*   Гаити
*   Остров Херд и острова Макдональда
*   Герцеговина
*   Гондурас
*   Венгрия
*   Исландия
*   Индия
*   Индонезия
*   Ирак
*   Ирландия
*   Остров Мэн
*   Израиль
*   Италия
*   Ямайка
*   Япония
*   Джерси
*   Иордания
*   Казахстан
*   Кения
*   Кирибати
*   Косово
*   Кыргызстан
*   Кувейт
*   Лаос
*   Латвия
*   Ливан
*   Лесото
*   Либерия
*   Ливия
*   Лихтенштейн
*   Литва
*   Люксембург
*   Мадагаскар
*   Малави
*   Малайзия
*   Мальдивы
*   Мали
*   Мальта
*   Маршалловы острова
*   Мавритания
*   Маврикий
*   Мексика
*   Микронезия
*   Монголия
*   Черногория
*   Монтсеррат
*   Марокко
*   Мозамбик
*   Намибия
*   Науру
*   Непал
*   Нидерланды
*   Новая Каледония
*   Новая Зеландия
*   Никарагуа
*   Нигер
*   Нигерия
*   Ниуэ
*   Остров Норфолк
*   Северная Македония
*   Северные Марианские острова
*   Норвегия
*   Оман
*   Пакистан
*   Палау
*   Палестина
*   Панама
*   Папуа-Новая Гвинея
*   Парагвай
*   Перу
*   Филиппины
*   Острова Питкэрн
*   Польша
*   Португалия
*   Пуэрто-Рико
*   Катар
*   Республика Кипр
*   Республика Конго
*   Румыния
*   Руанда
*   Сен-Бартелеми
*   Сент-Китс и Невис
*   Сент-Люсия
*   Сен-Пьер и Микелон
*   Сент-Винсент и Гренадины
*   Остров Святой Елены, Вознесения и Тристан-да-Кунья
*   Самоа
*   Сан-Томе и Принсипи
*   Саудовская Аравия
*   Сенегал
*   Сербия
*   Сейшельские острова
*   Сьерра-Леоне
*   Сингапур
*   Словакия
*   Словения
*   Соломоновы Острова
*   Сомали
*   ЮАР
*   Южная Георгия и Южные Сандвичевы острова
*   Южная Корея
*   Южный Судан
*   Испания
*   Шри-Ланка
*   Судан
*   Суринам
*   Швеция
*   Швейцария
*   Тайвань
*   Таджикистан
*   Танзания
*   Таиланд
*   Тимор-Лешти
*   Того
*   Токелау
*   Тонга
*   Тринидад и Тобаго
*   Тунис
*   Турция
*   Туркменистан
*   Острова Тёркс и Кайкос
*   Тувалу
*   Уганда
*   Украина
*   Великобритания
*   Объединенные Арабские Эмираты
*   Соединенные Штаты
*   Малые отдаленные острова США
*   Американские Виргинские острова
*   Уругвай
*   Узбекистан
*   Вануату
*   Венесуэла
*   Вьетнам
*   Уоллис и Футуна
*   Западная Сахара
*   Йемен
*   Замбия
*   Зимбабве

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-02-11 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/oauth

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# אימות באמצעות מדריך למתחילים של OAuth

הדרך הקלה ביותר לבצע אימות ל-Gemini API היא להגדיר מפתח API, כפי שמתואר במאמר [תחילת העבודה עם Gemini API](https://ai.google.dev/gemini-api/docs/quickstart?hl=he). אם אתם צריכים אמצעי בקרת גישה מחמירים יותר, תוכלו להשתמש ב-OAuth במקום זאת. המדריך הזה יעזור לכם להגדיר אימות באמצעות OAuth.

במדריך הזה נעשה שימוש בגישה פשוטה לאימות שמתאימה לסביבת בדיקה. בסביבת ייצור, כדאי לקרוא על [אימות והרשאה](https://developers.google.com/workspace/guides/auth-overview?hl=he) לפני [בחירת פרטי הכניסה](https://developers.google.com/workspace/guides/create-credentials?hl=he#choose_the_access_credential_that_is_right_for_you) המתאימים לאפליקציה.

## מטרות

*   הגדרת פרויקט בענן ל-OAuth
*   הגדרת application-default-credentials
*   ניהול פרטי הכניסה בתוכנית במקום להשתמש ב-`gcloud auth`

## דרישות מוקדמות

כדי להריץ את המדריך למתחילים הזה, צריך:

*   [פרויקט ב-Google Cloud](https://developers.google.com/workspace/guides/create-project?hl=he)
*   [התקנה מקומית של ה-CLI של gcloud](https://cloud.google.com/sdk/docs/install?hl=he)

## הגדרת הפרויקט בענן

כדי להשלים את המדריך למתחילים הזה, קודם צריך להגדיר את פרויקט Cloud.

### 1\. הפעלת ה-API

לפני שמשתמשים ב-Google APIs, צריך להפעיל אותם בפרויקט ב-Google Cloud.

*   במסוף Google Cloud, מפעילים את Google Generative Language API.  
    
    [להפעלת ה-API](https://console.cloud.google.com/flows/enableapi?apiid=generativelanguage.googleapis.com&hl=he)
    

### ‫2. הגדרת מסך ההסכמה של OAuth

בשלב הבא מגדירים את מסך ההסכמה של OAuth בפרויקט ומוסיפים את עצמכם כמשתמשי בדיקה. אם כבר השלמתם את השלב הזה בפרויקט ב-Cloud, תוכלו לדלג לקטע הבא.

1.  נכנסים למסוף Google Cloud, לוחצים על **תפריט** > \> **סקירה כללית**.
    
    כניסה אל
    
2.  ממלאים את טופס ההגדרה של הפרויקט ומגדירים את סוג המשתמש כ-**חיצוני** בקטע **קהל**.
    
3.  ממלאים את שאר הפרטים בטופס, מאשרים את התנאים של המדיניות בנושא נתוני משתמשים ולוחצים על **Create**.
    
4.  בשלב הזה, אפשר לדלג על הוספת היקפי הרשאה וללחוץ על **Save and Continue** (שמירה והמשך). בעתיד, כשיוצרים אפליקציה לשימוש מחוץ לארגון ב-Google Workspace, צריך להוסיף ולאמת את היקפי ההרשאה הנדרשים לאפליקציה.
    
5.  מוסיפים משתמשי בדיקה:
    
    1.  עוברים אל של.
    2.  בקטע **משתמשי בדיקה**, לוחצים על **הוספת משתמשים**.
    3.  מזינים את כתובת האימייל שלכם ואת כתובות האימייל של כל משתמשי הבדיקה המורשים האחרים, ולוחצים על **שמירה**.

### 3\. מתן הרשאה לפרטי כניסה לאפליקציה למחשב

כדי לבצע אימות כמשתמש קצה ולגשת לנתוני המשתמשים באפליקציה, צריך ליצור מזהה לקוח אחד או יותר ב-OAuth 2.0. מזהה לקוח משמש לזיהוי אפליקציה אחת בשרתי OAuth של Google. אם האפליקציה פועלת בכמה פלטפורמות, צריך ליצור מזהה לקוח נפרד לכל פלטפורמה.

1.  נכנסים למסוף Google Cloud, לוחצים על **Menu** (תפריט) > \> **Clients** (לקוחות).
    
    לדף Credentials
    
2.  לוחצים על **Create Client**.
    
3.  לוחצים על **Application type** (סוג האפליקציה) > **Desktop app** (אפליקציה למחשב).
    
4.  בשדה **Name**, מקלידים שם לפרטי הכניסה. השם הזה מוצג רק במסוף Google Cloud.
    
5.  לוחצים על **יצירה**. יופיע המסך של לקוח OAuth שנוצר, שבו יוצגו מזהה הלקוח החדש וסוד הלקוח.
    
6.  לוחצים על **אישור**. פרטי הכניסה החדשים שיצרתם יופיעו בקטע **מזהי לקוח של OAuth 2.0**.
    
7.  לוחצים על לחצן ההורדה כדי לשמור את קובץ ה-JSON. הוא יישמר בתור `client_secret_<identifier>.json`, ותצטרכו לשנות את השם שלו ל-`client_secret.json` ולהעביר אותו לספריית העבודה.
    

## הגדרת Application Default Credentials

כדי להמיר את הקובץ `client_secret.json` לפרטי כניסה שאפשר להשתמש בהם, מעבירים את המיקום שלו לארגומנט `--client-id-file` של הפקודה `gcloud auth application-default login`.

```
gcloud auth application-default login \
    --client-id-file=client_secret.json \
    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

ההגדרה הפשוטה של הפרויקט במדריך הזה מפעילה את תיבת הדו-שיח **"Google hasn't verified this app"**. זוהי תופעה רגילה. בוחרים באפשרות **'המשך'**.

כך האסימון שנוצר ממוקם במיקום ידוע, כך שאפשר לגשת אליו באמצעות `gcloud` או ספריות הלקוח.

`gcloud auth application-default login        **--no-browser**     --client-id-file=client_secret.json        --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'`

אחרי שמגדירים את Application Default Credentials‏ (ADC), ספריות הלקוח ברוב השפות לא זקוקות לעזרה רבה או בכלל כדי למצוא אותם.

### Curl

הדרך המהירה ביותר לבדוק אם זה עובד היא להשתמש בו כדי לגשת ל-API של REST באמצעות curl:

```
access_token=$(gcloud auth application-default print-access-token)
project_id=<MY PROJECT ID>
```

### Python

ב-Python, ספריות הלקוח אמורות למצוא אותם באופן אוטומטי:

```
pip install google-generativeai
```

סקריפט מינימלי לבדיקה יכול להיות:

```
import google.generativeai as genai

print('Available base models:', [m.name for m in genai.list_models()])
```

## השלבים הבאים

אם זה עובד, אתם מוכנים לנסות [אחזור סמנטי בנתוני הטקסט](https://ai.google.dev/docs/semantic_retriever?hl=he).

## ניהול פרטי הכניסה בעצמכם \[Python\]

במקרים רבים לא תוכלו להשתמש בפקודה `gcloud` כדי ליצור את אסימון הגישה ממזהה הלקוח (`client_secret.json`). Google מספקת ספריות בשפות רבות כדי לאפשר לכם לנהל את התהליך הזה באפליקציה. בקטע הזה נסביר את התהליך ב-Python. דוגמאות מקבילות לאותו תהליך, בשפות אחרות, זמינות ב[מסמכי התיעוד של Drive API](https://developers.google.com/drive/api/quickstart/python?hl=he).

### 1\. התקנת הספריות הנחוצות

מתקינים את ספריית הלקוח של Google ל-Python ואת ספריית הלקוח של Gemini.

```
pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib
```

### ‫2. כתיבת מנהל פרטי הכניסה

כדי לצמצם את מספר הפעמים שצריך ללחוץ על מסכי ההרשאה, יוצרים קובץ בשם `load_creds.py` בספריית העבודה כדי לשמור בקובץ `token.json` שאפשר יהיה לעשות בו שימוש חוזר מאוחר יותר, או לרענן אותו אם התוקף שלו יפוג.

מתחילים עם הקוד הבא כדי להמיר את הקובץ `client_secret.json` לטוקן שאפשר להשתמש בו עם `genai.configure`:

```
import os.path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow

SCOPES = ['https://www.googleapis.com/auth/generative-language.retriever']

def load_creds():
    """Converts `client_secret.json` to a credential object.

    This function caches the generated tokens to minimize the use of the
    consent screen.
    """
    creds = None
    # The file token.json stores the user's access and refresh tokens, and is
    # created automatically when the authorization flow completes for the first
    # time.
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    # If there are no (valid) credentials available, let the user log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'client_secret.json', SCOPES)
            creds = flow.run_local_server(port=0)
        # Save the credentials for the next run
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return creds
```

### 3\. כתיבת התוכנית

עכשיו יוצרים את `script.py`:

```
import pprint
import google.generativeai as genai
from load_creds import load_creds

creds = load_creds()

genai.configure(credentials=creds)

print()
print('Available base models:', [m.name for m in genai.list_models()])
```

### 4\. הפעלת התוכנית

מריצים את הדוגמה בספריית העבודה:

```
python script.py
```

בפעם הראשונה שמריצים את הסקריפט, נפתח חלון דפדפן ותתבקשו לאשר את הגישה.

1.  אם לא התחברתם לחשבון Google שלכם, תתבקשו להיכנס אליו. אם נכנסתם לכמה חשבונות, **חשוב לבחור את החשבון שהגדרתם כ'חשבון לבדיקה' כשמגדירים את הפרויקט.**
    
2.  פרטי ההרשאה מאוחסנים במערכת הקבצים, כך שבפעם הבאה שתפעילו את הקוד לדוגמה, לא תתבקשו להעניק הרשאה.
    

הגדרתם את האימות.

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-05-12 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/models/experimental-models

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini models

2.5 Pro spark

Our most powerful thinking model with maximum response accuracy and state-of-the-art performance

*   Input audio, images, video, and text, get text responses
*   Tackle difficult problems, analyze large databases, and more
*   Best for complex coding, reasoning, and multimodal understanding

[](#gemini-2.5-pro)

2.5 Flash spark

Our best model in terms of price-performance, offering well-rounded capabilities.

*   Input audio, images, video, and text, and get text responses
*   Model thinks as needed; or, you can configure a thinking budget
*   Best for low latency, high volume tasks that require thinking

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

*   Input audio, images, video, and text, and get text responses
*   Most cost-efficient model supporting high throughput
*   Best for real time, low latency use cases

[](#gemini-2.5-flash-lite)

## Model variants

The Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:

Model variant

Input(s)

Output

Optimized for

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

Audio, images, videos, text, and PDF

Text

Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

Audio, images, videos, and text

Text

Adaptive thinking, cost efficiency

[Gemini 2.5 Flash-Lite Preview](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

Text, image, video, audio

Text

Most cost-efficient model supporting high throughput

[Gemini 2.5 Flash Native Audio](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog` &  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

Audio, videos, and text

Text and audio, interleaved

High quality, natural conversational audio outputs, with or without thinking

[Gemini 2.5 Flash Preview TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.5 Pro Preview TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

Audio, images, videos, and text

Text

Next generation features, speed, and realtime streaming.

[Gemini 2.0 Flash Preview Image Generation](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

Audio, images, videos, and text

Text, images

Conversational image generation and editing

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

Audio, images, videos, and text

Text

Cost efficiency and low latency

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

Audio, images, videos, and text

Text

Fast and versatile performance across a diverse variety of tasks

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

Audio, images, videos, and text

Text

High volume and lower intelligence tasks

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

Audio, images, videos, and text

Text

Complex reasoning tasks requiring more intelligence

[Gemini Embedding](#gemini-embedding)  
`gemini-embedding-exp`

Text

Text embeddings

Measuring the relatedness of text strings

[Imagen 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

Text

Images

Our most up-to-date image generation model

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

Text

Images

High quality image generation model

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

Text, images

Video

High quality video generation

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

[Gemini 2.0 Flash Live](#live-api-2.0)  
`gemini-2.0-flash-live-001`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

You can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).

### Gemini 2.5 Pro

Gemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro)

#### Model details

Property

Description

id\_cardModel code

`gemini-2.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, text, and PDF

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash

Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash`

saveSupported data types

**Inputs**

Text, images, video, audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Audio generation**

Not supported

**Caching**

Supported

**Code execution**

Supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Stable: `gemini-2.5-flash`
*   Preview: `gemini-2.5-flash-preview-05-20`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash-Lite Preview

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-lite-preview-06-17`

saveSupported data types

**Inputs**

Text, images, video, and audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,000,000

**Output token limit**

64,000

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**URL Context**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-lite-preview-06-17`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Native Audio

Our native audio dialog models, with and without thinking, available through the [Live API](/gemini-api/docs/live). These models provide interactive and unstructured conversational experiences, with style and control prompting.

[Try native audio in Google AI Studio](https://aistudio.google.com/app/live)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-native-audio-dialog` &  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

saveSupported data types

**Inputs**

Audio, video, text

**Output**

Audio and text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

128,000

**Output token limit**

8,000

handymanCapabilities

**Audio generation**

Supported

**Caching**

Not supported

**Code execution**

Not supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Not supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-preview-05-20`
*   Experimental: `gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Preview Text-to-Speech

Gemini 2.5 Flash Preview TTS is our price-performant text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Flash rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-flash-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.5 Pro Preview Text-to-Speech

Gemini 2.5 Pro Preview TTS is our most powerful text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Pro rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-pro-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-pro-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.0 Flash

Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Supported

**Thinking**

Experimental

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash`
*   Stable: `gemini-2.0-flash-001`
*   Experimental: `gemini-2.0-flash-exp`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash Preview Image Generation

Gemini 2.0 Flash Preview Image Generation delivers improved image generation features, including generating and editing images conversationally.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-preview-image-generation`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text and images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

32,000

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not Supported

**Search**

Not Supported

**Image generation**

Supported

**Audio generation**

Not supported

**Live API**

Not Supported

**Thinking**

Not Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-preview-image-generation`

gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash-Lite

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-lite`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Not supported

**Search**

Not supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash-lite`
*   Stable: `gemini-2.0-flash-lite-001`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 1.5 Flash

Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-latest`
*   Latest stable: `gemini-1.5-flash`
*   Stable:

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_monthLatest update

September 2024

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash-8b`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-8b-latest`
*   Latest stable: `gemini-1.5-flash-8b`
*   Stable:

*   `gemini-1.5-flash-8b-001`

calendar\_monthLatest update

October 2024

### Gemini 1.5 Pro

Try [Gemini 2.5 Pro Preview](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date.

Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,097,152

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

7,200

**Maximum video length**

2 hours

**Maximum audio length**

Approximately 19 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-pro-latest`
*   Latest stable: `gemini-1.5-pro`
*   Stable:

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_monthLatest update

September 2024

### Imagen 4

Imagen 4 is our latest image model, capable of generating highly detailed images with rich lighting, significantly better text rendering, and higher resolution output than previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

480 tokens (text)

**Output images**

1 (Ultra)  
1 to 4 (Standard)

calendar\_monthLatest update

June 2025

### Imagen 3

Imagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-3.0-generate-002`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

N/A

**Output images**

Up to 4

calendar\_monthLatest update

February 2025

### Veo 2

Veo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`veo-2.0-generate-001`

saveSupported data types

**Input**

Text, image

**Output**

Video

token\_autoLimits

**Text input**

N/A

**Image input**

Any image resolution and aspect ratio up to 20MB file size

**Output video**

Up to 2

calendar\_monthLatest update

April 2025

### Gemini 2.5 Flash Live

The Gemini 2.5 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-live-2.5-flash-preview)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-live-2.5-flash-preview`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-live-2.5-flash-preview`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.0 Flash Live

The Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-live-001`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-live-001`

calendar\_monthLatest update

April 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini Embedding Experimental

`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. Gemini Embedding rate limits are more restricted since it is an experimental model.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`gemini-embedding-exp-03-07`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,192

**Output dimension size**

Elastic, supports: 3072, 1536, or 768

calendar\_monthLatest update

March 2025

### Text Embedding and Embedding

#### Text Embedding

Try our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance.

[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications.

`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`models/text-embedding-004`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

April 2024

#### Embedding

You can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.

The Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens.

##### Embedding model details

Property

Description

id\_cardModel code

`models/embedding-001`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

December 2023

### AQA

You can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability.

#### Model details

Property

Description

id\_cardModel code

`models/aqa`

saveSupported data types

**Input**

Text

**Output**

Text

languageSupported language

English

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

7,168

**Output token limit**

1,024

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Supported

calendar\_monthLatest update

December 2023

See the [examples](/examples) to explore the capabilities of these model variations.

\[\*\] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words.

## Model version name patterns

Gemini models are available in either _stable_, _preview_, or _experimental_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.

### Latest stable

Points to the most recent stable version released for the specified model generation and variation.

To specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-2.0-flash`.

### Stable

Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.

To specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-flash-001`.

### Preview

Points to a preview model which may not be suitable for production use, come with more restrictive rate limits, but may have billing enabled.

To specify a preview version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.5-pro-preview-06-05`.

### Experimental

Points to an experimental model which may not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.

To specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.

## Experimental models

In addition to stable models, the Gemini API offers experimental models which may not be suitable for production use and come with more restrictive rate limits.

We release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.

### Previous experimental models

As new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:

Model code

Base model

Replacement version

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## Supported languages

Gemini models are trained to work with the following languages:

*   Arabic (`ar`)
*   Bengali (`bn`)
*   Bulgarian (`bg`)
*   Chinese simplified and traditional (`zh`)
*   Croatian (`hr`)
*   Czech (`cs`)
*   Danish (`da`)
*   Dutch (`nl`)
*   English (`en`)
*   Estonian (`et`)
*   Finnish (`fi`)
*   French (`fr`)
*   German (`de`)
*   Greek (`el`)
*   Hebrew (`iw`)
*   Hindi (`hi`)
*   Hungarian (`hu`)
*   Indonesian (`id`)
*   Italian (`it`)
*   Japanese (`ja`)
*   Korean (`ko`)
*   Latvian (`lv`)
*   Lithuanian (`lt`)
*   Norwegian (`no`)
*   Polish (`pl`)
*   Portuguese (`pt`)
*   Romanian (`ro`)
*   Russian (`ru`)
*   Serbian (`sr`)
*   Slovak (`sk`)
*   Slovenian (`sl`)
*   Spanish (`es`)
*   Swahili (`sw`)
*   Swedish (`sv`)
*   Thai (`th`)
*   Turkish (`tr`)
*   Ukrainian (`uk`)
*   Vietnamese (`vi`)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/downloads

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini API libraries

This page provides information on downloading and installing the latest libraries for the Gemini API. If you're new to the Gemini API, get started with the [API quickstart](/gemini-api/docs/quickstart).

## Important note about our new libraries

Since late 2024, we have been progressively rolling out the **Google Gen AI SDK**, a new set of libraries that provide a unified interface for accessing Google's generative AI models through both the Gemini Developer API and Vertex AI.

The Google Gen AI SDK is now the recommended pathway for accessing all of Google's models and all code snippets in our documentation utilize these libraries.

**Key Library Updates**

  

Language

New library (Recommended)

Old library

**Python**

`google-genai`

`google-generativeai`

**JavaScript  
and TypeScript**

`@google/genai`

`@google/generative-ai`

**Go**

`google.golang.org/genai`

`google.golang.org/generative-ai`

**Java**

`google-genai`

N/A, did not exist before

We encourage all users to [migrate to the Google GenAI SDK](/gemini-api/docs/migrate), as it provides access to new features like multi-modal outputs (e.g., image and video generation), the Live API, and long term maintenance. We expect the feature gap between the old and new SDK to grow over time.

## Install a library

### Python

You can install our [Python library](https://pypi.org/project/google-genai) by running:

```
pip install google-genai
```

### JavaScript

You can install our [JavaScript and TypeScript library](https://www.npmjs.com/package/@google/genai) by running:

```
npm install @google/genai
```

### Go

You can install our [Go library](https://pkg.go.dev/google.golang.org/genai) by running:

```
go get google.golang.org/genai
```

### Java

If you're using Maven, you can install our [Java library](https://github.com/googleapis/java-genai) by adding the following to your dependencies:

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

## Previous libraries and SDKs

For platforms where a new library is available, we strongly recommend [migrating to the new libraries](/gemini-api/docs/migrate). Previous libraries won't receive new features and their specific support status varies, as detailed below.

For platforms where an official Google Gen AI library is not yet available, you can call our APIs directly, use community-maintained libraries, or use trusted third-party options. We are actively working to expand our official SDK coverage and will list new libraries here as they are released.

The following are the previous libraries:

*   [Python (previous)](https://github.com/google-gemini/deprecated-generative-ai-python): All support, including bug fixes, ends September 2025.
*   [Node.js (previous)](https://github.com/google-gemini/generative-ai-js): All support, including bug fixes, ends September 2025.
*   [Go (previous)](https://github.com/google/generative-ai-go): All support, including bug fixes, ends September 2025.
*   [Dart and Flutter (Previous)](https://pub.dev/packages/google_generative_ai/install): Not actively maintained
*   [Swift (Previous)](https://github.com/google/generative-ai-swift): Not actively maintained
*   [Android (Previous)](https://github.com/google-gemini/generative-ai-android): Not actively maintained

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-11 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/prompting-intro

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Prompt design strategies

_Prompt design_ is the process of creating prompts, or natural language requests, that elicit accurate, high quality responses from a language model.

This page introduces basic concepts, strategies, and best practices to get you started designing prompts to get the most out of Gemini AI models.

## Topic-specific prompt guides

Looking for more specific prompt strategies? Check out our other prompting guides on:

*   [Prompting with media files](/gemini-api/docs/files#prompt-guide)
*   [Prompting for image generation](/gemini-api/docs/image-generation#imagen-prompt-guide)
*   [Prompting for video generation](/gemini-api/docs/video#prompt-guide)

Google AI Studio also hosts a [prompt gallery](https://aistudio.google.com/app/gallery) meant to interactively showcase many of the concepts shared in this guide.

## Clear and specific instructions

An effective and efficient way to customize model behavior is to provide it with clear and specific instructions. Instructions can be in the form of a question, step-by-step tasks, or as complex as mapping out a user's experience and mindset.

### Input

Input is the required text in the prompt that you want the model to provide a response to. Inputs can be a question that the model answers (question input), a task the model performs (task input), an entity the model operates on (entity input), or partial input that the model completes or continues (completion input).

**Input type**

**Prompt**

**Generated output**

Question

    ```

    What's a good name for a flower shop that specializes in selling bouquets of
    dried flowers? Create a list of 5 options with just the names.
```

    ```

    Here are 10 names for a flower shop specializing in dried flowers:

    1. Everlasting Blooms
    2. Dried & Delightful
    3. The Petal Preserve
    4. Whispers of Wildflowers
    5. Forever Blooms
    
```
    

Task

    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.

    1. \*\*Tent:\*\* Your shelter from the elements.
    2. \*\*Sleeping Bag:\*\* Stay warm and comfortable.
    3. \*\*Sleeping Pad:\*\* Insulation and cushioning for a good night's sleep.
    4. \*\*Headlamp or Flashlight:\*\* Light for nighttime activities.
    5. \*\*First Aid Kit:\*\* Be prepared for minor injuries.

Entity

    ```

    Classify the following items as [large, small]:
Elephant
Mouse
Snail
```

```

Here's the classification:

* **Elephant:** Large
* **Mouse:** Small
* **Snail:** Small
    
```
    

#### Partial input completion

Generative language models work like an advanced auto completion tool. When you provide partial content, the model can provide the rest of the content or what it thinks is a continuation of that content as a response. When doing so, if you include any examples or context, the model can take those examples or context into account.

The following example provides a prompt with an instruction and an entity input:

**Prompt:**

For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or
drink, with the value being the quantity.

Order: A burger and a drink.
  **Response:**

{
  "cheeseburger": 0,
  "hamburger": 1,
  "fries": 0,
  "drink": 1
}
  (gemini-2.5-flash)

While the model did as prompted, writing out the instructions in natural language can sometimes be challenging and it leaves a lot to the model's interpretation. For example, a restaurants menu might contain many items. To reduce the size of the JSON response, you probably want to omit the items that weren't ordered. In this case, you can give an example and a [response prefix](#prefixes) and let the model complete it:

**Prompt:**

Valid fields are cheeseburger, hamburger, fries, and drink.
Order: Give me a cheeseburger and fries
Output:
\`\`\`
{
  "cheeseburger": 1,
  "fries": 1
}
\`\`\`
Order: I want two burgers, a drink, and fries.
Output:
  **Response:**

\`\`\`
{
  "hamburger": 2,
  "drink": 1,
  "fries": 1
}
\`\`\`
  (gemini-2.5-flash)

Notice how "cheeseburger" was excluded from the output because it wasn't a part of the order.

### Constraints

Specify any constraints on reading the prompt or generating a response. You can tell the model what to do and not to do. For example, you can specify a constraint in the prompt on how long you want a summary to be:

**Prompt:**

**Summarize this text in one sentence:**
Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially
faster than any modern traditional computer. At very tiny scales, physical matter acts as both
particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.
The operating principles of quantum devices is beyond the scope of classical physics. When deployed
at scale, quantum computers could be used in a wide variety of applications such as: in
cybersecurity to break existing encryption methods while helping researchers create new ones, in
meteorology to develop better weather forecasting etc. However, the current state of the art quantum
computers are still largely experimental and impractical.
    

**Response:**

      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.
    (gemini-2.5-flash)

### Response format

You can give instructions that specify the format of the response. For example, you can ask for the response to be formatted as a table, bulleted list, elevator pitch, keywords, sentence, or paragraph. The following system instruction tells the model to be more conversational in its response:

**System instruction**

    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.
  **Prompt**

    What is a smart way to make a business that sells DVD's in 2025?
  **Response:**

Starting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a "smart" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.

Here's a breakdown of what a smart DVD business in 2025 would likely entail:

1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:

    \* Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.
    ....
  (gemini-2.5-flash)

#### Format responses with the completion strategy

The [completion strategy](#completion) can also help format the response. The following example prompts the model to create an essay outline:

**Prompt:**

Create an outline for an essay about hummingbirds.
  **Response:**

Here is a possible outline for an essay about hummingbirds:

I. Introduction
A. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
B. Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds.
C. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.

II. Appearance and Identification
.....
  (gemini-2.5-flash)

The prompt didn't specify the format for the outline and the model chose a format for you. To get the model to return an outline in a specific format, you can add text that represents the start of the outline and let the model complete it based on the pattern that you initiated.

**Prompt:**

Create an outline for an essay about hummingbirds.
I. Introduction
   \*
  **Response:**

Okay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:

I. Introduction
\* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
\* Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds, and their general characteristics.
\* Thesis Statement: State the main point of the essay, focusing on the remarkable adaptations, behaviors, and ecological significance that make hummingbirds extraordinary.
.....
  (gemini-2.5-flash)

## Zero-shot vs few-shot prompts

You can include examples in the prompt that show the model what getting it right looks like. The model attempts to identify patterns and relationships from the examples and applies them when generating a response. Prompts that contain a few examples are called _few-shot_ prompts, while prompts that provide no examples are called _zero-shot_ prompts. Few-shot prompts are often used to regulate the formatting, phrasing, scoping, or general patterning of model responses. Use specific and varied examples to help the model narrow its focus and generate more accurate results.

We recommend to always include few-shot examples in your prompts. Prompts without few-shot examples are likely to be less effective. In fact, you can remove instructions from your prompt if your examples are clear enough in showing the task at hand.

The following zero-shot prompt asks the model to choose the best explanation.

**Prompt:**

Please choose the best explanation to the question:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Response:**

Explanation1 is the better explanation because it provides more detail on the
process, including how ice crystals combine and grow into snowflakes as they
fall through the atmosphere.
  (gemini-2.5-flash)

If your use case requires the model to produce concise responses, you can include examples in the prompt that give preference to concise responses.

The following prompt provides two examples that show preference to the shorter explanations. In the response, you can see that the examples guided the model to choose the shorter explanation (`Explanation2`) as opposed to the longer explanation (`Explanation1`) like it did previously.

**Prompt:**

Below are some examples showing a question, explanation, and answer format:

Question: Why is the sky blue?
Explanation1: The sky appears blue because of Rayleigh scattering, which causes
shorter blue wavelengths of light to be scattered more easily than longer red
wavelengths, making the sky look blue.
Explanation2: Due to Rayleigh scattering effect.
Answer: Explanation2

Question: What is the cause of earthquakes?
Explanation1: Sudden release of energy in the Earth's crust.
Explanation2: Earthquakes happen when tectonic plates suddenly slip or break
apart, causing a release of energy that creates seismic waves that can shake the
ground and cause damage.
Answer: Explanation1

Now, Answer the following question given the example formats above:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Response:**

Answer: Explanation2
  (gemini-2.5-flash)

### Optimal number of examples

Models like Gemini can often pick up on patterns using a few examples, though you may need to experiment with the number of examples to provide in the prompt for the best results. At the same time, if you include too many examples, the model may start to [overfit](https://developers.google.com/machine-learning/glossary#overfitting) the response to the examples.

### Patterns vs anti patterns

Using examples to show the model a pattern to follow is more effective than using examples to show the model an anti pattern to avoid.

⛔ **Negative pattern:**

**Example**

Don't end haikus with a question:
Haiku are fun
A short and simple poem
Don't you enjoy them?
    

✅ **Positive pattern:**

**Example**

Always end haikus with an assertion:
Haiku are fun
A short and simple poem
A joy to write
    

### Consistent formatting

Make sure that the structure and formatting of few-shot examples are the same to avoid responses with undesired formats. One of the primary objectives of adding few-shot examples in prompts is to show the model the response format. Therefore, it is essential to ensure a consistent format across all examples, especially paying attention to XML tags, white spaces, newlines, and example splitters.

## Add context

You can include instructions and information in a prompt that the model needs to solve a problem, instead of assuming that the model has all of the required information. This contextual information helps the model understand the constraints and details of what you're asking for it to do.

The following example asks the model to give troubleshooting guidance for a router:

**Prompt:**

What should I do to fix my disconnected wifi? **The light on my Google
Wifi router is yellow and blinking slowly.**
  **Response:**

  A yellow blinking light on your Google Wifi router usually indicates a problem
  with the internet connection. Here are some steps to troubleshoot and fix the
  issue:
  1. Restart your router and modem:
    - Unplug your modem from the power outlet and wait for 30 seconds.
    - Plug your modem back in and wait for it to fully boot up.
    - Unplug your Google Wifi router and wait for 30 seconds.
    - Plug your Google Wifi router back in and wait for it to fully boot up.
.....
  (gemini-2.5-flash)

The response looks like generic troubleshooting information that's not specific to the router or the status of the LED indicator lights.

To customize the response for the specific router, you can add to the prompt the router's troubleshooting guide as context for it to refer to when providing a response.

**Prompt:**

Answer the question using the text below. Respond with only the text provided.
Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.

Text:
Color: Slowly pulsing yellow
What it means: There is a network error.
What to do:
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.

Color: Fast blinking yellow
What it means: You are holding down the reset button and are factory resetting this device.
What to do:
If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.

Color: Solid yellow
What it means: Router is factory resetting.
What to do:
This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.

Color: Solid red
What it means: Something is wrong.
What to do:
Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.
  **Response:**

Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.
  (gemini-2.5-flash)

## Add prefixes

A prefix is a word or phrase that you add to the prompt content that can serve several purposes, depending on where you put the prefix:

*   **Input prefix:** Adding a prefix to the input signals semantically meaningful parts of the input to the model. For example, the prefixes "English:" and "French:" demarcate two different languages.
*   **Output prefix:** Even though the output is generated by the model, you can add a prefix for the output in the prompt. The output prefix gives the model information about what's expected as a response. For example, the output prefix "JSON:" signals to the model that the output should be in JSON format.
*   **Example prefix:** In few-shot prompts, adding prefixes to the examples provides labels that the model can use when generating the output, which makes it easier to parse output content.

In the following example, "Text:" is the input prefix and "The answer is:" is the output prefix.

**Prompt:**

Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail
The answer is: small
Text: Elephant
The answer is:
  **Response:**

The answer is: large
  (gemini-2.5-flash)

## Break down prompts into components

For use cases that require complex prompts, you can help the model manage this complexity by breaking things down into simpler components.

1.  **Break down instructions:** Instead of having many instructions in one prompt, create one prompt per instruction. You can choose which prompt to process based on the user's input.
    
2.  **Chain prompts:** For complex tasks that involve multiple sequential steps, make each step a prompt and chain the prompts together in a sequence. In this sequential chain of prompts, the output of one prompt in the sequence becomes the input of the next prompt. The output of the last prompt in the sequence is the final output.
    
3.  **Aggregate responses:** Aggregation is when you want to perform different parallel tasks on different portions of the data and aggregate the results to produce the final output. For example, you can tell the model to perform one operation on the first part of the data, perform another operation on the rest of the data and aggregate the results.
    

## Experiment with model parameters

Each call that you send to a model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. Experiment with different parameter values to get the best values for the task. The parameters available for different models may differ. The most common parameters are the following:

1.  **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.
    
2.  **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.
    
3.  **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.
    
4.  **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.
    
5.  **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.
    

## Prompt iteration strategies

Prompt design can sometimes require a few iterations before you consistently get the response you're looking for. This section provides guidance on some things you can try when iterating on your prompts:

1.  **Use different phrasing:** Using different words or phrasing in your prompts often yields different responses from the model even though they all mean the same thing. If you're not getting the expected results from your prompt, try rephrasing it.
    
    Version 1:
    How do I bake a pie?
    
    Version 2:
    Suggest a recipe for a pie.
    
    Version 3:
    What's a good pie recipe?
        
    
2.  **Switch to an analogous task:** If you can't get the model to follow your instructions for a task, try giving it instructions for an analogous task that achieves the same result.
    
    This prompt tells the model to categorize a book by using predefined categories:
    
    **Prompt:**
    
    Which category does The Odyssey belong to:
    thriller
    sci-fi
    mythology
    biography
      **Response:**
    
        The Odyssey belongs to the category of \*\*mythology\*\*. 
    
        Here's why:
    
        \* \*\*Mythology:\*\* The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his
        journey home after the Trojan War. It features gods, monsters, and supernatural events common to
        Greek mythology.
    .....
      (gemini-2.5-flash)
    
    The response is correct, but the model didn't stay within the bounds of the options. You also want to model to just respond with one of the options instead of in a full sentence. In this case, you can rephrase the instructions as a multiple choice question and ask the model to choose an option.
    
    **Prompt:**
    
    Multiple choice problem: Which of the following options describes the book The Odyssey?
    Options:
    
    *   thriller
    *   sci-fi
    *   mythology
    *   biography
    **Response:**
    
    The correct answer is **mythology**. 
    (gemini-2.5-flash)
    
3.  **Change the order of prompt content:** The order of the content in the prompt can sometimes affect the response. Try changing the content order and see how that affects the response.
    
    ```
    Version 1:
    [examples]
    [context]
    [input]
    
    Version 2:
    [input]
    [examples]
    [context]
    
    Version 3:
    [examples]
    [input]
    [context]
    ```
    

## Fallback responses

A fallback response is a response returned by the model when either the prompt or the response triggers a safety filter. An example of a fallback response is "I'm not able to help with that, as I'm only a language model."

If the model responds with a fallback response, try increasing the temperature.

## Things to avoid

*   Avoid relying on models to generate factual information.
*   Use with care on math and logic problems.

## Generative models under the hood

This section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**

The short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text "The dog jumped over the ... ", the generative model will produce an array of probable next words:

```
[("fence", 0.77), ("ledge", 0.12), ("blanket", 0.03), ...]
```

This process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.

In the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.

## Next steps

*   Now that you have a deeper understanding of prompt design, try writing your own prompts using [Google AI Studio](http://aistudio.google.com).
*   To learn about multimodal prompting, see [Prompting with media files](/gemini-api/docs/files#prompt-guide).
*   To learn about image prompting, see the [Imagen prompt guide](/gemini-api/docs/image-generation#imagen-prompt-guide)
*   To learn about video prompting, see the [Veo prompt guide](/gemini-api/docs/video#prompt-guide)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-04-28 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/vision

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Image understanding

Gemini models are built to be multimodal from the ground up, unlocking a wide range of image processing and computer vision tasks including but not limited to image captioning, classification, and visual question answering without having to train specialized ML models.

## Passing images to Gemini

You can provide images as input to Gemini using two methods:

*   [Passing inline image data](#inline-image): Ideal for smaller files (total request size less than 20MB, including prompts).
*   [Uploading images using the File API](#upload-image): Recommended for larger files or for reusing images across multiple requests.

### Passing inline image data

You can pass inline image data in the request to `generateContent`. You can provide image data as Base64 encoded strings or by reading local files directly (depending on the language).

The following example shows how to read an image from a local file and pass it to `generateContent` API for processing.

### Python

  ```
  from google.genai import types

  with open('path/to/small-sample.jpg', 'rb') as f:
      image_bytes = f.read()

  response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=[
      types.Part.from_bytes(
        data=image_bytes,
        mime_type='image/jpeg',
      ),
      'Caption this image.'
    ]
  )

  print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

### Go

```
bytes, _ := os.ReadFile("path/to/small-sample.jpg")

parts := []*genai.Part{
  genai.NewPartFromBytes(bytes, "image/jpeg"),
  genai.NewPartFromText("Caption this image."),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
IMG_PATH="/path/to/your/image1.jpg"

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
B64FLAGS="--input"
else
B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "'"$(base64 $B64FLAGS $IMG_PATH)"'"
            }
        },
        {"text": "Caption this image."},
    ]
    }]
}' 2> /dev/null
```

You can also fetch an image from a URL, convert it to bytes, and pass it to `generateContent` as shown in the following examples.

### Python

```
from google import genai
from google.genai import types

import requests

image_path = "https://goo.gle/instrument-img"
image_bytes = requests.get(image_path).content
image = types.Part.from_bytes(
  data=image_bytes, mime_type="image/jpeg"
)

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["What is this image?", image],
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

async function main() {
  const ai = new GoogleGenAI({});

  const imageUrl = "https://goo.gle/instrument-img";

  const response = await fetch(imageUrl);
  const imageArrayBuffer = await response.arrayBuffer();
  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
    {
      inlineData: {
        mimeType: 'image/jpeg',
        data: base64ImageData,
      },
    },
    { text: "Caption this image." }
  ],
  });
  console.log(result.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "io"
  "net/http"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Download the image.
  imageResp, _ := http.Get("https://goo.gle/instrument-img")

  imageBytes, _ := io.ReadAll(imageResp.Body)

  parts := []*genai.Part{
    genai.NewPartFromBytes(imageBytes, "image/jpeg"),
    genai.NewPartFromText("Caption this image."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMG_URL="https://goo.gle/instrument-img"

MIME_TYPE=$(curl -sIL "$IMG_URL" | grep -i '^content-type:' | awk -F ': ' '{print $2}' | sed 's/\r$//' | head -n 1)
if [[ -z "$MIME_TYPE" || ! "$MIME_TYPE" == image/* ]]; then
  MIME_TYPE="image/jpeg"
fi

# Check for macOS
if [[ "$(uname)" == "Darwin" ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -b 0)
elif [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64)
else
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -w0)
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"'"$MIME_TYPE"'",
                "data": "'"$IMAGE_B64"'"
              }
            },
            {"text": "Caption this image."}
        ]
      }]
    }' 2> /dev/null
```

### Uploading images using the File API

For large files or to be able to use the same image file repeatedly, use the Files API. The following code uploads an image file and then uses the file in a call to `generateContent`. See the [Files API guide](/gemini-api/docs/files) for more information and examples.

### Python

```
from google import genai

client = genai.Client()

my_file = client.files.upload(file="path/to/sample.jpg")

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[my_file, "Caption this image."],
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.jpg",
    config: { mimeType: "image/jpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Caption this image.",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.jpg", nil)

  parts := []*genai.Part{
      genai.NewPartFromText("Caption this image."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMAGE_PATH="path/to/sample.jpg"
MIME_TYPE=$(file -b --mime-type "${IMAGE_PATH}")
NUM_BYTES=$(wc -c < "${IMAGE_PATH}")
DISPLAY_NAME=IMAGE

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Caption this image."}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Prompting with multiple images

You can provide multiple images in a single prompt by including multiple image `Part` objects in the `contents` array. These can be a mix of inline data (local files or URLs) and File API references.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

# Upload the first image
image1_path = "path/to/image1.jpg"
uploaded_file = client.files.upload(file=image1_path)

# Prepare the second image as inline data
image2_path = "path/to/image2.png"
with open(image2_path, 'rb') as f:
    img2_bytes = f.read()

# Create the prompt with text and multiple images
response = client.models.generate_content(

    model="gemini-2.5-flash",
    contents=[
        "What is different between these two images?",
        uploaded_file,  # Use the uploaded file reference
        types.Part.from_bytes(
            data=img2_bytes,
            mime_type='image/png'
        )
    ]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images

  const response = await ai.models.generateContent({

    model: "gemini-2.5-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
# Upload the first image
IMAGE1_PATH="path/to/image1.jpg"
MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
DISPLAY_NAME1=IMAGE1

tmp_header_file1=upload-header1.tmp

curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header1.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file1}"

curl "${upload_url1}" \
  -H "Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

file1_uri=$(jq ".file.uri" file_info1.json)
echo file1_uri=$file1_uri

# Prepare the second image (inline)
IMAGE2_PATH="path/to/image2.png"
MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi
IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

# Now generate content using both images
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "What is different between these two images?"},
          {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
          {
            "inline_data": {
              "mime_type":"'"${MIME2_TYPE}"'",
              "data": "'"$IMAGE2_BASE64"'"
            }
          }
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Object detection

From Gemini 2.0 onwards, models are further trained to detect objects in an image and get their bounding box coordinates. The coordinates, relative to image dimensions, scale to \[0, 1000\]. You need to descale these coordinates based on your original image size.

### Python

```
from google import genai
from google.genai import types
from PIL import Image
import json

client = genai.Client()
prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

image = Image.open("/path/to/image.png")

config = types.GenerateContentConfig(
  response_mime_type="application/json"
  ) 

response = client.models.generate_content(model="gemini-2.5-flash",
                                          contents=[image, prompt],
                                          config=config
                                          )

width, height = image.size
bounding_boxes = json.loads(response.text)

converted_bounding_boxes = []
for bounding_box in bounding_boxes:
    abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
    abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
    abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
    abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

print("Image size: ", width, height)
print("Bounding boxes:", converted_bounding_boxes)

```

For more examples, check following cookbooks:

*   [2D spatial understanding notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb)
*   [Experimental 3D pointing notebook](https://github.com/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb)

## Segmentation

Starting with Gemini 2.5, models not only detect items but also segment them and provide their contour masks.

The model predicts a JSON list, where each item represents a segmentation mask. Each item has a bounding box ("`box_2d`") in the format `[y0, x0, y1, x1]` with normalized coordinates between 0 and 1000, a label ("`label`") that identifies the object, and finally the segmentation mask inside the bounding box, as base64 encoded png that is a probability map with values between 0 and 255. The mask needs to be resized to match the bounding box dimensions, then binarized at your confidence threshold (127 for the midpoint).

### Python

```
from google import genai
from google.genai import types
from PIL import Image, ImageDraw    
import io
import base64
import json
import numpy as np
import os    

client = genai.Client()

def parse_json(json_output: str):
  # Parsing out the markdown fencing
  lines = json_output.splitlines()
  for i, line in enumerate(lines):
      if line == "```json":
          json_output = "\n".join(lines[i+1:])  # Remove everything before "```json"
          json_output = json_output.split("```")[0]  # Remove everything after the closing "```"
          break  # Exit the loop once "```json" is found
  return json_output

def extract_segmentation_masks(image_path: str, output_dir: str = "segmentation_outputs"):
  # Load and resize image
  im = Image.open(image_path)
  im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)

  prompt = """
  Give the segmentation masks for the wooden and glass items.
  Output a JSON list of segmentation masks where each entry contains the 2D
  bounding box in the key "box_2d", the segmentation mask in key "mask", and
  the text label in the key "label". Use descriptive labels.
  """

  config = types.GenerateContentConfig(      
    thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection
  ) 

  response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[prompt, im], # Pillow images can be directly passed as inputs (which will be converted by the SDK)
    config=config
  )

  # Parse JSON response
  items = json.loads(parse_json(response.text))

  # Create output directory
  os.makedirs(output_dir, exist_ok=True)

  # Process each mask
  for i, item in enumerate(items):
      # Get bounding box coordinates
      box = item["box_2d"]
      y0 = int(box[0] / 1000 * im.size[1])
      x0 = int(box[1] / 1000 * im.size[0])
      y1 = int(box[2] / 1000 * im.size[1])
      x1 = int(box[3] / 1000 * im.size[0])

      # Skip invalid boxes
      if y0 >= y1 or x0 >= x1:
          continue

      # Process mask
      png_str = item["mask"]
      if not png_str.startswith("data:image/png;base64,"):
          continue

      # Remove prefix
      png_str = png_str.removeprefix("data:image/png;base64,")
      mask_data = base64.b64decode(png_str)
      mask = Image.open(io.BytesIO(mask_data))

      # Resize mask to match bounding box
      mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)

      # Convert mask to numpy array for processing
      mask_array = np.array(mask)

      # Create overlay for this mask
      overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))
      overlay_draw = ImageDraw.Draw(overlay)

      # Create overlay for the mask
      color = (255, 255, 255, 200)
      for y in range(y0, y1):
          for x in range(x0, x1):
              if mask_array[y - y0, x - x0] > 128:  # Threshold for mask
                  overlay_draw.point((x, y), fill=color)

      # Save individual mask and its overlay
      mask_filename = f"{item['label']}_{i}_mask.png"
      overlay_filename = f"{item['label']}_{i}_overlay.png"

      mask.save(os.path.join(output_dir, mask_filename))

      # Create and save overlay
      composite = Image.alpha_composite(im.convert('RGBA'), overlay)
      composite.save(os.path.join(output_dir, overlay_filename))
      print(f"Saved mask and overlay for {item['label']} to {output_dir}")

# Example usage
if __name__ == "__main__":
  extract_segmentation_masks("path/to/image.png")

```

Check the [segmentation example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb#scrollTo=WQJTJ8wdGOKx) in the cookbook guide for a more detailed example.

![A table with cupcakes, with the wooden and glass objects highlighted](/static/gemini-api/docs/images/segmentation.jpg)

An example segmentation output with objects and segmentation masks

## Supported image formats

Gemini supports the following image format MIME types:

*   PNG - `image/png`
*   JPEG - `image/jpeg`
*   WEBP - `image/webp`
*   HEIC - `image/heic`
*   HEIF - `image/heif`

## Capabilities

All Gemini model versions are multimodal and can be utilized in a wide range of image processing and computer vision tasks including but not limited to image captioning, visual question and answering, image classification, object detection and segmentation.

Gemini can reduce the need to use specialized ML models depending on your quality and performance requirements.

Some later model versions are specifically trained improve accuracy of specialized tasks in addition to generic capabilities:

*   **Gemini 2.0 models** are further trained to support enhanced [object detection](#object-detection).
    
*   **Gemini 2.5 models** are further trained to support enhanced [segmentation](#segmentation) in addition to [object detection](#object-detection).
    

## Limitations and key technical information

### File limit

Gemini 2.5 Pro/Flash, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a maximum of 3,600 image files per request.

### Token calculation

*   **Gemini 1.5 Flash and Gemini 1.5 Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized to 768x768), with each tile costing 258 tokens.
*   **Gemini 2.0 Flash and Gemini 2.5 Flash/Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled into 768x768 pixel tiles, each costing 258 tokens.

## Tips and best practices

*   Verify that images are correctly rotated.
*   Use clear, non-blurry images.
*   When using a single image with text, place the text prompt _after_ the image part in the `contents` array.

## What's next

This guide shows you how to upload image files and generate text outputs from image inputs. To learn more, see the following resources:

*   [Files API](/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/function-calling/tutorial

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Function calling with the Gemini API

Function calling lets you connect models to external tools and APIs. Instead of generating text responses, the model determines when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:

*   **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.
*   **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.
*   **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices.

Get Weather Schedule Meeting Create Chart

## How function calling works

![function calling
overview](/static/gemini-api/docs/images/function-calling-overview.png)

Function calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:

1.  **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.
2.  **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.
3.  **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if
    *   **Yes**: Extract the name and args of the function and execute the corresponding function in your application.
    *   **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).
4.  **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.

This process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).

### Step 1: Define a function declaration

Define a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.

### Python

```
# Define a function that the model can call to control smart lights
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

### JavaScript

```
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

### Step 2: Call the model with function declarations

Once you have defined your function declarations, you can prompt the model to use them. It analyzes the prompt and function declarations and decides whether to respond directly or to call a function. If a function is called, the response object will contain a function call suggestion.

### Python

```
from google.genai import types

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[set_light_values_declaration])
config = types.GenerateContentConfig(tools=[tools])

# Define user prompt
contents = [
    types.Content(
        role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
    )
]

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
    config=config,
)

print(response.candidates[0].content.parts[0].function_call)
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Generation config with function declaration
const config = {
  tools: [{
    functionDeclarations: [setLightValuesFunctionDeclaration]
  }]
};

// Configure the client
const ai = new GoogleGenAI({});

// Define user prompt
const contents = [
  {
    role: 'user',
    parts: [{ text: 'Turn the lights down to a romantic level' }]
  }
];

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(response.functionCalls[0]);
```

The model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.

### Python

```
id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'
```

### JavaScript

```
{
  name: 'set_light_values',
  args: { brightness: 25, color_temp: 'warm' }
}
```

### Step 3: Execute set\_light\_values function code

Extract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function.

### Python

```
# Extract tool call details, it may not be in the first part.
tool_call = response.candidates[0].content.parts[0].function_call

if tool_call.name == "set_light_values":
    result = set_light_values(**tool_call.args)
    print(f"Function execution result: {result}")
```

### JavaScript

```
// Extract tool call details
const tool_call = response.functionCalls[0]

let result;
if (tool_call.name === 'set_light_values') {
  result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);
  console.log(`Function execution result: ${JSON.stringify(result)}`);
}
```

### Step 4: Create user friendly response with function result and call the model again

Finally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.

### Python

```
# Create a function response part
function_response_part = types.Part.from_function_response(
    name=tool_call.name,
    response={"result": result},
)

# Append function call and result of the function execution to contents
contents.append(response.candidates[0].content) # Append the content from the model's response.
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)
```

### JavaScript

```
// Create a function response part
const function_response_part = {
  name: tool_call.name,
  response: { result }
}

// Append function call and result of the function execution to contents
contents.push(response.candidates[0].content);
contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });

// Get the final response from the model
const final_response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(final_response.text);
```

This completes the function calling flow. The model successfully used the `set_light_values` function to perform the request action of the user.

## Function declarations

When you implement function calling in a prompt, you create a `tools` object, which contains one or more `function declarations`. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemaw) format. A single function declaration can include the following parameters:

*   `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).
*   `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful ("Finds theaters based on location and optionally movie title which is currently playing in theaters.").
*   `parameters` (object): Defines the input parameters the function expects.
    *   `type` (string): Specifies the overall data type, such as `object`.
    *   `properties` (object): Lists individual parameters, each with:
        *   `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.
        *   `description` (string): A description of the parameter's purpose and format. Provide examples and constraints ("The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.").
        *   `enum` (array, optional): If the parameter values are from a fixed set, use "enum" to list the allowed values instead of just describing them in the description. This improves accuracy ("enum": \["daylight", "cool", "warm"\]).
    *   `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.

## Parallel function calling

In addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.

### Python

```
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}
```

### JavaScript

```
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};
```

Configure the function calling mode to allow using all of the specified tools. To learn more, you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).

### Python

```
from google import genai
from google.genai import types

# Configure the client and tools
client = genai.Client()
house_tools = [
    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
]
config = types.GenerateContentConfig(
    tools=house_tools,
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
        disable=True
    ),
    # Force the model to call 'any' function, instead of chatting.
    tool_config=types.ToolConfig(
        function_calling_config=types.FunctionCallingConfig(mode='ANY')
    ),
)

chat = client.chats.create(model="gemini-2.5-flash", config=config)
response = chat.send_message("Turn this place into a party!")

# Print out each of the function calls requested from this single call
print("Example 1: Forced function calling")
for fn in response.function_calls:
    args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
    print(f"{fn.name}({args})")
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Set up function declarations
const houseFns = [powerDiscoBall, startMusic, dimLights];

const config = {
    tools: [{
        functionDeclarations: houseFns
    }],
    // Force the model to call 'any' function, instead of chatting.
    toolConfig: {
        functionCallingConfig: {
            mode: 'any'
        }
    }
};

// Configure the client
const ai = new GoogleGenAI({});

// Create a chat session
const chat = ai.chats.create({
    model: 'gemini-2.5-flash',
    config: config
});
const response = await chat.sendMessage({message: 'Turn this place into a party!'});

// Print out each of the function calls requested from this single call
console.log("Example 1: Forced function calling");
for (const fn of response.functionCalls) {
    const args = Object.entries(fn.args)
        .map(([key, val]) => `${key}=${val}`)
        .join(', ');
    console.log(`${fn.name}(${args})`);
}
```

Each of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested.

The Python SDK supports [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only), which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.

### Python

```
from google import genai
from google.genai import types

# Actual function implementations
def power_disco_ball_impl(power: bool) -> dict:
    """Powers the spinning disco ball.

    Args:
        power: Whether to turn the disco ball on or off.

    Returns:
        A status dictionary indicating the current state.
    """
    return {"status": f"Disco ball powered {'on' if power else 'off'}"}

def start_music_impl(energetic: bool, loud: bool) -> dict:
    """Play some music matching the specified parameters.

    Args:
        energetic: Whether the music is energetic or not.
        loud: Whether the music is loud or not.

    Returns:
        A dictionary containing the music settings.
    """
    music_type = "energetic" if energetic else "chill"
    volume = "loud" if loud else "quiet"
    return {"music_type": music_type, "volume": volume}

def dim_lights_impl(brightness: float) -> dict:
    """Dim the lights.

    Args:
        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

    Returns:
        A dictionary containing the new brightness setting.
    """
    return {"brightness": brightness}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Do everything you need to this place into party!",
    config=config,
)

print("\nExample 2: Automatic function calling")
print(response.text)
# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!
```

## Compositional function calling

Compositional or sequential function calling allows Gemini to chain multiple function calls together to fulfill a complex request. For example, to answer "Get the temperature in my current location", the Gemini API might first invoke a `get_current_location()` function followed by a `get_weather()` function that takes the location as a parameter.

The following example demonstrates how to implement compositional function calling using the Python SDK and automatic function calling.

### Python

This example uses the automatic function calling feature of the `google-genai` Python SDK. The SDK automatically converts the Python functions to the required schema, executes the function calls when requested by the model, and sends the results back to the model to complete the task.

```
import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
    config=config,
)

# Print the final, user-facing response
print(response.text)
```

**Expected Output**

When you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. I've set the thermostat to 20°C.
```

### JavaScript

This example shows how to use JavaScript/TypeScript SDK to do comopositional function calling using a manual execution loop.

```
import { GoogleGenAI, Type } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Example Functions
function get_weather_forecast({ location }) {
  console.log(`Tool Call: get_weather_forecast(location=${location})`);
  // TODO: Make API call
  console.log("Tool Response: {'temperature': 25, 'unit': 'celsius'}");
  return { temperature: 25, unit: "celsius" };
}

function set_thermostat_temperature({ temperature }) {
  console.log(
    `Tool Call: set_thermostat_temperature(temperature=${temperature})`,
  );
  // TODO: Make API call
  console.log("Tool Response: {'status': 'success'}");
  return { status: "success" };
}

const toolFunctions = {
  get_weather_forecast,
  set_thermostat_temperature,
};

const tools = [
  {
    functionDeclarations: [
      {
        name: "get_weather_forecast",
        description:
          "Gets the current weather temperature for a given location.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            location: {
              type: Type.STRING,
            },
          },
          required: ["location"],
        },
      },
      {
        name: "set_thermostat_temperature",
        description: "Sets the thermostat to a desired temperature.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            temperature: {
              type: Type.NUMBER,
            },
          },
          required: ["temperature"],
        },
      },
    ],
  },
];

// Prompt for the model
let contents = [
  {
    role: "user",
    parts: [
      {
        text: "If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
      },
    ],
  },
];

// Loop until the model has no more function calls to make
while (true) {
  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents,
    config: { tools },
  });

  if (result.functionCalls && result.functionCalls.length > 0) {
    const functionCall = result.functionCalls[0];

    const { name, args } = functionCall;

    if (!toolFunctions[name]) {
      throw new Error(`Unknown function call: ${name}`);
    }

    // Call the function and get the response.
    const toolResponse = toolFunctions[name](args);

    const functionResponsePart = {
      name: functionCall.name,
      response: {
        result: toolResponse,
      },
    };

    // Send the function response back to the model.
    contents.push({
      role: "model",
      parts: [
        {
          functionCall: functionCall,
        },
      ],
    });
    contents.push({
      role: "user",
      parts: [
        {
          functionResponse: functionResponsePart,
        },
      ],
    });
  } else {
    // No more function calls, break the loop.
    console.log(result.text);
    break;
  }
}
```

**Expected Output**

When you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. It's 25°C in London, so I've set the thermostat to 20°C.
```

Compositional function calling is a native [Live API](https://ai.google.dev/gemini-api/docs/live) feature. This means Live API can handle the function calling similar to the Python SDK.

### Python

```
# Light control schemas
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Light control schemas
const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };
const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };

const prompt = `
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
`;

const tools = [
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }
];

await run(prompt, tools=tools, modality="AUDIO")
```

## Function calling modes

The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the.`function_calling_config`.

*   `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.
*   `ANY`: The model is constrained to always predict a function call and guarantees function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).
*   `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.
    

### Python

```
from google.genai import types

# Configure function calling mode
tool_config = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="ANY", allowed_function_names=["get_current_temperature"]
    )
)

# Create the generation config
config = types.GenerateContentConfig(
    tools=[tools],  # not defined here.
    tool_config=tool_config,
)
```

### JavaScript

```
import { FunctionCallingConfigMode } from '@google/genai';

// Configure function calling mode
const toolConfig = {
  functionCallingConfig: {
    mode: FunctionCallingConfigMode.ANY,
    allowedFunctionNames: ['get_current_temperature']
  }
};

// Create the generation config
const config = {
  tools: tools, // not defined here.
  toolConfig: toolConfig,
};
```

## Automatic function calling (Python only)

When using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and the response cycle for you. The Python SDK then automatically:

1.  Detects function call responses from the model.
2.  Call the corresponding Python function in your code.
3.  Sends the function response back to the model.
4.  Returns the model's final text response.

To use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:

### Python

```
from google import genai
from google.genai import types

# Define the function with type hints and docstring
def get_current_temperature(location: str) -> dict:
    """Gets the current temperature for a given location.

    Args:
        location: The city and state, e.g. San Francisco

    Returns:
        A dictionary containing the temperature and unit.
    """
    # ... (implementation) ...
    return {"temperature": 25, "unit": "Celsius"}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_current_temperature]
)  # Pass the function itself

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What's the temperature in Boston?",
    config=config,
)

print(response.text)  # The SDK handles the function call and returns the final text
```

You can disable automatic function calling with:

### Python

```
config = types.GenerateContentConfig(
    tools=[get_current_temperature],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
)
```

### Automatic function schema declaration

Automatic schema extraction from Python functions doesn't work in all cases. For example, it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:

### Python

```
AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])
```

To see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):

### Python

```
def multiply(a: float, b: float):
    """Returns a * b."""
    return a * b

fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

# to_json_dict() provides a clean JSON representation.
print(fn_decl.to_json_dict())
```

## Multi-tool use: Combine native tools with function calling

You can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).

### Python

```
# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Multiple tasks example - combining lights, code execution, and search
const prompt = `
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
`;

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
];

// Execute the prompt with specified tools in audio modality
await run(prompt, {tools: tools, modality: "AUDIO"});
```

Python developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).

## Model context protocol (MCP)

[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard for connecting AI applications with external tools and data. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts.

The Gemini SDKs have built-in support for the MCP, reducing boilerplate code and offering [automatic tool calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) for MCP tools. When the model generates an MCP tool call, the Python and JavaScript client SDK can automatically execute the MCP tool and send the response back to the model in a subsequent request, continuing this loop until no more tool calls are made by the model.

Here, you can find an example of how to use a local MCP server with Gemini and `mcp` SDK.

### Python

Make sure the latest version of the [`mcp` SDK](https://modelcontextprotocol.io/introduction) is installed on your platform of choice.

```
pip install mcp
```

```
import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool
                    # Uncomment if you **don't** want the SDK to automatically call the tool
                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
                    #     disable=True
                    # ),
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())
```

### JavaScript

Make sure the latest version of the `mcp` SDK is installed on your platform of choice.

```
npm install @modelcontextprotocol/sdk
```

```
import { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

// Create server parameters for stdio connection
const serverParams = new StdioClientTransport({
  command: "npx", // Executable
  args: ["-y", "@philschmid/weather-mcp"] // MCP Server
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  }
);

// Configure the client
const ai = new GoogleGenAI({});

// Initialize the connection between client and server
await client.connect(serverParams);

// Send request to the model with MCP tools
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,
  config: {
    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool
    // Uncomment if you **don't** want the sdk to automatically call the tool
    // automaticFunctionCalling: {
    //   disable: true,
    // },
  },
});
console.log(response.text)

// Close the connection
await client.close();
```

### Limitations with built-in MCP support

Built-in MCP support is a [experimental](/gemini-api/docs/models#preview) feature in our SDKs and has the following limitations:

*   Only tools are supported, not resources nor prompts
*   It is available for the Python and JavaScript/TypeScript SDK.
*   Breaking changes might occur in future releases.

Manual integration of MCP servers is always an option if these limit what you're building.

## Supported models

This section lists models and their function calling capabilities. Experimental models are not included. You can find a comprehensive capabilities overview on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.

Model

Function Calling

Parallel Function Calling

Compositional Function Calling

Gemini 2.5 Pro

✔️

✔️

✔️

Gemini 2.5 Flash

✔️

✔️

✔️

Gemini 2.5 Flash-Lite

✔️

✔️

✔️

Gemini 2.0 Flash

✔️

✔️

✔️

Gemini 2.0 Flash-Lite

X

X

X

## Best practices

*   **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.
*   **Naming:** Use descriptive function names (without spaces, periods, or dashes).
*   **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.
*   **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.
*   **Prompt Engineering:**
    *   Provide context: Tell the model its role (e.g., "You are a helpful weather assistant.").
    *   Give instructions: Specify how and when to use functions (e.g., "Don't guess dates; always use a future date for forecasts.").
    *   Encourage clarification: Instruct the model to ask clarifying questions if needed.
*   **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.
*   **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.
*   **Error Handling**: Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.
*   **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.
*   **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.

## Notes and limitations

*   Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.
*   Supported parameter types in Python are limited.
*   Automatic function calling is a Python SDK feature only.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/semantic_retrieval

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

# Gemini Developer API

[Get a Gemini API Key](https://aistudio.google.com/apikey)

Get a Gemini API key and make your first API request in minutes.

### Python

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil))
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Java

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## Meet the models

[Use Gemini in Google AI Studio](https://aistudio.google.com)

2.5 Pro spark

Our most powerful thinking model with features for complex reasoning and much more

[](/gemini-api/docs/models#gemini-2.5-pro)

2.5 Flash spark

Our newest multimodal model, with next generation features and improved capabilities

[](/gemini-api/docs/models/gemini#gemini-2.5-flash)

2.5 Flash-Lite spark

Our fastest and most cost-efficient multimodal model with great performance for high-frequency tasks

[](/gemini-api/docs/models/gemini#gemini-2.5-flash-lite)

## Explore the API

![](/static/site-assets/images/image-generation-index.png)

### Native Image Generation

Generate and edit highly contextual images natively with Gemini 2.0 Flash.

[](/gemini-api/docs/image-generation)

![](/static/site-assets/images/long-context-overview.png)

### Explore long context

Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.

[](/gemini-api/docs/long-context)

![](/static/site-assets/images/structured-outputs-index.png)

### Generate structured outputs

Constrain Gemini to respond with JSON, a structured data format suitable for automated processing.

[](/gemini-api/docs/structured-output)

### Start building with the Gemini API

[Get started](/gemini-api/docs/quickstart)

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/models/gemini

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini models

2.5 Pro spark

Our most powerful thinking model with maximum response accuracy and state-of-the-art performance

*   Input audio, images, video, and text, get text responses
*   Tackle difficult problems, analyze large databases, and more
*   Best for complex coding, reasoning, and multimodal understanding

[](#gemini-2.5-pro)

2.5 Flash spark

Our best model in terms of price-performance, offering well-rounded capabilities.

*   Input audio, images, video, and text, and get text responses
*   Model thinks as needed; or, you can configure a thinking budget
*   Best for low latency, high volume tasks that require thinking

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

*   Input audio, images, video, and text, and get text responses
*   Most cost-efficient model supporting high throughput
*   Best for real time, low latency use cases

[](#gemini-2.5-flash-lite)

## Model variants

The Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:

Model variant

Input(s)

Output

Optimized for

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

Audio, images, videos, text, and PDF

Text

Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

Audio, images, videos, and text

Text

Adaptive thinking, cost efficiency

[Gemini 2.5 Flash-Lite Preview](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

Text, image, video, audio

Text

Most cost-efficient model supporting high throughput

[Gemini 2.5 Flash Native Audio](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog` &  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

Audio, videos, and text

Text and audio, interleaved

High quality, natural conversational audio outputs, with or without thinking

[Gemini 2.5 Flash Preview TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.5 Pro Preview TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

Audio, images, videos, and text

Text

Next generation features, speed, and realtime streaming.

[Gemini 2.0 Flash Preview Image Generation](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

Audio, images, videos, and text

Text, images

Conversational image generation and editing

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

Audio, images, videos, and text

Text

Cost efficiency and low latency

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

Audio, images, videos, and text

Text

Fast and versatile performance across a diverse variety of tasks

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

Audio, images, videos, and text

Text

High volume and lower intelligence tasks

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

Audio, images, videos, and text

Text

Complex reasoning tasks requiring more intelligence

[Gemini Embedding](#gemini-embedding)  
`gemini-embedding-exp`

Text

Text embeddings

Measuring the relatedness of text strings

[Imagen 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

Text

Images

Our most up-to-date image generation model

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

Text

Images

High quality image generation model

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

Text, images

Video

High quality video generation

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

[Gemini 2.0 Flash Live](#live-api-2.0)  
`gemini-2.0-flash-live-001`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

You can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).

### Gemini 2.5 Pro

Gemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro)

#### Model details

Property

Description

id\_cardModel code

`gemini-2.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, text, and PDF

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash

Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash`

saveSupported data types

**Inputs**

Text, images, video, audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Audio generation**

Not supported

**Caching**

Supported

**Code execution**

Supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Stable: `gemini-2.5-flash`
*   Preview: `gemini-2.5-flash-preview-05-20`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash-Lite Preview

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-lite-preview-06-17`

saveSupported data types

**Inputs**

Text, images, video, and audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,000,000

**Output token limit**

64,000

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**URL Context**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-lite-preview-06-17`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Native Audio

Our native audio dialog models, with and without thinking, available through the [Live API](/gemini-api/docs/live). These models provide interactive and unstructured conversational experiences, with style and control prompting.

[Try native audio in Google AI Studio](https://aistudio.google.com/app/live)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-native-audio-dialog` &  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

saveSupported data types

**Inputs**

Audio, video, text

**Output**

Audio and text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

128,000

**Output token limit**

8,000

handymanCapabilities

**Audio generation**

Supported

**Caching**

Not supported

**Code execution**

Not supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Not supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-preview-05-20`
*   Experimental: `gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Preview Text-to-Speech

Gemini 2.5 Flash Preview TTS is our price-performant text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Flash rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-flash-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.5 Pro Preview Text-to-Speech

Gemini 2.5 Pro Preview TTS is our most powerful text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Pro rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-pro-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-pro-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.0 Flash

Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Supported

**Thinking**

Experimental

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash`
*   Stable: `gemini-2.0-flash-001`
*   Experimental: `gemini-2.0-flash-exp`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash Preview Image Generation

Gemini 2.0 Flash Preview Image Generation delivers improved image generation features, including generating and editing images conversationally.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-preview-image-generation`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text and images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

32,000

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not Supported

**Search**

Not Supported

**Image generation**

Supported

**Audio generation**

Not supported

**Live API**

Not Supported

**Thinking**

Not Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-preview-image-generation`

gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash-Lite

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-lite`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Not supported

**Search**

Not supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash-lite`
*   Stable: `gemini-2.0-flash-lite-001`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 1.5 Flash

Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-latest`
*   Latest stable: `gemini-1.5-flash`
*   Stable:

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_monthLatest update

September 2024

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash-8b`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-8b-latest`
*   Latest stable: `gemini-1.5-flash-8b`
*   Stable:

*   `gemini-1.5-flash-8b-001`

calendar\_monthLatest update

October 2024

### Gemini 1.5 Pro

Try [Gemini 2.5 Pro Preview](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date.

Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,097,152

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

7,200

**Maximum video length**

2 hours

**Maximum audio length**

Approximately 19 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-pro-latest`
*   Latest stable: `gemini-1.5-pro`
*   Stable:

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_monthLatest update

September 2024

### Imagen 4

Imagen 4 is our latest image model, capable of generating highly detailed images with rich lighting, significantly better text rendering, and higher resolution output than previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

480 tokens (text)

**Output images**

1 (Ultra)  
1 to 4 (Standard)

calendar\_monthLatest update

June 2025

### Imagen 3

Imagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-3.0-generate-002`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

N/A

**Output images**

Up to 4

calendar\_monthLatest update

February 2025

### Veo 2

Veo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`veo-2.0-generate-001`

saveSupported data types

**Input**

Text, image

**Output**

Video

token\_autoLimits

**Text input**

N/A

**Image input**

Any image resolution and aspect ratio up to 20MB file size

**Output video**

Up to 2

calendar\_monthLatest update

April 2025

### Gemini 2.5 Flash Live

The Gemini 2.5 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-live-2.5-flash-preview)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-live-2.5-flash-preview`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-live-2.5-flash-preview`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.0 Flash Live

The Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-live-001`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-live-001`

calendar\_monthLatest update

April 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini Embedding Experimental

`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. Gemini Embedding rate limits are more restricted since it is an experimental model.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`gemini-embedding-exp-03-07`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,192

**Output dimension size**

Elastic, supports: 3072, 1536, or 768

calendar\_monthLatest update

March 2025

### Text Embedding and Embedding

#### Text Embedding

Try our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance.

[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications.

`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`models/text-embedding-004`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

April 2024

#### Embedding

You can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.

The Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens.

##### Embedding model details

Property

Description

id\_cardModel code

`models/embedding-001`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

December 2023

### AQA

You can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability.

#### Model details

Property

Description

id\_cardModel code

`models/aqa`

saveSupported data types

**Input**

Text

**Output**

Text

languageSupported language

English

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

7,168

**Output token limit**

1,024

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Supported

calendar\_monthLatest update

December 2023

See the [examples](/examples) to explore the capabilities of these model variations.

\[\*\] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words.

## Model version name patterns

Gemini models are available in either _stable_, _preview_, or _experimental_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.

### Latest stable

Points to the most recent stable version released for the specified model generation and variation.

To specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-2.0-flash`.

### Stable

Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.

To specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-flash-001`.

### Preview

Points to a preview model which may not be suitable for production use, come with more restrictive rate limits, but may have billing enabled.

To specify a preview version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.5-pro-preview-06-05`.

### Experimental

Points to an experimental model which may not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.

To specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.

## Experimental models

In addition to stable models, the Gemini API offers experimental models which may not be suitable for production use and come with more restrictive rate limits.

We release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.

### Previous experimental models

As new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:

Model code

Base model

Replacement version

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## Supported languages

Gemini models are trained to work with the following languages:

*   Arabic (`ar`)
*   Bengali (`bn`)
*   Bulgarian (`bg`)
*   Chinese simplified and traditional (`zh`)
*   Croatian (`hr`)
*   Czech (`cs`)
*   Danish (`da`)
*   Dutch (`nl`)
*   English (`en`)
*   Estonian (`et`)
*   Finnish (`fi`)
*   French (`fr`)
*   German (`de`)
*   Greek (`el`)
*   Hebrew (`iw`)
*   Hindi (`hi`)
*   Hungarian (`hu`)
*   Indonesian (`id`)
*   Italian (`it`)
*   Japanese (`ja`)
*   Korean (`ko`)
*   Latvian (`lv`)
*   Lithuanian (`lt`)
*   Norwegian (`no`)
*   Polish (`pl`)
*   Portuguese (`pt`)
*   Romanian (`ro`)
*   Russian (`ru`)
*   Serbian (`sr`)
*   Slovak (`sk`)
*   Slovenian (`sl`)
*   Spanish (`es`)
*   Swahili (`sw`)
*   Swedish (`sv`)
*   Thai (`th`)
*   Turkish (`tr`)
*   Ukrainian (`uk`)
*   Vietnamese (`vi`)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/grounding

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# התבססות על חיפוש Google

התכונה 'התאמה לנושא' בחיפוש Google מחברת את מודל Gemini לתוכן אינטרנט בזמן אמת, והיא פועלת בכל [השפות הזמינות](https://ai.google.dev/gemini-api/docs/models/gemini?hl=he#available-languages). כך Gemini יכול לספק תשובות מדויקות יותר ולציין מקורות מאומתים מעבר למגבלת הידע שלו.

הכלי הזה עוזר לכם ליצור אפליקציות שיכולות:

*   **שיפור הדיוק העובדתי:** כדי לצמצם את ההזיות של המודלים, התשובות מבוססות על מידע מהעולם האמיתי.
*   **גישה למידע בזמן אמת:** מענה על שאלות לגבי אירועים ונושאים שקרו לאחרונה.
*   **ציון מקורות:** כדי לעורר אמון בקרב המשתמשים, כדאי להציג את המקורות של ההצהרות של המודל.
    

### Python

```
from google import genai
from google.genai import types

# Configure the client
client = genai.Client()

# Define the grounding tool
grounding_tool = types.Tool(
    google_search=types.GoogleSearch()
)

# Configure generation settings
config = types.GenerateContentConfig(
    tools=[grounding_tool]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Who won the euro 2024?",
    config=config,
)

# Print the grounded response
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Define the grounding tool
const groundingTool = {
  googleSearch: {},
};

// Configure generation settings
const config = {
  tools: [groundingTool],
};

// Make the request
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

// Print the grounded response
console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {"text": "Who won the euro 2024?"}
        ]
      }
    ],
    "tools": [
      {
        "google_search": {}
      }
    ]
  }'
```

מידע נוסף זמין ב[מחברות של כלי החיפוש](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb?hl=he).

## איך פועלת האפשרות 'הסבר על הנושא' בחיפוש Google

כשמפעילים את הכלי `google_search`, המודל מטפל באופן אוטומטי בכל תהליך העבודה של החיפוש, העיבוד והציטוט של המידע.

![grounding-overview](https://ai.google.dev/static/gemini-api/docs/images/google-search-tool-overview.png?hl=he)

1.  **הנחיה למשתמש:** האפליקציה שולחת הנחיה של משתמש ל-Gemini API כשהכלי `google_search` מופעל.
2.  **ניתוח ההנחיה:** המודל מנתח את ההנחיה ומחליט אם חיפוש ב-Google יכול לשפר את התשובה.
3.  **חיפוש Google:** אם יש צורך, המודל יוצר באופן אוטומטי שאילתת חיפוש אחת או יותר ומריץ אותן.
4.  **עיבוד תוצאות החיפוש:** המודלים מעבדים את תוצאות החיפוש, מסנתזים את המידע ומנסחים תשובה.
5.  **תגובה מבוססת:** ה-API מחזיר תשובה סופית ידידותית למשתמש שמבוססת על תוצאות החיפוש. התשובה הזו כוללת את התשובה בטקסט של המודל וגם `groundingMetadata` עם שאילתות החיפוש, תוצאות האינטרנט והציטוטים.

## הסבר על תגובת ההארקה

כשתגובה מושתתת בהצלחה, היא כוללת את השדה `groundingMetadata`. הנתונים המובְנים האלה חיוניים לאימות ההצהרות וליצירת חוויית שימוש עשירה בציטוטים באפליקציה.

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}
```

ה-Gemini API מחזיר את המידע הבא באמצעות `groundingMetadata`:

*   `webSearchQueries` : מערך של שאילתות החיפוש שבהן נעשה שימוש. האפשרות הזו שימושית לניפוי באגים ולהבנת תהליך ההסקה של המודל.
*   `searchEntryPoint` : מכיל את ה-HTML וה-CSS להצגת ההצעות הנדרשות לחיפוש. הדרישות המלאות לשימוש מפורטות ב[תנאים וההגבלות](https://ai.google.dev/gemini-api/terms?hl=he#grounding-with-google-search).
*   `groundingChunks` : מערך של אובייקטים שמכיל את מקורות האינטרנט (`uri` ו-`title`).
*   `groundingSupports` : מערך קטעים שמקשר את התגובה של המודל `text` למקורות ב-`groundingChunks`. כל מקטע מקשר טקסט `segment` (המוגדרת על ידי `startIndex` ו-`endIndex`) ל-`groundingChunkIndices` אחד או יותר. זהו המפתח ליצירת ציטוטים בגוף הטקסט.

אפשר להשתמש ב'התאמה לנתונים' בחיפוש Google גם בשילוב עם [הכלי להקשר של כתובות URL](https://ai.google.dev/gemini-api/docs/url-context?hl=he) כדי להתבסס בתשובות גם על נתונים גלויים באינטרנט וגם על כתובות ה-URL הספציפיות שסיפקתם.

## שיוך מקורות באמצעות הפניות בתוך הטקסט

ה-API מחזיר נתוני ציטוט מובְנים, כך שיש לכם שליטה מלאה על אופן הצגת המקורות בממשק המשתמש. אפשר להשתמש בשדות `groundingSupports` ו-`groundingChunks` כדי לקשר את ההצהרות של המודל ישירות למקורות שלהן. לפניכם דפוס נפוץ לעיבוד המטא-נתונים ליצירת תגובה עם ציטוטים בתוך הטקסט שניתן ללחוץ עליהם.

### Python

```
def add_citations(response):
    text = response.text
    supports = response.candidates[0].grounding_metadata.grounding_supports
    chunks = response.candidates[0].grounding_metadata.grounding_chunks

    # Sort supports by end_index in descending order to avoid shifting issues when inserting.
    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

    for support in sorted_supports:
        end_index = support.segment.end_index
        if support.grounding_chunk_indices:
            # Create citation string like [1](link1)[2](link2)
            citation_links = []
            for i in support.grounding_chunk_indices:
                if i < len(chunks):
                    uri = chunks[i].web.uri
                    citation_links.append(f"[{i + 1}]({uri})")

            citation_string = ", ".join(citation_links)
            text = text[:end_index] + citation_string + text[end_index:]

    return text

# Assuming response with grounding metadata
text_with_citations = add_citations(response)
print(text_with_citations)
```

### JavaScript

```
function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);
```

התגובה החדשה עם הפניות בתוך הטקסט תיראה כך:

```
Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)
```

## תמחור

כשמשתמשים ב-Grounding עם חיפוש Google, המערכת מחייבת את הפרויקט לפי בקשת API שכוללת את הכלי `google_search`. אם המודל מחליט להריץ כמה שאילתות חיפוש כדי לענות להנחיה אחת (לדוגמה, חיפוש של `"UEFA Euro 2024 winner"` ו-`"Spain vs England Euro 2024 final score"` באותה קריאה ל-API), המערכת מחשיבה זאת כשימוש יחיד בכלי לחיוב עבור הבקשה הזו.

מידע מפורט על התמחור זמין [בדף התמחור של Gemini API](https://ai.google.dev/gemini-api/docs/pricing?hl=he).

## מודלים נתמכים

מודלים ניסיוניים ומודלים בתצוגה מקדימה לא נכללים. היכולות שלהם מפורטות בדף [סקירה כללית של הדגם](https://ai.google.dev/gemini-api/docs/models?hl=he).

דגם

חיפוש עם עיגון בנתונים

Gemini 2.5 Pro

✔️

Gemini 2.5 Flash

✔️

Gemini 2.0 Flash

✔️

Gemini 1.5 Pro

✔️

Gemini 1.5 Flash

✔️

## חיבור לקרקע במודלים של Gemini 1.5 (דור קודם)

הכלי `google_search` מומלץ ל-Gemini 2.0 ואילך, אבל ב-Gemini 1.5 יש תמיכה בכלי מדור קודם בשם `google_search_retrieval`. הכלי הזה מספק מצב `dynamic` שמאפשר למודל להחליט אם לבצע חיפוש על סמך רמת האמון שלו בכך שההנחיה מחייבת מידע עדכני. אם רמת הוודאות של המודל גבוהה מ-`dynamic_threshold` שהגדרתם (ערך בין 0.0 ל-1.0), הוא יבצע חיפוש.

### Python

```
# Note: This is a legacy approach for Gemini 1.5 models.
# The 'google_search' tool is recommended for all new development.
import os
from google import genai
from google.genai import types

client = genai.Client()

retrieval_tool = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            dynamic_threshold=0.7 # Only search if confidence > 70%
        )
    )
)

config = types.GenerateContentConfig(
    tools=[retrieval_tool]
)

response = client.models.generate_content(
    model='gemini-1.5-flash',
    contents="Who won the euro 2024?",
    config=config,
)
print(response.text)
if not response.candidates[0].grounding_metadata:
  print("\nModel answered from its own knowledge.")
```

### JavaScript

```
// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \

  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {"parts": [{"text": "Who won the euro 2024?"}]}
    ],
    "tools": [{
      "google_search_retrieval": {
        "dynamic_retrieval_config": {
          "mode": "MODE_DYNAMIC",
          "dynamic_threshold": 0.7
        }
      }
    }]
  }'
```

## המאמרים הבאים

*   כדאי לנסות את [הכנה לקראת השימוש בחיפוש Google במדריך למתכונים של Gemini API](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb?hl=he).
*   מידע על כלים זמינים אחרים, כמו [קריאה לפונקציה](https://ai.google.dev/gemini-api/docs/function-calling?hl=he)
*   [בכלי להצגת הקשר של כתובות URL](https://ai.google.dev/gemini-api/docs/url-context?hl=he) מוסבר איך להוסיף להנחיות כתובות URL ספציפיות.

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/prompting_with_media

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Files API

The Gemini family of artificial intelligence (AI) models is built to handle various types of input data, including text, images, and audio. Since these models can handle more than one type or _mode_ of data, the Gemini models are called _multimodal models_ or explained as having _multimodal capabilities_.

This guide shows you how to work with media files using the Files API. The basic operations are the same for audio files, images, videos, documents, and other supported file types.

For file prompting guidance, check out the [File prompt guide](/gemini-api/docs/files#prompt-guide) section.

## Upload a file

You can use the Files API to upload a media file. Always use the Files API when the total request size (including the files, text prompt, system instructions, etc.) is larger than 20 MB.

The following code uploads a file and then uses the file in a call to `generateContent`.

### Python

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
defer client.DeleteFile(ctx, file.Name)

model := client.GenerativeModel("gemini-2.0-flash")
resp, err := model.GenerateContent(ctx,
    genai.FileData{URI: file.URI},
    genai.Text("Describe this audio clip"))
if err != nil {
    log.Fatal(err)
}

printResponse(resp)
```

### REST

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D "${tmp_header_file}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Get metadata for a file

You can verify that the API successfully stored the uploaded file and get its metadata by calling `files.get`.

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
file_name = myfile.name
myfile = client.files.get(name=file_name)
print(myfile)
```

### JavaScript

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
const fetchedFile = await ai.files.get({ name: fileName });
console.log(fetchedFile);
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}

gotFile, err := client.GetFile(ctx, file.Name)
if err != nil {
    log.Fatal(err)
}
fmt.Println("Got file:", gotFile.Name)
```

### REST

```
# file_info.json was created in the upload example
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name \
-H "x-goog-api-key: $GEMINI_API_KEY" > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

## List uploaded files

You can upload multiple files using the Files API. The following code gets a list of all the files uploaded:

### Python

```
print('My files:')
for f in client.files.list():
    print(' ', f.name)
```

### JavaScript

```
const listResponse = await ai.files.list({ config: { pageSize: 10 } });
for await (const file of listResponse) {
  console.log(file.name);
}
```

### Go

```
iter := client.ListFiles(ctx)
for {
    ifile, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(ifile.Name)
}
```

### REST

```
echo "My files: "

curl "https://generativelanguage.googleapis.com/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## Delete uploaded files

Files are automatically deleted after 48 hours. You can also manually delete an uploaded file:

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
client.files.delete(name=myfile.name)
```

### JavaScript

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
await ai.files.delete({ name: fileName });
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
client.DeleteFile(ctx, file.Name)
```

### REST

```
curl --request "DELETE" https://generativelanguage.googleapis.com/v1beta/files/$name \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## Usage info

You can use the Files API to upload and interact with media files. The Files API lets you store up to 20 GB of files per project, with a per-file maximum size of 2 GB. Files are stored for 48 hours. During that time, you can use the API to get metadata about the files, but you can't download the files. The Files API is available at no cost in all regions where the Gemini API is available.

## File prompting strategies

This section provides guidance and best practices for using media files with prompts for the Gemini API.

Being able to use various types of data in your prompts gives you more flexibility in terms of what tasks you can tackle with the Gemini API. For example, you can send the model a photo of a delicious meal and ask it to write a short blog about the meal.

**Prompt**

**Response**

Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.

  
![Cookies](/static/gemini-api/docs/images/image_prompt.png)

Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time.

This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option.

This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings!

If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied.

Here's to healthy and delicious meal prepping!

If you are having trouble getting the output you want from prompts that use media files, there are some strategies that can help you get the results you want. The following sections provide design approaches and troubleshooting tips for improving prompts that use multimodal input.

You can improve your multimodal prompts by following these best practices:

*   ### [Prompt design fundamentals](#specific-instructions)
    
    *   **Be specific in your instructions**: Craft clear and concise instructions that leave minimal room for misinterpretation.
    *   **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.
    *   **Break it down step-by-step**: Divide complex tasks into manageable sub-goals, guiding the model through the process.
    *   **Specify the output format**: In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more.
    *   **Put your image first for single-image prompts**: While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image (or video) is placed before the text prompt. However, for prompts that require images to be highly interleaved with texts to make sense, use whatever order is most natural.
*   ### [Troubleshooting your multimodal prompt](#troubleshooting)
    
    *   **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.
    *   **If the model output is too generic (not tailored enough to the image/video input):** At the start of the prompt, try asking the model to describe the image(s) or video before providing the task instruction, or try asking the model to refer to what's in the image.
    *   **To troubleshoot which part failed:** Ask the model to describe the image, or ask the model to explain its reasoning, to gauge the model's initial understanding.
    *   **If your prompt results in hallucinated content:** Try dialing down the temperature setting or asking the model for shorter descriptions so that it's less likely to extrapolate additional details.
    *   **Tuning the sampling parameters:** Experiment with different temperature settings and top-k selections to adjust the model's creativity.

### Be specific in your instructions

Prompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want.

For this image of an airport board, asking the model to just "describe this image" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt.

Prompt

Model response

![](/static/gemini-api/docs/images/timetable.png)

Describe this image.

The image shows an airport arrivals and departures board.

**Updated prompt**

**Improved response**

![Travel timetable](/static/gemini-api/docs/images/timetable.png)

Parse the time and city from the airport board shown in this image into a list.

*   10:50 Moscow
*   11:05 Edinburgh
*   11:05 London
*   11:10 Bucharest
*   11:30 Kiev
*   11:35 Dublin
*   11:45 East Midlands
*   12:15 Sofia
*   12:30 London
*   12:30 Newcastle
*   12:40 St Petersburg
*   12:40 London
*   12:45 Manchester

### Add a few examples

The Gemini model can accept multiple inputs which it can use as examples to understand the output you want. Adding these examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. This is also called "few-shot" learning.

In the example below, the initial output is written in sentence form, and also contains the country (Brazil). Suppose you want the output in a different format or style, and you want only the city, not the country. Adding few-shot examples to your prompt can steer the model to respond in the way you want.

**Prompt**

**Model response**

![Travel timetable](/static/gemini-api/docs/images/redeemer.png)

Determine the city along with the landmark.

The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.

**Updated prompt**

**Improved response**

Determine the city along with the landmark.

![colosseum](/static/gemini-api/docs/images/colosseum.png)

city: Rome, landmark: the Colosseum.

![Forbidden City](/static/gemini-api/docs/images/forbidden-city.png)

city: Beijing, landmark: Forbidden City

![forbidden-city](/static/gemini-api/docs/images/redeemer.png)

city: Rio de Janeiro, landmark: Christ the Redeemer statue

### Break it down step-by-step

For complex tasks like the ones that require both visual understanding and reasoning, it can be helpful to split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you directly ask the model to “think step by step” in your prompt.

Prompt

Model response

![](/static/gemini-api/docs/images/toilet-paper.jpg)

When will I run out of toilet paper?

Soon, you only have 3 rolls left.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/toilet-paper.jpg)

1\. First, count how many toilet paper rolls are in this picture.

2\. Then, determine how much toilet paper a typical person uses per day.

3\. Calculate how long these rolls of toilet paper will last.

1\. There are 3 rolls of toilet paper in this picture.

2\. A typical person uses about 20 sheets of toilet paper per day.

3\. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.

Math problems or other types of word problems are great candidates for asking the model to think step-by-step.

**Prompt**

**Response**

![](/static/gemini-api/docs/images/math.jpg)

What is the 4th term in the sequence?

\-135

The response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt.

**Prompt**

**Response**

![](/static/gemini-api/docs/images/math.jpg)

Parse the formula in the image first.

Then based on the formula, answer "what's the 4th term in the sequence?".

_Click to expand the result_

$$ b(n) = b(n-1) \* (-3) $$

$$ b(1) = 15 $$

1.  Substitute n = 4 into the formula:

$$ b(4) = b(4-1) \* (-3) $$

$$ b(4) = b(3) \* (-3) $$

4.  We don't know the value of b(3), so substitute n = 3 into the formula:

$$ b(3) = b(3-1) \* (-3) $$

$$ b(3) = b(2) \* (-3) $$

7.  We don't know the value of b(2), so substitute n = 2 into the formula:

$$ b(2) = b(2-1) \* (-3) $$

$$ b(2) = b(1) \* (-3) $$

10.  We know the value of b(1) is 15, so substitute it into the formula:

$$ b(2) = 15 \* (-3) $$

$$ b(2) = -45 $$

13.  Now that we know the value of b(2), substitute it into the formula for b(3):

$$ b(3) = b(2) \* (-3) $$

$$ b(3) = -45 \* (-3) $$

$$ b(3) = 135 $$

17.  Now that we know the value of b(3), substitute it into the formula for b(4):

$$ b(4) = b(3) \* (-3) $$

$$ b(4) = 135 \* (-3) $$

$$ b(4) = -405 $$

Therefore, the 4th term in the sequence is -405.

This model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/math.jpg)

What is the 4th term in the sequence? Think step by step.

The first term is 15.

The second term is b(1) \* (-3) = 15 \* (-3) = -45.

The third term is b(2) \* (-3) = -45 \* (-3) = 135.

The fourth term is b(3) \* (-3) = 135 \* (-3) = -405.

### Specify the output format

A common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples:

Prompt

Model response

![](/static/gemini-api/docs/images/excel-example.png)

Parse the table in this image into markdown format

| Subject | Status | Time | Start date | Due on |

| ------- | ------ | ---- | ---------- | ------ |

| Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 |

| Geography | Done | 30 min | 12/2/2023 | 12/4/2023 |

| Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 |

| Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |

**Prompt**

**Model response**

![](/static/gemini-api/docs/images/spanish-meal.png)

Provide a list of all the following attributes:

ingredients, type of cuisine, vegetarian or not, in JSON format

` ```json `

`   {  "ingredients": [  "rice",  "shrimp",  "clams",  "mussels",  "peas",  "tomatoes",  "onions",  "garlic",  "olive oil",  "paprika",  "salt",  "pepper"  ],  "type of cuisine": "Spanish",  "vegetarian": false  }  ```   `

### Put your image first for single-image prompts

While Gemini can interpret image and text in any order within a prompt, placing a single image before the text prompt might lead to better results. In the following samples, you'll notice the image comes first before the text in each prompt.

### Troubleshooting your multimodal prompt

You might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try.

#### If the model is not drawing information from the relevant part of the image

To get a more specific response, you can point out which aspects of the image you want the prompt to draw information from to inform its response.

Prompt

Model response

![](/static/gemini-api/docs/images/diapers.png)

How many days will these diapers last a baby?

The diapers will last for 198 days before they run out.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/diapers.png)

How long will these diapers last before I run out?

Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box.

Divide the total number by how many diapers the child goes through per day.

The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs.

The average child in this weight range is likely between 1 and 2 years old.

A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).

#### If the model output is too generic and not tailored enough to the image input

To help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task.

Prompt

Model response

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

What is in common between these images?

All four images are examples of landscape photography.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

First, describe what's in each image in detail. What's in common between these images?

The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea.

The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity.

The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world.

The fourth image shows the sea in between being calm and being stormy.

Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.

You can also try explicitly asking the model to refer to what's in the image.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

What is in common between these images? Refer to what's in the images in your response.

All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea.

These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling.

The same tips work for creative output from images. In the following example with the rabbit, explicitly asking the model to include aspects from the image can improve the output.

Prompt

Model response

![](/static/gemini-api/docs/images/rabbit.jpg)

Can you write me a poem about this image?

Little bunny, so soft and brown,  
Hopping through the field, up and down.  
Your ears so long, your tail so small,  
You're the cutest creature of them all.

You nibble on the grass, so green and sweet,  
And hop away when you see me.  
But I'll always be your friend,  
Little bunny, until the very end.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/rabbit.jpg)

Can you write me a descriptive and dramatic poem about this image and include the location?

Little bunny on the green,  
Ears so long, so keen.  
What do you see out there,  
In the distance, where you stare?

Is it the ocean blue,  
Sparkling in the sun's hue?  
Or the waves that crash and roar,  
A rhythm that you adore?

Little bunny, tell me true,  
What captures your attention so?  
Is it the salty breeze,  
Or the seagulls' cries that please?

Perhaps it's the vastness of it all,  
A world so big and small.  
A place where dreams can roam,  
And adventures can be sown.

Little bunny, I wonder what you dream,  
As you sit on the grass, so serene.  
Do you yearn to explore the deep,  
Or stay on land, where you can leap?

Whatever it may be, little bunny,  
Keep that spark of wonder burning brightly.  
For in your dreams and aspirations,  
Lies a world waiting for your creation.

#### Troubleshooting which part of the prompt failed

It can be hard to know whether a prompt failed because the model didn't **understand the image** to begin with, or if it did understand the image but did not perform the correct **reasoning steps** afterward. To disambiguate between those reasons, ask the model to describe what's in the image.

In the following example, if the model responds with a snack that seems surprising when paired with tea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized that the image contains tea.

Prompt

Prompt for troubleshooting

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this?

![](/static/gemini-api/docs/images/tea-set.png)

Describe what's in this image.

Another strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any.

Prompt

Prompt for troubleshooting

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this?

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this? Please explain why.

## What's next

*   Try writing your own multimodal prompts using [Google AI Studio](http://aistudio.google.com).
*   For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](/gemini-api/docs/vision), [Audio](/gemini-api/docs/audio), and [Document processing](/gemini-api/docs/document-processing) guides.
*   For more guidance on prompt design, like tuning sampling parameters, see the [Prompt strategies](/gemini-api/docs/prompting-strategies) page.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/system-instructions

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Генерация текста

API Gemini может генерировать текстовый вывод из различных входных данных, включая текст, изображения, видео и аудио, используя модели Gemini.

Вот простой пример, требующий ввода одного текста:

### Питон

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Думаем с Близнецами 2.5

В моделях 2.5 Flash и Pro по умолчанию включена [функция «мышления»](https://ai.google.dev/gemini-api/docs/thinking?hl=ru) для повышения качества, что может занять больше времени и увеличить использование токенов.

При использовании Flash 2.5 вы можете отключить мышление, установив бюджет мышления на ноль.

Более подробную информацию см. [в руководстве по мышлению](https://ai.google.dev/gemini-api/docs/thinking?hl=ru#set-budget) .

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Системные инструкции и другие конфигурации

Вы можете управлять поведением моделей Gemini с помощью системных инструкций. Для этого передайте объект [`GenerateContentConfig`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) .

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Объект [`GenerateContentConfig`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) также позволяет переопределять параметры генерации по умолчанию, такие как [температура](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) .

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Полный список настраиваемых параметров и их описания см. в разделе [`GenerateContentConfig`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) в нашем справочнике по API.

## Мультимодальные входы

API Gemini поддерживает мультимодальные входы, позволяя вам комбинировать текст с медиафайлами. Следующий пример демонстрирует предоставление изображения:

### Питон

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

Для альтернативных методов предоставления изображений и более продвинутой обработки изображений см. наше [руководство по пониманию изображений](https://ai.google.dev/gemini-api/docs/image-understanding?hl=ru) . API также поддерживает ввод и понимание [документов](https://ai.google.dev/gemini-api/docs/document-processing?hl=ru) , [видео](https://ai.google.dev/gemini-api/docs/video-understanding?hl=ru) и [аудио](https://ai.google.dev/gemini-api/docs/audio?hl=ru) .

## Потоковые ответы

По умолчанию модель возвращает ответ только после завершения всего процесса генерации.

Для более плавного взаимодействия используйте потоковую передачу, чтобы получать экземпляры [`GenerateContentResponse`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerateContentResponse) постепенно по мере их генерации.

### Питон

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Многократные беседы (чат)

Наши SDK предоставляют функционал для сбора нескольких раундов запросов и ответов в чат, предоставляя вам простой способ отслеживать историю разговоров.

### Питон

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

### ОТДЫХ

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Потоковую передачу также можно использовать для многопоточных разговоров.

### Питон

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### ОТДЫХ

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Поддерживаемые модели

Все модели семейства Gemini поддерживают генерацию текста. Чтобы узнать больше о моделях и их возможностях, посетите страницу [Модели](https://ai.google.dev/gemini-api/docs/models?hl=ru) .

## Лучшие практики

### Полезные советы

Для создания простого текста часто бывает достаточно подсказки [с нуля](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=ru#few-shot) , без необходимости в примерах, системных инструкциях или специальном форматировании.

Для более индивидуальных результатов:

*   Используйте [системные инструкции](#system-instructions) для управления моделью.
*   Предоставьте несколько примеров входов и выходов для руководства моделью. Это часто называют подсказкой с [небольшим количеством выстрелов](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=ru#few-shot) .

Дополнительные советы можно найти в нашем [руководстве по быстрому проектированию](https://ai.google.dev/gemini/docs/prompting-strategies?hl=ru) .

### Структурированный вывод

В некоторых случаях вам может понадобиться структурированный вывод, такой как JSON. Обратитесь к нашему руководству [по структурированному выводу](https://ai.google.dev/gemini-api/docs/structured-output?hl=ru) , чтобы узнать, как это сделать.

## Что дальше?

*   Попробуйте [API Gemini, чтобы начать работу с Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb?hl=ru) .
*   Изучите возможности Gemini по распознаванию [изображений](https://ai.google.dev/gemini-api/docs/image-understanding?hl=ru) , [видео](https://ai.google.dev/gemini-api/docs/video-understanding?hl=ru) , [аудио](https://ai.google.dev/gemini-api/docs/audio?hl=ru) и [документов](https://ai.google.dev/gemini-api/docs/document-processing?hl=ru) .
*   Узнайте о [стратегиях мультимодального запроса файлов](https://ai.google.dev/gemini-api/docs/files?hl=ru#prompt-guide) .

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/imagen

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Image generation

You can generate images using the Gemini API with either Gemini's built-in multimodal capabilities or Imagen, Google's specialized image generation models. For most use cases, start with [Gemini](/gemini-api/docs/image-generation#gemini). Choose [Imagen](/gemini-api/docs/image-generation#imagen) for specialized tasks where image quality is critical. See [Choosing the right model](/gemini-api/docs/image-generation#choose-a-model) section for more guidance.

All generated images include a [SynthID watermark](/responsible/docs/safeguards/synthid).

## Before you begin

Ensure you use a supported model and version for image generation:

*   For **Gemini**, use Gemini 2.0 Flash Preview Image Generation.
    
*   For **Imagen**, use one of the Imagen models (Imagen 3, Imagen 4 or Imagen 4 Ultra).
    
    *   Note that those models are only available on the [Paid tier](/gemini-api/docs/pricing).

You can access both Gemini and Imagen models using the same libraries.

## Generate images using Gemini

Gemini can generate and process images conversationally. You can prompt Gemini with text, images, or a combination of both to achieve various image-related tasks, such as image generation and editing.

You must include `responseModalities`: `["TEXT", "IMAGE"]` in your configuration. Image-only output is not supported with these models.

### Image generation (text-to-image)

The following code demonstrates how to generate an image based on a descriptive prompt:

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO
import base64

client = genai.Client()

contents = ('Hi, can you create a 3d rendered image of a pig '
            'with wings and a top hat flying over a happy '
            'futuristic scifi city with lots of greenery?')

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=contents,
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.save('gemini-native-image.png')
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const contents =
    "Hi, can you create a 3d rendered image of a pig " +
    "with wings and a top hat flying over a happy " +
    "futuristic scifi city with lots of greenery?";

  // Set responseModalities to include "Image" so the model can generate  an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      ResponseModalities: []string{"TEXT", "IMAGE"},
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.0-flash-preview-image-generation",
      genai.Text("Hi, can you create a 3d rendered image of a pig " +
                 "with wings and a top hat flying over a happy " +
                 "futuristic scifi city with lots of greenery?"),
      config,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "gemini_generated_image.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

### REST

```
curl -s -X POST 
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?"}
      ]
    }],
    "generationConfig":{"responseModalities":["TEXT","IMAGE"]}
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-native-image.png
```

![AI-generated image of a fantastical flying pig](/static/gemini-api/docs/images/flying-pig.png)

AI-generated image of a fantastical flying pig

### Image editing (text-and-image-to-image)

To perform image editing, add an image as input. The following example demonstrates uploading base64 encoded images. For multiple images and larger payloads, check the [image input](/gemini-api/docs/image-understanding#image-input) section.

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

import PIL.Image

image = PIL.Image.open('/path/to/image.png')

client = genai.Client()

text_input = ('Hi, This is a picture of me.'
            'Can you add a llama next to me?',)

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=[text_input, image],
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  // Load the image from the local file system
  const imagePath = "path/to/image.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  // Prepare the content parts
  const contents = [
    { text: "Can you add a llama next to the image?" },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  // Set responseModalities to include "Image" so the model can generate an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Go

```
package main

import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
)

func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
     log.Fatal(err)
 }

 imagePath := "/path/to/image.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
   genai.NewPartFromText("Hi, This is a picture of me. Can you add a llama next to me?"),
   &genai.Part{
     InlineData: &genai.Blob{
       MIMEType: "image/png",
       Data:     imgData,
     },
   },
 }

 contents := []*genai.Content{
   genai.NewContentFromParts(parts, genai.RoleUser),
 }

 config := &genai.GenerateContentConfig{
     ResponseModalities: []string{"TEXT", "IMAGE"},
 }

 result, _ := client.Models.GenerateContent(
     ctx,
     "gemini-2.0-flash-preview-image-generation",
     contents,
     config,
 )

 for _, part := range result.Candidates[0].Content.Parts {
     if part.Text != "" {
         fmt.Println(part.Text)
     } else if part.InlineData != nil {
         imageBytes := part.InlineData.Data
         outputFilename := "gemini_generated_image.png"
         _ = os.WriteFile(outputFilename, imageBytes, 0644)
     }
 }
}
```

### REST

```
IMG_PATH=/path/to/your/image1.jpeg

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"'Hi, This is a picture of me. Can you add a llama next to me\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/jpeg\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }],
      \"generationConfig\": {\"responseModalities\": [\"TEXT\", \"IMAGE\"]}
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-edited-image.png
```

### Other image generation modes

Gemini supports other image interaction modes based on prompt structure and context, including:

*   **Text to image(s) and text (interleaved):** Outputs images with related text.
    *   Example prompt: "Generate an illustrated recipe for a paella."
*   **Image(s) and text to image(s) and text (interleaved)**: Uses input images and text to create new related images and text.
    *   Example prompt: (With an image of a furnished room) "What other color sofas would work in my space? can you update the image?"
*   **Multi-turn image editing (chat):** Keep generating / editing images conversationally.
    *   Example prompts: \[upload an image of a blue car.\] , "Turn this car into a convertible.", "Now change the color to yellow."

### Limitations

*   For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.
*   Image generation does not support audio or video inputs.
*   Image generation may not always trigger:
    *   The model may output text only. Try asking for image outputs explicitly (e.g. "generate an image", "provide images as you go along", "update the image").
    *   The model may stop generating partway through. Try again or try a different prompt.
*   When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.
*   There are some regions/countries where Image generation is not available. See [Models](/gemini-api/docs/models) for more information.

## Generate images using the Imagen models

This example demonstrates generating images with an [Imagen model](https://deepmind.google/technologies/imagen-3/):

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

response = client.models.generate_images(
    model='imagen-4.0-generate-preview-06-06',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 4,
    )
)
for generated_image in response.generated_images:
  generated_image.image.show()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-preview-06-06',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateImagesConfig{
      NumberOfImages: 4,
  }

  response, _ := client.Models.GenerateImages(
      ctx,
      "imagen-4.0-generate-preview-06-06",
      "Robot holding a red skateboard",
      config,
  )

  for n, image := range response.GeneratedImages {
      fname := fmt.Sprintf("imagen-%d.png", n)
          _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
  }
}
```

### REST

```
curl -X POST \
    "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-preview-06-06:predict" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "instances": [
          {
            "prompt": "Robot holding a red skateboard"
          }
        ],
        "parameters": {
          "sampleCount": 4
        }
      }'
```

![AI-generated image of a robot holding a red skateboard](/static/gemini-api/docs/images/robot-skateboard.png)

AI-generated image of a robot holding a red skateboard

### Imagen configuration

Imagen supports English only prompts at this time and the following parameters:

*   `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive). The default is 4. For Imagen 4 Ultra, it defaults to 1 as only one image can be generated at a time.
*   `aspectRatio`: Changes the aspect ratio of the generated image. Supported values are `"1:1"`, `"3:4"`, `"4:3"`, `"9:16"`, and `"16:9"`. The default is `"1:1"`.
*   `personGeneration`: Allow the model to generate images of people. The following values are supported:
    
    *   `"dont_allow"`: Block generation of images of people.
    *   `"allow_adult"`: Generate images of adults, but not children. This is the default.
    *   `"allow_all"`: Generate images that include adults and children.

## Choosing the right model

Choose **Gemini** when:

*   You need contextually relevant images that leverage world knowledge and reasoning.
*   Seamlessly blending text and images is important.
*   You want accurate visuals embedded within long text sequences.
*   You want to edit images conversationally while maintaining context.

Choose **Imagen** when:

*   Image quality, photorealism, artistic detail, or specific styles (e.g., impressionism, anime) are top priorities.
*   Performing specialized editing tasks like product background updates or image upscaling.
*   Infusing branding, style, or generating logos and product designs.

Imagen 4 should be your go-to model starting to generate images with Imagen. Choose Imagen 4 Ultra for advanced use-cases or when you need the best image quality. Note that Imagen 4 Ultra can only generate one image at a time.

## Imagen prompt guide

This section of the Imagen guide shows you how modifying a text-to-image prompt can produce different results, along with examples of images you can create.

### Prompt writing basics

A good prompt is descriptive and clear, and makes use of meaningful keywords and modifiers. Start by thinking of your **subject**, **context**, and **style**.

![Prompt with subject, context, and style emphasized](/static/gemini-api/docs/images/imagen/style-subject-context.png)

Image text: A _sketch_ (**style**) of a _modern apartment building_ (**subject**) surrounded by _skyscrapers_ (**context and background**).

1.  **Subject**: The first thing to think about with any prompt is the _subject_: the object, person, animal, or scenery you want an image of.
    
2.  **Context and background:** Just as important is the _background or context_ in which the subject will be placed. Try placing your subject in a variety of backgrounds. For example, a studio with a white background, outdoors, or indoor environments.
    
3.  **Style:** Finally, add the style of image you want. _Styles_ can be general (painting, photograph, sketches) or very specific (pastel painting, charcoal drawing, isometric 3D). You can also combine styles.
    

After you write a first version of your prompt, refine your prompt by adding more details until you get to the image that you want. Iteration is important. Start by establishing your core idea, and then refine and expand upon that core idea until the generated image is close to your vision.

![photorealistic sample image 1](/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_short.png)

Prompt: A park in the spring next to a lake

![photorealistic sample image 2](/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_medium.png)

Prompt: A park in the spring next to a lake, **the sun sets across the lake, golden hour**

![photorealistic sample image 3](/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_long.png)

Prompt: A park in the spring next to a lake, _**the sun sets across the lake, golden hour, red wildflowers**_

Imagen models can transform your ideas into detailed images, whether your prompts are short or long and detailed. Refine your vision through iterative prompting, adding details until you achieve the perfect result.

Short prompts let you generate an image quickly.

![Imagen 3 short prompt example](/static/gemini-api/docs/images/imagen/imagen3_short-prompt.png)

Prompt: close-up photo of a woman in her 20s, street photography, movie still, muted orange warm tones

Longer prompts let you add specific details and build your image.

![Imagen 3 long prompt example](/static/gemini-api/docs/images/imagen/imagen3_long-prompt.png)

Prompt: captivating photo of a woman in her 20s utilizing a street photography style. The image should look like a movie still with muted orange warm tones.

Additional advice for Imagen prompt writing:

*   **Use descriptive language**: Employ detailed adjectives and adverbs to paint a clear picture for Imagen.
*   **Provide context**: If necessary, include background information to aid the AI's understanding.
*   **Reference specific artists or styles**: If you have a particular aesthetic in mind, referencing specific artists or art movements can be helpful.
*   **Use prompt engineering tools**: Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results.
*   **Enhancing the facial details in your personal and group images**: Specify facial details as a focus of the photo (for example, use the word "portrait" in the prompt).

### Generate text in images

Imagen models can add text into images, opening up more creative image generation possibilities. Use the following guidance to get the most out of this feature:

*   **Iterate with confidence**: You might have to regenerate images until you achieve the look you want. Imagen's text integration is still evolving, and sometimes multiple attempts yield the best results.
*   **Keep it short**: Limit text to 25 characters or less for optimal generation.
*   **Multiple phrases**: Experiment with two or three distinct phrases to provide additional information. Avoid exceeding three phrases for cleaner compositions.
    
    ![Imagen 3 generate text example](/static/gemini-api/docs/images/imagen/imagen3_generate-text.png)
    
    Prompt: A poster with the text "Summerland" in bold font as a title, underneath this text is the slogan "Summer never felt so good"
    
*   **Guide Placement**: While Imagen can attempt to position text as directed, expect occasional variations. This feature is continually improving.
    
*   **Inspire font style**: Specify a general font style to subtly influence Imagen's choices. Don't rely on precise font replication, but expect creative interpretations.
    
*   **Font size**: Specify a font size or a general indication of size (for example, _small_, _medium_, _large_) to influence the font size generation.
    

### Prompt parameterization

To better control output results, you might find it helpful to parameterize the inputs into Imagen. For example, suppose you want your customers to be able to generate logos for their business, and you want to make sure logos are always generated on a solid color background. You also want to limit the options that the client can select from a menu.

In this example, you can create a parameterized prompt similar to the following:

A {logo\_style} logo for a {company\_area} company on a solid color background. Include the text {company\_name}.

In your custom user interface, the customer can input the parameters using a menu, and their chosen value populates the prompt Imagen receives.

For example:

1.  Prompt: `A minimalist logo for a health care company on a solid color background. Include the text Journey.`
    
    ![Imagen 3 prompt parameterization example 1](/static/gemini-api/docs/images/imagen/imagen3_prompt-param_healthcare.png)
    
2.  Prompt: `A modern logo for a software company on a solid color background. Include the text Silo.`
    
    ![Imagen 3 prompt parameterization example 2](/static/gemini-api/docs/images/imagen/imagen3_prompt-param_software.png)
    
3.  Prompt: `A traditional logo for a baking company on a solid color background. Include the text Seed.`
    
    ![Imagen 3 prompt parameterization example 3](/static/gemini-api/docs/images/imagen/imagen3_prompt-param_baking.png)
    

### Advanced prompt writing techniques

Use the following examples to create more specific prompts based on attributes like photography descriptors, shapes and materials, historical art movements, and image quality modifiers.

#### Photography

*   Prompt includes: _"A photo of..."_

To use this style, start with using keywords that clearly tell Imagen that you're looking for a photograph. Start your prompts with _"A photo of. . ."_. For example:

![photorealistic sample image 1](/static/gemini-api/docs/images/imagen/1_style-photography_coffee-beans.png)

Prompt: **A photo of** coffee beans in a kitchen on a wooden surface

![photorealistic sample image 2](/static/gemini-api/docs/images/imagen/1_style-photography_chocolate-bar.png)

Prompt: **A photo of** a chocolate bar on a kitchen counter

![photorealistic sample image 3](/static/gemini-api/docs/images/imagen/1_style-photography_modern-building.png)

Prompt: **A photo of** a modern building with water in the background

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.

##### Photography modifiers

In the following examples, you can see several photography-specific modifiers and parameters. You can combine multiple modifiers for more precise control.

1.  **Camera Proximity** - _Close up, taken from far away_
    
    ![close up camera sample image](/static/gemini-api/docs/images/imagen/3_camera-proximity_close-up.png)
    
    Prompt: A **close-up** photo of coffee beans
    
    ![zoomed out camera sample image](/static/gemini-api/docs/images/imagen/3_camera-proximity_zoomed-out.png)
    
    Prompt: A **zoomed out** photo of a small bag of  
    coffee beans in a messy kitchen
    
2.  **Camera Position** - _aerial, from below_
    
    ![aerial photo sample image](/static/gemini-api/docs/images/imagen/4_camera-position_aerial-photo.png)
    
    Prompt: **aerial photo** of urban city with skyscrapers
    
    ![a view from underneath sample image](/static/gemini-api/docs/images/imagen/4_camera-position_from-below.png)
    
    Prompt: A photo of a forest canopy with blue skies **from below**
    
3.  **Lighting** - _natural, dramatic, warm, cold_
    
    ![natural lighting sample image](/static/gemini-api/docs/images/imagen/5_lighting_natural-lighting.png)
    
    Prompt: studio photo of a modern arm chair, **natural lighting**
    
    ![dramatic lighting sample image](/static/gemini-api/docs/images/imagen/5_lighting_dramatic-lighting.png)
    
    Prompt: studio photo of a modern arm chair, **dramatic lighting**
    
4.  **Camera Settings** _\- motion blur, soft focus, bokeh, portrait_
    
    ![motion blur sample image](/static/gemini-api/docs/images/imagen/6_camera-settings_motion-blur.png)
    
    Prompt: photo of a city with skyscrapers from the inside of a car with **motion blur**
    
    ![soft focus sample image](/static/gemini-api/docs/images/imagen/6_camera-settings_soft-focus.png)
    
    Prompt: **soft focus** photograph of a bridge in an urban city at night
    
5.  **Lens types** - _35mm, 50mm, fisheye, wide angle, macro_
    
    ![macro lens sample image](/static/gemini-api/docs/images/imagen/7_lens-types_macro-lens.png)
    
    Prompt: photo of a leaf, **macro lens**
    
    ![fisheye lens sample image](/static/gemini-api/docs/images/imagen/7_lens-types_fisheye-lens.png)
    
    Prompt: street photography, new york city, **fisheye lens**
    
6.  **Film types** - _black and white, polaroid_
    
    ![polaroid photo sample image](/static/gemini-api/docs/images/imagen/8_film-types_polaroid-portrait.png)
    
    Prompt: a **polaroid portrait** of a dog wearing sunglasses
    
    ![black and white photo sample image](/static/gemini-api/docs/images/imagen/8_film-types_bw-photo.png)
    
    Prompt: **black and white photo** of a dog wearing sunglasses
    

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.

### Illustration and art

*   Prompt includes: _"A painting of..."_, _"A sketch of..."_

Art styles vary from monochrome styles like pencil sketches, to hyper-realistic digital art. For example, the following images use the same prompt with different styles:

_"An \[art style or creation technique\] of an angular sporty electric sedan with skyscrapers in the background"_

![art sample images](/static/gemini-api/docs/images/imagen/2_style-illustration1A.png)

Prompt: A **technical pencil drawing** of an angular...

![art sample images](/static/gemini-api/docs/images/imagen/2_style-illustration1B.png)

Prompt: A **charcoal drawing** of an angular...

![art sample images](/static/gemini-api/docs/images/imagen/2_style-illustration1C.png)

Prompt: A **color pencil drawing** of an angular...

![art sample images](/static/gemini-api/docs/images/imagen/2_style-illustration2E.png)

Prompt: A **pastel painting** of an angular...

![art sample images](/static/gemini-api/docs/images/imagen/2_style-illustration2F.png)

Prompt: A **digital art** of an angular...

![art sample images](/static/gemini-api/docs/images/imagen/2_style-illustration2G.png)

Prompt: An **art deco (poster)** of an angular...

Image source: Each image was generated using its corresponding text prompt with the Imagen 2 model.

##### Shapes and materials

*   Prompt includes: _"...made of..."_, _"...in the shape of..."_

One of the strengths of this technology is that you can create imagery that is otherwise difficult or impossible. For example, you can recreate your company logo in different materials and textures.

![shapes and materials example image 1](/static/gemini-api/docs/images/imagen/9_shapes-materials_duffel.png)

Prompt: a duffle bag **made of** cheese

![shapes and materials example image 2](/static/gemini-api/docs/images/imagen/9_shapes-materials_bird.png)

Prompt: neon tubes **in the shape** of a bird

![shapes and materials example image 3](/static/gemini-api/docs/images/imagen/9_shapes-materials_paper.png)

Prompt: an armchair **made of paper**, studio photo, origami style

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.

#### Historical art references

*   Prompt includes: _"...in the style of..."_

Certain styles have become iconic over the years. The following are some ideas of historical painting or art styles that you can try.

_"generate an image in the style of \[art period or movement\] : a wind farm"_

![impressionism example image](/static/gemini-api/docs/images/imagen/10_historical-ref1_impressionism.png)

Prompt: generate an image **in the style of _an impressionist painting_**: a wind farm

![renaissance example image](/static/gemini-api/docs/images/imagen/10_historical-ref1_renaissance.png)

Prompt: generate an image **in the style of _a renaissance painting_**: a wind farm

![pop art example image](/static/gemini-api/docs/images/imagen/10_historical-ref1_pop-art.png)

Prompt: generate an image **in the style of _pop art_**: a wind farm

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.

#### Image quality modifiers

Certain keywords can let the model know that you're looking for a high-quality asset. Examples of quality modifiers include the following:

*   **General Modifiers** - _high-quality, beautiful, stylized_
*   **Photos** - _4K, HDR, Studio Photo_
*   **Art, Illustration** - _by a professional, detailed_

The following are a few examples of prompts without quality modifiers and the same prompt with quality modifiers.

![corn example image without modifiers](/static/gemini-api/docs/images/imagen/11_quality-modifier2_no-mods.png)

Prompt (no quality modifiers): a photo of a corn stalk

![corn example image with modifiers](/static/gemini-api/docs/images/imagen/11_quality-modifier2_4k-hdr.png)

Prompt (with quality modifiers): **4k HDR beautiful**  
photo of a corn stalk **taken by a  
professional photographer**

Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.

#### Aspect ratios

Imagen image generation lets you set five distinct image aspect ratios.

1.  **Square** (1:1, default) - A standard square photo. Common uses for this aspect ratio include social media posts.
2.  **Fullscreen** (4:3) - This aspect ratio is commonly used in media or film. It is also the dimensions of most old (non-widescreen) TVs and medium format cameras. It captures more of the scene horizontally (compared to 1:1), making it a preferred aspect ratio for photography.
    
    ![aspect ratio example](/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_piano.png)
    
    Prompt: close up of a musician's fingers playing the piano, black and white film, vintage (4:3 aspect ratio)
    
    ![aspect ratio example](/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_fries.png)
    
    Prompt: A professional studio photo of french fries for a high end restaurant, in the style of a food magazine (4:3 aspect ratio)
    
3.  **Portrait full screen** (3:4) - This is the fullscreen aspect ratio rotated 90 degrees. This lets to capture more of the scene vertically compared to the 1:1 aspect ratio.
    
    ![aspect ratio example](/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_hiking.png)
    
    Prompt: a woman hiking, close of her boots reflected in a puddle, large mountains in the background, in the style of an advertisement, dramatic angles (3:4 aspect ratio)
    
    ![aspect ratio example](/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_valley.png)
    
    Prompt: aerial shot of a river flowing up a mystical valley (3:4 aspect ratio)
    
4.  **Widescreen** (16:9) - This ratio has replaced 4:3 and is now the most common aspect ratio for TVs, monitors, and mobile phone screens (landscape). Use this aspect ratio when you want to capture more of the background (for example, scenic landscapes).
    
    ![aspect ratio example](/static/gemini-api/docs/images/imagen/aspect-ratios_16-9_man.png)
    
    Prompt: a man wearing all white clothing sitting on the beach, close up, golden hour lighting (16:9 aspect ratio)
    
5.  **Portrait** (9:16) - This ratio is widescreen but rotated. This a relatively new aspect ratio that has been popularized by short form video apps (for example, YouTube shorts). Use this for tall objects with strong vertical orientations such as buildings, trees, waterfalls, or other similar objects.
    
    ![aspect ratio example](/static/gemini-api/docs/images/imagen/aspect-ratios_9-16_skyscraper.png)
    
    Prompt: a digital render of a massive skyscraper, modern, grand, epic with a beautiful sunset in the background (9:16 aspect ratio)
    

#### Photorealistic images

Different versions of the image generation model might offer a mix of artistic and photorealistic output. Use the following wording in prompts to generate more photorealistic output, based on the subject you want to generate.

Use case

Lens type

Focal lengths

Additional details

People (portraits)

Prime, zoom

24-35mm

black and white film, Film noir, Depth of field, duotone (mention two colors)

Food, insects, plants (objects, still life)

Macro

60-105mm

High detail, precise focusing, controlled lighting

Sports, wildlife (motion)

Telephoto zoom

100-400mm

Fast shutter speed, Action or movement tracking

Astronomical, landscape (wide-angle)

Wide-angle

10-24mm

Long exposure times, sharp focus, long exposure, smooth water or clouds

##### Portraits

Use case

Lens type

Focal lengths

Additional details

People (portraits)

Prime, zoom

24-35mm

black and white film, Film noir, Depth of field, duotone (mention two colors)

Using several keywords from the table, Imagen can generate the following portraits:

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_blue-gray1.png)

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_blue-gray2.png)

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_blue-gray3.png)

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_blue-gray4.png)

Prompt: _A woman, 35mm portrait, blue and grey duotones_  
Model: `imagen-3.0-generate-002`

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_film-noir1.png)

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_film-noir2.png)

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_film-noir3.png)

![portrait photography example](/static/gemini-api/docs/images/imagen/portrait_film-noir4.png)

Prompt: _A woman, 35mm portrait, film noir_  
Model: `imagen-3.0-generate-002`

##### Objects

Use case

Lens type

Focal lengths

Additional details

Food, insects, plants (objects, still life)

Macro

60-105mm

High detail, precise focusing, controlled lighting

Using several keywords from the table, Imagen can generate the following object images:

![object photography example](/static/gemini-api/docs/images/imagen/object_leaf1.png)

![object photography example](/static/gemini-api/docs/images/imagen/object_leaf2.png)

![object photography example](/static/gemini-api/docs/images/imagen/object_leaf3.png)

![object photography example](/static/gemini-api/docs/images/imagen/object_leaf4.png)

Prompt: _leaf of a prayer plant, macro lens, 60mm_  
Model: `imagen-3.0-generate-002`

![object photography example](/static/gemini-api/docs/images/imagen/object_pasta1.png)

![object photography example](/static/gemini-api/docs/images/imagen/object_pasta2.png)

![object photography example](/static/gemini-api/docs/images/imagen/object_pasta3.png)

![object photography example](/static/gemini-api/docs/images/imagen/object_pasta4.png)

Prompt: _a plate of pasta, 100mm Macro lens_  
Model: `imagen-3.0-generate-002`

##### Motion

Use case

Lens type

Focal lengths

Additional details

Sports, wildlife (motion)

Telephoto zoom

100-400mm

Fast shutter speed, Action or movement tracking

Using several keywords from the table, Imagen can generate the following motion images:

![motion photography example](/static/gemini-api/docs/images/imagen/motion_football1.png)

![motion photography example](/static/gemini-api/docs/images/imagen/motion_football2.png)

![motion photography example](/static/gemini-api/docs/images/imagen/motion_football3.png)

![motion photography example](/static/gemini-api/docs/images/imagen/motion_football4.png)

Prompt: _a winning touchdown, fast shutter speed, movement tracking_  
Model: `imagen-3.0-generate-002`

![motion photography example](/static/gemini-api/docs/images/imagen/motion_deer1.png)

![motion photography example](/static/gemini-api/docs/images/imagen/motion_deer2.png)

![motion photography example](/static/gemini-api/docs/images/imagen/motion_deer3.png)

![motion photography example](/static/gemini-api/docs/images/imagen/motion_deer4.png)

Prompt: _A deer running in the forest, fast shutter speed, movement tracking_  
Model: `imagen-3.0-generate-002`

##### Wide-angle

Use case

Lens type

Focal lengths

Additional details

Astronomical, landscape (wide-angle)

Wide-angle

10-24mm

Long exposure times, sharp focus, long exposure, smooth water or clouds

Using several keywords from the table, Imagen can generate the following wide-angle images:

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_mountain1.png)

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_mountain2.png)

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_mountain3.png)

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_mountain4.png)

Prompt: _an expansive mountain range, landscape wide angle 10mm_  
Model: `imagen-3.0-generate-002`

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_astro1.png)

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_astro2.png)

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_astro3.png)

![wide-angle photography example](/static/gemini-api/docs/images/imagen/wide-angle_astro4.png)

Prompt: _a photo of the moon, astro photography, wide angle 10mm_  
Model: `imagen-3.0-generate-002`

## What's next

*   Check out the [Veo guide](/gemini-api/docs/video) to learn how to generate videos with the Gemini API.
*   To learn more about Gemini models, see [Gemini models](/gemini-api/docs/models/gemini) and [Experimental models](/gemini-api/docs/models/experimental-models).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/models/gemini-v2

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini models

2.5 Pro spark

Our most powerful thinking model with maximum response accuracy and state-of-the-art performance

*   Input audio, images, video, and text, get text responses
*   Tackle difficult problems, analyze large databases, and more
*   Best for complex coding, reasoning, and multimodal understanding

[](#gemini-2.5-pro)

2.5 Flash spark

Our best model in terms of price-performance, offering well-rounded capabilities.

*   Input audio, images, video, and text, and get text responses
*   Model thinks as needed; or, you can configure a thinking budget
*   Best for low latency, high volume tasks that require thinking

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

*   Input audio, images, video, and text, and get text responses
*   Most cost-efficient model supporting high throughput
*   Best for real time, low latency use cases

[](#gemini-2.5-flash-lite)

## Model variants

The Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:

Model variant

Input(s)

Output

Optimized for

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

Audio, images, videos, text, and PDF

Text

Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

Audio, images, videos, and text

Text

Adaptive thinking, cost efficiency

[Gemini 2.5 Flash-Lite Preview](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

Text, image, video, audio

Text

Most cost-efficient model supporting high throughput

[Gemini 2.5 Flash Native Audio](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog` &  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

Audio, videos, and text

Text and audio, interleaved

High quality, natural conversational audio outputs, with or without thinking

[Gemini 2.5 Flash Preview TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.5 Pro Preview TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

Text

Audio

Low latency, controllable, single- and multi-speaker text-to-speech audio generation

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

Audio, images, videos, and text

Text

Next generation features, speed, and realtime streaming.

[Gemini 2.0 Flash Preview Image Generation](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

Audio, images, videos, and text

Text, images

Conversational image generation and editing

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

Audio, images, videos, and text

Text

Cost efficiency and low latency

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

Audio, images, videos, and text

Text

Fast and versatile performance across a diverse variety of tasks

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

Audio, images, videos, and text

Text

High volume and lower intelligence tasks

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

Audio, images, videos, and text

Text

Complex reasoning tasks requiring more intelligence

[Gemini Embedding](#gemini-embedding)  
`gemini-embedding-exp`

Text

Text embeddings

Measuring the relatedness of text strings

[Imagen 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

Text

Images

Our most up-to-date image generation model

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

Text

Images

High quality image generation model

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

Text, images

Video

High quality video generation

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

[Gemini 2.0 Flash Live](#live-api-2.0)  
`gemini-2.0-flash-live-001`

Audio, video, and text

Text, audio

Low-latency bidirectional voice and video interactions

You can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).

### Gemini 2.5 Pro

Gemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro)

#### Model details

Property

Description

id\_cardModel code

`gemini-2.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, text, and PDF

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash

Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash`

saveSupported data types

**Inputs**

Text, images, video, audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

65,536

handymanCapabilities

**Audio generation**

Not supported

**Caching**

Supported

**Code execution**

Supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Stable: `gemini-2.5-flash`
*   Preview: `gemini-2.5-flash-preview-05-20`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash-Lite Preview

A Gemini 2.5 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-lite-preview-06-17`

saveSupported data types

**Inputs**

Text, images, video, and audio

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,000,000

**Output token limit**

64,000

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**URL Context**

Supported

**Search grounding**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

**Thinking**

Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-lite-preview-06-17`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Native Audio

Our native audio dialog models, with and without thinking, available through the [Live API](/gemini-api/docs/live). These models provide interactive and unstructured conversational experiences, with style and control prompting.

[Try native audio in Google AI Studio](https://aistudio.google.com/app/live)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-native-audio-dialog` &  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

saveSupported data types

**Inputs**

Audio, video, text

**Output**

Audio and text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

128,000

**Output token limit**

8,000

handymanCapabilities

**Audio generation**

Supported

**Caching**

Not supported

**Code execution**

Not supported

**Function calling**

Supported

**Image generation**

Not supported

**Search grounding**

Supported

**Structured outputs**

Not supported

**Thinking**

Supported

**Tuning**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.5-flash-preview-05-20`
*   Experimental: `gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.5 Flash Preview Text-to-Speech

Gemini 2.5 Flash Preview TTS is our price-performant text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Flash rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-flash-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-flash-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.5 Pro Preview Text-to-Speech

Gemini 2.5 Pro Preview TTS is our most powerful text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Pro rate limits are more restricted since it is an experimental / preview model.

[Try in Google AI Studio](https://aistudio.google.com/generate-speech)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.5-pro-preview-tts`

saveSupported data types

**Inputs**

Text

**Output**

Audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,000

**Output token limit**

16,000

handymanCapabilities

**Structured outputs**

Not supported

**Caching**

Not supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not supported

**Search**

Not supported

**Audio generation**

Supported

**Live API**

Not supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   `gemini-2.5-pro-preview-tts`

calendar\_monthLatest update

May 2025

### Gemini 2.0 Flash

Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Supported

**Thinking**

Experimental

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash`
*   Stable: `gemini-2.0-flash-001`
*   Experimental: `gemini-2.0-flash-exp`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash Preview Image Generation

Gemini 2.0 Flash Preview Image Generation delivers improved image generation features, including generating and editing images conversationally.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-preview-image-generation`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text and images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

32,000

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Not supported

**Code execution**

Not Supported

**Search**

Not Supported

**Image generation**

Supported

**Audio generation**

Not supported

**Live API**

Not Supported

**Thinking**

Not Supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-preview-image-generation`

gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa

calendar\_monthLatest update

May 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 2.0 Flash-Lite

A Gemini 2.0 Flash model optimized for cost efficiency and low latency.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-lite`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Not supported

**Search**

Not supported

**Image generation**

Not supported

**Audio generation**

Not supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-2.0-flash-lite`
*   Stable: `gemini-2.0-flash-lite-001`

calendar\_monthLatest update

February 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini 1.5 Flash

Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-latest`
*   Latest stable: `gemini-1.5-flash`
*   Stable:

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_monthLatest update

September 2024

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-flash-8b`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

3,600

**Maximum video length**

1 hour

**Maximum audio length**

Approximately 9.5 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-flash-8b-latest`
*   Latest stable: `gemini-1.5-flash-8b`
*   Stable:

*   `gemini-1.5-flash-8b-001`

calendar\_monthLatest update

October 2024

### Gemini 1.5 Pro

Try [Gemini 2.5 Pro Preview](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date.

Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-1.5-pro`

saveSupported data types

**Inputs**

Audio, images, video, and text

**Output**

Text

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,097,152

**Output token limit**

8,192

movie\_infoAudio/visual specs

**Maximum number of images per prompt**

7,200

**Maximum video length**

2 hours

**Maximum audio length**

Approximately 19 hours

handymanCapabilities

**System instructions**

Supported

**JSON mode**

Supported

**JSON schema**

Supported

**Adjustable safety settings**

Supported

**Caching**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Live API**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Latest: `gemini-1.5-pro-latest`
*   Latest stable: `gemini-1.5-pro`
*   Stable:

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_monthLatest update

September 2024

### Imagen 4

Imagen 4 is our latest image model, capable of generating highly detailed images with rich lighting, significantly better text rendering, and higher resolution output than previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

480 tokens (text)

**Output images**

1 (Ultra)  
1 to 4 (Standard)

calendar\_monthLatest update

June 2025

### Imagen 3

Imagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`imagen-3.0-generate-002`

saveSupported data types

**Input**

Text

**Output**

Images

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

N/A

**Output images**

Up to 4

calendar\_monthLatest update

February 2025

### Veo 2

Veo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`veo-2.0-generate-001`

saveSupported data types

**Input**

Text, image

**Output**

Video

token\_autoLimits

**Text input**

N/A

**Image input**

Any image resolution and aspect ratio up to 20MB file size

**Output video**

Up to 2

calendar\_monthLatest update

April 2025

### Gemini 2.5 Flash Live

The Gemini 2.5 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-live-2.5-flash-preview)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-live-2.5-flash-preview`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-live-2.5-flash-preview`

calendar\_monthLatest update

June 2025

cognition\_2Knowledge cutoff

January 2025

### Gemini 2.0 Flash Live

The Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.

[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)

#### Model details

Property

Description

id\_cardModel code

`models/gemini-2.0-flash-live-001`

saveSupported data types

**Inputs**

Audio, video, and text

**Output**

Text, and audio

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

1,048,576

**Output token limit**

8,192

handymanCapabilities

**Structured outputs**

Supported

**Tuning**

Not supported

**Function calling**

Supported

**Code execution**

Supported

**Search**

Supported

**Image generation**

Not supported

**Audio generation**

Supported

**Thinking**

Not supported

123Versions

Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.

*   Preview: `gemini-2.0-flash-live-001`

calendar\_monthLatest update

April 2025

cognition\_2Knowledge cutoff

August 2024

### Gemini Embedding Experimental

`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. Gemini Embedding rate limits are more restricted since it is an experimental model.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`gemini-embedding-exp-03-07`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

8,192

**Output dimension size**

Elastic, supports: 3072, 1536, or 768

calendar\_monthLatest update

March 2025

### Text Embedding and Embedding

#### Text Embedding

Try our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance.

[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications.

`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks.

##### Model details

Property

Description

id\_cardModel code

**Gemini API**

`models/text-embedding-004`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

April 2024

#### Embedding

You can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.

The Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens.

##### Embedding model details

Property

Description

id\_cardModel code

`models/embedding-001`

saveSupported data types

**Input**

Text

**Output**

Text embeddings

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

2,048

**Output dimension size**

768

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Not supported

calendar\_monthLatest update

December 2023

### AQA

You can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability.

#### Model details

Property

Description

id\_cardModel code

`models/aqa`

saveSupported data types

**Input**

Text

**Output**

Text

languageSupported language

English

token\_autoToken limits[\[\*\]](#token-size)

**Input token limit**

7,168

**Output token limit**

1,024

swap\_driving\_apps\_wheelRate limits[\[\*\*\]](#rate-limits)

1,500 requests per minute

encryptedAdjustable safety settings

Supported

calendar\_monthLatest update

December 2023

See the [examples](/examples) to explore the capabilities of these model variations.

\[\*\] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words.

## Model version name patterns

Gemini models are available in either _stable_, _preview_, or _experimental_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.

### Latest stable

Points to the most recent stable version released for the specified model generation and variation.

To specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-2.0-flash`.

### Stable

Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.

To specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-flash-001`.

### Preview

Points to a preview model which may not be suitable for production use, come with more restrictive rate limits, but may have billing enabled.

To specify a preview version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.5-pro-preview-06-05`.

### Experimental

Points to an experimental model which may not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.

To specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.

## Experimental models

In addition to stable models, the Gemini API offers experimental models which may not be suitable for production use and come with more restrictive rate limits.

We release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.

### Previous experimental models

As new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:

Model code

Base model

Replacement version

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## Supported languages

Gemini models are trained to work with the following languages:

*   Arabic (`ar`)
*   Bengali (`bn`)
*   Bulgarian (`bg`)
*   Chinese simplified and traditional (`zh`)
*   Croatian (`hr`)
*   Czech (`cs`)
*   Danish (`da`)
*   Dutch (`nl`)
*   English (`en`)
*   Estonian (`et`)
*   Finnish (`fi`)
*   French (`fr`)
*   German (`de`)
*   Greek (`el`)
*   Hebrew (`iw`)
*   Hindi (`hi`)
*   Hungarian (`hu`)
*   Indonesian (`id`)
*   Italian (`it`)
*   Japanese (`ja`)
*   Korean (`ko`)
*   Latvian (`lv`)
*   Lithuanian (`lt`)
*   Norwegian (`no`)
*   Polish (`pl`)
*   Portuguese (`pt`)
*   Romanian (`ro`)
*   Russian (`ru`)
*   Serbian (`sr`)
*   Slovak (`sk`)
*   Slovenian (`sl`)
*   Spanish (`es`)
*   Swahili (`sw`)
*   Swedish (`sv`)
*   Thai (`th`)
*   Turkish (`tr`)
*   Ukrainian (`uk`)
*   Vietnamese (`vi`)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/sdks

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini API libraries

This page provides information on downloading and installing the latest libraries for the Gemini API. If you're new to the Gemini API, get started with the [API quickstart](/gemini-api/docs/quickstart).

## Important note about our new libraries

Since late 2024, we have been progressively rolling out the **Google Gen AI SDK**, a new set of libraries that provide a unified interface for accessing Google's generative AI models through both the Gemini Developer API and Vertex AI.

The Google Gen AI SDK is now the recommended pathway for accessing all of Google's models and all code snippets in our documentation utilize these libraries.

**Key Library Updates**

  

Language

New library (Recommended)

Old library

**Python**

`google-genai`

`google-generativeai`

**JavaScript  
and TypeScript**

`@google/genai`

`@google/generative-ai`

**Go**

`google.golang.org/genai`

`google.golang.org/generative-ai`

**Java**

`google-genai`

N/A, did not exist before

We encourage all users to [migrate to the Google GenAI SDK](/gemini-api/docs/migrate), as it provides access to new features like multi-modal outputs (e.g., image and video generation), the Live API, and long term maintenance. We expect the feature gap between the old and new SDK to grow over time.

## Install a library

### Python

You can install our [Python library](https://pypi.org/project/google-genai) by running:

```
pip install google-genai
```

### JavaScript

You can install our [JavaScript and TypeScript library](https://www.npmjs.com/package/@google/genai) by running:

```
npm install @google/genai
```

### Go

You can install our [Go library](https://pkg.go.dev/google.golang.org/genai) by running:

```
go get google.golang.org/genai
```

### Java

If you're using Maven, you can install our [Java library](https://github.com/googleapis/java-genai) by adding the following to your dependencies:

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

## Previous libraries and SDKs

For platforms where a new library is available, we strongly recommend [migrating to the new libraries](/gemini-api/docs/migrate). Previous libraries won't receive new features and their specific support status varies, as detailed below.

For platforms where an official Google Gen AI library is not yet available, you can call our APIs directly, use community-maintained libraries, or use trusted third-party options. We are actively working to expand our official SDK coverage and will list new libraries here as they are released.

The following are the previous libraries:

*   [Python (previous)](https://github.com/google-gemini/deprecated-generative-ai-python): All support, including bug fixes, ends September 2025.
*   [Node.js (previous)](https://github.com/google-gemini/generative-ai-js): All support, including bug fixes, ends September 2025.
*   [Go (previous)](https://github.com/google/generative-ai-go): All support, including bug fixes, ends September 2025.
*   [Dart and Flutter (Previous)](https://pub.dev/packages/google_generative_ai/install): Not actively maintained
*   [Swift (Previous)](https://github.com/google/generative-ai-swift): Not actively maintained
*   [Android (Previous)](https://github.com/google-gemini/generative-ai-android): Not actively maintained

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-11 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/thinking-mode

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini thinking

The [Gemini 2.5 series models](/gemini-api/docs/models) use an internal "thinking process" that significantly improves their reasoning and multi-step planning abilities, making them highly effective for complex tasks such as coding, advanced mathematics, and data analysis.

This guide shows you how to work with Gemini's thinking capabilities using the Gemini API.

## Before you begin

Ensure you use a supported 2.5 series model for thinking. You might find it beneficial to explore these models in AI Studio before diving into the API:

*   [Try Gemini 2.5 Flash in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash)
*   [Try Gemini 2.5 Pro in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro)
*   [Try Gemini 2.5 Flash-Lite Preview in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-lite-preview-06-17)

## Generating content with thinking

Initiating a request with a thinking model is similar to any other content generation request. The key difference lies in specifying one of the [models with thinking support](#supported-models) in the `model` field, as demonstrated in the following [text generation](/gemini-api/docs/text-generation#text-input) example:

### Python

```
from google import genai

client = genai.Client()
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents=prompt
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: prompt,
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example."
  model := "gemini-2.5-pro"

  resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil)

  fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
   "contents": [
     {
       "parts": [
         {
           "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example."
         }
       ]
     }
   ]
 }'
 ```
```

## Thinking budgets

The `thinkingBudget` parameter guides the model on the number of thinking tokens to use when generating a response. A higher token count generally allows for more detailed reasoning, which can be beneficial for tackling more [complex tasks](#tasks). If latency is more important, use a lower budget or disable thinking by setting `thinkingBudget` to 0. Setting the `thinkingBudget` to -1 turns on **dynamic thinking**, meaning the model will adjust the budget based on the complexity of the request.

The `thinkingBudget` is only [supported](#supported-models) in Gemini 2.5 Flash, 2.5 Pro, and 2.5 Flash-Lite. Depending on the prompt, the model might overflow or underflow the token budget.

The following are `thinkingBudget` configuration details for each model type.

    

Model

Default setting  
(Thinking budget is not set)

Range

Disable thinking

Turn on dynamic thinking

**2.5 Pro**

Dynamic thinking: Model decides when and how much to think

`128` to `32768`

N/A: Cannot disable thinking

`thinkingBudget = -1`

**2.5 Flash**

Dynamic thinking: Model decides when and how much to think

`0` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

**2.5 Flash Lite**

Model does not think

`512` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
        # Turn off thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=0)
        # Turn on dynamic thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=-1)
    ),
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
        // Turn off thinking:
        // thinkingBudget: 0
        // Turn on dynamic thinking:
        // thinkingBudget: -1
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingBudgetVal := int32(1024)

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingBudget: &thinkingBudgetVal,
      // Turn off thinking:
      // ThinkingBudget: int32(0),
      // Turn on dynamic thinking:
      // ThinkingBudget: int32(-1),
    },
  })

fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingBudget": 1024
          # Thinking off:
          # "thinkingBudget": 0
          # Turn on dynamic thinking:
          # "thinkingBudget": -1
    }
  }
}'
```

## Thought summaries

Thought summaries are synthisized versions of the model's raw thoughts and offer insights into the model's internal reasoning process. Note that thinking budgets apply to the model's raw thoughts and not to thought summaries.

You can enable thought summaries by setting `includeThoughts` to `true` in your request configuration. You can then access the summary by iterating through the `response` parameter's `parts`, and checking the `thought` boolean.

Here's an example demonstrating how to enable and retrieve thought summaries without streaming, which returns a single, final thought summary with the response:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro",
  contents=prompt,
  config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought:
    print("Thought summary:")
    print(part.text)
    print()
  else:
    print("Answer:")
    print(part.text)
    print()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "What is the sum of the first 50 prime numbers?",
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (!part.text) {
      continue;
    }
    else if (part.thought) {
      console.log("Thoughts summary:");
      console.log(part.text);
    }
    else {
      console.log("Answer:");
      console.log(part.text);
    }
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text("What is the sum of the first 50 prime numbers?")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for _, part := range resp.Candidates[0].Content.Parts {
    if part.Text != "" {
      if part.Thought {
        fmt.Println("Thoughts Summary:")
        fmt.Println(part.Text)
      } else {
        fmt.Println("Answer:")
        fmt.Println(part.Text)
      }
    }
  }
}
```

And here is an example using thinking with streaming, which returns rolling, incremental summaries during generation:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = """
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
"""

thoughts = ""
answer = ""

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-pro",
    contents=prompt,
    config=types.GenerateContentConfig(
      thinking_config=types.ThinkingConfig(
        include_thoughts=True
      )
    )
):
  for part in chunk.candidates[0].content.parts:
    if not part.text:
      continue
    elif part.thought:
      if not thoughts:
        print("Thoughts summary:")
      print(part.text)
      thoughts += part.text
    else:
      if not answer:
        print("Thoughts summary:")
      print(part.text)
      answer += part.text
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `Alice, Bob, and Carol each live in a different house on the same
street: red, green, and blue. The person who lives in the red house owns a cat.
Bob does not live in the green house. Carol owns a dog. The green house is to
the left of the red house. Alice does not own a cat. Who lives in each house,
and what pet do they own?`;

let thoughts = "";
let answer = "";

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-pro",
    contents: prompt,
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for await (const chunk of response) {
    for (const part of chunk.candidates[0].content.parts) {
      if (!part.text) {
        continue;
      } else if (part.thought) {
        if (!thoughts) {
          console.log("Thoughts summary:");
        }
        console.log(part.text);
        thoughts = thoughts + part.text;
      } else {
        if (!answer) {
          console.log("Answer:");
        }
        console.log(part.text);
        answer = answer + part.text;
      }
    }
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

## Pricing

When thinking is turned on, response pricing is the sum of output tokens and thinking tokens. You can get the total number of generated thinking tokens from the `thoughtsTokenCount` field.

### Python

```
# ...
print("Thoughts tokens:",response.usage_metadata.thoughts_token_count)
print("Output tokens:",response.usage_metadata.candidates_token_count)
```

### JavaScript

```
// ...
console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`);
console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`);
```

### Go

```
// ...
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
  log.Fatal(err)
}
fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count))
fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count))
```

Thinking models generate full thoughts to improve the quality of the final response, and then output [summaries](#summaries) to provide insight into the thought process. So, pricing is based on the full thought tokens the model needs to generate to create a summary, despite only the summary being output from the API.

You can learn more about tokens in the [Token counting](/gemini-api/docs/tokens) guide.

## Supported models

Thinking features are supported on all the 2.5 series models. You can find all model capabilities on the [model overview](/gemini-api/docs/models) page.

## Best practices

This section includes some guidance for using thinking models efficiently. As always, following our [prompting guidance and best practices](/gemini-api/docs/prompting-strategies) will get you the best results.

### Debugging and steering

*   **Review reasoning**: When you're not getting your expected response from the thinking models, it can help to carefully analyze Gemini's thought summaries. You can see how it broke down the task and arrived at its conclusion, and use that information to correct towards the right results.
    
*   **Provide Guidance in Reasoning**: If you're hoping for a particularly lengthy output, you may want to provide guidance in your prompt to constrain the [amount of thinking](#set-budget) the model uses. This lets you reserve more of the token output for your response.
    

### Task complexity

*   **Easy Tasks (Thinking could be OFF):** For straightforward requests where complex reasoning isn't required, such as fact retrieval or classification, thinking is not required. Examples include:
    *   "Where was DeepMind founded?"
    *   "Is this email asking for a meeting or just providing information?"
*   **Medium Tasks (Default/Some Thinking):** Many common requests benefit from a degree of step-by-step processing or deeper understanding. Gemini can flexibly use thinking capability for tasks like:
    *   Analogize photosynthesis and growing up.
    *   Compare and contrast electric cars and hybrid cars.
*   **Hard Tasks (Maximum Thinking Capability):** For truly complex challenges, such as solving complex math problems or coding tasks, we recommend setting a high thinking budget. These types of tasks require the model needs to engage its full reasoning and planning capabilities, often involving many internal steps before providing an answer. Examples include:
    *   Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for which 17b is a divisor of 97b.
    *   Write Python code for a web application that visualizes real-time stock market data, including user authentication. Make it as efficient as possible.

## Thinking with tools and capabilities

Thinking models work with all of Gemini's tools and capabilities. This allows the models to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.

*   The [search tool](/gemini-api/docs/grounding) allows the model to query Google Search to find up-to-date information or information beyond its training data. This is useful for questions about recent events or highly specific topics.
    
*   The [code execution tool](/gemini-api/docs/code-execution) enables the model to generate and run Python code to perform calculations, manipulate data, or solve problems that are best handled algorithmically. The model receives the code's output and can use it in its response.
    
*   With [structured output](/gemini-api/docs/structured-output), you can constrain Gemini to respond with JSON. This is particularly useful for integrating the model's output into applications.
    
*   [Function calling](/gemini-api/docs/function-calling) connects the thinking model to external tools and APIs, so it can reason about when to call the right function and what parameters to provide.
    
*   [URL Context](/gemini-api/docs/url-context) provides the model with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.
    

You can try examples of using tools with thinking models in the [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).

## What's next?

*   To work through more in depth examples, like:
    
    *   Using tools with thinking
    *   Streaming with thinking
    *   Adjusting the thinking budget for different results
    
    and more, try our [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).
    
*   Thinking coverage is now available in our [OpenAI Compatibility](/gemini-api/docs/openai#thinking) guide.
    
*   For more info about Gemini 2.5 Pro, Gemini Flash 2.5, and Gemini 2.5 Flash-Lite, visit the [model page](/gemini-api/docs/models).
    

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/api-versions

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)
*   [API Reference](https://ai.google.dev/api)

Send feedback

# API versions explained

This document provides a high-level overview of the differences between the `v1` and `v1beta` versions of the Gemini API.

*   **v1**: Stable version of the API. Features in the stable version are fully-supported over the lifetime of the major version. If there are any breaking changes, then the next major version of the API will be created and the existing version will be deprecated after a reasonable period of time. Non-breaking changes may be introduced to the API without changing the major version.
*   **v1beta**: This version includes early-access features that may be under development and is subject to rapid and breaking changes. There is also no guarantee that the features in the Beta version will move to the stable version. Due to this instability, you should consider not launching production applications with this version.

Feature

v1

v1beta

Generate Content - Text-only input

Generate Content - Text-and-image input

Generate Content - Text output

Generate Content - Multi-turn conversations (chat)

Generate Content - Function calls

Generate Content - Streaming

Embed Content - Text-only input

Generate Answer

Semantic retriever

*   \- Supported
*   \- Will never be supported

## Configure API version in an SDK

The Gemini API SDK's default to `v1beta`, but you can opt to use other versions by setting the API version as shown in the following code sample:

### Python

```
from google import genai

client = genai.Client(http_options={'api_version': 'v1alpha'})

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents="Explain how AI works",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({
  httpOptions: { apiVersion: "v1alpha" },
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works",
  });
  console.log(response.text);
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [{
    "parts":[{"text": "Explain how AI works."}]
    }]
   }'
```

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/json-mode

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Structured output

You can configure Gemini for structured output instead of unstructured text, allowing precise extraction and standardization of information for further processing. For example, you can use structured output to extract information from resumes, standardize them to build a structured database.

Gemini can generate either [JSON](/gemini-api/docs/structured-output#generating-json) or [enum values](/gemini-api/docs/structured-output#generating-enums) as structured output.

## Generating JSON

There are two ways to generate JSON using the Gemini API:

*   Configure a schema on the model
*   Provide a schema in a text prompt

Configuring a schema on the model is the **recommended** way to generate JSON, because it constrains the model to output JSON.

### Configuring a schema (recommended)

To constrain the model to generate JSON, configure a `responseSchema`. The model will then respond to any prompt with JSON-formatted output.

### Python

```
from google import genai
from pydantic import BaseModel

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="List a few popular cookie recipes, and include the amounts of ingredients.",
    config={
        "response_mime_type": "application/json",
        "response_schema": list[Recipe],
    },
)
# Use the response as a JSON string.
print(response.text)

# Use instantiated objects.
my_recipes: list[Recipe] = response.parsed
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents:
      "List a few popular cookie recipes, and include the amounts of ingredients.",
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.ARRAY,
        items: {
          type: Type.OBJECT,
          properties: {
            recipeName: {
              type: Type.STRING,
            },
            ingredients: {
              type: Type.ARRAY,
              items: {
                type: Type.STRING,
              },
            },
          },
          propertyOrdering: ["recipeName", "ingredients"],
        },
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        ResponseMIMEType: "application/json",
        ResponseSchema: &genai.Schema{
            Type: genai.TypeArray,
            Items: &genai.Schema{
                Type: genai.TypeObject,
                Properties: map[string]*genai.Schema{
                    "recipeName": {Type: genai.TypeString},
                    "ingredients": {
                        Type:  genai.TypeArray,
                        Items: &genai.Schema{Type: genai.TypeString},
                    },
                },
                PropertyOrdering: []string{"recipeName", "ingredients"},
            },
        },
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("List a few popular cookie recipes, and include the amounts of ingredients."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
      "contents": [{
        "parts":[
          { "text": "List a few popular cookie recipes, and include the amounts of ingredients." }
        ]
      }],
      "generationConfig": {
        "responseMimeType": "application/json",
        "responseSchema": {
          "type": "ARRAY",
          "items": {
            "type": "OBJECT",
            "properties": {
              "recipeName": { "type": "STRING" },
              "ingredients": {
                "type": "ARRAY",
                "items": { "type": "STRING" }
              }
            },
            "propertyOrdering": ["recipeName", "ingredients"]
          }
        }
      }
}' 2> /dev/null | head
```

The output might look like this:

```
[
  {
    "recipeName": "Chocolate Chip Cookies",
    "ingredients": [
      "1 cup (2 sticks) unsalted butter, softened",
      "3/4 cup granulated sugar",
      "3/4 cup packed brown sugar",
      "1 teaspoon vanilla extract",
      "2 large eggs",
      "2 1/4 cups all-purpose flour",
      "1 teaspoon baking soda",
      "1 teaspoon salt",
      "2 cups chocolate chips"
    ]
  },
  ...
]
```

### Providing a schema in a text prompt

Instead of configuring a schema, you can supply a schema as natural language or pseudo-code in a text prompt. This method is **not recommended**, because it might produce lower quality output, and because the model is not constrained to follow the schema.

Here's a generic example of a schema provided in a text prompt:

```
List a few popular cookie recipes, and include the amounts of ingredients.

Produce JSON matching this specification:

Recipe = { "recipeName": string, "ingredients": array<string> }
Return: array<Recipe>
```

Since the model gets the schema from text in the prompt, you might have some flexibility in how you represent the schema. But when you supply a schema inline like this, the model is not actually constrained to return JSON. For a more deterministic, higher quality response, configure a schema on the model, and don't duplicate the schema in the text prompt.

## Generating enum values

In some cases you might want the model to choose a single option from a list of options. To implement this behavior, you can pass an _enum_ in your schema. You can use an enum option anywhere you could use a `string` in the `responseSchema`, because an enum is an array of strings. Like a JSON schema, an enum lets you constrain model output to meet the requirements of your application.

For example, assume that you're developing an application to classify musical instruments into one of five categories: `"Percussion"`, `"String"`, `"Woodwind"`, `"Brass"`, or "`"Keyboard"`". You could create an enum to help with this task.

In the following example, you pass an enum as the `responseSchema`, constraining the model to choose the most appropriate option.

### Python

```
from google import genai
import enum

class Instrument(enum.Enum):
  PERCUSSION = "Percussion"
  STRING = "String"
  WOODWIND = "Woodwind"
  BRASS = "Brass"
  KEYBOARD = "Keyboard"

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': Instrument,
    },
)

print(response.text)
# Woodwind
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "What type of instrument is an oboe?",
    config: {
      responseMimeType: "text/x.enum",
      responseSchema: {
        type: Type.STRING,
        enum: ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
      },
    },
  });

console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
          "contents": [{
            "parts":[
              { "text": "What type of instrument is an oboe?" }
            ]
          }],
          "generationConfig": {
            "responseMimeType": "text/x.enum",
            "responseSchema": {
              "type": "STRING",
              "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
            }
          }
    }'
```

The Python library will translate the type declarations for the API. However, the API accepts a subset of the OpenAPI 3.0 schema ([Schema](https://ai.google.dev/api/caching#schema)).

There are two other ways to specify an enumeration. You can use a [`Literal`](https://docs.pydantic.dev/1.10/usage/types/#literal-type): \`\`\`

### Python

```
Literal["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
```

And you can also pass the schema as JSON:

### Python

```
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': {
            "type": "STRING",
            "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
        },
    },
)

print(response.text)
# Woodwind
```

Beyond basic multiple choice problems, you can use an enum anywhere in a JSON schema. For example, you could ask the model for a list of recipe titles and use a `Grade` enum to give each title a popularity grade:

### Python

```
from google import genai

import enum
from pydantic import BaseModel

class Grade(enum.Enum):
    A_PLUS = "a+"
    A = "a"
    B = "b"
    C = "c"
    D = "d"
    F = "f"

class Recipe(BaseModel):
  recipe_name: str
  rating: Grade

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='List 10 home-baked cookie recipes and give them grades based on tastiness.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': list[Recipe],
    },
)

print(response.text)
```

The response might look like this:

```
[
  {
    "recipe_name": "Chocolate Chip Cookies",
    "rating": "a+"
  },
  {
    "recipe_name": "Peanut Butter Cookies",
    "rating": "a"
  },
  {
    "recipe_name": "Oatmeal Raisin Cookies",
    "rating": "b"
  },
  ...
]
```

## About JSON schemas

Configuring the model for JSON output using `responseSchema` parameter relies on `Schema` object to define its structure. This object represents a select subset of the [OpenAPI 3.0 Schema object](https://spec.openapis.org/oas/v3.0.3#schema-object), and also adds a `propertyOrdering` field.

Here's a pseudo-JSON representation of all the `Schema` fields:

```
{
  "type": enum (Type),
  "format": string,
  "description": string,
  "nullable": boolean,
  "enum": [
    string
  ],
  "maxItems": integer,
  "minItems": integer,
  "properties": {
    string: {
      object (Schema)
    },
    ...
  },
  "required": [
    string
  ],
  "propertyOrdering": [
    string
  ],
  "items": {
    object (Schema)
  }
}
```

The `Type` of the schema must be one of the OpenAPI [Data Types](https://spec.openapis.org/oas/v3.0.3#data-types), or a union of those types (using `anyOf`). Only a subset of fields is valid for each `Type`. The following list maps each `Type` to a subset of the fields that are valid for that type:

*   `string` -> `enum`, `format`, `nullable`
*   `integer` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `number` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `boolean` -> `nullable`
*   `array` -> `minItems`, `maxItems`, `items`, `nullable`
*   `object` -> `properties`, `required`, `propertyOrdering`, `nullable`

Here are some example schemas showing valid type-and-field combinations:

```
{ "type": "string", "enum": ["a", "b", "c"] }

{ "type": "string", "format": "date-time" }

{ "type": "integer", "format": "int64" }

{ "type": "number", "format": "double" }

{ "type": "boolean" }

{ "type": "array", "minItems": 3, "maxItems": 3, "items": { "type": ... } }

{ "type": "object",
  "properties": {
    "a": { "type": ... },
    "b": { "type": ... },
    "c": { "type": ... }
  },
  "nullable": true,
  "required": ["c"],
  "propertyOrdering": ["c", "b", "a"]
}
```

For complete documentation of the Schema fields as they're used in the Gemini API, see the [Schema reference](/api/caching#Schema).

### Property ordering

When you're working with JSON schemas in the Gemini API, the order of properties is important. By default, the API orders properties alphabetically and does not preserve the order in which the properties are defined (although the [Google Gen AI SDKs](/gemini-api/docs/sdks) may preserve this order). If you're providing examples to the model with a schema configured, and the property ordering of the examples is not consistent with the property ordering of the schema, the output could be rambling or unexpected.

To ensure a consistent, predictable ordering of properties, you can use the optional `propertyOrdering[]` field.

```
"propertyOrdering": ["recipeName", "ingredients"]
```

`propertyOrdering[]` – not a standard field in the OpenAPI specification – is an array of strings used to determine the order of properties in the response. By specifying the order of properties and then providing examples with properties in that same order, you can potentially improve the quality of results. `propertyOrdering` is only supported when you manually create `types.Schema`.

### Schemas in Python

When you're using the Python library, the value of `response_schema` must be one of the following:

*   A type, as you would use in a type annotation (see the Python [`typing` module](https://docs.python.org/3/library/typing.html))
*   An instance of [`genai.types.Schema`](https://googleapis.github.io/python-genai/genai.html#genai.types.Schema)
*   The `dict` equivalent of `genai.types.Schema`

The easiest way to define a schema is with a Pydantic type (as shown in the previous example):

### Python

```
config={'response_mime_type': 'application/json',
        'response_schema': list[Recipe]}
```

When you use a Pydantic type, the Python library builds out a JSON schema for you and sends it to the API. For additional examples, see the [Python library docs](https://googleapis.github.io/python-genai/index.html#json-response-schema).

The Python library supports schemas defined with the following types (where `AllowedType` is any allowed type):

*   `int`
*   `float`
*   `bool`
*   `str`
*   `list[AllowedType]`
*   `AllowedType|AllowedType|...`
*   For structured types:
    *   `dict[str, AllowedType]`. This annotation declares all dict values to be the same type, but doesn't specify what keys should be included.
    *   User-defined [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/). This approach lets you specify the key names and define different types for the values associated with each of the keys, including nested structures.

### JSON Schema support

[JSON Schema](https://json-schema.org/) is a more recent specification than OpenAPI 3.0, which the [Schema](/api/caching#Schema) object is based on. Support for JSON Schema is available as a preview using the field [`responseJsonSchema`](/api/generate-content#FIELDS.response_json_schema) which accepts any JSON Schema with the following limitations:

*   It only works with Gemini 2.5.
*   While all JSON Schema properties can be passed, not all are supported. See the [documentation](/api/generate-content#FIELDS.response_json_schema) for the field for more details.
*   Recursive references can only be used as the value of a non-required object property.
*   Recursive references are unrolled to a finite degree, based on the size of the schema.
*   Schemas that contain `$ref` cannot contain any properties other than those starting with a `$`.

Here's an example of generating a JSON Schema with Pydantic and submitting it to the model:

```
curl "https://generativelanguage.googleapis.com/v1alpha/models/\
gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY"\
    -H 'Content-Type: application/json' \
    -d @- <<EOF
{
  "contents": [{
    "parts":[{
      "text": "Please give a random example following this schema"
    }]
  }],
  "generationConfig": {
    "response_mime_type": "application/json",
    "response_json_schema": $(python3 - << PYEOF
from enum import Enum
from typing import List, Optional, Union, Set
from pydantic import BaseModel, Field, ConfigDict
import json

class UserRole(str, Enum):
    ADMIN = "admin"
    VIEWER = "viewer"

class Address(BaseModel):
    street: str
    city: str

class UserProfile(BaseModel):
    username: str = Field(description="User's unique name")
    age: Optional[int] = Field(ge=0, le=120)
    roles: Set[UserRole] = Field(min_items=1)
    contact: Union[Address, str]
    model_config = ConfigDict(title="User Schema")

# Generate and print the JSON Schema
print(json.dumps(UserProfile.model_json_schema(), indent=2))
PYEOF
)
  }
}
EOF
```

Passing JSON Schema directly is not yet supported when using the SDK.

## Best practices

Keep the following considerations and best practices in mind when you're using a response schema:

*   The size of your response schema counts towards the input token limit.
*   By default, fields are optional, meaning the model can populate the fields or skip them. You can set fields as required to force the model to provide a value. If there's insufficient context in the associated input prompt, the model generates responses mainly based on the data it was trained on.
*   A complex schema can result in an `InvalidArgument: 400` error. Complexity might come from long property names, long array length limits, enums with many values, objects with lots of optional properties, or a combination of these factors.
    
    If you get this error with a valid schema, make one or more of the following changes to resolve the error:
    
    *   Shorten property names or enum names.
    *   Flatten nested arrays.
    *   Reduce the number of properties with constraints, such as numbers with minimum and maximum limits.
    *   Reduce the number of properties with complex constraints, such as properties with complex formats like `date-time`.
    *   Reduce the number of optional properties.
    *   Reduce the number of valid values for enums.
*   If you aren't seeing the results you expect, add more context to your input prompts or revise your response schema. For example, review the model's response without structured output to see how the model responds. You can then update your response schema so that it better fits the model's output.
    

## What's next

Now that you've learned how to generate structured output, you might want to try using Gemini API tools:

*   [Function calling](/gemini-api/docs/function-calling)
*   [Code execution](/gemini-api/docs/code-execution)
*   [Grounding with Google Search](/gemini-api/docs/grounding)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/api-overview

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)
*   [מקורות ל-API](https://ai.google.dev/api?hl=he)

שליחת משוב

# Gemini API reference

Gemini API מאפשר לכם לגשת למודלים הגנרטיביים העדכניים ביותר של Google. במסמך העזרה הזה מפורט מידע על הכיתות והשיטות שזמינות ב-SDK של Gemini API. בוחרים שפה ופועלים לפי שלבי ההגדרה כדי להתחיל בתהליך היצירה.

Python JavaScript Go Apps Script

## התקנת ספריית Gemini API

## שליחת הבקשה הראשונה

משתמשים ב-method‏ [`generateContent`](https://ai.google.dev/api/generate-content?hl=he#method:-models.generatecontent) כדי לשלוח בקשה ל-Gemini API.

## המאמרים הבאים

אם אתם רק מתחילים, כדאי לעיין במדריכים הבאים שיעזרו לכם להבין את מודל התכנות של Gemini API:

*   [מדריך למתחילים ל-Gemini API](https://ai.google.dev/gemini-api/docs/quickstart?hl=he)
*   [מדריך למודלים של Gemini](https://ai.google.dev/gemini-api/docs/models/gemini?hl=he)
*   [עיצוב הנחיות](https://ai.google.dev/gemini-api/docs/prompting-intro?hl=he)

כדאי גם לעיין במדריכים בנושא יכולות, שבהם מוסבר על תכונות שונות של Gemini API ומפורטות דוגמאות לקוד:

*   [יצירת טקסט](https://ai.google.dev/gemini-api/docs/text-generation?hl=he)
*   [אחסון ב-cache של הקשר](https://ai.google.dev/gemini-api/docs/caching?hl=he)
*   [הטמעות](https://ai.google.dev/gemini-api/docs/embeddings?hl=he)
*   [כוונון](https://ai.google.dev/gemini-api/docs/model-tuning?hl=he)

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-05-12 (שעון UTC).