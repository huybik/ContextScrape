---

# Content from: https://ai.google.dev/gemini-api/docs/gemini-for-research

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)

# Accelerate discovery with Gemini for Research

[Get a Gemini API Key](https://aistudio.google.com/apikey)

Gemini models can be used to advance foundational research across disciplines. Here are ways that you can explore Gemini for your research:

*   **Fine-tuning**: You can fine-tune Gemini models for a variety of modalities to advance your research. [Learn more](/gemini-api/docs/model-tuning/tutorial).
*   **Analyze and control model outputs**: For further analysis, you can examine a response candidate generated by the model using tools like `Logprobs` and `CitationMetadata`. You can also configure options for model generation and outputs, such as `responseSchema`, `topP`, and `topK`. [Learn more](/api/generate-content).
*   **Multimodal inputs**: Gemini can process images, audio, and videos, enabling a multitude of exciting research directions. [Learn more](/gemini-api/docs/vision).
*   **Long-context capabilities**: Gemini 1.5 Flash comes with a 1-million-token context window, and Gemini 1.5 Pro comes with a 2-million-token context window. [Learn more](/gemini-api/docs/long-context).
*   **Grow with Google**: Quickly access Gemini models through the API and Google AI Studio for production use cases. If you're looking for a Google Cloud-based platform, Vertex AI can provide additional supporting infrastructure.

To support academic research and drive cutting-edge research, Google provides access to Gemini API credits for scientists and academic researchers through the [Gemini Academic Program](/gemini-api/docs/gemini-for-research#gemini-academic-program).

## Get started with Gemini

The Gemini API and Google AI Studio help you start working with Google's latest models and turn your ideas into applications that scale.

### Python

```
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="How large is the universe?",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "How large is the universe?",
  });
  console.log(response.text);
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [{
    "parts":[{"text": "How large is the universe?"}]
    }]
   }'
```

## Featured academics

![](/static/site-assets/images/diyi-yang.png)

"Our research investigates Gemini as a visual language model (VLM) and its agentic behaviors in diverse environments from robustness and safety perspectives. So far, we have evaluated Gemini's robustness against distractions such as pop-up windows when VLM agents perform computer tasks, and have leveraged Gemini to analyze social interaction, temporal events as well as risk factors based on video input."

[](https://cs.stanford.edu/~diyiy/)

![](/static/site-assets/images/lerrel-pinto.png)

"Gemini Pro and Flash, with their long context window, have been helping us in OK-Robot, our open-vocabulary mobile manipulation project. Gemini enables complex natural language queries and commands over the robot's "memory": in this case, previous observations made by the robot over a long operation duration. Mahi Shafiullah and I are also using Gemini to decompose tasks into code that the robot can execute in the real world."

[](https://www.lerrelpinto.com/)

## Gemini Academic Program

Qualified academic researchers (such as faculty, staff, and PhD students) in [supported countries](/gemini-api/docs/available-regions) can apply to receive Gemini API credits and higher rate limits for research projects. This support enables higher throughput for scientific experiments and advances research.

We are particularly interested in the research areas in the following section, but we welcome applications from diverse scientific disciplines:

*   **Evaluations and benchmarks**: Community-endorsed evaluation methods that can provide a strong performance signal in areas such as factuality, safety, instruction following, reasoning, and planning.
    
*   **Accelerating scientific discovery to benefit humanity**: Potential applications of AI in interdisciplinary scientific research, including areas such as rare and neglected diseases, experimental biology, materials science, and sustainability.
    
*   **Embodiment and interactions**: Utilizing large language models to investigate novel interactions within the fields of embodied AI, ambient interactions, robotics, and human-computer interaction.
    
*   **Emergent capabilities**: Exploring new agentic capabilities required to enhance reasoning and planning, and how capabilities can be expanded during inference (e.g., by utilizing Gemini Flash).
    
*   **Multimodal interaction and understanding**: Identifying gaps and opportunities for multimodal foundational models for analysis, reasoning, and planning across a variety of tasks.
    

Eligibility: Only individuals (faculty members, researchers or equivalent) affiliated with a valid academic institution, or academic research organization can apply. Note that API access and credits will be granted and removed at Google's discretion. We review applications on a monthly basis.

### Start researching with the Gemini API

[Apply now](https://forms.gle/HMviQstU8PxC5iCt5)

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

# Gemini Developer API

[Get a Gemini API Key](https://aistudio.google.com/apikey)

Get a Gemini API key and make your first API request in minutes.

### Python

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil))
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Java

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## Meet the models

[Use Gemini in Google AI Studio](https://aistudio.google.com)

2.5 Pro spark

Our most powerful thinking model with features for complex reasoning and much more

[](/gemini-api/docs/models#gemini-2.5-pro)

2.5 Flash spark

Our newest multimodal model, with next generation features and improved capabilities

[](/gemini-api/docs/models/gemini#gemini-2.5-flash)

2.5 Flash-Lite spark

Our fastest and most cost-efficient multimodal model with great performance for high-frequency tasks

[](/gemini-api/docs/models/gemini#gemini-2.5-flash-lite)

## Explore the API

![](/static/site-assets/images/image-generation-index.png)

### Native Image Generation

Generate and edit highly contextual images natively with Gemini 2.0 Flash.

[](/gemini-api/docs/image-generation)

![](/static/site-assets/images/long-context-overview.png)

### Explore long context

Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.

[](/gemini-api/docs/long-context)

![](/static/site-assets/images/structured-outputs-index.png)

### Generate structured outputs

Constrain Gemini to respond with JSON, a structured data format suitable for automated processing.

[](/gemini-api/docs/structured-output)

### Start building with the Gemini API

[Get started](/gemini-api/docs/quickstart)

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/video

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Generate video using Veo

The Gemini API provides access to [Veo 2](https://deepmind.google/technologies/veo/), Google's most capable video generation model to date. Veo generates videos in a wide range of cinematic and visual styles, capturing prompt nuance to render intricate details consistently across frames. This guide will help you get started with Veo using the Gemini API.

For video prompting guidance, check out the [Veo prompt guide](/gemini-api/docs/video#prompt-guide) section.

### Before you begin

Before calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.

To use Veo with the Google Gen AI SDKs, ensure that you have one of the following versions installed:

*   [Python](https://pypi.org/project/google-genai/) v1.10.0 or later
*   [TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) v0.8.0 or later
*   [Go](https://pkg.go.dev/google.golang.org/genai) v1.0.0 or later

## Generate videos

This section provides code examples for generating videos [using text prompts](#generate-from-text) and [using images](#generate-from-images).

### Generate from text

You can use the following code to generate videos with Veo:

### Python

```
import time
from google import genai
from google.genai import types

client = genai.Client()

operation = client.models.generate_videos(
    model="veo-2.0-generate-001",
    prompt="Panning wide shot of a calico kitten sleeping in the sunshine",
    config=types.GenerateVideosConfig(
        person_generation="dont_allow",  # "dont_allow" or "allow_adult"
        aspect_ratio="16:9",  # "16:9" or "9:16"
    ),
)

while not operation.done:
    time.sleep(20)
    operation = client.operations.get(operation)

for n, generated_video in enumerate(operation.response.generated_videos):
    client.files.download(file=generated_video.video)
    generated_video.video.save(f"video{n}.mp4")  # save the video

```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import { createWriteStream } from "fs";
import { Readable } from "stream";

const ai = new GoogleGenAI({});

async function main() {
  let operation = await ai.models.generateVideos({
    model: "veo-2.0-generate-001",
    prompt: "Panning wide shot of a calico kitten sleeping in the sunshine",
    config: {
      personGeneration: "dont_allow",
      aspectRatio: "16:9",
    },
  });

  while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
      operation: operation,
    });
  }

  operation.response?.generatedVideos?.forEach(async (generatedVideo, n) => {
    const resp = await fetch(`${generatedVideo.video?.uri}&key=GEMINI_API_KEY`); // append your API key
    const writer = createWriteStream(`video${n}.mp4`);
    Readable.fromWeb(resp.body).pipe(writer);
  });
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "time"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  videoConfig := &genai.GenerateVideosConfig{
      AspectRatio:      "16:9",
      PersonGeneration: "dont_allow",
  }

  operation, _ := client.Models.GenerateVideos(
      ctx,
      "veo-2.0-generate-001",
      "Panning wide shot of a calico kitten sleeping in the sunshine",
      nil,
      videoConfig,
  )

  for !operation.Done {
      time.Sleep(20 * time.Second)
      operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
  }

  for n, video := range operation.Response.GeneratedVideos {
      client.Files.Download(ctx, video.Video, nil)
      fname := fmt.Sprintf("video_%d.mp4", n)
      _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
  }
}
```

### REST

```
# Use curl to send a POST request to the predictLongRunning endpoint.
# The request body includes the prompt for video generation.
curl "${BASE_URL}/models/veo-2.0-generate-001:predictLongRunning" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X "POST" \
  -d '{
    "instances": [{
        "prompt": "Panning wide shot of a calico kitten sleeping in the sunshine"
      }
    ],
    "parameters": {
      "aspectRatio": "16:9",
      "personGeneration": "dont_allow",
    }
  }' | tee result.json | jq .name | sed 's/"//g' > op_name

# Obtain operation name to download video.
op_name=$(cat op_name)

# Check against status of operation.
while true; do
  is_done=$(curl -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${op_name}" | tee op_check.json | jq .done)

  if [ "${is_done}" = "true" ]; then
    cat op_check.json
    echo "** Attach API_KEY to download video, or examine error message."
    break
  fi

  echo "** Video ${op_name} has not downloaded yet!  Check again after 5 seconds..."

  # Wait for 5 seoncds to check again.
  sleep 5

done
```

![Kitten sleeping in the sun.](https://storage.googleapis.com/generativeai-downloads/images/calico.gif)

This code takes about 2-3 minutes to run, though it may take longer if resources are constrained. Once it's done running, you should see a video that looks something like this:

If you see an error message instead of a video, this means that resources are constrained and your request couldn't be completed. In this case, run the code again.

Generated videos are stored on the server for 2 days, after which they are removed. If you want to save a local copy of your generated video, you must run `result()` and `save()` within 2 days of generation.

### Generate from images

You can also generate videos using images. The following code generates an image using Imagen, then uses the generated image as the starting frame for the generated video.

First, generate an image using [Imagen](/gemini-api/docs/image-generation#imagen):

### Python

```
prompt="Panning wide shot of a calico kitten sleeping in the sunshine",

imagen = client.models.generate_images(
    model="imagen-3.0-generate-002",
    prompt=prompt,
    config=types.GenerateImagesConfig(
      aspect_ratio="16:9",
      number_of_images=1
    )
)

imagen.generated_images[0].image
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});
const response = await ai.models.generateImages({
  model: "imagen-3.0-generate-002",
  prompt: "Panning wide shot of a calico kitten sleeping in the sunshine",
  config: {
    numberOfImages: 1,
  },
});

// you'll pass response.generatedImages[0].image.imageBytes to Veo
```

### Go

```
package main

import (
    "context"
    "fmt"
    "os"
    "time"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

    config := &genai.GenerateImagesConfig{
        AspectRatio:    "16:9",
        NumberOfImages: 1,
    }

    response, _ := client.Models.GenerateImages(
        ctx,
        "imagen-3.0-generate-002",
        "Panning wide shot of a calico kitten sleeping in the sunshine",
        config,
    )

    // you'll pass response.GeneratedImages[0].Image to Veo
}
```

Then, generate a video using the resulting image as the first frame:

### Python

```
operation = client.models.generate_videos(
    model="veo-2.0-generate-001",
    prompt=prompt,
    image = imagen.generated_images[0].image,
    config=types.GenerateVideosConfig(
      person_generation="dont_allow",  # "dont_allow" or "allow_adult"
      aspect_ratio="16:9",  # "16:9" or "9:16"
      number_of_videos=2
    ),
)

# Wait for videos to generate
 while not operation.done:
  time.sleep(20)
  operation = client.operations.get(operation)

for n, video in enumerate(operation.response.generated_videos):
    fname = f'with_image_input{n}.mp4'
    print(fname)
    client.files.download(file=video.video)
    video.video.save(fname)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import { createWriteStream } from "fs";
import { Readable } from "stream";

const ai = new GoogleGenAI({});

async function main() {
  // get image bytes from Imagen, as shown above

  let operation = await ai.models.generateVideos({
    model: "veo-2.0-generate-001",
    prompt: "Panning wide shot of a calico kitten sleeping in the sunshine",
    image: {
      imageBytes: response.generatedImages[0].image.imageBytes, // response from Imagen
      mimeType: "image/png",
    },
    config: {
      aspectRatio: "16:9",
      numberOfVideos: 2,
    },
  });

  while (!operation.done) {
    await new Promise((resolve) => setTimeout(resolve, 10000));
    operation = await ai.operations.getVideosOperation({
      operation: operation,
    });
  }

  operation.response?.generatedVideos?.forEach(async (generatedVideo, n) => {
    const resp = await fetch(
      `${generatedVideo.video?.uri}&key=GEMINI_API_KEY`, // append your API key
    );
    const writer = createWriteStream(`video${n}.mp4`);
    Readable.fromWeb(resp.body).pipe(writer);
  });
}

main();
```

### Go

    ```
    image := response.GeneratedImages[0].Image

    videoConfig := &genai.GenerateVideosConfig{
      AspectRatio:    "16:9",
      NumberOfVideos: 2,
    }

    operation, _ := client.Models.GenerateVideos(
        ctx,
        "veo-2.0-generate-001",
        "A dramatic scene based on the input image",
        image,
        videoConfig,
    )

    for !operation.Done {
        time.Sleep(20 * time.Second)
        operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
    }

    for n, video := range operation.Response.GeneratedVideos {
        client.Files.Download(ctx, video.Video, nil)
        fname := fmt.Sprintf("video_with_image_input_%d.mp4", n)
        _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
    }
```

## Veo model parameters

(Naming conventions vary by programming language.)

*   `prompt`: The text prompt for the video. When present, the `image` parameter is optional.
*   `image`: The image to use as the first frame for the video. When present, the `prompt` parameter is optional.
*   `negativePrompt`: Text string that describes anything you want to _discourage_ the model from generating
*   `aspectRatio`: Changes the aspect ratio of the generated video. Supported values are `"16:9"` and `"9:16"`. The default is `"16:9"`.
*   `personGeneration`: Allow the model to generate videos of people. The following values are supported:
    *   Text-to-video generation:
        *   `"dont_allow"`: Don't allow the inclusion of people or faces.
        *   `"allow_adult"`: Generate videos that include adults, but not children.
        *   `"allow_all"`: Generate videos that include adults and children.
    *   Image-to-video generation:
        *   `"dont_allow"`: Don't allow the inclusion of people or faces.
        *   `"allow_adult"`: Generate videos that include adults, but not children.
    *   See [Limitations](#limitations).
*   `numberOfVideos`: Output videos requested, either `1` or `2`.
*   `durationSeconds`: Length of each output video in seconds, between `5` and `8`.
*   `enhance_prompt`: Enable or disable the prompt rewriter. Enabled by default.

## Specifications

**Modalities**

*   Text-to-video generation
*   Image-to-video generation

**Request latency**

*   Min: 11 seconds
*   Max: 6 minutes (during peak hours)

**Variable length generation**

5-8 seconds

**Resolution**

720p

**Frame rate**

24fps

**Aspect ratio**

*   16:9 - landscape
*   9:16 - portrait

**Input languages (text-to-video)**

English

**Limitations**

*   Image-to-video `personGeneration` is not allowed in EU, UK, CH, MENA locations
*   Text-to-video `personGeneration: "allow_all"` is not allowed in EU, UK, CH, MENA locations

Videos created by Veo are watermarked using [SynthID](https://deepmind.google/technologies/synthid/), our tool for watermarking and identifying AI-generated content, and are passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks.

## Things to try

To get the most out of Veo, incorporate video-specific terminology into your prompts. Veo understands a wide range of terms related to:

*   **Shot composition:** Specify the framing and number of subjects in the shot (e.g., "single shot," "two shot," "over-the-shoulder shot").
*   **Camera positioning and movement:** Control the camera's location and movement using terms like "eye level," "high angle," "worms eye," "dolly shot," "zoom shot," "pan shot," and "tracking shot."
*   **Focus and lens effects:** Use terms like "shallow focus," "deep focus," "soft focus," "macro lens," and "wide-angle lens" to achieve specific visual effects.
*   **Overall style and subject:** Guide Veo's creative direction by specifying styles like "sci-fi," "romantic comedy," "action movie," or "animation." You can also describe the subjects and backgrounds you want, such as "cityscape," "nature," "vehicles," or "animals."

## Veo prompt guide

This section of the Veo guide contains examples of videos you can create using Veo, and shows you how to modify prompts to produce distinct results.

### Safety filters

Veo applies safety filters across Gemini to help ensure that generated videos and uploaded photos don't contain offensive content. Prompts that violate our [terms and guidelines](/gemini-api/docs/usage-policies#abuse-monitoring) are blocked.

### Prompt writing basics

Good prompts are descriptive and clear. To get your generated video as close as possible to what you want, start with identifying your core idea, and then refine your idea by adding keywords and modifiers.

The following elements should be included in your prompt:

*   **Subject**: The object, person, animal, or scenery that you want in your video.
*   **Context**: The background or context in which the subject is placed.
*   **Action**: What the subject is doing (for example, _walking_, _running_, or _turning their head_).
*   **Style**: This can be general or very specific. Consider using specific film style keywords, such as _horror film_, _film noir_, or animated styles like _cartoon_ style.
*   **Camera motion**: \[Optional\] What the camera is doing, such as _aerial view_, _eye-level_, _top-down shot_, or _low-angle shot_.
*   **Composition**: \[Optional\] How the shot is framed, such as _wide shot_, _close-up_, or _extreme close-up_.
*   **Ambiance**: \[Optional\] How the color and light contribute to the scene, such as _blue tones_, _night_, or _warm tones_.

#### More tips for writing prompts

The following tips help you write prompts that generate your videos:

*   **Use descriptive language**: Use adjectives and adverbs to paint a clear picture for Veo.
*   **Provide context**: If necessary, include background information to help your model understand what you want.
*   **Reference specific artistic styles**: If you have a particular aesthetic in mind, reference specific artistic styles or art movements.
*   **Utilize prompt engineering tools**: Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results. For more information, visit [Introduction to prompt design](/gemini-api/docs/prompting-intro).
*   **Enhance the facial details in your personal and group images**: Specify facial details as a focus of the photo like using the word _portrait_ in the prompt.

### Example prompts and output

This section presents several prompts, highlighting how descriptive details can elevate the outcome of each video.

#### Icicles

This video demonstrates how you can use the elements of [prompt writing basics](#basics) in your prompt.

**Prompt**

**Generated output**

Close up shot (composition) of melting icicles (subject) on a frozen rock wall (context) with cool blue tones (ambiance), zoomed in (camera motion) maintaining close-up detail of water drips (action).

![Dripping icicles with a blue background.](/static/gemini-api/docs/video/images/icicles2.gif)

#### Man on the phone

These videos demonstrate how you can revise your prompt with increasingly specific details to get Veo to refine the output to your liking.

**Prompt**

**Generated output**

**Analysis**

The camera dollies to show a close up of a desperate man in a green trench coat. He's making a call on a rotary-style wall phone with a green neon light. It looks like a movie scene.

![Man talking on the phone.](/static/gemini-api/docs/video/images/phonebooth.gif)

This is the first generated video based on the prompt.

A close-up cinematic shot follows a desperate man in a weathered green trench coat as he dials a rotary phone mounted on a gritty brick wall, bathed in the eerie glow of a green neon sign. The camera dollies in, revealing the tension in his jaw and the desperation etched on his face as he struggles to make the call. The shallow depth of field focuses on his furrowed brow and the black rotary phone, blurring the background into a sea of neon colors and indistinct shadows, creating a sense of urgency and isolation.

![Man talking on the phone](/static/gemini-api/docs/video/images/phonebooth2.gif)

A more detailed prompt results in a video that is more focused with a richer environment.

A video with smooth motion that dollies in on a desperate man in a green trench coat, using a vintage rotary phone against a wall bathed in an eerie green neon glow. The camera starts from a medium distance, slowly moving closer to the man's face, revealing his frantic expression and the sweat on his brow as he urgently dials the phone. The focus is on the man's hands, his fingers fumbling with the dial as he desperately tries to connect. The green neon light casts long shadows on the wall, adding to the tense atmosphere. The scene is framed to emphasize the isolation and desperation of the man, highlighting the stark contrast between the vibrant glow of the neon and the man's grim determination.

![Man talking on the phone.](/static/gemini-api/docs/video/images/phonebooth3.gif)

Adding more detail gives the subject a realistic expression and creates an intense and vibrant scene.

#### Snow leopard

This example demonstrates the output Veo might generate for a simple prompt.

**Prompt**

**Generated output**

A cute creature with snow leopard-like fur is walking in winter forest, 3D cartoon style render.

![Snow leopard is lethargic.](/static/gemini-api/docs/video/images/snow_leopard_short.gif)

#### Running snow leopard

This prompt has more detail and demonstrates generated output that might be closer to what you want in your video.

**Prompt**

**Generated output**

Create a short 3D animated scene in a joyful cartoon style. A cute creature with snow leopard-like fur, large expressive eyes, and a friendly, rounded form happily prances through a whimsical winter forest. The scene should feature rounded, snow-covered trees, gentle falling snowflakes, and warm sunlight filtering through the branches. The creature's bouncy movements and wide smile should convey pure delight. Aim for an upbeat, heartwarming tone with bright, cheerful colors and playful animation.

![Snow leopard is running faster.](/static/gemini-api/docs/video/images/running_snow_leopard.gif)

### Examples by writing elements

These examples show you how to refine your prompts by each basic element.

#### Subject

This example shows you how to specify a subject description.

**Subject description**

**Prompt**

**Generated output**

The description can include a subject, or multiple subjects and actions. Here, our subject is "white concrete apartment building."

An architectural rendering of a white concrete apartment building with flowing organic shapes, seamlessly blending with lush greenery and futuristic elements

![Placeholder.](/static/gemini-api/docs/video/images/white_building.gif)

#### Context

This example shows you how to specify context.

**Context**

**Prompt**

**Generated output**

The background or context in which the subject will be placed is very important. Try placing your subject in a variety of backgrounds like on a busy street, or in outer space.

A satellite floating through outer space with the moon and some stars in the background.

![Satellite floating in the atmosphere.](/static/gemini-api/docs/video/images/satellite2.gif)

#### Action

This example shows you how to specify action.

**Action**

**Prompt**

**Generated output**

What is the subject doing like walking, running, or turning their head.

A wide shot of a woman walking along the beach, looking content and relaxed towards the horizon at sunset.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/sunset.gif)

#### Style

This example shows you how to specify style.

**Style**

**Prompt**

**Generated output**

You can add keywords to improve generation quality and steer it closer to intended style, such as shallow depth of field, movie still, minimalistic, surreal, vintage, futuristic, or double-exposure.

Film noir style, man and woman walk on the street, mystery, cinematic, black and white.

![Film noir style is absolutely beautiful.](/static/gemini-api/docs/video/images/film_noir.gif)

#### Camera motion

This example shows you how to specify camera motion.

**Camera motion**

**Prompt**

**Generated output**

Options for camera motion include POV shot, aerial view, tracking drone view, or tracking shot.

A POV shot from a vintage car driving in the rain, Canada at night, cinematic.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/pov_shot.gif)

#### Composition

This example shows you how to specify composition.

**Composition**

**Prompt**

**Generated output**

How the shot is framed (wide shot, close-up, low angle).

Extreme close-up of a an eye with city reflected in it.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/composition_eye_close_up.gif)

Create a video of a wide shot of surfer walking on a beach with a surfboard, beautiful sunset, cinematic.

![Sunset is absolutely beautiful.](/static/gemini-api/docs/video/images/composition_surfer.gif)

#### Ambiance

This example shows you how to specify ambiance.

**Ambiance**

**Prompt**

**Generated output**

Color palettes play a vital role in photography, influencing the mood and conveying intended emotions. Try things like "muted orange warm tones," "natural light," "sunrise" or "sunset". For example, a warm, golden palette can infuse a romantic and atmospheric feel into a photograph.

A close-up of a girl holding adorable golden retriever puppy in the park, sunlight.

![A puppy in a young girl's arms.](/static/gemini-api/docs/video/images/ambiance_puppy.gif)

Cinematic close-up shot of a sad woman riding a bus in the rain, cool blue tones, sad mood.

![A woman riding on a bus that feels sad.](/static/gemini-api/docs/video/images/ambiance_sad.gif)

### Use reference images to generate videos

You can bring images to life by using Veo's [image-to-video](/gemini-api/docs/video#generate-from-images) capability. You can use existing assets, or try [Imagen](/gemini-api/docs/image-generation) to generate something new.

**Prompt**

**Generated output**

Bunny with a chocolate candy bar.

![Bunny is running away.](/static/gemini-api/docs/video/images/static_bunny.png)

Bunny runs away.

![Bunny is running away.](/static/gemini-api/docs/video/images/bunny_runs_away.gif)

### Negative prompts

Negative prompts can be a powerful tool to help specify elements you _don't_ want in the video. Describe what you want to discourage the model from generating after the phrase "Negative prompt". Follow these tips:

*   ❌ Don't use instructive language or words like _no_ or _don't_. For example, "No walls" or "don't show walls".
    
*   ✅ Do describe what you don't want to see. For example, "wall, frame", which means that you don't want a wall or a frame in the video.
    

**Prompt**

**Generated output**

Generate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette.

![Tree with using words.](/static/gemini-api/docs/video/images/tree_with_no_negative.gif)

Generate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette.  
  
With negative prompt - urban background, man-made structures, dark, stormy, or threatening atmosphere.

![Tree with no negative words.](/static/gemini-api/docs/video/images/tree_with_negative.gif)

### Aspect ratios

Gemini Veo video generation supports the following two aspect ratios:

**Aspect ratio**

**Description**

Widescreen or 16:9

The most common aspect ratio for televisions, monitors, and mobile phone screens (landscape). Use this when you want to capture more of the background, like in scenic landscapes.

Portrait or 9:16

Rotated widescreen. This aspect ratio has been popularized by short form video applications, such as Youtube shorts. Use this for portraits or tall objects with strong vertical orientations, such as buildings, trees, waterfall, or buildings.  

#### Widescreen

This prompt is an example of the widescreen aspect ratio of 16:9.

**Prompt**

**Generated output**

Create a video with a tracking drone view of a man driving a red convertible car in Palm Springs, 1970s, warm sunlight, long shadows.

![Waterfall is absolutely beautiful.](/static/gemini-api/docs/video/images/widescreen_palm_springs.gif)

#### Portrait

This prompt is an example of the portrait aspect ratio of 9:16.

**Prompt**

**Generated output**

Create a video highlighting the smooth motion of a majestic Hawaiian waterfall within a lush rainforest. Focus on realistic water flow, detailed foliage, and natural lighting to convey tranquility. Capture the rushing water, misty atmosphere, and dappled sunlight filtering through the dense canopy. Use smooth, cinematic camera movements to showcase the waterfall and its surroundings. Aim for a peaceful, realistic tone, transporting the viewer to the serene beauty of the Hawaiian rainforest.

![Waterfall is absolutely beautiful.](/static/gemini-api/docs/video/images/waterfall.gif)

## What's next

*   Gain more experience generating AI videos with the [Veo Colab](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_Veo.ipynb).
*   Check out cool examples using Veo 2 on the [Google DeepMind site](https://deepmind.google/technologies/veo/)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/quickstart

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# המדריך למתחילים של Gemini API

במדריך למתחילים הזה נסביר איך להתקין את [הספריות](https://ai.google.dev/gemini-api/docs/libraries?hl=he) שלנו ולשלוח את הבקשה הראשונה ל-Gemini API.

## לפני שמתחילים

צריך מפתח API של Gemini. אם עדיין אין לכם חשבון, תוכלו [להירשם בחינם ל-Google AI Studio](https://aistudio.google.com/app/apikey?hl=he).

## התקנה של Google GenAI SDK

### Python

באמצעות [Python 3.9 ואילך](https://www.python.org/downloads/), מתקינים את [חבילת `google-genai`](https://pypi.org/project/google-genai/) באמצעות [פקודת pip](https://packaging.python.org/en/latest/tutorials/installing-packages/) הבאה:

```
pip install -q -U google-genai
```

### JavaScript

באמצעות [Node.js v18 ואילך](https://nodejs.org/en/download/package-manager), מתקינים את [Google Gen AI SDK ל-TypeScript ול-JavaScript](https://www.npmjs.com/package/@google/genai) באמצעות [הפקודה הבאה של npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm):

```
npm install @google/genai
```

### Go

מתקינים את [‎google.golang.org/genai](https://pkg.go.dev/google.golang.org/genai) בספריית המודול באמצעות [הפקודה go get](https://go.dev/doc/code):

```
go get google.golang.org/genai
```

### Java

אם אתם משתמשים ב-Maven, תוכלו להתקין את [google-genai](https://github.com/googleapis/java-genai) על ידי הוספת הקוד הבא ליחסי התלות:

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

### Apps Script

1.  כדי ליצור פרויקט חדש של Apps Script, עוברים אל [script.new](https://script.google.com/u/0/home/projects/create?hl=he).
2.  לוחצים על **Untitled project**.
3.  משנים את שם הפרויקט ב-Apps Script ל-**AI Studio** ולוחצים על **Rename**.
4.  מגדירים את [מפתח ה-API](https://developers.google.com/apps-script/guides/properties?hl=he#manage_script_properties_manually)
    1.  בצד ימין, לוחצים על **הגדרות הפרויקט** ![הסמל של הגדרות הפרויקט](https://fonts.gstatic.com/s/i/short-term/release/googlesymbols/settings/default/24px.svg).
    2.  בקטע **מאפייני סקריפט**, לוחצים על **הוספת מאפיין סקריפט**.
    3.  בשדה **נכס**, מזינים את שם המפתח: `GEMINI_API_KEY`.
    4.  בשדה **ערך**, מזינים את הערך של מפתח ה-API.
    5.  לוחצים על **Save script properties**.
5.  מחליפים את תוכן הקובץ `Code.gs` בקוד הבא:

## שליחת הבקשה הראשונה

הנה דוגמה שבה נעשה שימוש בשיטה [`generateContent`](https://ai.google.dev/api/generate-content?hl=he#method:-models.generatecontent) כדי לשלוח בקשה ל-Gemini API באמצעות מודל Gemini 2.5 Flash.

אם [מגדירים את מפתח ה-API](https://ai.google.dev/gemini-api/docs/api-key?hl=he#set-api-env-var) כמשתנה הסביבה `GEMINI_API_KEY`, הלקוח יזהה אותו באופן אוטומטי כשמשתמשים ב[ספריות של Gemini API](https://ai.google.dev/gemini-api/docs/libraries?hl=he). אחרת, תצטרכו [להעביר את מפתח ה-API](https://ai.google.dev/gemini-api/docs/api-key?hl=he#provide-api-key-explicitly) כארגומנט בזמן האתחול של הלקוח.

חשוב לזכור שכל דוגמאות הקוד במסמכי התיעוד של Gemini API מבוססות על ההנחה שהגדרתם את משתנה הסביבה `GEMINI_API_KEY`.

### Python

```
from google import genai

# The client gets the API key from the environment variable `GEMINI_API_KEY`.
client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

// The client gets the API key from the environment variable `GEMINI_API_KEY`.
const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Java

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    // The client gets the API key from the environment variable `GEMINI_API_KEY`.
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');
function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## האפשרות 'חשיבה' מופעלת כברירת מחדל בדוגמאות רבות לקוד שלנו

בדוגמאות קוד רבות באתר הזה נעשה שימוש במודל [Gemini 2.5 Flash](https://ai.google.dev/gemini-api/docs/models?hl=he#gemini-2.5-flash), שבו התכונה ['חשיבה'](https://ai.google.dev/gemini-api/docs/thinking?hl=he) מופעלת כברירת מחדל כדי לשפר את איכות התשובות. חשוב לדעת שהפעולה הזו עלולה להגדיל את זמן התגובה ואת השימוש באסימונים. אם אתם נותנים עדיפות למהירות או רוצים למזער את העלויות, תוכלו להשבית את התכונה הזו על ידי הגדרת תקציב החשיבה לאפס, כפי שמתואר בדוגמאות שבהמשך. מידע נוסף זמין ב[מדריך לחשיבה](https://ai.google.dev/gemini-api/docs/thinking?hl=he#set-budget).

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## המאמרים הבאים

אחרי ששלחתם את הבקשה הראשונה ל-API, כדאי לעיין במדריכים הבאים שבהם מוסבר איך משתמשים ב-Gemini:

*   [Thinking](https://ai.google.dev/gemini-api/docs/thinking?hl=he)
*   [יצירת טקסט](https://ai.google.dev/gemini-api/docs/text-generation?hl=he)
*   [ראייה](https://ai.google.dev/gemini-api/docs/vision?hl=he)
*   [הקשר ארוך](https://ai.google.dev/gemini-api/docs/long-context?hl=he)

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/libraries

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini API libraries

This page provides information on downloading and installing the latest libraries for the Gemini API. If you're new to the Gemini API, get started with the [API quickstart](/gemini-api/docs/quickstart).

## Important note about our new libraries

Since late 2024, we have been progressively rolling out the **Google Gen AI SDK**, a new set of libraries that provide a unified interface for accessing Google's generative AI models through both the Gemini Developer API and Vertex AI.

The Google Gen AI SDK is now the recommended pathway for accessing all of Google's models and all code snippets in our documentation utilize these libraries.

**Key Library Updates**

  

Language

New library (Recommended)

Old library

**Python**

`google-genai`

`google-generativeai`

**JavaScript  
and TypeScript**

`@google/genai`

`@google/generative-ai`

**Go**

`google.golang.org/genai`

`google.golang.org/generative-ai`

**Java**

`google-genai`

N/A, did not exist before

We encourage all users to [migrate to the Google GenAI SDK](/gemini-api/docs/migrate), as it provides access to new features like multi-modal outputs (e.g., image and video generation), the Live API, and long term maintenance. We expect the feature gap between the old and new SDK to grow over time.

## Install a library

### Python

You can install our [Python library](https://pypi.org/project/google-genai) by running:

```
pip install google-genai
```

### JavaScript

You can install our [JavaScript and TypeScript library](https://www.npmjs.com/package/@google/genai) by running:

```
npm install @google/genai
```

### Go

You can install our [Go library](https://pkg.go.dev/google.golang.org/genai) by running:

```
go get google.golang.org/genai
```

### Java

If you're using Maven, you can install our [Java library](https://github.com/googleapis/java-genai) by adding the following to your dependencies:

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

## Previous libraries and SDKs

For platforms where a new library is available, we strongly recommend [migrating to the new libraries](/gemini-api/docs/migrate). Previous libraries won't receive new features and their specific support status varies, as detailed below.

For platforms where an official Google Gen AI library is not yet available, you can call our APIs directly, use community-maintained libraries, or use trusted third-party options. We are actively working to expand our official SDK coverage and will list new libraries here as they are released.

The following are the previous libraries:

*   [Python (previous)](https://github.com/google-gemini/deprecated-generative-ai-python): All support, including bug fixes, ends September 2025.
*   [Node.js (previous)](https://github.com/google-gemini/generative-ai-js): All support, including bug fixes, ends September 2025.
*   [Go (previous)](https://github.com/google/generative-ai-go): All support, including bug fixes, ends September 2025.
*   [Dart and Flutter (Previous)](https://pub.dev/packages/google_generative_ai/install): Not actively maintained
*   [Swift (Previous)](https://github.com/google/generative-ai-swift): Not actively maintained
*   [Android (Previous)](https://github.com/google-gemini/generative-ai-android): Not actively maintained

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-11 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/image-generation

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Генерация изображения

Вы можете генерировать изображения с помощью Gemini API либо с помощью встроенных мультимодальных возможностей Gemini, либо с помощью Imagen, специализированных моделей генерации изображений от Google. Для большинства случаев использования начните с [Gemini](https://ai.google.dev/gemini-api/docs/image-generation?hl=ru#gemini) . Выберите [Imagen](https://ai.google.dev/gemini-api/docs/image-generation?hl=ru#imagen) для специализированных задач, где качество изображения имеет решающее значение. Дополнительные рекомендации см. в разделе [Выбор правильной модели](https://ai.google.dev/gemini-api/docs/image-generation?hl=ru#choose-a-model) .

Все сгенерированные изображения содержат [водяной знак SynthID](https://ai.google.dev/responsible/docs/safeguards/synthid?hl=ru) .

## Прежде чем начать

Убедитесь, что вы используете поддерживаемую модель и версию для генерации изображения:

*   Для **Gemini** используйте Gemini 2.0 Flash Preview Image Generation.
    
*   В качестве **Imagen** используйте одну из моделей Imagen (Imagen 3, Imagen 4 или Imagen 4 Ultra).
    
    *   Обратите внимание, что эти модели доступны только на [платном уровне](https://ai.google.dev/gemini-api/docs/pricing?hl=ru) .

Вы можете получить доступ к моделям Gemini и Imagen, используя одни и те же библиотеки.

## Генерация изображений с помощью Gemini

Gemini может генерировать и обрабатывать изображения в разговорном режиме. Вы можете подсказывать Gemini текст, изображения или их комбинацию для выполнения различных задач, связанных с изображениями, таких как генерация и редактирование изображений.

Вы должны включить `responseModalities` : `["TEXT", "IMAGE"]` в вашу конфигурацию. Вывод только изображений не поддерживается этими моделями.

### Генерация изображений (текст-изображение)

Следующий код демонстрирует, как создать изображение на основе описательной подсказки:

### Питон

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO
import base64

client = genai.Client()

contents = ('Hi, can you create a 3d rendered image of a pig '
            'with wings and a top hat flying over a happy '
            'futuristic scifi city with lots of greenery?')

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=contents,
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.save('gemini-native-image.png')
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const contents =
    "Hi, can you create a 3d rendered image of a pig " +
    "with wings and a top hat flying over a happy " +
    "futuristic scifi city with lots of greenery?";

  // Set responseModalities to include "Image" so the model can generate  an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      ResponseModalities: []string{"TEXT", "IMAGE"},
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.0-flash-preview-image-generation",
      genai.Text("Hi, can you create a 3d rendered image of a pig " +
                 "with wings and a top hat flying over a happy " +
                 "futuristic scifi city with lots of greenery?"),
      config,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "gemini_generated_image.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

### ОТДЫХ

```
curl -s -X POST 
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?"}
      ]
    }],
    "generationConfig":{"responseModalities":["TEXT","IMAGE"]}
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-native-image.png
```

![Изображение фантастической летающей свиньи, созданное искусственным интеллектом](https://ai.google.dev/static/gemini-api/docs/images/flying-pig.png?hl=ru)

Изображение фантастической летающей свиньи, созданное искусственным интеллектом

### Редактирование изображений (текст и изображение в изображение)

Чтобы выполнить редактирование изображения, добавьте изображение в качестве входных данных. Следующий пример демонстрирует загрузку изображений в кодировке base64. Для нескольких изображений и больших полезных нагрузок проверьте раздел [ввода изображений](https://ai.google.dev/gemini-api/docs/image-understanding?hl=ru#image-input) .

### Питон

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

import PIL.Image

image = PIL.Image.open('/path/to/image.png')

client = genai.Client()

text_input = ('Hi, This is a picture of me.'
            'Can you add a llama next to me?',)

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=[text_input, image],
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  // Load the image from the local file system
  const imagePath = "path/to/image.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  // Prepare the content parts
  const contents = [
    { text: "Can you add a llama next to the image?" },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  // Set responseModalities to include "Image" so the model can generate an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Идти

```
package main

import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
)

func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
     log.Fatal(err)
 }

 imagePath := "/path/to/image.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
   genai.NewPartFromText("Hi, This is a picture of me. Can you add a llama next to me?"),
   &genai.Part{
     InlineData: &genai.Blob{
       MIMEType: "image/png",
       Data:     imgData,
     },
   },
 }

 contents := []*genai.Content{
   genai.NewContentFromParts(parts, genai.RoleUser),
 }

 config := &genai.GenerateContentConfig{
     ResponseModalities: []string{"TEXT", "IMAGE"},
 }

 result, _ := client.Models.GenerateContent(
     ctx,
     "gemini-2.0-flash-preview-image-generation",
     contents,
     config,
 )

 for _, part := range result.Candidates[0].Content.Parts {
     if part.Text != "" {
         fmt.Println(part.Text)
     } else if part.InlineData != nil {
         imageBytes := part.InlineData.Data
         outputFilename := "gemini_generated_image.png"
         _ = os.WriteFile(outputFilename, imageBytes, 0644)
     }
 }
}
```

### ОТДЫХ

```
IMG_PATH=/path/to/your/image1.jpeg

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"'Hi, This is a picture of me. Can you add a llama next to me\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/jpeg\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }],
      \"generationConfig\": {\"responseModalities\": [\"TEXT\", \"IMAGE\"]}
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-edited-image.png
```

### Другие режимы генерации изображений

Gemini поддерживает другие режимы взаимодействия с изображениями на основе структуры подсказок и контекста, в том числе:

*   **Текст с изображением(ями) и текстом (чередование):** выводит изображения с соответствующим текстом.
    *   Пример подсказки: «Создайте иллюстрированный рецепт паэльи».
*   **Изображение(я) и текст в изображение(я) и текст (чередование)** : использует входные изображения и текст для создания новых связанных изображений и текста.
    *   Пример подсказки: (С изображением меблированной комнаты) «Какие еще цвета диванов подойдут для моего помещения? Можете ли вы обновить изображение?»
*   **Многопоточное редактирование изображений (чат):** продолжайте создавать/редактировать изображения в режиме диалога.
    *   Примеры подсказок: \[загрузите изображение синего автомобиля.\], «Превратите этот автомобиль в кабриолет.», «Теперь измените цвет на желтый».

### Ограничения

*   Для лучшей производительности используйте следующие языки: EN, es-MX, ja-JP, zh-CN, hi-IN.
*   Генерация изображений не поддерживает аудио- и видеовходы.
*   Генерация изображения не всегда может вызвать:
    *   Модель может выводить только текст. Попробуйте явно запросить вывод изображений (например, «сгенерировать изображение», «предоставить изображения по мере продвижения», «обновить изображение»).
    *   Модель может перестать генерировать на полпути. Попробуйте еще раз или попробуйте другую подсказку.
*   При генерации текста для изображения Gemini работает лучше всего, если вы сначала генерируете текст, а затем запрашиваете изображение с текстом.
*   Есть некоторые регионы/страны, где генерация изображений недоступна. См. [Модели](https://ai.google.dev/gemini-api/docs/models?hl=ru) для получения дополнительной информации.

## Генерация изображений с использованием моделей Imagen

В этом примере демонстрируется генерация изображений с помощью [модели Imagen](https://deepmind.google/technologies/imagen-3/?hl=ru) :

### Питон

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

response = client.models.generate_images(
    model='imagen-4.0-generate-preview-06-06',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 4,
    )
)
for generated_image in response.generated_images:
  generated_image.image.show()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-preview-06-06',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateImagesConfig{
      NumberOfImages: 4,
  }

  response, _ := client.Models.GenerateImages(
      ctx,
      "imagen-4.0-generate-preview-06-06",
      "Robot holding a red skateboard",
      config,
  )

  for n, image := range response.GeneratedImages {
      fname := fmt.Sprintf("imagen-%d.png", n)
          _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
  }
}
```

### ОТДЫХ

```
curl -X POST \
    "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-preview-06-06:predict" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "instances": [
          {
            "prompt": "Robot holding a red skateboard"
          }
        ],
        "parameters": {
          "sampleCount": 4
        }
      }'
```

![Сгенерированное искусственным интеллектом изображение робота, держащего красный скейтборд](https://ai.google.dev/static/gemini-api/docs/images/robot-skateboard.png?hl=ru)

Сгенерированное искусственным интеллектом изображение робота, держащего красный скейтборд

### Конфигурация изображения

В настоящее время Imagen поддерживает только подсказки на английском языке и следующие параметры:

*   `numberOfImages` : Количество изображений для генерации от 1 до 4 (включительно). Значение по умолчанию — 4. Для Imagen 4 Ultra значение по умолчанию — 1, поскольку за раз может быть сгенерировано только одно изображение.
*   `aspectRatio` : Изменяет соотношение сторон сгенерированного изображения. Поддерживаемые значения: `"1:1"` , `"3:4"` , `"4:3"` , `"9:16"` и `"16:9"` . Значение по умолчанию: `"1:1"` .
*   `personGeneration` : Разрешить модели генерировать изображения людей. Поддерживаются следующие значения:
    
    *   `"dont_allow"` : Блокировать генерацию изображений людей.
    *   `"allow_adult"` : Генерировать изображения взрослых, но не детей. Это значение по умолчанию.
    *   `"allow_all"` : создание изображений, включающих взрослых и детей.

## Выбор правильной модели

Выбирайте **Близнецов,** когда:

*   Вам нужны контекстно-релевантные изображения, которые задействуют знания и рассуждения о мире.
*   Важно обеспечить плавное сочетание текста и изображений.
*   Вам нужны точные визуальные эффекты, встроенные в длинные текстовые последовательности.
*   Вы хотите редактировать изображения в диалоговом режиме, сохраняя при этом контекст.

Выбирайте **Imagen** , когда:

*   Главными приоритетами являются качество изображения, фотореализм, художественная детализация или определенные стили (например, импрессионизм, аниме).
*   Выполнение специализированных задач по редактированию, таких как обновление фона продукта или масштабирование изображений.
*   Внедрение брендинга, стиля или создание логотипов и дизайнов продукции.

Imagen 4 должен стать вашей основной моделью, если вы начинаете генерировать изображения с помощью Imagen. Выберите Imagen 4 Ultra для расширенных вариантов использования или когда вам нужно наилучшее качество изображения. Обратите внимание, что Imagen 4 Ultra может генерировать только одно изображение за раз.

## Руководство по подсказкам Imagen

В этом разделе руководства Imagen показано, как изменение запроса на преобразование текста в изображение может привести к различным результатам, а также приведены примеры изображений, которые вы можете создать.

### Основы письма в режиме онлайн

Хорошее предложение является описательным и ясным, и использует значимые ключевые слова и модификаторы. Начните с размышлений о **предмете** , **контексте** и **стиле** .

![Подсказка с акцентом на тему, контекст и стиль](https://ai.google.dev/static/gemini-api/docs/images/imagen/style-subject-context.png?hl=ru)

Текст изображения: _Эскиз_ ( **стиль** ) _современного многоквартирного дома_ ( **предмет** ), окруженного _небоскребами_ ( **контекст и фон** ).

1.  **Тема** : Первое, о чем следует подумать при создании любой подсказки, — это _тема_ : объект, человек, животное или пейзаж, изображение которого вы хотите получить.
    
2.  **Контекст и фон:** Не менее важен _фон или контекст_ , в котором будет размещен объект. Попробуйте разместить объект на разных фонах. Например, студия с белым фоном, на открытом воздухе или в помещении.
    
3.  **Стиль:** Наконец, добавьте желаемый стиль изображения. _Стили_ могут быть общими (живопись, фотография, эскизы) или очень конкретными (живопись пастелью, рисунок углем, изометрическое 3D). Вы также можете комбинировать стили.
    

После того, как вы напишете первую версию вашего запроса, усовершенствуйте его, добавляя больше деталей, пока не получите желаемое изображение. Итерация важна. Начните с определения вашей основной идеи, а затем усовершенствуйте и расширьте эту основную идею, пока сгенерированное изображение не будет близко к вашему видению.

![фотореалистичное изображение образца 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_short.png?hl=ru)

Подсказка: Парк весной рядом с озером.

![фотореалистичное изображение образца 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_medium.png?hl=ru)

Подсказка: Весенний парк рядом с озером, **солнце садится за озером, золотой час.**

![фотореалистичное изображение образца 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_long.png?hl=ru)

Подсказка: Весенний парк рядом с озером, _**солнце садится за озером, золотой час, красные полевые цветы.**_

Модели Imagen могут преобразовать ваши идеи в подробные изображения, будь то короткие или длинные и подробные подсказки. Уточняйте свое видение с помощью итеративных подсказок, добавляя детали, пока не достигнете идеального результата.

Короткие подсказки позволят вам быстро создать изображение.

![Пример краткой подсказки Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_short-prompt.png?hl=ru)

Подсказка: фотография крупным планом женщины в возрасте 20 лет, уличная фотография, кадр из фильма, приглушенные оранжевые теплые тона.

Более длинные подсказки позволяют вам добавлять конкретные детали и создавать свой образ.

![Пример длинной подсказки Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_long-prompt.png?hl=ru)

Подсказка: захватывающее фото женщины в возрасте 20 лет в стиле уличной фотографии. Изображение должно выглядеть как кадр из фильма с приглушенными оранжевыми теплыми тонами.

Дополнительные советы по написанию подсказок Imagen:

*   **Используйте описательный язык** : используйте подробные прилагательные и наречия, чтобы нарисовать ясную картину для Imagen.
*   **Предоставьте контекст** : при необходимости включите справочную информацию, которая поможет ИИ понять ситуацию.
*   **Ссылайтесь на конкретных художников или стили** : если вы придерживаетесь определенной эстетики, ссылки на конкретных художников или художественные течения могут быть полезны.
*   **Используйте инструменты проектирования подсказок** : рассмотрите возможность изучения инструментов или ресурсов проектирования подсказок, которые помогут вам усовершенствовать подсказки и достичь оптимальных результатов.
*   **Улучшение деталей лица на личных и групповых снимках** : укажите детали лица в качестве фокуса фотографии (например, используйте слово «портрет» в подсказке).

### Генерация текста на изображениях

Модели Imagen могут добавлять текст в изображения, открывая более творческие возможности создания изображений. Используйте следующие рекомендации, чтобы максимально использовать эту функцию:

*   **Повторяйте с уверенностью** : вам, возможно, придется перегенерировать изображения, пока вы не достигнете желаемого вида. Интеграция текста в Imagen все еще развивается, и иногда несколько попыток дают наилучшие результаты.
*   **Сокращайте** текст: для оптимальной генерации ограничьте его длиной не более 25 символов.
*   **Несколько фраз** : Экспериментируйте с двумя или тремя отдельными фразами, чтобы предоставить дополнительную информацию. Избегайте использования более трех фраз для более чистых композиций.
    
    ![Пример генерации текста с помощью Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_generate-text.png?hl=ru)
    
    Подсказка: Плакат с текстом «Summerland» жирным шрифтом в качестве заголовка, под этим текстом находится слоган «Лето никогда не было таким прекрасным».
    
*   **Размещение руководства** : хотя Imagen может попытаться расположить текст в соответствии с указаниями, ожидайте случайных изменений. Эта функция постоянно совершенствуется.
    
*   **Вдохновляйте стиль шрифта** : укажите общий стиль шрифта, чтобы тонко повлиять на выбор Imagen. Не полагайтесь на точное копирование шрифта, но ожидайте творческих интерпретаций.
    
*   **Размер шрифта** : укажите размер шрифта или общее указание размера (например, _маленький_ , _средний_ , _большой_ ), чтобы повлиять на генерацию размера шрифта.
    

### Быстрая параметризация

Для лучшего контроля результатов вывода вам может быть полезно параметризовать входные данные в Imagen. Например, предположим, что вы хотите, чтобы ваши клиенты могли генерировать логотипы для своего бизнеса, и вы хотите убедиться, что логотипы всегда генерируются на сплошном цветном фоне. Вы также хотите ограничить опции, которые клиент может выбрать из меню.

В этом примере вы можете создать параметризованное приглашение, подобное следующему:

A {logo\_style} logo for a {company\_area} company on a solid color background. Include the text {company\_name}.

В вашем пользовательском интерфейсе клиент может вводить параметры с помощью меню, а выбранное им значение заполняет подсказку, которую получает Imagen.

Например:

1.  Подсказка: `A minimalist logo for a health care company on a solid color background. Include the text Journey .`
    
    ![Пример параметризации подсказки Imagen 3 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_healthcare.png?hl=ru)
    
2.  Подсказка: `A modern logo for a software company on a solid color background. Include the text Silo .`
    
    ![Пример параметризации подсказки Imagen 3 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_software.png?hl=ru)
    
3.  Подсказка: `A traditional logo for a baking company on a solid color background. Include the text Seed .`
    
    ![Пример параметризации подсказки Imagen 3 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_baking.png?hl=ru)
    

### Продвинутые методы письма в сжатом виде

Используйте следующие примеры для создания более конкретных подсказок на основе таких атрибутов, как описания фотографий, формы и материалы, исторические направления в искусстве и модификаторы качества изображения.

#### Фотография

*   Подсказка включает: _«Фотография...»_

Чтобы использовать этот стиль, начните с ключевых слов, которые четко сообщат Imagen, что вы ищете фотографию. Начните свои подсказки с _"Фото..."_ . Например:

![фотореалистичное изображение образца 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_coffee-beans.png?hl=ru)

Подсказка: **Фотография** кофейных зерен на кухне на деревянной поверхности.

![фотореалистичное изображение образца 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_chocolate-bar.png?hl=ru)

Подсказка: **Фотография** плитки шоколада на кухонном столе.

![фотореалистичное изображение образца 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_modern-building.png?hl=ru)

Подсказка: **Фотография** современного здания с водой на заднем плане.

Источник изображения: каждое изображение было создано с использованием соответствующей текстовой подсказки с помощью модели Imagen 3.

##### Модификаторы фотографии

В следующих примерах вы можете увидеть несколько модификаторов и параметров, специфичных для фотографии. Вы можете комбинировать несколько модификаторов для более точного управления.

1.  **Приближение камеры** — _крупный план, снято издалека_
    
    ![пример изображения с камеры крупным планом](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_close-up.png?hl=ru)
    
    Подсказка: фотография кофейных зерен **крупным планом.**
    
    ![увеличенное изображение образца камеры](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_zoomed-out.png?hl=ru)
    
    Подсказка: **Уменьшенное** фото небольшого пакета  
    кофейные зерна на грязной кухне
    
2.  **Положение камеры** - _вид сверху, снизу_
    
    ![Образец изображения аэрофотоснимка](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_aerial-photo.png?hl=ru)
    
    Подсказка: **аэрофотоснимок** города с небоскребами
    
    ![вид снизу образец изображения](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_from-below.png?hl=ru)
    
    Подсказка: Фотография полога леса с голубым небом **снизу.**
    
3.  **Освещение** - _естественное, драматическое, теплое, холодное_
    
    ![образец изображения естественного освещения](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_natural-lighting.png?hl=ru)
    
    Подсказка: студийное фото современного кресла, **естественное освещение.**
    
    ![Образец изображения драматического освещения](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_dramatic-lighting.png?hl=ru)
    
    Подсказка: студийное фото современного кресла, **эффектное освещение.**
    
4.  **Настройки камеры** _— размытие движения, мягкий фокус, боке, портрет_
    
    ![пример изображения размытия движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_motion-blur.png?hl=ru)
    
    Подсказка: фотография города с небоскребами из салона автомобиля с **размытием движения**
    
    ![образец изображения с мягким фокусом](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_soft-focus.png?hl=ru)
    
    Подсказка: фотография моста в городе ночью **с мягким фокусом.**
    
5.  **Типы объективов** - _35 мм, 50 мм, «рыбий глаз», широкоугольный, макро_
    
    ![пример изображения макрообъектива](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_macro-lens.png?hl=ru)
    
    Подсказка: фотография листа, **макрообъектив**
    
    ![Образец изображения объектива «рыбий глаз»](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_fisheye-lens.png?hl=ru)
    
    Подсказка: уличная фотография, Нью-Йорк, **объектив «рыбий глаз»**
    
6.  **Типы пленки** - _черно-белая, поляроид_
    
    ![образец фотографии Polaroid](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_polaroid-portrait.png?hl=ru)
    
    Подсказка: **полароидный портрет** собаки в солнцезащитных очках.
    
    ![черно-белое фото образец изображения](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_bw-photo.png?hl=ru)
    
    Подсказка: **черно-белое фото** собаки в солнцезащитных очках.
    

Источник изображения: каждое изображение было создано с использованием соответствующей текстовой подсказки с помощью модели Imagen 3.

### Иллюстрация и искусство

*   В подсказку включены: _« painting с изображением...»_ , _« sketch ...»._

Художественные стили варьируются от монохромных стилей, таких как карандашные наброски, до гиперреалистичного цифрового искусства. Например, следующие изображения используют одну и ту же подсказку с разными стилями:

_« \[art style or creation technique\] угловатого спортивного электрического седана на фоне небоскребов»_

![образцы художественных изображений](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1A.png?hl=ru)

Подсказка: **Технический карандашный рисунок** угловатого...

![образцы художественных изображений](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1B.png?hl=ru)

Подсказка: **Рисунок углем** угловатого...

![образцы художественных изображений](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1C.png?hl=ru)

Подсказка: **Рисунок цветным карандашом** угловатого...

![образцы художественных изображений](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2E.png?hl=ru)

Подсказка: **Пастельная картина** с изображением угловатого...

![образцы художественных изображений](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2F.png?hl=ru)

Подсказка: **Цифровое искусство** угловатого...

![образцы художественных изображений](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2G.png?hl=ru)

Подсказка: **Плакат в стиле ар-деко** с изображением угловатого...

Источник изображения: каждое изображение было создано с использованием соответствующей текстовой подсказки с помощью модели Imagen 2.

##### Формы и материалы

*   Подсказка включает: _«...изготовлен из...»_ , _«...в форме...»_

Одной из сильных сторон этой технологии является то, что вы можете создавать изображения, которые в противном случае было бы трудно или невозможно создать. Например, вы можете воссоздать логотип своей компании в разных материалах и текстурах.

![Пример формы и материала изображение 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_duffel.png?hl=ru)

Подсказка: дорожная сумка **из** сыра.

![Пример формы и материала изображение 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_bird.png?hl=ru)

Подсказка: неоновые трубки **в форме** птицы

![Пример формы и материала изображение 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_paper.png?hl=ru)

Подсказка: кресло **из бумаги** , студийное фото, стиль оригами

Источник изображения: каждое изображение было создано с использованием соответствующей текстовой подсказки с помощью модели Imagen 3.

#### Исторические ссылки на искусство

*   Подсказка включает: _«...в стиле...»_

Определенные стили стали культовыми на протяжении многих лет. Ниже приведены некоторые идеи исторической живописи или художественных стилей, которые вы можете попробовать.

_"создать изображение в стиле \[art period or movement\] : ветряная электростанция"_

![импрессионизм пример изображения](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_impressionism.png?hl=ru)

Подсказка: создать изображение **в стиле _импрессионистской картины_** : ветряная электростанция.

![пример изображения эпохи возрождения](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_renaissance.png?hl=ru)

Подсказка: создать изображение **в стиле _живописи эпохи Возрождения_** : ветряная электростанция.

![поп-арт пример изображения](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_pop-art.png?hl=ru)

Подсказка: сгенерировать изображение **в стиле _поп-арт_** : ветряная электростанция

Источник изображения: каждое изображение было создано с использованием соответствующей текстовой подсказки с помощью модели Imagen 3.

#### Модификаторы качества изображения

Определенные ключевые слова могут дать модели понять, что вы ищете высококачественный актив. Примеры модификаторов качества включают следующее:

*   **Общие модификаторы** - _качественные, красивые, стилизованные_
*   **Фото** - _4K, HDR, студийное фото_
*   **Искусство, Иллюстрация** - _профессиональная, подробная_

Ниже приведены несколько примеров подсказок без модификаторов качества и тех же подсказок с модификаторами качества.

![пример изображения кукурузы без модификаторов](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_no-mods.png?hl=ru)

Подсказка (без модификаторов качества): фотография стебля кукурузы

![пример изображения кукурузы с модификаторами](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_4k-hdr.png?hl=ru)

Подсказка (с модификаторами качества): **4k HDR прекрасно**  
фотография стебля кукурузы **, сделанная  
профессиональный фотограф**

Источник изображения: каждое изображение было создано с использованием соответствующей текстовой подсказки с помощью модели Imagen 3.

#### Соотношения сторон

Генератор изображений Imagen позволяет вам устанавливать пять различных соотношений сторон изображения.

1.  **Квадрат** (1:1, по умолчанию) — стандартное квадратное фото. Обычное использование этого соотношения сторон включает публикации в социальных сетях.
2.  **Полный экран** (4:3) — это соотношение сторон обычно используется в медиа или кино. Это также размеры большинства старых (не широкоэкранных) телевизоров и среднеформатных камер. Оно захватывает большую часть сцены по горизонтали (по сравнению с 1:1), что делает его предпочтительным соотношением сторон для фотографии.
    
    ![пример соотношения сторон](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_piano.png?hl=ru)
    
    Подсказка: крупный план пальцев музыканта, играющего на пианино, черно-белая пленка, винтаж (соотношение сторон 4:3)
    
    ![пример соотношения сторон](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_fries.png?hl=ru)
    
    Подсказка: Профессиональная студийная фотография картофеля фри для элитного ресторана в стиле кулинарного журнала (соотношение сторон 4:3)
    
3.  **Портретный полноэкранный** (3:4) — это полноэкранное соотношение сторон, повернутое на 90 градусов. Это позволяет захватить больше сцены по вертикали по сравнению с соотношением сторон 1:1.
    
    ![пример соотношения сторон](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_hiking.png?hl=ru)
    
    Подсказка: женщина на прогулке, ее ботинки отражаются в луже, на заднем плане большие горы, в рекламном стиле, драматичные ракурсы (соотношение сторон 3:4)
    
    ![пример соотношения сторон](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_valley.png?hl=ru)
    
    Подсказка: аэрофотоснимок реки, текущей вверх по мистической долине (соотношение сторон 3:4)
    
4.  **Широкоэкранный** (16:9) — это соотношение заменило 4:3 и теперь является наиболее распространенным соотношением сторон для телевизоров, мониторов и экранов мобильных телефонов (альбомная ориентация). Используйте это соотношение сторон, когда хотите захватить больше фона (например, живописные пейзажи).
    
    ![пример соотношения сторон](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_16-9_man.png?hl=ru)
    
    Подсказка: мужчина в белой одежде сидит на пляже, крупный план, освещение «золотого часа» (соотношение сторон 16:9)
    
5.  **Портрет** (9:16) — это широкоэкранное соотношение, но повернутое. Это относительно новое соотношение сторон, которое стало популярным благодаря приложениям для коротких видео (например, YouTube shorts). Используйте его для высоких объектов с четкой вертикальной ориентацией, таких как здания, деревья, водопады или другие подобные объекты.
    
    ![пример соотношения сторон](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_9-16_skyscraper.png?hl=ru)
    
    Подсказка: цифровая визуализация огромного небоскреба, современного, грандиозного, эпического с красивым закатом на заднем плане (соотношение сторон 9:16)
    

#### Фотореалистичные изображения

Различные версии модели генерации изображений могут предлагать смесь художественного и фотореалистичного вывода. Используйте следующую формулировку в подсказках для создания более фотореалистичного вывода, основанного на объекте, который вы хотите создать.

Вариант использования

Тип линзы

Фокусные расстояния

Дополнительные подробности

Люди (портреты)

Prime, зум

24-35мм

черно-белая пленка, фильм нуар, глубина резкости, дуотон (укажите два цвета)

Еда, насекомые, растения (предметы, натюрморт)

Макро

60-105мм

Высокая детализация, точная фокусировка, контролируемое освещение

Спорт, дикая природа (движение)

Телефото зум

100-400мм

Короткая выдержка, отслеживание действий или движения

Астрономический, пейзажный (широкоугольный)

Широкоугольный

10-24мм

Длительное время экспозиции, четкий фокус, длительная выдержка, гладкая вода или облака

##### Портреты

Вариант использования

Тип линзы

Фокусные расстояния

Дополнительные подробности

Люди (портреты)

Prime, зум

24-35мм

черно-белая пленка, фильм нуар, глубина резкости, дуотон (укажите два цвета)

Используя несколько ключевых слов из таблицы, Imagen может сгенерировать следующие портреты:

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray1.png?hl=ru)

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray2.png?hl=ru)

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray3.png?hl=ru)

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray4.png?hl=ru)

Подсказка: _Женщина, портрет, 35 мм, синие и серые дуотона._  
Модель: `imagen-3.0-generate-002`

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir1.png?hl=ru)

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir2.png?hl=ru)

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir3.png?hl=ru)

![пример портретной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir4.png?hl=ru)

Подсказка: _Женщина, 35-мм портрет, фильм-нуар_  
Модель: `imagen-3.0-generate-002`

##### Объекты

Вариант использования

Тип линзы

Фокусные расстояния

Дополнительные подробности

Еда, насекомые, растения (предметы, натюрморт)

Макро

60-105мм

Высокая детализация, точная фокусировка, контролируемое освещение

Используя несколько ключевых слов из таблицы, Imagen может генерировать следующие изображения объектов:

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf1.png?hl=ru)

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf2.png?hl=ru)

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf3.png?hl=ru)

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf4.png?hl=ru)

Подсказка: _лист молитвенного растения, макрообъектив, 60 мм._  
Модель: `imagen-3.0-generate-002`

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta1.png?hl=ru)

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta2.png?hl=ru)

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta3.png?hl=ru)

![пример предметной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta4.png?hl=ru)

Подсказка: _тарелка пасты, макрообъектив 100 мм._  
Модель: `imagen-3.0-generate-002`

##### Движение

Вариант использования

Тип линзы

Фокусные расстояния

Дополнительные подробности

Спорт, дикая природа (движение)

Телефото зум

100-400мм

Короткая выдержка, отслеживание действий или движения

Используя несколько ключевых слов из таблицы, Imagen может генерировать следующие движущиеся изображения:

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football1.png?hl=ru)

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football2.png?hl=ru)

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football3.png?hl=ru)

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football4.png?hl=ru)

Подсказка: _победный тачдаун, быстрая выдержка, отслеживание движения_  
Модель: `imagen-3.0-generate-002`

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer1.png?hl=ru)

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer2.png?hl=ru)

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer3.png?hl=ru)

![пример фотографии движения](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer4.png?hl=ru)

Подсказка: _Олень бежит по лесу, короткая выдержка, отслеживание движения._  
Модель: `imagen-3.0-generate-002`

##### Широкоугольный

Вариант использования

Тип линзы

Фокусные расстояния

Дополнительные подробности

Астрономический, пейзажный (широкоугольный)

Широкоугольный

10-24мм

Длительное время экспозиции, четкий фокус, длительная выдержка, гладкая вода или облака

Используя несколько ключевых слов из таблицы, Imagen может генерировать следующие широкоугольные изображения:

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain1.png?hl=ru)

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain2.png?hl=ru)

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain3.png?hl=ru)

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain4.png?hl=ru)

Подсказка: _обширный горный хребет, пейзаж, широкоугольный объектив 10 мм_  
Модель: `imagen-3.0-generate-002`

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro1.png?hl=ru)

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro2.png?hl=ru)

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro3.png?hl=ru)

![пример широкоугольной фотографии](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro4.png?hl=ru)

Подсказка: _фотография Луны, астрофотография, широкоугольный объектив 10 мм._  
Модель: `imagen-3.0-generate-002`

## Что дальше?

*   Ознакомьтесь [с руководством Veo](https://ai.google.dev/gemini-api/docs/video?hl=ru) , чтобы узнать, как создавать видео с помощью API Gemini.
*   Дополнительную информацию о моделях Gemini см. в разделах [Модели Gemini](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ru) и [Экспериментальные модели](https://ai.google.dev/gemini-api/docs/models/experimental-models?hl=ru) .

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/openai

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# OpenAI compatibility

Gemini models are accessible using the OpenAI libraries (Python and TypeScript / Javascript) along with the REST API, by updating three lines of code and using your [Gemini API key](https://aistudio.google.com/apikey). If you aren't already using the OpenAI libraries, we recommend that you call the [Gemini API directly](https://ai.google.dev/gemini-api/docs/quickstart).

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ]
    }'
```

What changed? Just three lines!

*   **`api_key="GEMINI_API_KEY"`**: Replace "`GEMINI_API_KEY`" with your actual Gemini API key, which you can get in [Google AI Studio](https://aistudio.google.com).
    
*   **`base_url="https://generativelanguage.googleapis.com/v1beta/openai/"`:** This tells the OpenAI library to send requests to the Gemini API endpoint instead of the default URL.
    
*   **`model="gemini-2.0-flash"`**: Choose a compatible Gemini model
    

## Thinking

Gemini 2.5 models are trained to think through complex problems, leading to significantly improved reasoning. The Gemini API comes with a ["thinking budget" parameter](/gemini-api/docs/thinking#set-budget) which gives fine grain control over how much the model will think.

Unlike the Gemini API, the OpenAI API offers three levels of thinking control: `"low"`, `"medium"`, and `"high"`, which map to 1,024, 8,192, and 24,576 tokens, respectively.

If you want to disable thinking, you can set `reasoning_effort` to `"none"` (note that reasoning cannot be turned off for 2.5 Pro models).

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    reasoning_effort="low",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    reasoning_effort: "low",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
    "reasoning_effort": "low",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
      ]
    }'
```

Gemini thinking models also produce [thought summaries](/gemini-api/docs/thinking#summaries) and can use exact [thinking budgets](/gemini-api/docs/thinking#set-budget). You can use the [`extra_body`](#extra-body) field to include these fields in your request.

Note that `reasoning_effort` and `thinking_budget` overlap functionality, so they can't be used at the same time.

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[{"role": "user", "content": "Explain to me how AI works"}],
    extra_body={
      'extra_body': {
        "google": {
          "thinking_config": {
            "thinking_budget": 800,
            "include_thoughts": True
          }
        }
      }
    }
)

print(response.choices[0].message)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.5-flash",
    messages: [{role: "user", content: "Explain to me how AI works",}],
    extra_body: {
      "google": {
        "thinking_config": {
          "thinking_budget": 800,
          "include_thoughts": true
        }
      }
    }
});

console.log(response.choices[0].message);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.5-flash",
      "messages": [{"role": "user", "content": "Explain to me how AI works"}],
      "extra_body": {
        "google": {
           "thinking_config": {
             "include_thoughts": true
           }
        }
      }
    }'
```

## Streaming

The Gemini API supports [streaming responses](/gemini-api/docs/text-generation?lang=python#generate-a-text-stream).

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  stream=True
)

for chunk in response:
    print(chunk.choices[0].delta)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    stream: true,
  });

  for await (const chunk of completion) {
    console.log(chunk.choices[0].delta.content);
  }
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "model": "gemini-2.0-flash",
    "messages": [
        {"role": "user", "content": "Explain to me how AI works"}
    ],
    "stream": true
  }'
```

## Function calling

Function calling makes it easier for you to get structured data outputs from generative models and is [supported in the Gemini API](/gemini-api/docs/function-calling/tutorial).

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

tools = [
  {
    "type": "function",
    "function": {
      "name": "get_weather",
      "description": "Get the weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. Chicago, IL",
          },
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
        },
        "required": ["location"],
      },
    }
  }
]

messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}]
response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=messages,
  tools=tools,
  tool_choice="auto"
)

print(response)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}];
  const tools = [
      {
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Get the weather in a given location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. Chicago, IL",
              },
              "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
          },
        }
      }
  ];

  const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: messages,
    tools: tools,
    tool_choice: "auto",
  });

  console.log(response);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
  "model": "gemini-2.0-flash",
  "messages": [
    {
      "role": "user",
      "content": "What'\''s the weather like in Chicago today?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. Chicago, IL"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "tool_choice": "auto"
}'
```

## Image understanding

Gemini models are natively multimodal and provide best in class performance on [many common vision tasks](/gemini-api/docs/vision).

### Python

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# Function to encode the image
def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

# Getting the base64 string
base64_image = encode_image("Path/to/agi/image.jpeg")

response = client.chat.completions.create(
  model="gemini-2.0-flash",
  messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url":  f"data:image/jpeg;base64,{base64_image}"
          },
        },
      ],
    }
  ],
)

print(response.choices[0])
```

### JavaScript

```
import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
```

### REST

```
bash -c '
  base64_image=$(base64 -i "Path/to/agi/image.jpeg");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"What is in this image?\" },
            {
              \"type\": \"image_url\",
              \"image_url\": { \"url\": \"data:image/jpeg;base64,${base64_image}\" }
            }
          ]
        }
      ]
    }"
'
```

## Generate an image

Generate an image:

### Python

```
import base64
from openai import OpenAI
from PIL import Image
from io import BytesIO

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

response = client.images.generate(
    model="imagen-3.0-generate-002",
    prompt="a portrait of a sheepadoodle wearing a cape",
    response_format='b64_json',
    n=1,
)

for image_data in response.data:
  image = Image.open(BytesIO(base64.b64decode(image_data.b64_json)))
  image.show()
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const image = await openai.images.generate(
    {
      model: "imagen-3.0-generate-002",
      prompt: "a portrait of a sheepadoodle wearing a cape",
      response_format: "b64_json",
      n: 1,
    }
  );

  console.log(image.data);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/images/generations" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer GEMINI_API_KEY" \
  -d '{
        "model": "imagen-3.0-generate-002",
        "prompt": "a portrait of a sheepadoodle wearing a cape",
        "response_format": "b64_json",
        "n": 1,
      }'
```

## Audio understanding

Analyze audio input:

### Python

```
import base64
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

with open("/path/to/your/audio/file.wav", "rb") as audio_file:
  base64_audio = base64.b64encode(audio_file.read()).decode('utf-8')

response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Transcribe this audio",
        },
        {
              "type": "input_audio",
              "input_audio": {
                "data": base64_audio,
                "format": "wav"
          }
        }
      ],
    }
  ],
)

print(response.choices[0].message.content)
```

### JavaScript

```
import fs from "fs";
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

const audioFile = fs.readFileSync("/path/to/your/audio/file.wav");
const base64Audio = Buffer.from(audioFile).toString("base64");

async function main() {
  const response = await client.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Transcribe this audio",
          },
          {
            type: "input_audio",
            input_audio: {
              data: base64Audio,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

  console.log(response.choices[0].message.content);
}

main();
```

### REST

```
bash -c '
  base64_audio=$(base64 -i "/path/to/your/audio/file.wav");
  curl "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer GEMINI_API_KEY" \
    -d "{
      \"model\": \"gemini-2.0-flash\",
      \"messages\": [
        {
          \"role\": \"user\",
          \"content\": [
            { \"type\": \"text\", \"text\": \"Transcribe this audio file.\" },
            {
              \"type\": \"input_audio\",
              \"input_audio\": {
                \"data\": \"${base64_audio}\",
                \"format\": \"wav\"
              }
            }
          ]
        }
      ]
    }"
'
```

## Structured output

Gemini models can output JSON objects in any [structure you define](/gemini-api/docs/structured-output).

### Python

```
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model="gemini-2.0-flash",
    messages=[
        {"role": "system", "content": "Extract the event information."},
        {"role": "user", "content": "John and Susan are going to an AI conference on Friday."},
    ],
    response_format=CalendarEvent,
)

print(completion.choices[0].message.parsed)
```

### JavaScript

```
import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai"
});

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const completion = await openai.beta.chat.completions.parse({
  model: "gemini-2.0-flash",
  messages: [
    { role: "system", content: "Extract the event information." },
    { role: "user", content: "John and Susan are going to an AI conference on Friday" },
  ],
  response_format: zodResponseFormat(CalendarEvent, "event"),
});

const event = completion.choices[0].message.parsed;
console.log(event);
```

## Embeddings

Text embeddings measure the relatedness of text strings and can be generated using the [Gemini API](/gemini-api/docs/embeddings).

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key="GEMINI_API_KEY",
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-004"
)

print(response.data[0].embedding)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const embedding = await openai.embeddings.create({
    model: "text-embedding-004",
    input: "Your text string goes here",
  });

  console.log(embedding);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/openai/embeddings" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer GEMINI_API_KEY" \
-d '{
    "input": "Your text string goes here",
    "model": "text-embedding-004"
  }'
```

## `extra_body`

There are several features supported by Gemini that are not available in OpenAI models but can be enabled using the `extra_body` field.

**`extra_body` features**

`safety_settings`

Corresponds to Gemini's `SafetySetting`.

`cached_content`

Corresponds to Gemini's `GenerateContentRequest.cached_content`.

`thinking_config`

Corresponds to Gemini's `ThinkingConfig`.

### `cached_content`

Here's an example of using `extra_body` to set `cached_content`:

### Python

```
from openai import OpenAI

client = OpenAI(
    api_key=MY_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)

stream = client.chat.completions.create(
    model="gemini-2.5-pro",
    n=1,
    messages=[
        {
            "role": "user",
            "content": "Summarize the video"
        }
    ],
    stream=True,
    stream_options={'include_usage': True},
    extra_body={
        'extra_body':
        {
            'google': {
              'cached_content': "cachedContents/0000aaaa1111bbbb2222cccc3333dddd4444eeee"
          }
        }
    }
)

for chunk in stream:
    print(chunk)
    print(chunk.usage.to_dict())
```

## List models

Get a list of available Gemini models:

### Python

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

models = client.models.list()
for model in models:
  print(model.id)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const list = await openai.models.list();

  for await (const model of list) {
    console.log(model);
  }
}
main();
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models \
-H "Authorization: Bearer GEMINI_API_KEY"
```

## Retrieve a model

Retrieve a Gemini model:

### Python

```
from openai import OpenAI

client = OpenAI(
  api_key="GEMINI_API_KEY",
  base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

model = client.models.retrieve("gemini-2.0-flash")
print(model.id)
```

### JavaScript

```
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const model = await openai.models.retrieve("gemini-2.0-flash");
  console.log(model.id);
}

main();
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/openai/models/gemini-2.0-flash \
-H "Authorization: Bearer GEMINI_API_KEY"
```

## Current limitations

Support for the OpenAI libraries is still in beta while we extend feature support.

If you have questions about supported parameters, upcoming features, or run into any issues getting started with Gemini, join our [Developer Forum](https://discuss.ai.google.dev/c/gemini-api/4).

## What's next

Try our [OpenAI Compatibility Colab](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_OpenAI_Compatibility.ipynb) to work through more detailed examples.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-18 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/api-key

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# שימוש במפתחות API של Gemini

כדי להשתמש ב-Gemini API, אתם צריכים מפתח API. אתם יכולים ליצור מפתח בחינם ב[Google AI Studio](https://aistudio.google.com/app/apikey?hl=he) בכמה קליקים.

אחרי שתקבלו מפתח API, תוכלו להתחבר ל-Gemini API באמצעות האפשרויות הבאות:

*   [הגדרת מפתח ה-API כמשתנה סביבה](#set-api-env-var)
*   [ציון מפתח ה-API באופן מפורש](#provide-api-key-explicitly)

לבדיקה ראשונית, אפשר להטמיע מפתח API בקוד, אבל זה צריך להיות זמני בלבד כי זה לא מאובטח. דוגמאות להטמעת מפתח ה-API בקוד מפורטות בקטע [הצגת מפתח API באופן מפורש](#provide-api-key-explicitly).

## הגדרת מפתח API כמשתנה סביבה

אם מגדירים את משתנה הסביבה `GEMINI_API_KEY` או `GOOGLE_API_KEY`, הלקוח יזהה את מפתח ה-API באופן אוטומטי כשמשתמשים באחת מ[ספריות Gemini API](https://ai.google.dev/gemini-api/docs/libraries?hl=he). מומלץ להגדיר רק אחד מהמשתנים האלה, אבל אם מגדירים את שניהם, הערך של `GOOGLE_API_KEY` מקבל עדיפות.

אם אתם משתמשים ב-API ל-REST או ב-JavaScript בדפדפן, תצטרכו לספק את מפתח ה-API באופן מפורש.

כך מגדירים את מפתח ה-API באופן מקומי כמשתנה הסביבה `GEMINI_API_KEY` במערכות הפעלה שונות.

### Linux/macOS – Bash

Bash היא הגדרה נפוצה של מסוף Linux ו-macOS. כדי לבדוק אם יש לכם קובץ תצורה, מריצים את הפקודה הבאה:

```
~/.bashrc
```

אם התשובה היא 'No such file or directory', תצטרכו ליצור את הקובץ הזה ולפתוח אותו באמצעות הפקודות הבאות, או להשתמש ב-`zsh`:

```
touch ~/.bashrc
```

בשלב הבא, צריך להגדיר את מפתח ה-API על ידי הוספת פקודת הייצוא הבאה:

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

אחרי שמירת הקובץ, מחילים את השינויים באמצעות הפקודה:

```
source ~/.bashrc
```

### macOS – Zsh

Zsh היא הגדרה נפוצה של מסוף Linux ו-macOS. כדי לבדוק אם יש לכם קובץ תצורה, מריצים את הפקודה הבאה:

```
~/.zshrc
```

אם התשובה היא 'No such file or directory', תצטרכו ליצור את הקובץ הזה ולפתוח אותו באמצעות הפקודות הבאות, או להשתמש ב-`bash`:

```
touch ~/.zshrc
```

בשלב הבא, צריך להגדיר את מפתח ה-API על ידי הוספת פקודת הייצוא הבאה:

```
export GEMINI_API_KEY=<YOUR_API_KEY_HERE>
```

אחרי שמירת הקובץ, מחילים את השינויים באמצעות הפקודה:

```
source ~/.zshrc
```

### Windows

1.  מחפשים את 'משתני סביבה' בהגדרות המערכת
2.  עורכים את 'משתני משתמש' (למשתמש הנוכחי) או את 'משתני מערכת' (לכל המשתמשים – יש להשתמש בזהירות).
3.  יוצרים את המשתנה ומוסיפים את `export GEMINI_API_KEY=your_key_here`
4.  החלה של השינויים

## מתן מפתח API באופן מפורש

במקרים מסוימים, כדאי לספק מפתח API באופן מפורש. לדוגמה:

*   אתם מבצעים קריאה פשוטה ל-API ומעדיפים להטמיע את מפתח ה-API בקוד.
*   אתם רוצים לשלוט באופן מפורש בלי להסתמך על זיהוי אוטומטי של משתני הסביבה על ידי ספריות ה-API של Gemini
*   אתם משתמשים בסביבה שבה משתני סביבה לא נתמכים (למשל, אינטרנט) או שאתם מבצעים קריאות ל-REST.

בהמשך מפורטות דוגמאות לאופן שבו אפשר לספק מפתח API באופן מפורש:

### Python

```
from google import genai

client = genai.Client(api_key="YOUR_API_KEY")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "YOUR_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, &genai.ClientConfig{
        APIKey:  "YOUR_API_KEY",
        Backend: genai.BackendGeminiAPI,
    })
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Java

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = Client.builder().apiKey("YOUR_API_KEY").build();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=$YOUR_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## שמירה על אבטחת מפתח ה-API

חשוב לשמור על אבטחת מפתח ה-API של Gemini. ריכזנו כאן כמה דברים שחשוב לזכור כשמשתמשים במפתח Gemini API:

*   ב-Google AI Gemini API נעשה שימוש במפתחות API לצורך הרשאה. אם משתמשים אחרים יקבלו גישה למפתח Gemini API, הם יוכלו לבצע קריאות באמצעות המכסה של הפרויקט שלכם. כתוצאה מכך, יכול להיות שתאבדו את המכסה או שתצברו חיובים נוספים על פרויקטים שמחויבים, בנוסף לגישה למודלים ולקבצים שעברו כוונון.
    
*   הוספת [הגבלות על מפתחות API](https://cloud.google.com/api-keys/docs/add-restrictions-api-keys?hl=he#add-api-restrictions) יכולה לעזור להגביל את שטח ההתקפה שאפשר להשתמש בו דרך כל מפתח API.
    
*   אתם אחראים על אבטחת מפתח ה-API של Gemini.
    
    *   אין להטמיע מפתחות Gemini API במערכת בקרת הגרסאות.
    *   אפליקציות בצד הלקוח (אינטרנט ו-Dart/Flutter) עלולות לחשוף מפתחות API. אנחנו לא ממליצים להשתמש ב-SDK של לקוח Google AI באפליקציות ייצור כדי לבצע קריאה ל-Google AI Gemini API ישירות מאפליקציות האינטרנט שלכם.

[במאמר התמיכה הזה](https://support.google.com/googleapi/answer/6310037?hl=he) מפורטות גם שיטות מומלצות כלליות.

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/pricing

Memperkenalkan pembaruan pada rangkaian model pemikiran 2.5 kami. [Pelajari lebih lanjut](https://ai.google.dev/gemini-api/docs/models?hl=id)

![](https://ai.google.dev/_static/images/translated.svg?hl=id) Halaman ini diterjemahkan oleh [Cloud Translation API](//cloud.google.com/translate/?hl=id).

*   [Beranda](https://ai.google.dev/?hl=id)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=id)
*   [Model](https://ai.google.dev/gemini-api/docs?hl=id)

# Harga Gemini Developer API

"Paket gratis" Gemini API ditawarkan melalui layanan API dengan batas kapasitas yang lebih rendah untuk keperluan pengujian. Penggunaan Google AI Studio sepenuhnya gratis di semua negara yang tersedia. "Paket berbayar" Gemini API dilengkapi dengan [batas kapasitas yang lebih tinggi](https://ai.google.dev/gemini-api/docs/rate-limits?hl=id), fitur tambahan, dan penanganan data yang berbeda.

[Mengupgrade ke Tingkat Berbayar](https://aistudio.google.com/apikey?hl=id)

## Gemini 2.5 Pro

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro&hl=id)

Model serbaguna canggih kami, yang unggul dalam tugas coding dan penalaran kompleks.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$1,25, perintah <= 200 ribu token  
$2,50, perintah > 200 ribu token

Harga output (termasuk token pemikiran)

Tanpa biaya

$10,00, perintah <= 200 ribu token  
$15,00, perintah > 200 ribu

Harga cache konteks

Tidak tersedia

$0,31, perintah <= 200 ribu token  
$0,625, perintah > 200 ribu  
$4,50 / 1.000.000 token per jam (harga penyimpanan)

Grounding dengan Google Penelusuran

Tidak tersedia

1.500 RPD (gratis), lalu $35 / 1.000 permintaan

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Gemini 2.5 Flash

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash&hl=id)

Model penalaran campuran pertama kami yang mendukung jendela konteks token 1 juta dan memiliki anggaran pemikiran.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$0,30 (teks / gambar / video)  
$1,00 (audio)

Harga output (termasuk token pemikiran)

Tanpa biaya

$2,50

Harga cache konteks

Tidak tersedia

$0,075 (teks / gambar / video)  
$0,25 (audio)  
$1,00 / 1.000.000 token per jam (harga penyimpanan)

Grounding dengan Google Penelusuran

Gratis, hingga 500 RPD (batas yang dibagikan dengan RPD Flash-Lite)

1.500 RPD (gratis, batas yang dibagikan dengan RPD Flash-Lite), lalu $35 / 1.000 permintaan

Live API

Tanpa biaya

Input: $0,50 (teks), $3,00 (audio / gambar \[video\])  
Output: $2,00 (teks), $12,00 (audio)

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Pratinjau Gemini 2.5 Flash-Lite

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite-06-17&hl=id)

Model terkecil dan paling hemat biaya kami, yang dibuat untuk penggunaan dalam skala besar.

Model pratinjau dapat berubah sebelum menjadi stabil dan memiliki batas kecepatan yang lebih ketat.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Harga input (teks, gambar, video)

Tanpa biaya

$0,10 (teks / gambar / video)  
$0,50 (audio)

Harga output (termasuk token pemikiran)

Tanpa biaya

$0,40

Harga cache konteks

Tidak tersedia

$0,025 (teks / gambar / video)  
$0,125 (audio)  
$1,00 / 1.000.000 token per jam (harga penyimpanan)

Grounding dengan Google Penelusuran

Gratis, hingga 500 RPD (batas yang dibagikan dengan Flash RPD)

1.500 RPD (gratis, batas yang dibagikan dengan Flash RPD), lalu $35 / 1.000 permintaan

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Audio Native Gemini 2.5 Flash

[Coba di Google AI Studio](https://aistudio.google.com/app/live?hl=id#gemini-2.5-flash-preview-native-audio-dialog)

Model audio native kami dioptimalkan untuk output audio berkualitas lebih tinggi dengan kecepatan, kealamian suara, panjang, dan suasana hati yang lebih baik.

Model pratinjau dapat berubah sebelum menjadi stabil dan memiliki batas kecepatan yang lebih ketat.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tidak tersedia

$0,50 (teks)  
$3,00 (audio / video)

Harga output (termasuk token pemikiran)

Tidak tersedia

$2,00 (teks)  
$12,00 (audio)

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## TTS Pratinjau Gemini 2.5 Flash

[Coba di Google AI Studio](https://aistudio.google.com/generate-speech?hl=id)

Model audio text-to-speech Flash 2.5 kami dioptimalkan untuk pembuatan ucapan yang dapat dikontrol, latensi rendah, dan berperforma harga.

Model pratinjau dapat berubah sebelum menjadi stabil dan memiliki batas kecepatan yang lebih ketat.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$0,50 (teks)

Harga output

Tanpa biaya

$10,00 (audio)

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Pratinjau TTS Gemini 2.5 Pro

[Coba di Google AI Studio](https://aistudio.google.com/generate-speech?hl=id)

Model audio text-to-speech 2.5 Pro kami dioptimalkan untuk pembuatan ucapan yang canggih dan latensi rendah untuk output yang lebih alami dan perintah yang lebih mudah diarahkan.

Model pratinjau dapat berubah sebelum menjadi stabil dan memiliki batas kecepatan yang lebih ketat.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tidak tersedia

$1,00 (teks)

Harga output

Tidak tersedia

$20,00 (audio)

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Gemini 2.0 Flash

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash&hl=id)

Model multimodal kami yang paling seimbang dengan performa yang luar biasa di semua tugas, dengan jendela konteks 1 juta token, dan dibuat untuk era Agen.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$0,10 (teks / gambar / video)  
$0,70 (audio)

Harga output

Tanpa biaya

$0,40

Harga cache konteks

Tanpa biaya

$0,025 / 1.000.000 token (teks/gambar/video)  
$0,175 / 1.000.000 token (audio)

Context caching (penyimpanan)

Tanpa biaya, hingga 1.000.000 token penyimpanan per jam

$1,00 / 1.000.000 token per jam

Harga pembuatan gambar

Tanpa biaya

$0,039 per gambar\*

Harga penyesuaian

Tidak tersedia

Tidak tersedia

Grounding dengan Google Penelusuran

Tanpa biaya, hingga 500 RPD

1.500 RPD (gratis), lalu $35 / 1.000 permintaan

Live API

Tanpa biaya

Input: $0,35 (teks), $2,10 (audio / gambar \[video\])  
Output: $1,50 (teks), $8,50 (audio)

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

\[\*\] Output gambar dihargai $30 per 1.000.000 token. Gambar output hingga 1024x1024 piksel menggunakan 1.290 token dan setara dengan $0,039 per gambar.

## Gemini 2.0 Flash-Lite

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite&hl=id)

Model terkecil dan paling hemat biaya kami, yang dibuat untuk penggunaan dalam skala besar.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$0,075

Harga output

Tanpa biaya

$0,30

Harga cache konteks

Tidak tersedia

Tidak tersedia

Context caching (penyimpanan)

Tidak tersedia

Tidak tersedia

Harga penyesuaian

Tidak tersedia

Tidak tersedia

Grounding dengan Google Penelusuran

Tidak tersedia

Tidak tersedia

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Pratinjau Imagen 4

[Coba di Google AI Studio](https://aistudio.google.com/generate-image?hl=id)

Model pembuatan gambar terbaru kami, dengan rendering teks yang jauh lebih baik dan kualitas gambar yang lebih baik secara keseluruhan.

Model pratinjau dapat berubah sebelum menjadi stabil dan memiliki batas kecepatan yang lebih ketat.

  

Paket Gratis

Tingkat Berbayar, per Gambar dalam USD

Harga gambar standar Imagen 4

Tidak tersedia

$0,04

Harga gambar Imagen 4 Ultra

Tidak tersedia

$0,06

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Imagen 3

[Coba di Google AI Studio](https://aistudio.google.com/generate-image?hl=id)

Model pembuatan gambar canggih kami, yang tersedia untuk developer di tingkat berbayar Gemini API.

  

Paket Gratis

Tingkat Berbayar, per Gambar dalam USD

Harga gambar

Tidak tersedia

$0,03

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Veo 2

[Coba API](https://ai.google.dev/gemini-api/docs/video?hl=id)

Model pembuatan video canggih kami, yang tersedia untuk developer di tingkat berbayar Gemini API.

  

Paket Gratis

Tingkat Berbayar, per detik dalam USD

Harga video

Tidak tersedia

$0,35

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Gemma 3

[Coba Gemma 3](https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it&hl=id)

Model terbuka yang ringan dan canggih, yang dibuat dari teknologi yang sama dengan yang mendukung model Gemini kami.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

Tidak tersedia

Harga output

Tanpa biaya

Tidak tersedia

Harga cache konteks

Tanpa biaya

Tidak tersedia

Context caching (penyimpanan)

Tanpa biaya

Tidak tersedia

Harga penyesuaian

Tidak tersedia

Tidak tersedia

Grounding dengan Google Penelusuran

Tidak tersedia

Tidak tersedia

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Gemma 3n

[Coba Gemma 3n](https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it&hl=id)

Model terbuka kami dibuat untuk performa yang efisien di perangkat sehari-hari seperti ponsel, laptop, dan tablet.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

Tidak tersedia

Harga output

Tanpa biaya

Tidak tersedia

Harga cache konteks

Tanpa biaya

Tidak tersedia

Context caching (penyimpanan)

Tanpa biaya

Tidak tersedia

Harga penyesuaian

Tidak tersedia

Tidak tersedia

Grounding dengan Google Penelusuran

Tidak tersedia

Tidak tersedia

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Gemini 1.5 Flash

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash&hl=id)

Model multimodal tercepat kami dengan performa yang luar biasa untuk tugas berulang yang beragam dan jendela konteks 1 juta token.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$0,075, perintah <= 128 ribu token  
$0,15, perintah > 128 ribu token

Harga output

Tanpa biaya

$0,30, perintah <= 128 ribu token  
$0,60, perintah > 128 ribu token

Harga cache konteks

Gratis, hingga 1 juta token penyimpanan per jam

$0,01875, perintah <= 128 ribu token  
$0,0375, perintah > 128 ribu token

Context caching (penyimpanan)

Tanpa biaya

$1,00 per jam

Harga penyesuaian

Harga token sama untuk model yang disesuaikan  
Layanan penyesuaian tidak dikenai biaya.

Harga token sama untuk model yang disesuaikan  
Layanan penyesuaian tidak dikenai biaya.

Grounding dengan Google Penelusuran

Tidak tersedia

$35 / 1.000 permintaan pembumian

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Gemini 1.5 Flash-8B

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash-8b&hl=id)

Model terkecil kami untuk kasus penggunaan kecerdasan yang lebih rendah, dengan jendela konteks 1 juta token.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$0,0375, perintah <= 128 ribu token  
$0,075, perintah > 128 ribu token

Harga output

Tanpa biaya

$0,15, perintah <= 128 ribu token  
$0,30, perintah > 128 ribu token

Harga cache konteks

Gratis, hingga 1 juta token penyimpanan per jam

$0,01, perintah <= 128 ribu token  
$0,02, perintah > 128 ribu token

Context caching (penyimpanan)

Tanpa biaya

$0,25 per jam

Harga penyesuaian

Harga token sama untuk model yang disesuaikan  
Layanan penyesuaian tidak dikenai biaya.

Harga token sama untuk model yang disesuaikan  
Layanan penyesuaian tidak dikenai biaya.

Grounding dengan Google Penelusuran

Tidak tersedia

$35 / 1.000 permintaan pembumian

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Gemini 1.5 Pro

[Coba di Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro&hl=id)

Model seri Gemini 1.5 dengan kecerdasan tertinggi kami, dengan jendela konteks 2 juta token yang inovatif.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

$1,25, perintah <= 128 ribu token  
$2,50, perintah > 128 ribu token

Harga output

Tanpa biaya

$5,00, perintah <= 128 ribu token  
$10,00, perintah > 128 ribu token

Harga cache konteks

Tidak tersedia

$0,3125, perintah <= 128 ribu token  
$0,625, perintah > 128 ribu token

Context caching (penyimpanan)

Tidak tersedia

$4,50 per jam

Harga penyesuaian

Tidak tersedia

Tidak tersedia

Grounding dengan Google Penelusuran

Tidak tersedia

$35 / 1.000 permintaan pembumian

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

## Penyematan Teks 004

Model embedding teks canggih kami.

  

Paket Gratis

Paket Berbayar, per 1 juta token dalam USD

Input harga

Tanpa biaya

Tidak tersedia

Harga output

Tanpa biaya

Tidak tersedia

Harga penyesuaian

Tidak tersedia

Tidak tersedia

Digunakan untuk meningkatkan kualitas produk kami

[Ya](https://ai.google.dev/gemini-api/terms?hl=id)

[Tidak](https://ai.google.dev/gemini-api/terms?hl=id)

\[\*\] Penggunaan Google AI Studio tidak dikenai biaya di semua [wilayah yang tersedia](https://ai.google.dev/gemini-api/docs/available-regions?hl=id). Lihat [FAQ Penagihan](https://ai.google.dev/gemini-api/docs/billing?hl=id) untuk mengetahui detailnya.

\[\*\*\] Harga dapat berbeda dari harga yang tercantum di sini dan harga yang ditawarkan di Vertex AI. Untuk harga Vertex, lihat [halaman harga Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/pricing?hl=id).

\[\*\*\*\] Jika Anda menggunakan [pengambilan dinamis](https://ai.google.dev/gemini-api/docs/grounding?hl=id) untuk mengoptimalkan biaya, hanya permintaan yang berisi setidaknya satu URL dukungan grounding dari web dalam responsnya yang dikenai biaya untuk Grounding dengan Google Penelusuran. Biaya untuk Gemini selalu berlaku. Batas kapasitas dapat berubah sewaktu-waktu.

Kecuali dinyatakan lain, konten di halaman ini dilisensikan berdasarkan [Lisensi Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/), sedangkan contoh kode dilisensikan berdasarkan [Lisensi Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Untuk mengetahui informasi selengkapnya, lihat [Kebijakan Situs Google Developers](https://developers.google.com/site-policies?hl=id). Java adalah merek dagang terdaftar dari Oracle dan/atau afiliasinya.

Terakhir diperbarui pada 2025-07-03 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/text-generation

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Text generation

The Gemini API can generate text output from various inputs, including text, images, video, and audio, leveraging Gemini models.

Here's a basic example that takes a single text input:

### Python

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Thinking with Gemini 2.5

2.5 Flash and Pro models have ["thinking"](/gemini-api/docs/thinking) enabled by default to enhance quality, which may take longer to run and increase token usage.

When using 2.5 Flash, you can disable thinking by setting the thinking budget to zero.

For more details, see the [thinking guide](/gemini-api/docs/thinking#set-budget).

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## System instructions and other configurations

You can guide the behavior of Gemini models with system instructions. To do so, pass a [`GenerateContentConfig`](/api/generate-content#v1beta.GenerationConfig) object.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

The [`GenerateContentConfig`](/api/generate-content#v1beta.GenerationConfig) object also lets you override default generation parameters, such as [temperature](/api/generate-content#v1beta.GenerationConfig).

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Refer to the [`GenerateContentConfig`](/api/generate-content#v1beta.GenerationConfig) in our API reference for a complete list of configurable parameters and their descriptions.

## Multimodal inputs

The Gemini API supports multimodal inputs, allowing you to combine text with media files. The following example demonstrates providing an image:

### Python

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

For alternative methods of providing images and more advanced image processing, see our [image understanding guide](/gemini-api/docs/image-understanding). The API also supports [document](/gemini-api/docs/document-processing), [video](/gemini-api/docs/video-understanding), and [audio](/gemini-api/docs/audio) inputs and understanding.

## Streaming responses

By default, the model returns a response only after the entire generation process is complete.

For more fluid interactions, use streaming to receive [`GenerateContentResponse`](/api/generate-content#v1beta.GenerateContentResponse) instances incrementally as they're generated.

### Python

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Multi-turn conversations (Chat)

Our SDKs provide functionality to collect multiple rounds of prompts and responses into a chat, giving you an easy way to keep track of the conversation history.

### Python

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Streaming can also be used for multi-turn conversations.

### Python

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### REST

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Apps Script

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Supported models

All models in the Gemini family support text generation. To learn more about the models and their capabilities, visit the [Models](/gemini-api/docs/models) page.

## Best practices

### Prompting tips

For basic text generation, a [zero-shot](/gemini-api/docs/prompting-strategies#few-shot) prompt often suffices without needing examples, system instructions or specific formatting.

For more tailored outputs:

*   Use [System instructions](#system-instructions) to guide the model.
*   Provide few example inputs and outputs to guide the model. This is often referred to as [few-shot](/gemini-api/docs/prompting-strategies#few-shot) prompting.

Consult our [prompt engineering guide](/gemini/docs/prompting-strategies) for more tips.

### Structured output

In some cases, you may need structured output, such as JSON. Refer to our [structured output](/gemini-api/docs/structured-output) guide to learn how.

## What's next

*   Try the [Gemini API getting started Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb).
*   Explore Gemini's [image](/gemini-api/docs/image-understanding), [video](/gemini-api/docs/video-understanding), [audio](/gemini-api/docs/audio) and [document](/gemini-api/docs/document-processing) understanding capabilities.
*   Learn about multimodal [file prompting strategies](/gemini-api/docs/files#prompt-guide).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/rate-limits

2.5 ファミリーの思考モデルの更新をご紹介します。[詳細](https://ai.google.dev/gemini-api/docs/models?hl=ja)

![](https://ai.google.dev/_static/images/translated.svg?hl=ja) このページは [Cloud Translation API](//cloud.google.com/translate/?hl=ja) によって翻訳されました。

*   [ホーム](https://ai.google.dev/?hl=ja)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ja)
*   [モデル](https://ai.google.dev/gemini-api/docs?hl=ja)

フィードバックを送信

# レート制限

レート制限は、特定の期間内に Gemini API に対して実行できるリクエストの数を制御します。これらの上限は、公正な使用を維持し、不正使用を防止し、すべてのユーザーがシステム パフォーマンスを維持できるようにするためのものです。

## レート制限の仕組み

レート制限は通常、次の 3 つのディメンションで測定されます。

*   1 分あたりのリクエスト数（**RPM**）
*   1 日あたりのリクエスト数（**RPD**）
*   1 分あたりのトークン数（入力）（**TPM**）

使用量は各上限に対して評価され、いずれかの上限を超えると、レート制限エラーが発生します。たとえば、RPM の上限が 20 の場合、1 分以内に 21 件のリクエストを実行すると、TPM などの上限を超えていない場合でもエラーが発生します。

レート制限は、API キーではなくプロジェクトごとに適用されます。

上限は、使用している特定のモデルによって異なり、一部の上限は特定のモデルにのみ適用されます。たとえば、1 分あたりの画像数（IPM）は、画像を生成できるモデル（Imagen 3）でのみ計算されますが、コンセプトは TPM に似ています。他のモデルには、1 日あたりのトークン数の上限（TPD）が設定されている場合があります。

試験運用版モデルとプレビュー版モデルでは、レート制限がより厳しく制限されています。

## 使用量ティア

レート制限はプロジェクトの使用量階層に関連付けられています。API の使用量と費用が増加すると、料金の上限を引き上げ、より高い階層にアップグレードできます。

階層

要件

無料

[対象となる国](https://ai.google.dev/gemini-api/docs/available-regions?hl=ja)のユーザー

Tier 1

[プロジェクトにリンクされている](https://ai.google.dev/gemini-api/docs/billing?hl=ja#enable-cloud-billing)請求先アカウント

Tier 2

合計費用: 250 ドル超、支払いが完了してから 30 日以上経過している

Tier 3

合計費用: 1,000 ドル超、支払いが完了してから 30 日以上経過している

アップグレードをリクエストすると、Google の自動不正使用防止システムによって追加のチェックが行われます。通常、記載されている資格要件を満たしていれば承認されますが、審査プロセスで特定されたその他の要因に基づいて、アップグレード リクエストが拒否されることもあります。

このシステムは、すべてのユーザーに対して Gemini API プラットフォームのセキュリティと整合性を維持するのに役立ちます。

## 現在のレートの上限

### 無料枠

モデル

RPM

TPM

RPD

Gemini 2.5 Pro

5

250,000

100

Gemini 2.5 Flash

10

250,000

250

Gemini 2.5 Flash-Lite プレビュー 06-17

15

250,000

1,000

Gemini 2.5 Flash プレビュー TTS

3

10,000

15

Gemini 2.5 Pro プレビュー TTS

\--

\--

\--

Gemini 2.0 Flash

15

1,000,000

200

Gemini 2.0 Flash プレビュー画像の生成

10

200,000

100

Gemini 2.0 Flash-Lite

30

1,000,000

200

Imagen 3

\--

\--

\--

Veo 2

\--

\--

\--

Gemini 1.5 Flash（非推奨）

15

250,000

50

Gemini 1.5 Flash-8B（非推奨）

15

250,000

50

Gemini 1.5 Pro（非推奨）

\--

\--

\--

Gemma 3 および 3n

30

15,000

14,400

Gemini エンベディング（試験運用版）03-07

5

\--

100

### Tier 1

モデル

RPM

TPM

RPD

Gemini 2.5 Pro

150

2,000,000

1,000

Gemini 2.5 Flash

1,000

1,000,000

10,000

Gemini 2.5 Flash-Lite プレビュー 06-17

4,000

4,000,000

\--

Gemini 2.5 Flash プレビュー TTS

10

10,000

100

Gemini 2.5 Pro プレビュー TTS

10

10,000

50

Gemini 2.0 Flash

2,000

4,000,000

\--

Gemini 2.0 Flash プレビュー画像の生成

1,000

1,000,000

10,000

Gemini 2.0 Flash-Lite

4,000

4,000,000

\--

Imagen 4 スタンダード

10

\--

70

Imagen 4 Ultra

5

\--

30

Imagen 3

20

\--

\--

Veo 2

1 分あたり 2 本の動画

\--

1 日あたり 50 本

Gemini 1.5 Flash（非推奨）

2,000

4,000,000

\--

Gemini 1.5 Flash-8B（非推奨）

4,000

4,000,000

\--

Gemini 1.5 Pro（非推奨）

1,000

4,000,000

\--

Gemma 3 および 3n

30

15,000

14,400

Gemini エンベディング（試験運用版）03-07

10

\--

1,000

### Tier 2

モデル

RPM

TPM

RPD

Gemini 2.5 Pro

1,000

5,000,000

50,000

Gemini 2.5 Flash

2,000

3,000,000

100,000

Gemini 2.5 Flash-Lite プレビュー 06-17

10,000

10,000,000

100,000

Gemini 2.5 Flash プレビュー TTS

1,000

100,000

10,000

Gemini 2.5 Pro プレビュー TTS

100

25,000

1,000

Gemini 2.0 Flash

10,000

10,000,000

\--

Gemini 2.0 Flash プレビュー画像の生成

2,000

3,000,000

100,000

Gemini 2.0 Flash-Lite

20,000

10,000,000

\--

Imagen 4 スタンダード

10

\--

70

Imagen 4 Ultra

5

\--

30

Imagen 3

20

\--

\--

Veo 2

1 分あたり 2 本の動画

\--

1 日あたり 50 本

Gemini 1.5 Flash（非推奨）

2,000

4,000,000

\--

Gemini 1.5 Flash-8B（非推奨）

4,000

4,000,000

\--

Gemini 1.5 Pro（非推奨）

1,000

4,000,000

\--

Gemma 3 および 3n

30

15,000

14,400

Gemini エンベディング（試験運用版）03-07

10

\--

1,000

### Tier 3

モデル

RPM

TPM

RPD

Gemini 2.5 Pro

2,000

8,000,000

\--

Gemini 2.5 Flash

10,000

8,000,000

\--

Gemini 2.5 Flash-Lite プレビュー 06-17

30,000

30,000,000

\--

Gemini 2.5 Flash プレビュー TTS

1,000

1,000,000

\--

Gemini 2.5 Pro プレビュー TTS

100

1,000,000

\--

Gemini 2.0 Flash

30,000

30,000,000

\--

Gemini 2.0 Flash プレビュー画像の生成

5,000

5,000,000

\--

Gemini 2.0 Flash-Lite

30,000

30,000,000

\--

Imagen 4 スタンダード

10

\--

70

Imagen 4 Ultra

5

\--

30

Imagen 3

20

\--

\--

Veo 2

1 分あたり 2 本の動画

\--

1 日あたり 50 本

Gemini 1.5 Flash（非推奨）

2,000

4,000,000

\--

Gemini 1.5 Flash-8B（非推奨）

4,000

4,000,000

\--

Gemini 1.5 Pro（非推奨）

1,000

4,000,000

\--

Gemma 3 および 3n

30

15,000

14,400

Gemini エンベディング（試験運用版）03-07

10

\--

1,000

指定されたレートの上限は保証されず、実際の容量は変動する可能性があります。

### Live API のレート上限

### 無料枠

名前

同時セッション数

TPM

RPD

Gemini 2.5 Flash ライブ

3

1,000,000

\--

Gemini 2.0 Flash Live

3

1,000,000

\--

Gemini 2.5 Flash プレビューのネイティブ音声ダイアログ

1

25,000

5

Gemini 2.5 Flash 試験運用版ネイティブ音声思考ダイアログ

1

10,000

5

### Tier 1

名前

同時セッション数

TPM

RPD

Gemini 2.5 Flash ライブ

50

4,000,000

\--

Gemini 2.0 Flash Live

50

4,000,000

\--

Gemini 2.5 Flash プレビューのネイティブ音声ダイアログ

3

50,000

50

Gemini 2.5 Flash 試験運用版ネイティブ音声思考ダイアログ

1

25,000

50

### Tier 2

名前

同時セッション数

TPM

RPD

Gemini 2.5 Flash ライブ

1000

10,000,000

\--

Gemini 2.0 Flash Live

1000

10,000,000

\--

Gemini 2.5 Flash プレビューのネイティブ音声ダイアログ

100

1,000,000

無制限

Gemini 2.5 Flash 試験運用版ネイティブ音声思考ダイアログ

\--

\--

\--

### Tier 3

名前

同時セッション数

TPM

RPD

Gemini 2.5 Flash ライブ

1000

10,000,000

\--

Gemini 2.0 Flash Live

1000

10,000,000

\--

Gemini 2.5 Flash プレビューのネイティブ音声ダイアログ

\--

\--

\--

Gemini 2.5 Flash 試験運用版ネイティブ音声思考ダイアログ

\--

\--

\--

指定されたレートの上限は保証されず、実際の容量は変動する可能性があります。

## 次の階層にアップグレードする方法

Gemini API は、すべての課金サービスに Cloud Billing を使用します。無料枠から有料枠に移行するには、まず Google Cloud プロジェクトで Cloud 課金を有効にする必要があります。

プロジェクトが指定された条件を満たすと、次の階層へのアップグレードの対象になります。アップグレードをリクエストする手順は次のとおりです。

*   AI Studio の \[[API キー](https://aistudio.google.com/app/apikey?hl=ja)\] ページに移動します。
*   アップグレードするプロジェクトを見つけて、\[アップグレード\] をクリックします。\[アップグレード\] オプションは、[次の階層の要件](https://ai.google.dev/gemini-api/docs/rate-limits?hl=ja#usage-tiers)を満たすプロジェクトにのみ表示されます。

簡単な検証の後、プロジェクトは次の階層にアップグレードされます。

## レート制限の引き上げをリクエストする

各モデルのバリエーションには、レート制限（1 分あたりのリクエスト数、RPM）が関連付けられています。これらのレート制限の詳細については、[Gemini モデル](https://ai.google.dev/models/gemini?hl=ja)をご覧ください。

[有料ティアのレート制限の引き上げをリクエストする](https://forms.gle/ETzX94k8jf7iSotH9)

レート制限の引き上げについて保証はいたしかねますが、リクエストを審査し、容量のニーズに対応できる場合はご連絡いたします。

フィードバックを送信

特に記載のない限り、このページのコンテンツは[クリエイティブ・コモンズの表示 4.0 ライセンス](https://creativecommons.org/licenses/by/4.0/)により使用許諾されます。コードサンプルは [Apache 2.0 ライセンス](https://www.apache.org/licenses/LICENSE-2.0)により使用許諾されます。詳しくは、[Google Developers サイトのポリシー](https://developers.google.com/site-policies?hl=ja)をご覧ください。Java は Oracle および関連会社の登録商標です。

最終更新日 2025-06-28 UTC。

---

# Content from: https://ai.google.dev/gemini-api/docs/models

2.5 ファミリーの思考モデルの更新をご紹介します。[詳細](https://ai.google.dev/gemini-api/docs/models?hl=ja)

![](https://ai.google.dev/_static/images/translated.svg?hl=ja) このページは [Cloud Translation API](//cloud.google.com/translate/?hl=ja) によって翻訳されました。

*   [ホーム](https://ai.google.dev/?hl=ja)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ja)
*   [モデル](https://ai.google.dev/gemini-api/docs?hl=ja)

フィードバックを送信

# Gemini モデル

2.5 Pro spark

レスポンスの精度とパフォーマンスを最大限に高めた Google の最も強力な思考モデル

*   音声、画像、動画、テキストを入力して、テキスト レスポンスを取得する
*   難しい問題に取り組む、大規模なデータベースを分析する、など
*   複雑なコーディング、推論、マルチモーダル理解に最適

[](#gemini-2.5-pro)

2.5 Flash spark

価格とパフォーマンスの点で最適なモデルで、幅広い機能を提供します。

*   音声、画像、動画、テキストを入力して、テキスト レスポンスを取得する
*   モデルは必要に応じて思考します。または、思考予算を構成することもできます。
*   思考を必要とする低レイテンシの大容量タスクに最適

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

費用対効果と低レイテンシを重視して最適化された Gemini 2.5 Flash モデル。

*   音声、画像、動画、テキストを入力して、テキスト レスポンスを取得する
*   高スループットをサポートする最も費用対効果の高いモデル
*   リアルタイムで低レイテンシのユースケースに最適

[](#gemini-2.5-flash-lite)

## モデル バリエーション

Gemini API には、特定のユースケースに最適化されたさまざまなモデルが用意されています。利用可能な Gemini のバリエーションの概要は次のとおりです。

モデル バリエーション

入力

出力

適した用途

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

音声、画像、動画、テキスト、PDF

テキスト

思考と推論の強化、マルチモーダル理解、高度なコーディングなど

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

音声、画像、動画、テキスト

テキスト

適応的思考、費用対効果

[Gemini 2.5 Flash-Lite プレビュー](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

テキスト、画像、動画、オーディオ

テキスト

高スループットをサポートする最も費用対効果の高いモデル

[Gemini 2.5 Flash ネイティブ音声](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog`、  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

音声、動画、テキスト

テキストと音声（インターリーブ）

思考ありまたは思考なしで、高品質で自然な会話音声を出力

[Gemini 2.5 Flash プレビュー TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

テキスト

音声

低レイテンシで制御可能な、単一スピーカーと複数スピーカーのテキスト読み上げ音声の生成

[Gemini 2.5 Pro プレビュー版 TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

テキスト

音声

低レイテンシで制御可能な、単一スピーカーと複数スピーカーのテキスト読み上げ音声生成

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

音声、画像、動画、テキスト

テキスト

次世代の機能、速度、リアルタイム ストリーミング。

[Gemini 2.0 Flash プレビュー画像の生成](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

音声、画像、動画、テキスト

テキスト、画像

会話型の画像生成と編集

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

音声、画像、動画、テキスト

テキスト

費用対効果と低レイテンシ

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

音声、画像、動画、テキスト

テキスト

さまざまなタスクで高速で汎用性の高いパフォーマンスを実現

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

音声、画像、動画、テキスト

テキスト

大規模でインテリジェンスが低いタスク

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

音声、画像、動画、テキスト

テキスト

より高度なインテリジェンスを必要とする複雑な推論タスク

[Gemini エンベディング](#gemini-embedding)  
`gemini-embedding-exp`

テキスト

テキスト エンベディング

テキスト文字列の関連性の測定

[Imagen 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

テキスト

画像

Google の最新の画像生成モデル

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

テキスト

画像

高品質の画像生成モデル

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

テキスト、画像

動画

高画質動画の生成

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

音声、動画、テキスト

テキスト、音声

低レイテンシの双方向の音声と動画によるやり取り

[Gemini 2.0 Flash ライブ](#live-api-2.0)  
`gemini-2.0-flash-live-001`

音声、動画、テキスト

テキスト、音声

低レイテンシの双方向の音声と動画によるやり取り

各モデルのレート制限は、[レート制限ページ](https://ai.google.dev/gemini-api/docs/rate-limits?hl=ja)で確認できます。

### Gemini 2.5 Pro

Gemini 2.5 Pro は、コード、数学、STEM の複雑な問題の推論や、長いコンテキストを使用した大規模なデータセット、コードベース、ドキュメントの分析が可能な、最先端の思考モデルです。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-2.5-pro&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`gemini-2.5-pro`

保存: サポートされるデータ型

**入力**

音声、画像、動画、テキスト、PDF

**出力**

テキスト

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

65,536

handyman の機能

**構造化出力**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**検索によるグラウンディング**

サポート対象

**画像生成**

サポート対象外

**音声の生成**

サポート対象外

**Live API**

サポート対象外

**思考**

サポート対象

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_month最終更新日

2025 年 6 月

cognition\_2 ナレッジ カットオフ

2025 年 1 月

### Gemini 2.5 Flash

価格とパフォーマンスの点で最適なモデルで、幅広い機能を提供します。2.5 Flash は、大規模な処理、低レイテンシ、思考を必要とする大量のタスク、エージェント ユースケースに最適です。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-2.5-flash&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.5-flash`

保存: サポートされるデータ型

**入力**

テキスト、画像、動画、音声

**出力**

テキスト

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

65,536

handyman の機能

**音声の生成**

サポート対象外

**キャッシュ保存**

サポート対象

**コードの実行**

サポート対象

**関数呼び出し**

サポート対象

**画像生成**

サポート対象外

**検索によるグラウンディング**

サポート対象

**構造化出力**

サポート対象

**思考**

サポート対象

**チューニング**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   安定版: `gemini-2.5-flash`
*   プレビュー: `gemini-2.5-flash-preview-05-20`

calendar\_month最終更新日

2025 年 6 月

cognition\_2 ナレッジ カットオフ

2025 年 1 月

### Gemini 2.5 Flash-Lite プレビュー

費用対効果と低レイテンシを重視して最適化された Gemini 2.5 Flash モデル。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.5-flash-lite-preview-06-17`

保存: サポートされるデータ型

**入力**

テキスト、画像、動画、音声

**出力**

テキスト

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,000,000

**出力トークンの上限**

64,000

handyman の機能

**構造化出力**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**URL コンテキスト**

サポート対象

**検索によるグラウンディング**

サポート対象

**画像生成**

サポート対象外

**音声の生成**

サポート対象外

**Live API**

サポート対象外

**思考**

サポート対象

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   プレビュー: `gemini-2.5-flash-lite-preview-06-17`

calendar\_month最終更新日

2025 年 6 月

cognition\_2 ナレッジ カットオフ

2025 年 1 月

### Gemini 2.5 Flash ネイティブ音声

思考ありと思考なしの Google のネイティブ音声ダイアログ モデルは、[Live API](https://ai.google.dev/gemini-api/docs/live?hl=ja) で利用できます。これらのモデルは、スタイルとコントロールのプロンプトを使用して、インタラクティブで非構造化の会話エクスペリエンスを提供します。

[Google AI Studio でネイティブ音声を試す](https://aistudio.google.com/app/live?hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.5-flash-preview-native-audio-dialog`、  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

保存: サポートされるデータ型

**入力**

音声、動画、テキスト

**出力**

音声とテキスト

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

128,000

**出力トークンの上限**

8,000

handyman の機能

**音声の生成**

サポート対象

**キャッシュ保存**

サポート対象外

**コードの実行**

サポート対象外

**関数呼び出し**

サポート対象

**画像生成**

サポート対象外

**検索によるグラウンディング**

サポート対象

**構造化出力**

サポート対象外

**思考**

サポート対象

**チューニング**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   プレビュー: `gemini-2.5-flash-preview-05-20`
*   試験運用版: `gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_month最終更新日

2025 年 5 月

cognition\_2 ナレッジ カットオフ

2025 年 1 月

### Gemini 2.5 Flash プレビューの Text-to-Speech

Gemini 2.5 Flash プレビュー版 TTS は、価格とパフォーマンスに優れたテキスト読み上げモデルです。ポッドキャストの生成、オーディオブック、カスタマー サポートなどの構造化ワークフローで、高い制御性と透明性を提供します。Gemini 2.5 Flash は試験運用版 / プレビュー版のモデルであるため、レート制限がより厳しくなっています。

[Google AI Studio で試す](https://aistudio.google.com/generate-speech?hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.5-flash-preview-tts`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

音声

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

8,000

**出力トークンの上限**

16,000

handyman の機能

**構造化出力**

サポート対象外

**キャッシュ保存**

サポート対象外

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象外

**コードの実行**

サポート対象外

**検索**

サポート対象外

**音声の生成**

サポート対象

**Live API**

サポート対象外

**思考**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   `gemini-2.5-flash-preview-tts`

calendar\_month最終更新日

2025 年 5 月

### Gemini 2.5 Pro プレビュー版 Text-to-Speech

Gemini 2.5 Pro プレビュー版 TTS は、Google の最も強力なテキスト読み上げモデルです。ポッドキャストの生成、オーディオブック、カスタマー サポートなどの構造化ワークフローで、高い制御性と透明性を提供します。Gemini 2.5 Pro は試験運用版 / プレビュー版モデルであるため、レート制限がより厳しくなっています。

[Google AI Studio で試す](https://aistudio.google.com/generate-speech?hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.5-pro-preview-tts`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

音声

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

8,000

**出力トークンの上限**

16,000

handyman の機能

**構造化出力**

サポート対象外

**キャッシュ保存**

サポート対象外

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象外

**コードの実行**

サポート対象外

**検索**

サポート対象外

**音声の生成**

サポート対象

**Live API**

サポート対象外

**思考**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   `gemini-2.5-pro-preview-tts`

calendar\_month最終更新日

2025 年 5 月

### Gemini 2.0 Flash

Gemini 2.0 Flash は、優れた速度、ネイティブ ツールの使用、100 万トークンのコンテキスト ウィンドウなど、次世代の機能と強化された機能を提供します。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-2.0-flash-001&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.0-flash`

保存: サポートされるデータ型

**入力**

音声、画像、動画、テキスト

**出力**

テキスト

token\_auto トークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

8,192

handyman の機能

**構造化出力**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**検索**

サポート対象

**画像生成**

サポート対象外

**音声の生成**

サポート対象外

**Live API**

サポート対象

**思考**

試験運用版

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   最新: `gemini-2.0-flash`
*   安定版: `gemini-2.0-flash-001`
*   試験運用版: `gemini-2.0-flash-exp`

calendar\_month最終更新日

2025 年 2 月

cognition\_2 ナレッジ カットオフ

2024 年 8 月

### Gemini 2.0 Flash プレビュー画像の生成

Gemini 2.0 Flash プレビュー版の画像生成では、会話による画像の生成と編集など、画像生成機能が強化されています。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.0-flash-preview-image-generation`

保存: サポートされるデータ型

**入力**

音声、画像、動画、テキスト

**出力**

テキストと画像

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

32,000

**出力トークンの上限**

8,192

handyman の機能

**構造化出力**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象外

**コードの実行**

サポート対象外

**検索**

サポート対象外

**画像生成**

サポート対象

**音声の生成**

サポート対象外

**Live API**

サポート対象外

**思考**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   プレビュー: `gemini-2.0-flash-preview-image-generation`

gemini-2.0-flash-preview-image-generation は現在、欧州、中東、アフリカの多くの国でサポートされていません

calendar\_month最終更新日

2025 年 5 月

cognition\_2 ナレッジ カットオフ

2024 年 8 月

### Gemini 2.0 Flash-Lite

費用対効果と低レイテンシを重視して最適化された Gemini 2.0 Flash モデル。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-2.0-flash-lite&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.0-flash-lite`

保存: サポートされるデータ型

**入力**

音声、画像、動画、テキスト

**出力**

テキスト

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

8,192

handyman の機能

**構造化出力**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象外

**検索**

サポート対象外

**画像生成**

サポート対象外

**音声の生成**

サポート対象外

**Live API**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   最新: `gemini-2.0-flash-lite`
*   安定版: `gemini-2.0-flash-lite-001`

calendar\_month最終更新日

2025 年 2 月

cognition\_2 ナレッジ カットオフ

2024 年 8 月

### Gemini 1.5 Flash

Gemini 1.5 Flash は、さまざまなタスクにスケーリングできる、高速で汎用性の高いマルチモーダル モデルです。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-1.5-flash&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-1.5-flash`

保存: サポートされるデータ型

**入力**

音声、画像、動画、テキスト

**出力**

テキスト

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

8,192

movie\_info 音声/映像の仕様

**プロンプトあたりの画像の最大数**

3,600

**動画の長さの上限**

1 時間

**音声の最大長**

約 9.5 時間

handyman の機能

**システム指示**

サポート対象

**JSON モード**

サポート対象

**JSON スキーマ**

サポート対象

**調整可能な安全性設定**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**Live API**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   最新: `gemini-1.5-flash-latest`
*   最新の安定版: `gemini-1.5-flash`
*   安定版:

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_month最終更新日

2024 年 9 月

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B は、低インテリジェンス タスク向けに設計された小型モデルです。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-1.5-flash&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-1.5-flash-8b`

保存: サポートされるデータ型

**入力**

音声、画像、動画、テキスト

**出力**

テキスト

token\_auto トークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

8,192

movie\_info 音声/映像の仕様

**プロンプトあたりの画像の最大数**

3,600

**動画の長さの上限**

1 時間

**音声の最大長**

約 9.5 時間

handyman の機能

**システム指示**

サポート対象

**JSON モード**

サポート対象

**JSON スキーマ**

サポート対象

**調整可能な安全性設定**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**Live API**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   最新: `gemini-1.5-flash-8b-latest`
*   最新の安定版: `gemini-1.5-flash-8b`
*   安定版:

*   `gemini-1.5-flash-8b-001`

calendar\_month最終更新日

2024 年 10 月

### Gemini 1.5 Pro

これまでで最も高度な Gemini モデルである [Gemini 2.5 Pro プレビュー版](https://ai.google.dev/gemini-api/docs/models/experimental-models?hl=ja#available-models)をお試しください。

Gemini 1.5 Pro は、さまざまな推論タスクに最適化された中規模のマルチモーダル モデルです。1.5 Pro では、2 時間の動画、19 時間の音声、60,000 行のコードを含むコードベース、2,000 ページのテキストなど、大量のデータを一度に処理できます。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-1.5-pro&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-1.5-pro`

保存: サポートされるデータ型

**入力**

音声、画像、動画、テキスト

**出力**

テキスト

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

2,097,152

**出力トークンの上限**

8,192

movie\_info 音声/映像の仕様

**プロンプトあたりの画像の最大数**

7,200

**動画の長さの上限**

2 時間

**音声の最大長**

約 19 時間

handyman の機能

**システム指示**

サポート対象

**JSON モード**

サポート対象

**JSON スキーマ**

サポート対象

**調整可能な安全性設定**

サポート対象

**キャッシュ保存**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**Live API**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   最新: `gemini-1.5-pro-latest`
*   最新の安定版: `gemini-1.5-pro`
*   安定版:

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_month最終更新日

2024 年 9 月

### Imagen 4

Imagen 4 は Google の最新の画像モデルです。豊かな照明、大幅に改善されたテキスト レンダリング、以前のモデルよりも高い解像度の出力で、非常に詳細な画像を生成できます。

##### モデルの詳細

プロパティ

説明

id\_card モデルコード

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

画像

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

480 トークン（テキスト）

**出力画像**

1（Ultra）  
1 ～ 4（Standard）

calendar\_month最終更新日

2025 年 6 月

### Imagen 3

Imagen 3 は、Google の最高品質のテキスト画像変換モデルであり、以前のモデルよりも鮮明なディテールと豊かな照明効果を備え、邪魔なアーティファクトの少ない画像を生成できます。

##### モデルの詳細

プロパティ

説明

id\_card モデルコード

**Gemini API**

`imagen-3.0-generate-002`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

画像

token\_auto トークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

なし

**出力画像**

最大 4 個

calendar\_month最終更新日

2025 年 2 月

### Veo 2

Veo 2 は、Google のテキストと画像から動画を生成する高品質モデルです。プロンプトの芸術的なニュアンスを捉え、詳細な動画を生成できます。

##### モデルの詳細

プロパティ

説明

id\_card モデルコード

**Gemini API**

`veo-2.0-generate-001`

保存: サポートされるデータ型

**入力**

テキスト、画像

**出力**

動画

token\_auto の上限

**テキスト入力**

なし

**画像入力**

任意の画像解像度とアスペクト比、ファイルサイズは最大 20 MB

**動画を出力する**

最大 2 個

calendar\_month最終更新日

2025 年 4 月

### Gemini 2.5 Flash ライブ

Gemini 2.5 Flash Live モデルは Live API と連携して、Gemini との双方向の音声と動画による低レイテンシのやり取りを可能にします。このモデルは、テキスト、音声、動画の入力を処理でき、テキストと音声の出力を生成できます。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-live-2.5-flash-preview&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-live-2.5-flash-preview`

保存: サポートされるデータ型

**入力**

音声、動画、テキスト

**出力**

テキスト、音声

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

8,192

handyman の機能

**構造化出力**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**検索**

サポート対象

**画像生成**

サポート対象外

**音声の生成**

サポート対象

**思考**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   プレビュー: `gemini-live-2.5-flash-preview`

calendar\_month最終更新日

2025 年 6 月

cognition\_2 ナレッジ カットオフ

2025 年 1 月

### Gemini 2.0 Flash Live

Gemini 2.0 Flash Live モデルは Live API と連携して、Gemini との双方向の音声と動画による低レイテンシのやり取りを可能にします。このモデルは、テキスト、音声、動画の入力を処理でき、テキストと音声の出力を生成できます。

[Google AI Studio で試す](https://aistudio.google.com?model=gemini-2.0-flash-live-001&hl=ja)

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/gemini-2.0-flash-live-001`

保存: サポートされるデータ型

**入力**

音声、動画、テキスト

**出力**

テキスト、音声

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

1,048,576

**出力トークンの上限**

8,192

handyman の機能

**構造化出力**

サポート対象

**チューニング**

サポート対象外

**関数呼び出し**

サポート対象

**コードの実行**

サポート対象

**検索**

サポート対象

**画像生成**

サポート対象外

**音声の生成**

サポート対象

**思考**

サポート対象外

123 バージョン

詳細については、[モデル バージョン パターン](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ja#model-versions)をご覧ください。

*   プレビュー: `gemini-2.0-flash-live-001`

calendar\_month最終更新日

2025 年 4 月

cognition\_2 ナレッジ カットオフ

2024 年 8 月

### Gemini 埋め込み（試験運用版）

`Gemini embedding` は、コード、多言語、検索など、多くの重要なディメンションで[SOTA のパフォーマンス](https://deepmind.google/research/publications/157741/?hl=ja)を実現します。 Gemini エンベディングは試験運用版であるため、レートの上限がより厳しく制限されています。

##### モデルの詳細

プロパティ

説明

id\_card モデルコード

**Gemini API**

`gemini-embedding-exp-03-07`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

テキスト エンベディング

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

8,192

**出力ディメンションのサイズ**

伸縮可能、3072、1536、768 をサポート

calendar\_month最終更新日

2025 年 3 月

### テキスト エンベディングとエンベディング

#### テキスト エンベディング

最新のパフォーマンスを実現する新しい [Gemini エンベディング モデルの試験運用版](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/)をお試しください。

[テキスト エンベディング](https://ai.google.dev/gemini-api/docs/embeddings?hl=ja)は、文字列の関連性を測定するために使用され、多くの AI アプリケーションで広く使用されています。

`text-embedding-004` は、標準の MTEB エンベディング ベンチマークで、同等のディメンションを持つ既存のモデルよりも[優れた検索パフォーマンスを実現し、優れたパフォーマンスを発揮](https://arxiv.org/pdf/2403.20327)します。

##### モデルの詳細

プロパティ

説明

id\_card モデルコード

**Gemini API**

`models/text-embedding-004`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

テキスト エンベディング

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

2,048

**出力ディメンションのサイズ**

768

swap\_driving\_apps\_wheel レート制限[\[\*\*\]](#rate-limits)

1 分あたり 1,500 件のリクエスト

暗号化された調整可能な安全性設定

サポート対象外

calendar\_month最終更新日

2024 年 4 月

#### 埋め込み

エンベディング モデルを使用して、入力テキストの[テキスト エンベディング](https://ai.google.dev/gemini-api/docs/embeddings?hl=ja)を生成できます。

エンベディング モデルは、最大 2,048 トークンのテキストに対して 768 次元のエンベディングを作成するように最適化されています。

##### エンベディング モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/embedding-001`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

テキスト エンベディング

token\_auto トークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

2,048

**出力ディメンションのサイズ**

768

swap\_driving\_apps\_wheel レート制限[\[\*\*\]](#rate-limits)

1 分あたり 1,500 件のリクエスト

暗号化された調整可能な安全性設定

サポート対象外

calendar\_month最終更新日

2023 年 12 月

### AQA

AQA モデルを使用すると、ドキュメント、コーパス、または一連のパラグラフに対して[属性付き質問応答](https://ai.google.dev/gemini-api/docs/semantic_retrieval?hl=ja)（AQA）関連のタスクを実行できます。AQA モデルは、指定されたソースに基づく質問に対する回答と、回答可能な確率の推定値を返します。

#### モデルの詳細

プロパティ

説明

id\_card モデルコード

`models/aqa`

保存: サポートされるデータ型

**入力**

テキスト

**出力**

テキスト

languageサポートされている言語

英語

token\_autoトークンの上限[\[\*\]](#token-size)

**入力トークンの上限**

7,168

**出力トークンの上限**

1,024

swap\_driving\_apps\_wheel レート制限[\[\*\*\]](#rate-limits)

1 分あたり 1,500 件のリクエスト

暗号化された調整可能な安全性設定

サポート対象

calendar\_month最終更新日

2023 年 12 月

これらのモデルのバリエーションの機能については、[例](https://ai.google.dev/examples?hl=ja)をご覧ください。

\[\*\] Gemini モデルの場合、1 個のトークンは約 4 文字に相当します。100 個のトークンは、約 60 ～ 80 ワード（英語）です。

## モデル バージョン名のパターン

Gemini モデルは、_安定版_、_プレビュー版_、_試験運用版_のいずれかでご利用いただけます。コードでは、次のいずれかのモデル名形式を使用して、使用するモデルとバージョンを指定できます。

### 最新の安定版

指定したモデルの生成とバリエーションに対してリリースされた最新の安定バージョンを参照します。

最新の安定版を指定するには、`<model>-<generation>-<variation>` というパターンを使用します。例: `gemini-2.0-flash`

### Stable

特定の安定モデルを参照します。通常、安定したモデルは変更されません。ほとんどの本番環境アプリでは、特定の安定モデルを使用する必要があります。

安定版を指定するには、`<model>-<generation>-<variation>-<version>` の形式を使用します。例: `gemini-2.0-flash-001`

### プレビュー

本番環境での使用には適さない可能性があるプレビューモデルを指し、より制限の厳しいレート制限が適用されますが、請求が有効になっている場合があります。

プレビュー バージョンを指定するには、`<model>-<generation>-<variation>-<version>` の形式を使用します。例: `gemini-2.5-pro-preview-06-05`

### 試験運用版

本番環境での使用に適さない可能性があり、より制限の厳しいレート制限が適用される試験運用版モデルを指します。試験運用版モデルは、フィードバックを収集し、最新のアップデートをデベロッパーに迅速に提供するためにリリースされます。

試験運用版を指定するには、`<model>-<generation>-<variation>-<version>` の形式を使用します。例: `gemini-2.0-pro-exp-02-05`

## 試験運用版モデル

Gemini API には、安定版モデルに加えて試験運用版モデルも用意されています。試験運用版モデルは、本番環境での使用には適さず、レートの上限がより厳しくなっています。

Google は、フィードバックの収集、最新のアップデートを迅速にデベロッパーに提供すること、Google のイノベーションのペースを強調することを目的として、試験運用版モデルをリリースしています。試験運用版リリースから得られた情報は、モデルの提供対象の拡大の際に反映させていただきます。試験運用版モデルは、事前の通知なく別のモデルと置き換わる可能性があります。試験運用版モデルは、将来的に安定版モデルになるとは限りません。

### 以前の試験運用版モデル

新しいバージョンまたは安定版が利用可能になると、試験運用版モデルは削除され、置き換えられます。以前リリースされた試験運用版モデルは、次のセクションで、置き換えバージョンとともに確認できます。

モデルコード

ベースモデル

交換用バージョン

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## サポートされている言語

Gemini モデルは、次の言語で動作するようにトレーニングされています。

*   アラビア語（`ar`）
*   ベンガル語（`bn`）
*   ブルガリア語（`bg`）
*   中国語（簡体および繁体）（`zh`）
*   クロアチア語（`hr`）
*   チェコ語（`cs`）
*   デンマーク語（`da`）
*   オランダ語（`nl`）
*   英語（`en`）
*   エストニア語（`et`）
*   フィンランド語（`fi`）
*   フランス語（`fr`）
*   ドイツ語（`de`）
*   ギリシャ語（`el`）
*   ヘブライ語（`iw`）
*   ヒンディー語（`hi`）
*   ハンガリー語（`hu`）
*   インドネシア語（`id`）
*   イタリア語（`it`）
*   日本語（`ja`）
*   韓国語（`ko`）
*   ラトビア語（`lv`）
*   リトアニア語（`lt`）
*   ノルウェー語（`no`）
*   ポーランド語（`pl`）
*   ポルトガル語（`pt`）
*   ルーマニア語（`ro`）
*   ロシア語（`ru`）
*   セルビア語（`sr`）
*   スロバキア語（`sk`）
*   スロベニア語（`sl`）
*   スペイン語（`es`）
*   スワヒリ語（`sw`）
*   スウェーデン語（`sv`）
*   タイ語（`th`）
*   トルコ語（`tr`）
*   ウクライナ語（`uk`）
*   ベトナム語（`vi`）

フィードバックを送信

特に記載のない限り、このページのコンテンツは[クリエイティブ・コモンズの表示 4.0 ライセンス](https://creativecommons.org/licenses/by/4.0/)により使用許諾されます。コードサンプルは [Apache 2.0 ライセンス](https://www.apache.org/licenses/LICENSE-2.0)により使用許諾されます。詳しくは、[Google Developers サイトのポリシー](https://developers.google.com/site-policies?hl=ja)をご覧ください。Java は Oracle および関連会社の登録商標です。

最終更新日 2025-06-26 UTC。

---

# Content from: https://ai.google.dev/gemini-api/docs/billing

আমাদের 2.5 ফ্যামিলি অফ থিংকিং মডেলের আপডেট পেশ করা হচ্ছে। [আরও জানুন](https://ai.google.dev/gemini-api/docs/models?hl=bn)

![](https://ai.google.dev/_static/images/translated.svg?hl=bn) এই পৃষ্ঠাটি [Cloud Translation API](//cloud.google.com/translate/?hl=bn) অনুবাদ করেছে।

*   [হোম](https://ai.google.dev/?hl=bn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=bn)
*   [মডেল](https://ai.google.dev/gemini-api/docs?hl=bn)

মতামত জানান

# বিলিং

এই নির্দেশিকা বিভিন্ন জেমিনি API বিলিং বিকল্পগুলির একটি ওভারভিউ প্রদান করে, বিলিং এবং ব্যবহার নিরীক্ষণ কিভাবে সক্ষম করতে হয় তা ব্যাখ্যা করে এবং বিলিং সম্পর্কে প্রায়শই জিজ্ঞাসিত প্রশ্নগুলির উত্তর প্রদান করে।

[Gemini API প্রদত্ত স্তরে আপগ্রেড করুন৷](https://aistudio.google.com/apikey?hl=bn)

## বিলিং সম্পর্কে

Gemini API-এর জন্য বিলিং দুটি মূল্যের স্তরের উপর ভিত্তি করে করা হয়: _বিনামূল্যে_ (বা _বিনামূল্যে_ ) এবং _পে-যেমন-ইউ-গো_ (বা _প্রদত্ত_ )৷ মূল্য এবং হারের সীমা এই স্তরগুলির মধ্যে আলাদা এবং মডেল অনুসারেও পরিবর্তিত হয়। আপনি আরও জানতে [রেট সীমা](https://ai.google.dev/gemini-api/docs/rate-limits?hl=bn) এবং [মূল্য](https://ai.google.dev/gemini-api/docs/pricing?hl=bn) পৃষ্ঠাগুলি পরীক্ষা করতে পারেন৷ ক্ষমতার মডেল-বাই-মডেল ব্রেকডাউনের জন্য, [মিথুন মডেল পৃষ্ঠাটি](https://ai.google.dev/gemini-api/docs/models/gemini?hl=bn) দেখুন।

#### কিভাবে একটি আপগ্রেড অনুরোধ করতে হয়

বিনামূল্যের স্তর থেকে পে-যেমন-তুমি-গো পরিকল্পনায় স্থানান্তর করতে, আপনাকে আপনার Google ক্লাউড প্রকল্পের জন্য বিলিং সক্ষম করতে হবে৷ আপনি Google AI স্টুডিওতে যে বোতামটি দেখছেন তা আপনার প্রকল্পের বর্তমান পরিকল্পনার উপর নির্ভর করে।

*   আপনি যদি বিনামূল্যের স্তরে থাকেন, আপনি আপনার প্রকল্পের জন্য একটি **সেট আপ বিলিং** বোতাম দেখতে পাবেন৷
*   আপনি যদি ইতিমধ্যেই অর্থপ্রদানের স্তরে থাকেন এবং একটি পরিকল্পনা পরিবর্তনের মানদণ্ড পূরণ করেন তবে আপনি একটি **আপগ্রেড** বোতাম দেখতে পারেন৷

প্রক্রিয়া শুরু করতে, এই পদক্ষেপগুলি অনুসরণ করুন:

1.  [এআই স্টুডিও এপিআই কী পৃষ্ঠায়](https://aistudio.google.com/app/apikey?hl=bn) যান।
2.  আপনি যে প্রজেক্টটি পেইড প্ল্যানে যেতে চান সেটি খুঁজুন এবং প্রদর্শিত বোতামের উপর নির্ভর করে **বিলিং সেট আপ করুন** বা **আপগ্রেড করুন এ** ক্লিক করুন।
3.  পরবর্তী ধাপ আপনি ক্লিক করা বোতামের উপর নির্ভর করে:
    *   **আপনি যদি সেট আপ বিলিং ক্লিক করেন:** আপনার প্রকল্পে একটি বিলিং অ্যাকাউন্ট লিঙ্ক করতে আপনাকে Google ক্লাউড কনসোলে পুনঃনির্দেশিত করা হবে৷ প্রক্রিয়াটি সম্পূর্ণ করতে অন-স্ক্রীন নির্দেশাবলী অনুসরণ করুন।
    *   **আপনি আপগ্রেড ক্লিক করলে:** সিস্টেম স্বয়ংক্রিয়ভাবে আপনার প্রকল্পের যোগ্যতা যাচাই করবে। যদি আপনার প্রকল্পটি সমস্ত প্রয়োজনীয়তা পূরণ করে, তাহলে তা অবিলম্বে পরবর্তী স্তরে আপগ্রেড করা হবে।

### কেন প্রদত্ত স্তর ব্যবহার?

আপনি যখন বিলিং সক্ষম করেন এবং প্রদত্ত স্তর ব্যবহার করেন, আপনি [উচ্চ হারের সীমা](https://ai.google.dev/gemini-api/docs/rate-limits?hl=bn) থেকে উপকৃত হন এবং আপনার প্রম্পট এবং প্রতিক্রিয়াগুলি Google পণ্যগুলিকে উন্নত করতে ব্যবহার করা হয় না৷ প্রদত্ত পরিষেবার জন্য ডেটা ব্যবহার সম্পর্কে আরও তথ্যের জন্য, [পরিষেবার শর্তাবলী](https://ai.google.dev/gemini-api/terms?hl=bn#data-use-paid) দেখুন৷

### ক্লাউড বিলিং

Gemini API বিলিং পরিষেবার জন্য [ক্লাউড বিলিং](https://cloud.google.com/billing/docs/concepts?hl=bn) ব্যবহার করে। প্রদত্ত স্তর ব্যবহার করতে, আপনাকে অবশ্যই আপনার ক্লাউড প্রকল্পে ক্লাউড বিলিং সেট আপ করতে হবে৷ আপনি ক্লাউড বিলিং সক্ষম করার পরে, আপনি খরচ ট্র্যাক করতে, খরচ বুঝতে, অর্থপ্রদান করতে এবং ক্লাউড বিলিং সমর্থন অ্যাক্সেস করতে ক্লাউড বিলিং সরঞ্জামগুলি ব্যবহার করতে পারেন৷

## বিলিং সক্ষম করুন

আপনি Google AI স্টুডিও থেকে শুরু করে ক্লাউড বিলিং সক্ষম করতে পারেন:

1.  [গুগল এআই স্টুডিও](https://aistudio.google.com/?hl=bn) খুলুন।
    
2.  বাম সাইডবারের নীচে, **সেটিংস** > **পরিকল্পনা তথ্য** নির্বাচন করুন।
    
3.  ক্লাউড বিলিং সক্ষম করতে আপনার নির্বাচিত প্রকল্পের জন্য **সেট আপ বিলিং এ** ক্লিক করুন৷
    

## ব্যবহার মনিটর

আপনি ক্লাউড বিলিং সক্ষম করার পরে, আপনি [Google ক্লাউড কনসোলে](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com?hl=bn) আপনার Gemini API-এর ব্যবহার নিরীক্ষণ করতে পারেন৷ API-এর পরিষেবার নাম হল `generativelanguage.googleapis.com` , এবং কনসোলে Gemini API-কে **Generative Language API** নামেও উল্লেখ করা হয়।

![Cloud console](https://ai.google.dev/static/gemini-api/docs/images/console-apis-services.png?hl=bn "ক্লাউড কনসোল")

আরও জানতে, [API ব্যবহার নিরীক্ষণের](https://cloud.google.com/apis/docs/monitoring?hl=bn) জন্য Google ক্লাউড ডকুমেন্টেশন দেখুন।

## প্রায়শই জিজ্ঞাসিত প্রশ্ন

এই বিভাগটি প্রায়শই জিজ্ঞাসিত প্রশ্নের উত্তর প্রদান করে।

### আমি কি জন্য বিল করা হয়?

Gemini API মূল্য নিম্নলিখিত উপর ভিত্তি করে:

*   ইনপুট টোকেন গণনা
*   আউটপুট টোকেন গণনা
*   ক্যাশে টোকেন গণনা
*   ক্যাশে টোকেন স্টোরেজ সময়কাল

মূল্য সংক্রান্ত তথ্যের জন্য, [মূল্য পৃষ্ঠাটি](https://ai.google.dev/pricing?hl=bn) দেখুন।

### আমি আমার কোটা কোথায় দেখতে পারি?

আপনি [Google ক্লাউড কনসোলে](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas?hl=bn) আপনার কোটা এবং সিস্টেমের সীমা দেখতে পারেন৷

### আমি কিভাবে আরও কোটা অনুরোধ করব?

আরও কোটার অনুরোধ করতে, [কিভাবে আপগ্রেডের অনুরোধ করতে হয়-](#request-an-upgrade) এর নির্দেশাবলী অনুসরণ করুন।

### আমি কি EEA (EU সহ), UK এবং CH-তে বিনামূল্যে Gemini API ব্যবহার করতে পারি?

হ্যাঁ, আমরা [অনেক অঞ্চলে](https://ai.google.dev/gemini-api/docs/available-regions?hl=bn) বিনামূল্যে স্তর এবং অর্থ প্রদানের স্তর উপলব্ধ করি৷

### আমি যদি Gemini API দিয়ে বিলিং সেট আপ করি, তাহলে কি আমার Google AI স্টুডিও ব্যবহারের জন্য চার্জ করা হবে?

না, আপনি সমস্ত সমর্থিত অঞ্চল জুড়ে বিলিং সেট-আপ করলেও তা নির্বিশেষে Google AI স্টুডিও ব্যবহার বিনামূল্যে থাকবে।

### আমি কি মুক্ত স্তরে 1M টোকেন ব্যবহার করতে পারি?

জেমিনি API-এর জন্য বিনামূল্যের স্তর নির্বাচিত মডেলের উপর ভিত্তি করে পৃথক হয়। আপাতত, আপনি নিম্নলিখিত উপায়ে 1M টোকেন প্রসঙ্গ উইন্ডোটি চেষ্টা করতে পারেন:

*   গুগল এআই স্টুডিওতে
*   সাথে পে-যেমন-ই-গো প্ল্যান
*   নির্বাচিত মডেলের জন্য বিনামূল্যের প্ল্যান সহ

[রেট লিমিট পৃষ্ঠায়](https://ai.google.dev/gemini-api/docs/rate-limits?hl=bn) মডেল প্রতি সর্বশেষ বিনামূল্যের হারের সীমা দেখুন।

### আমি যে টোকেন ব্যবহার করছি তার সংখ্যা কীভাবে গণনা করতে পারি?

টোকেনের সংখ্যা গণনা করতে [`GenerativeModel.count_tokens`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel?hl=bn#count_tokens) পদ্ধতি ব্যবহার করুন। টোকেন সম্পর্কে আরও জানতে [টোকেন গাইড](https://ai.google.dev/gemini-api/docs/tokens?hl=bn) পড়ুন।

### আমি কি আমার Google ক্লাউড ক্রেডিট Gemini API এর সাথে ব্যবহার করতে পারি?

হ্যাঁ, Google ক্লাউড ক্রেডিটগুলি Gemini API ব্যবহারের জন্য ব্যবহার করা যেতে পারে৷

### কিভাবে বিলিং পরিচালনা করা হয়?

Gemini API-এর জন্য বিলিং [ক্লাউড বিলিং](https://cloud.google.com/billing/docs/concepts?hl=bn) সিস্টেম দ্বারা পরিচালিত হয়।

### আমি ব্যর্থ অনুরোধের জন্য চার্জ করা হয়?

আপনার অনুরোধ 400 বা 500 ত্রুটির সাথে ব্যর্থ হলে, আপনাকে ব্যবহৃত টোকেনের জন্য চার্জ করা হবে না। যাইহোক, অনুরোধটি এখনও আপনার কোটার সাথে গণনা করা হবে।

### মডেল সূক্ষ্ম-টিউনিং জন্য একটি চার্জ আছে?

[মডেল টিউনিং](https://ai.google.dev/gemini-api/docs/model-tuning?hl=bn) বিনামূল্যে, কিন্তু টিউন করা মডেলের অনুমান বেস মডেলের মতো একই হারে চার্জ করা হয়।

### GetTokens বিল করা হয়?

GetTokens API-এর অনুরোধগুলি বিল করা হয় না এবং সেগুলি অনুমান কোটার সাথে গণনা করা হয় না।

### যদি আমার একটি প্রদত্ত API অ্যাকাউন্ট থাকে তবে আমার Google AI স্টুডিও ডেটা কীভাবে পরিচালনা করা হয়?

ক্লাউড বিলিং সক্ষম হলে ডেটা কীভাবে পরিচালনা করা হয় তার বিশদ বিবরণের জন্য [শর্তাবলী](https://ai.google.dev/gemini-api/terms?hl=bn#paid-services) পড়ুন ("প্রদান পরিষেবাগুলির" অধীনে "Google আপনার ডেটা কীভাবে ব্যবহার করে" দেখুন)। মনে রাখবেন যে আপনার Google AI স্টুডিও প্রম্পটগুলিকে একই "পেইড পরিষেবা" শর্তাবলীর অধীনে বিবেচনা করা হয় যতক্ষণ না অন্তত 1টি API প্রকল্পের বিলিং সক্ষম করা থাকে, আপনি যদি "প্ল্যান" এর অধীনে "প্রদত্ত" হিসাবে চিহ্নিত কোনও প্রকল্প দেখতে পান তবে আপনি [Gemini API কী পৃষ্ঠায়](https://aistudio.google.com/apikey?hl=bn) যাচাই করতে পারেন।

### আমি কোথায় বিলিং এর জন্য সাহায্য পেতে পারি?

বিলিং সংক্রান্ত সাহায্য পেতে, [ক্লাউড বিলিং সমর্থন পান](https://cloud.google.com/support/billing?hl=bn) দেখুন।

মতামত জানান

অন্য কিছু উল্লেখ না করা থাকলে, এই পৃষ্ঠার কন্টেন্ট [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/)\-এর অধীনে এবং কোডের নমুনাগুলি [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0)\-এর অধীনে লাইসেন্স প্রাপ্ত। আরও জানতে, [Google Developers সাইট নীতি](https://developers.google.com/site-policies?hl=bn) দেখুন। Java হল Oracle এবং/অথবা তার অ্যাফিলিয়েট সংস্থার রেজিস্টার্ড ট্রেডমার্ক।

2025-05-31 UTC-তে শেষবার আপডেট করা হয়েছে।

---

# Content from: https://ai.google.dev/gemini-api/docs/music-generation

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Music generation using Lyria RealTime

The Gemini API, using [Lyria RealTime](https://deepmind.google/technologies/lyria/realtime/), provides access to a state-of-the-art, real-time, streaming music generation model. It allows developers to build applications where users can interactively create, continuously steer, and perform instrumental music.

To experience what can be built using Lyria RealTime, try it on AI Studio using the [Prompt DJ](https://aistudio.google.com/apps/bundled/promptdj) or the [MIDI DJ](https://aistudio.google.com/apps/bundled/promptdj-midi) apps!

## How music generation works

Lyria RealTime music generation uses a persistent, bidirectional, low-latency streaming connection using [WebSocket](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).

## Generate and control music

Lyria RealTime works a bit like the [Live API](https://ai.google.dev/gemini-api/docs/live) in the sense that it is using websockets to keep a real-time communication with the model. It's still not exactly the same as you can't talk to the model and you have to use a specific format to prompt it.

The following code demonstrates how to generate music:

### Python

This example initializes the Lyria RealTime session using `client.aio.live.music.connect()`, then sends an initial prompt with `session.set_weighted_prompts()` along with an initial configuration using `session.set_music_generation_config`, starts the music generation using `session.play()` and sets up `receive_audio()` to process the audio chunks it receives.

  ```
  import asyncio
  from google import genai
  from google.genai import types

  client = genai.Client(http_options={'api_version': 'v1alpha'})

  async def main():
      async def receive_audio(session):
        """Example background task to process incoming audio."""
        while True:
          async for message in session.receive():
            audio_data = message.server_content.audio_chunks[0].data
            # Process audio...
            await asyncio.sleep(10**-12)

      async with (
        client.aio.live.music.connect(model='models/lyria-realtime-exp') as session,
        asyncio.TaskGroup() as tg,
      ):
        # Set up task to receive server messages.
        tg.create_task(receive_audio(session))

        # Send initial prompts and config
        await session.set_weighted_prompts(
          prompts=[
            types.WeightedPrompt(text='minimal techno', weight=1.0),
          ]
        )
        await session.set_music_generation_config(
          config=types.LiveMusicGenerationConfig(bpm=90, temperature=1.0)
        )

        # Start streaming music
        await session.play()
  if __name__ == "__main__":
      asyncio.run(main())
```

### JavaScript

This example initializes the Lyria RealTime session using `client.live.music.connect()`, then sends an initial prompt with `session.setWeightedPrompts()` along with an initial configuration using `session.setMusicGenerationConfig`, starts the music generation using `session.play()` and sets up an `onMessage` callback to process the audio chunks it receives.

```
import { GoogleGenAI } from '@google/genai';

const ai = new GoogleGenAI({
  apiVersion: 'v1alpha',
});

// Create session object to control music generation.
const session: MusicSession = client.live.music.connect({
  model: 'models/lyria-realtime-exp',
  callbacks: {
    onMessage: (message) => {
      // Application logic: buffer and play using Web Audio API etc.
    },
    onError: (error) => {
      console.error('music session error:', error);
    },
    onClose: () => {
      console.log('Lyria RealTime stream closed.');
    }
  }
}); 

// Send initial prompts and config
await session.setWeightedPrompts({
  weightedPrompts: [{ text: 'minimal techno', weight: 1.0 }],
});
await session.setMusicGenerationConfig({
  musicGenerationConfig: { bpm: 90, temperature: 1.0 },
});

// Start generation
await session.play();
```

You can then use `session.play()`, `session.pause()`, `session.stop()` and `session.reset_context()` to start, pause, stop or reset the session.

## Steer music in real-time

### Prompt Lyria RealTime

While the stream is active, you can send new `WeightedPrompt` messages at any time to alter the generated music. The model will smoothly transition based on the new input.

The prompts need to follow the right format with a `text` (the actual prompt), and a `weight`. The `weight` can take any value except `0`. `1.0` is usually a good starting point.

### Python

  ```
  await session.set_weighted_prompts(
    prompts=[
      {"text": "Piano", "weight": 2.0},
      types.WeightedPrompt(text="Meditation", weight=0.5),
      types.WeightedPrompt(text="Live Performance", weight=1.0),
    ]
  )
```

### JavaScript

  ```
  await session.setMusicGenerationConfig({
    weightedPrompts: [
      { text: 'Harmonica', weight: 0.3 },
      { text: 'Afrobeat', weight: 0.7 }
    ],
  });
```

Note that the model transitions can be a bit abrupt when drastically changing the prompts so it's recommended to implement some kind of cross-fading by sending intermediate weight values to the model.

### Update the configuration

You can also update the music generation parameters in real time. You can't just update a parameter, you need to set the whole configuration otherwise the other fields will be reset back to their default values.

Since updating the bpm or the scale is a drastic change for the model you'll also need to tell it to reset its context using `reset_context()` to take the new config into account. It won't stop the stream, but it will be a hard transition. You don't need to do it for the other parameters.

### Python

  ```
  await session.set_music_generation_config(
    config=types.LiveMusicGenerationConfig(
      bpm=128,
      scale=types.Scale.D_MAJOR_B_MINOR,
    )
  )
  await session.reset_context();
```

### JavaScript

  ```
  await session.setMusicGenerationConfig({
    musicGenerationConfig: { bpm: 120, density: 0.75 },
  });
  await session.reset_context();
```

## Prompt guide for Lyria RealTime

Here's a non-exhaustive list of prompts you can use to prompt Lyria RealTime:

*   Instruments: `303 Acid Bass, 808 Hip Hop Beat, Accordion, Alto Saxophone, Bagpipes, Balalaika Ensemble, Banjo, Bass Clarinet, Bongos, Boomy Bass, Bouzouki, Buchla Synths, Cello, Charango, Clavichord, Conga Drums, Didgeridoo, Dirty Synths, Djembe, Drumline, Dulcimer, Fiddle, Flamenco Guitar, Funk Drums, Glockenspiel, Guitar, Hang Drum, Harmonica, Harp, Harpsichord, Hurdy-gurdy, Kalimba, Koto, Lyre, Mandolin, Maracas, Marimba, Mbira, Mellotron, Metallic Twang, Moog Oscillations, Ocarina, Persian Tar, Pipa, Precision Bass, Ragtime Piano, Rhodes Piano, Shamisen, Shredding Guitar, Sitar, Slide Guitar, Smooth Pianos, Spacey Synths, Steel Drum, Synth Pads, Tabla, TR-909 Drum Machine, Trumpet, Tuba, Vibraphone, Viola Ensemble, Warm Acoustic Guitar, Woodwinds, ...`
*   Music Genre: `Acid Jazz, Afrobeat, Alternative Country, Baroque, Bengal Baul, Bhangra, Bluegrass, Blues Rock, Bossa Nova, Breakbeat, Celtic Folk, Chillout, Chiptune, Classic Rock, Contemporary R&B, Cumbia, Deep House, Disco Funk, Drum & Bass, Dubstep, EDM, Electro Swing, Funk Metal, G-funk, Garage Rock, Glitch Hop, Grime, Hyperpop, Indian Classical, Indie Electronic, Indie Folk, Indie Pop, Irish Folk, Jam Band, Jamaican Dub, Jazz Fusion, Latin Jazz, Lo-Fi Hip Hop, Marching Band, Merengue, New Jack Swing, Minimal Techno, Moombahton, Neo-Soul, Orchestral Score, Piano Ballad, Polka, Post-Punk, 60s Psychedelic Rock, Psytrance, R&B, Reggae, Reggaeton, Renaissance Music, Salsa, Shoegaze, Ska, Surf Rock, Synthpop, Techno, Trance, Trap Beat, Trip Hop, Vaporwave, Witch house, ...`
*   Mood/Description: `Acoustic Instruments, Ambient, Bright Tones, Chill, Crunchy Distortion, Danceable, Dreamy, Echo, Emotional, Ethereal Ambience, Experimental, Fat Beats, Funky, Glitchy Effects, Huge Drop, Live Performance, Lo-fi, Ominous Drone, Psychedelic, Rich Orchestration, Saturated Tones, Subdued Melody, Sustained Chords, Swirling Phasers, Tight Groove, Unsettling, Upbeat, Virtuoso, Weird Noises, ...`

These are just some examples, Lyria RealTime can do much more. Experiment with your own prompts!

## Best practices

*   Client applications must implement robust audio buffering to ensure smooth playback. This helps account for network jitter and slight variations in generation latency.
*   Effective prompting:
    *   Be descriptive. Use adjectives describing mood, genre, and instrumentation.
    *   Iterate and steer gradually. Rather than completely changing the prompt, try adding or modifying elements to morph the music more smoothly.
    *   Experiment with weight on `WeightedPrompt` to influence how strongly a new prompt affects the ongoing generation.

## Technical details

This section describes the specifics of how to use Lyria RealTime music generation.

### Specifications

*   Output format: Raw 16-bit PCM Audio
*   Sample rate: 48kHz
*   Channels: 2 (stereo)

### Controls

Music generation can be influenced in real time by sending messages containing:

*   `WeightedPrompt`: A text string describing a musical idea, genre, instrument, mood, or characteristic. Multiple prompts can potentially be supplied to blend influences. See [above](/gemini-api/docs/:#steer-music) for more details on how to best prompt Lyria RealTime.
*   `MusicGenerationConfig`: Configuration for the music generation process, influencing the characteristics of the output audio.). Parameters include:
    *   `guidance`: (float) Range: `[0.0, 6.0]`. Default: `4.0`. Controls how strictly the model follows the prompts. Higher guidance improves adherence to the prompt, but makes transitions more abrupt.
    *   `bpm`: (int) Range: `[60, 200]`. Sets the Beats Per Minute you want for the generated music. You need to stop/play or reset the context for the model it take into account the new bpm.
    *   `density`: (float) Range: `[0.0, 1.0]`. Controls the density of musical notes/sounds. Lower values produce sparser music; higher values produce "busier" music.
    *   `brightness`: (float) Range: `[0.0, 1.0]`. Adjusts the tonal quality. Higher values produce "brighter" sounding audio, generally emphasizing higher frequencies.
    *   `scale`: (Enum) Sets the musical scale (Key and Mode) for the generation. Use the [`Scale` enum values](#scale-enum) provided by the SDK. You need to stop/play or reset the context for the model it take into account the new scale.
    *   `mute_bass`: (bool) Default: `False`. Controls whether the model reduces the outputs' bass.
    *   `mute_drums`: (bool) Default: `False`. Controls whether the model outputs reduces the outputs' drums.
    *   `only_bass_and_drums`: (bool) Default: `False`. Steer the model to try to only output bass and drums.
*   `PlaybackControl`: Commands to control playback aspects, such as play, pause, stop or reset the context.

For `bpm`, `density`, `brightness` and `scale`, if no value is provided, the model will decide what's best according to your initial prompts.

More classical parameters like `temperature` (0.0 to 3.0, default 1.1), `top_k` (1 to 1000, default 40), and `seed` (0 to 2 147 483 647, randomly selected by default) are also customizable in the `MusicGenerationConfig`.

#### Scale Enum Values

Here are all the scale values that the model can accept:

Enum Value

Scale / Key

`C_MAJOR_A_MINOR`

C major / A minor

`D_FLAT_MAJOR_B_FLAT_MINOR`

D♭ major / B♭ minor

`D_MAJOR_B_MINOR`

D major / B minor

`E_FLAT_MAJOR_C_MINOR`

E♭ major / C minor

`E_MAJOR_D_FLAT_MINOR`

E major / C♯/D♭ minor

`F_MAJOR_D_MINOR`

F major / D minor

`G_FLAT_MAJOR_E_FLAT_MINOR`

G♭ major / E♭ minor

`G_MAJOR_E_MINOR`

G major / E minor

`A_FLAT_MAJOR_F_MINOR`

A♭ major / F minor

`A_MAJOR_G_FLAT_MINOR`

A major / F♯/G♭ minor

`B_FLAT_MAJOR_G_MINOR`

B♭ major / G minor

`B_MAJOR_A_FLAT_MINOR`

B major / G♯/A♭ minor

`SCALE_UNSPECIFIED`

Default / The model decides

The model is capable of guiding the notes that are played, but does not distinguish between relative keys. Thus each enum corresponds both to the relative major and minor. For example, `C_MAJOR_A_MINOR` would correspond to all the white keys of a piano, and `F_MAJOR_D_MINOR` would be all the white keys except B flat.

### Limitations

*   Instrumental only: The model generates instrumental music only.
*   Safety: Prompts are checked by safety filters. Prompts triggering the filters will be ignored in which case an explanation will be written in the output's `filtered_prompt` field.
*   Watermarking: Output audio is always watermarked for identification following our [Responsible AI](https://ai.google/responsibility/principles/) principles.

## What's next

*   Instead of music, learn how to generate multi-speakers conversation using the [TTS models](https://ai.google.dev/gemini-api/docs/audio-generation),
*   Discover how to generate [images](https://ai.google.dev/gemini-api/docs/image-generation) or [videos](https://ai.google.dev/gemini-api/docs/video),
*   Instead of generation music or audio, find out how to Gemini can [understand Audio files](https://ai.google.dev/gemini-api/docs/audio),
*   Have a real-time conversation with Gemini using the [Live API](https://ai.google.dev/gemini-api/docs/live).

Explore the [Cookbook](https://github.com/google-gemini/cookbook) for more code examples and tutorials.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/thinking

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini thinking

The [Gemini 2.5 series models](/gemini-api/docs/models) use an internal "thinking process" that significantly improves their reasoning and multi-step planning abilities, making them highly effective for complex tasks such as coding, advanced mathematics, and data analysis.

This guide shows you how to work with Gemini's thinking capabilities using the Gemini API.

## Before you begin

Ensure you use a supported 2.5 series model for thinking. You might find it beneficial to explore these models in AI Studio before diving into the API:

*   [Try Gemini 2.5 Flash in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash)
*   [Try Gemini 2.5 Pro in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro)
*   [Try Gemini 2.5 Flash-Lite Preview in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-lite-preview-06-17)

## Generating content with thinking

Initiating a request with a thinking model is similar to any other content generation request. The key difference lies in specifying one of the [models with thinking support](#supported-models) in the `model` field, as demonstrated in the following [text generation](/gemini-api/docs/text-generation#text-input) example:

### Python

```
from google import genai

client = genai.Client()
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents=prompt
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: prompt,
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example."
  model := "gemini-2.5-pro"

  resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil)

  fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
   "contents": [
     {
       "parts": [
         {
           "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example."
         }
       ]
     }
   ]
 }'
 ```
```

## Thinking budgets

The `thinkingBudget` parameter guides the model on the number of thinking tokens to use when generating a response. A higher token count generally allows for more detailed reasoning, which can be beneficial for tackling more [complex tasks](#tasks). If latency is more important, use a lower budget or disable thinking by setting `thinkingBudget` to 0. Setting the `thinkingBudget` to -1 turns on **dynamic thinking**, meaning the model will adjust the budget based on the complexity of the request.

The `thinkingBudget` is only [supported](#supported-models) in Gemini 2.5 Flash, 2.5 Pro, and 2.5 Flash-Lite. Depending on the prompt, the model might overflow or underflow the token budget.

The following are `thinkingBudget` configuration details for each model type.

    

Model

Default setting  
(Thinking budget is not set)

Range

Disable thinking

Turn on dynamic thinking

**2.5 Pro**

Dynamic thinking: Model decides when and how much to think

`128` to `32768`

N/A: Cannot disable thinking

`thinkingBudget = -1`

**2.5 Flash**

Dynamic thinking: Model decides when and how much to think

`0` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

**2.5 Flash Lite**

Model does not think

`512` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
        # Turn off thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=0)
        # Turn on dynamic thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=-1)
    ),
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
        // Turn off thinking:
        // thinkingBudget: 0
        // Turn on dynamic thinking:
        // thinkingBudget: -1
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingBudgetVal := int32(1024)

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingBudget: &thinkingBudgetVal,
      // Turn off thinking:
      // ThinkingBudget: int32(0),
      // Turn on dynamic thinking:
      // ThinkingBudget: int32(-1),
    },
  })

fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingBudget": 1024
          # Thinking off:
          # "thinkingBudget": 0
          # Turn on dynamic thinking:
          # "thinkingBudget": -1
    }
  }
}'
```

## Thought summaries

Thought summaries are synthisized versions of the model's raw thoughts and offer insights into the model's internal reasoning process. Note that thinking budgets apply to the model's raw thoughts and not to thought summaries.

You can enable thought summaries by setting `includeThoughts` to `true` in your request configuration. You can then access the summary by iterating through the `response` parameter's `parts`, and checking the `thought` boolean.

Here's an example demonstrating how to enable and retrieve thought summaries without streaming, which returns a single, final thought summary with the response:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro",
  contents=prompt,
  config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought:
    print("Thought summary:")
    print(part.text)
    print()
  else:
    print("Answer:")
    print(part.text)
    print()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "What is the sum of the first 50 prime numbers?",
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (!part.text) {
      continue;
    }
    else if (part.thought) {
      console.log("Thoughts summary:");
      console.log(part.text);
    }
    else {
      console.log("Answer:");
      console.log(part.text);
    }
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text("What is the sum of the first 50 prime numbers?")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for _, part := range resp.Candidates[0].Content.Parts {
    if part.Text != "" {
      if part.Thought {
        fmt.Println("Thoughts Summary:")
        fmt.Println(part.Text)
      } else {
        fmt.Println("Answer:")
        fmt.Println(part.Text)
      }
    }
  }
}
```

And here is an example using thinking with streaming, which returns rolling, incremental summaries during generation:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = """
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
"""

thoughts = ""
answer = ""

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-pro",
    contents=prompt,
    config=types.GenerateContentConfig(
      thinking_config=types.ThinkingConfig(
        include_thoughts=True
      )
    )
):
  for part in chunk.candidates[0].content.parts:
    if not part.text:
      continue
    elif part.thought:
      if not thoughts:
        print("Thoughts summary:")
      print(part.text)
      thoughts += part.text
    else:
      if not answer:
        print("Thoughts summary:")
      print(part.text)
      answer += part.text
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `Alice, Bob, and Carol each live in a different house on the same
street: red, green, and blue. The person who lives in the red house owns a cat.
Bob does not live in the green house. Carol owns a dog. The green house is to
the left of the red house. Alice does not own a cat. Who lives in each house,
and what pet do they own?`;

let thoughts = "";
let answer = "";

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-pro",
    contents: prompt,
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for await (const chunk of response) {
    for (const part of chunk.candidates[0].content.parts) {
      if (!part.text) {
        continue;
      } else if (part.thought) {
        if (!thoughts) {
          console.log("Thoughts summary:");
        }
        console.log(part.text);
        thoughts = thoughts + part.text;
      } else {
        if (!answer) {
          console.log("Answer:");
        }
        console.log(part.text);
        answer = answer + part.text;
      }
    }
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

## Pricing

When thinking is turned on, response pricing is the sum of output tokens and thinking tokens. You can get the total number of generated thinking tokens from the `thoughtsTokenCount` field.

### Python

```
# ...
print("Thoughts tokens:",response.usage_metadata.thoughts_token_count)
print("Output tokens:",response.usage_metadata.candidates_token_count)
```

### JavaScript

```
// ...
console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`);
console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`);
```

### Go

```
// ...
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
  log.Fatal(err)
}
fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count))
fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count))
```

Thinking models generate full thoughts to improve the quality of the final response, and then output [summaries](#summaries) to provide insight into the thought process. So, pricing is based on the full thought tokens the model needs to generate to create a summary, despite only the summary being output from the API.

You can learn more about tokens in the [Token counting](/gemini-api/docs/tokens) guide.

## Supported models

Thinking features are supported on all the 2.5 series models. You can find all model capabilities on the [model overview](/gemini-api/docs/models) page.

## Best practices

This section includes some guidance for using thinking models efficiently. As always, following our [prompting guidance and best practices](/gemini-api/docs/prompting-strategies) will get you the best results.

### Debugging and steering

*   **Review reasoning**: When you're not getting your expected response from the thinking models, it can help to carefully analyze Gemini's thought summaries. You can see how it broke down the task and arrived at its conclusion, and use that information to correct towards the right results.
    
*   **Provide Guidance in Reasoning**: If you're hoping for a particularly lengthy output, you may want to provide guidance in your prompt to constrain the [amount of thinking](#set-budget) the model uses. This lets you reserve more of the token output for your response.
    

### Task complexity

*   **Easy Tasks (Thinking could be OFF):** For straightforward requests where complex reasoning isn't required, such as fact retrieval or classification, thinking is not required. Examples include:
    *   "Where was DeepMind founded?"
    *   "Is this email asking for a meeting or just providing information?"
*   **Medium Tasks (Default/Some Thinking):** Many common requests benefit from a degree of step-by-step processing or deeper understanding. Gemini can flexibly use thinking capability for tasks like:
    *   Analogize photosynthesis and growing up.
    *   Compare and contrast electric cars and hybrid cars.
*   **Hard Tasks (Maximum Thinking Capability):** For truly complex challenges, such as solving complex math problems or coding tasks, we recommend setting a high thinking budget. These types of tasks require the model needs to engage its full reasoning and planning capabilities, often involving many internal steps before providing an answer. Examples include:
    *   Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for which 17b is a divisor of 97b.
    *   Write Python code for a web application that visualizes real-time stock market data, including user authentication. Make it as efficient as possible.

## Thinking with tools and capabilities

Thinking models work with all of Gemini's tools and capabilities. This allows the models to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.

*   The [search tool](/gemini-api/docs/grounding) allows the model to query Google Search to find up-to-date information or information beyond its training data. This is useful for questions about recent events or highly specific topics.
    
*   The [code execution tool](/gemini-api/docs/code-execution) enables the model to generate and run Python code to perform calculations, manipulate data, or solve problems that are best handled algorithmically. The model receives the code's output and can use it in its response.
    
*   With [structured output](/gemini-api/docs/structured-output), you can constrain Gemini to respond with JSON. This is particularly useful for integrating the model's output into applications.
    
*   [Function calling](/gemini-api/docs/function-calling) connects the thinking model to external tools and APIs, so it can reason about when to call the right function and what parameters to provide.
    
*   [URL Context](/gemini-api/docs/url-context) provides the model with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.
    

You can try examples of using tools with thinking models in the [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).

## What's next?

*   To work through more in depth examples, like:
    
    *   Using tools with thinking
    *   Streaming with thinking
    *   Adjusting the thinking budget for different results
    
    and more, try our [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).
    
*   Thinking coverage is now available in our [OpenAI Compatibility](/gemini-api/docs/openai#thinking) guide.
    
*   For more info about Gemini 2.5 Pro, Gemini Flash 2.5, and Gemini 2.5 Flash-Lite, visit the [model page](/gemini-api/docs/models).
    

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/long-context

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# 长上下文

许多 Gemini 模型都具有大上下文窗口，可容纳 100 万个或更多词元。过去，大语言模型 (LLM) 受到一次可传递给模型的文本（或词元）数量的极大限制。Gemini 长上下文窗口发掘了许多新的应用场景和开发者模式。

您已用于[文本生成](https://ai.google.dev/gemini-api/docs/text-generation?hl=zh-cn)或[多模态输入](https://ai.google.dev/gemini-api/docs/vision?hl=zh-cn)等场景的代码可以直接用于长上下文，无需进行任何更改。

本文档简要介绍了使用上下文窗口大小为 100 万个词元或以上的模型可以实现哪些功能。本页面简要介绍了上下文窗口，并探讨了开发者应如何考虑长上下文、长上下文的各种实际应用场景，以及优化长上下文使用的方法。

如需了解特定模型的上下文窗口大小，请参阅[模型](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)页面。

## 什么是上下文窗口？

使用 Gemini 模型的基本方法是将信息（上下文）传递给模型，模型随后会生成回答。上下文窗口可以比作短期记忆。人们的短期记忆可以存储有限的信息量，生成模型也是如此。

您可以参阅我们的[生成模型指南](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=zh-cn#under-the-hood)，详细了解模型在后台的工作原理。

## 开始使用长上下文

早期版本的生成式模型一次只能处理 8,000 个令牌。较新的模型进一步提高了此数字，可接受 32,000 个甚至 128,000 个词元。Gemini 是第一个能够接受 100 万个令牌的模型。

在实际中，100 万个词元相当于：

*   50,000 行代码（标准为每行 80 个字符）
*   您在过去 5 年内发送的所有短信
*   8 部平均长度的英语小说
*   200 多个平均时长播客剧集的转写内容

许多其他模型中常见的上下文窗口较为有限，因此通常需要采用一些策略，例如任意舍弃旧消息、总结内容、将 RAG 与向量数据库搭配使用，或过滤提示以节省令牌。

虽然这些技术在特定场景中仍然很有价值，但 Gemini 的宽广上下文窗口更提倡采用更直接的方法：预先提供所有相关信息。由于 Gemini 模型是专门构建的，具有强大的上下文处理能力，因此它们具有强大的上下文学习能力。例如，仅使用情境教学材料（500 页的参考语法书、一本字典和大约 400 个平行句子），Gemini 就[学会了从英语翻译成 Kalamang](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)（一种使用人数不到 200 人的巴布亚新几内亚语言），翻译质量与使用相同材料的人类学习者相当。这说明了 Gemini 的长上下文带来的范式转变，通过强大的上下文学习功能，赋予了新可能。

## 长上下文应用场景

虽然大多数生成模型的标准应用场景仍然是文本输入，但 Gemini 模型系列可实现多模态应用场景的新模式。这些模型可采用原生方式理解文本、视频、音频和图片。它们附带[可接受多模态文件类型的 Gemini API](https://ai.google.dev/gemini-api/docs/prompting_with_media?hl=zh-cn)，以方便使用。

### 长文本

事实证明，文本是支撑 LLM 大部分发展势头的智能层。如前所述，LLM 的很多实际限制是因为没有足够大的上下文窗口来执行某些任务。这导致了检索增强生成 (RAG) 和其他技术的快速采用，这些技术可为模型动态提供相关的上下文信息。现在，随着上下文窗口越来越大，出现了一些新技术可用于发掘新的应用场景。

基于文本的长上下文的一些新兴和标准应用场景包括：

*   总结大型文本语料库
    *   之前使用较小上下文模型的总结方法需要使用滑动窗口或其他技术，以便在新词元传递给模型时保留之前部分的状态
*   问答
    *   过去在上下文数量有限且模型的真实召回率较低的情况下，只有使用 RAG 才能实现这一目的
*   智能体工作流
    *   文本是智能体如何保存已完成的操作和需要执行的操作的状态的基础；如果没有关于实际世界和智能体目标的足够信息，会限制智能体的可靠性

[多样本上下文学习](https://arxiv.org/pdf/2404.11018)是长上下文模型发掘的最独特功能之一。研究表明，采用常见的“单样本”或“多样本”示例模式，在其中向模型提供一个或几个任务示例，然后扩展到多达数百个、数千个甚至数十万个示例，这可能形成全新的模型功能。事实证明，这种多样本方法的性能与针对特定任务进行了微调的模型类似。对于 Gemini 模型的性能尚不足以满足生产发布的应用场景，您可以尝试多样本方法。正如您稍后将在长上下文优化部分中所了解的那样，上下文缓存使这种高输入词元工作负载类型在经济上更加可行，在某些场景中甚至可降低延迟。

### 长视频

无法访问媒体本身长期以来一直限制着视频内容的实用性。浏览内容并非易事，转写通常无法捕获视频的细微差别，而且大多数工具无法同时处理图片、文本和音频。借助 Gemini，长上下文文本功能可转换为以持续的性能推理和回答有关多模态输入的问题的能力。

视频长上下文的一些新兴和标准应用场景包括：

*   视频问答
*   视频内存，如 [Google 的 Project Astra](https://deepmind.google/technologies/gemini/project-astra/?hl=zh-cn) 所示
*   视频字幕
*   视频推荐系统，通过新的多模态理解来丰富现有元数据
*   视频自定义，可查看数据以及关联视频元数据的语料库，然后移除与观看者无关的视频部分
*   视频内容审核
*   实时视频处理

处理视频时，重要的是考虑如何[将视频处理为词元](https://ai.google.dev/gemini-api/docs/tokens?hl=zh-cn#media-token)，这会影响结算和用量限额。如需详细了解如何使用视频文件进行提示，请参阅[提示指南](https://ai.google.dev/gemini-api/docs/prompting_with_media?lang=python&hl=zh-cn#prompting-with-videos)。

### 长音频

Gemini 模型是首批能够理解音频的原生多模态大语言模型。传统上，典型的开发者工作流涉及将多个特定于领域的模型（例如语音转文字模型和文本到文本模型）串联起来，以便处理音频。这会导致执行多次往返请求所需的延迟时间增加并且性能下降，这通常归因于多模型设置的分离架构。

音频上下文的一些新兴和标准应用场景包括：

*   实时转写和翻译
*   播客/视频问答
*   会议转写和总结
*   语音助理

如需详细了解如何使用音频文件进行提示，请参阅[提示指南](https://ai.google.dev/gemini-api/docs/prompting_with_media?lang=python&hl=zh-cn#prompting-with-videos)。

## 长上下文优化

使用长上下文和 Gemini 模型时，主要优化方法是使用[上下文缓存](https://ai.google.dev/gemini-api/docs/caching?hl=zh-cn)。除了以前无法在单个请求中处理大量词元之外，另一个主要限制是费用。如果您有一个“与数据聊天”应用，用户在其中上传了 10 个 PDF 文件、一个视频和一些工作文档，那么过去您必须使用更复杂的检索增强生成 (RAG) 工具/框架来处理这些请求，并为移入上下文窗口的词元支付大量费用。现在，您可以缓存用户上传的文件，并按小时为存储这些文件付费。例如，使用 Gemini Flash 时，每个请求的输入/输出费用约为标准输入/输出费用的 1/4，因此如果用户与其数据进行足够多的聊天，便可为作为开发者的您节省大量费用。

## 长上下文限制

在本指南的各个部分中，我们讨论了 Gemini 模型如何在各种“大海捞针”检索评估中实现高性能。这些测试考虑了最基本的设置，您在其中只需寻找一根“针”。如果您要寻找多根“针”或特定信息，模型执行的准确率会有所不同。性能可能会因上下文而变化很大。考虑这一点很重要，因为在检索到正确信息与费用之间存在固有的权衡。您在单个查询中可获得大约 99% 的准确率，但每次发送该查询时，您都必须支付输入词元费用。因此，要检索 100 条信息，如果您需要 99% 的性能，则可能需要发送 100 个请求。这是一个很好的示例，上下文缓存在其中可显著降低与使用 Gemini 模型关联的费用，同时保持高性能。

## 常见问题解答

### 在上下文窗口中，查询的最佳放置位置在哪里？

在大多数情况下（尤其是在总上下文较长的情况下），如果您将询问 / 问题放在提示的末尾（所有其他上下文之后），模型的效果会更好。

### 如果向查询添加更多令牌，模型性能会下降吗？

通常，如果您不需要将令牌传递给模型，最好避免传递令牌。不过，如果您有大量包含某些信息的令牌，并且想要询问与这些信息相关的问题，该模型非常擅长提取这些信息（在许多情况下，准确率高达 99%）。

### 如何通过长情境查询降低费用？

如果您有一组类似的令牌 / 上下文，并且希望多次重复使用，[上下文缓存](https://ai.google.dev/gemini-api/docs/caching?hl=zh-cn)可以帮助您降低与询问此类信息相关的费用。

### 上下文长度是否会影响模型延迟时间？

任何给定请求（无论大小）都会存在一定的延迟时间，但通常，查询越长，延迟时间（首次获取令牌的时间）就越长。

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-05-21。

---

# Content from: https://ai.google.dev/gemini-api/docs/speech-generation

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Генерация речи (преобразование текста в речь)

API Gemini может преобразовывать текстовый ввод в аудио для одного или нескольких говорящих, используя собственные возможности генерации текста в речь (TTS). Генерация текста в речь (TTS) _[управляема](#controllable)_ , то есть вы можете использовать естественный язык для структурирования взаимодействий и управления _стилем_ , _акцентом_ , _темпом_ и _тоном_ аудио.

Возможность TTS отличается от генерации речи, предоставляемой через [Live API](https://ai.google.dev/gemini-api/docs/live?hl=ru) , которая разработана для интерактивного, неструктурированного аудио и мультимодальных входов и выходов. В то время как Live API отлично подходит для динамических разговорных контекстов, TTS через Gemini API предназначен для сценариев, требующих точного чтения текста с детальным контролем стиля и звука, например, для генерации подкастов или аудиокниг.

В этом руководстве показано, как создать из текста аудиозапись для одного или нескольких говорящих.

## Прежде чем начать

Убедитесь, что вы используете вариант модели Gemini 2.5 с собственными возможностями преобразования текста в речь (TTS), как указано в разделе [Поддерживаемые модели](https://ai.google.dev/gemini-api/docs/speech-generation?hl=ru#supported-models) . Для получения оптимальных результатов рассмотрите, какая модель лучше всего подходит для вашего конкретного варианта использования.

Перед началом сборки вам может быть полезно [протестировать модели Gemini 2.5 TTS в AI Studio](https://aistudio.google.com/generate-speech?hl=ru) .

## Преобразование текста в речь одним диктором

Чтобы преобразовать текст в аудиосигнал одного динамика, установите модальность ответа на «аудио» и передайте объект `SpeechConfig` с набором `VoiceConfig` . Вам нужно будет выбрать имя голоса из готовых [выходных голосов](#voices) .

В этом примере выходной аудиосигнал модели сохраняется в файле WAV:

### Питон

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents="Say cheerfully: Have a wonderful day!",
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(
               voice_name='Kore',
            )
         )
      ),
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: 'Say cheerfully: Have a wonderful day!' }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               voiceConfig: {
                  prebuiltVoiceConfig: { voiceName: 'Kore' },
               },
            },
      },
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}
await main();
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
        "contents": [{
          "parts":[{
            "text": "Say cheerfully: Have a wonderful day!"
          }]
        }],
        "generationConfig": {
          "responseModalities": ["AUDIO"],
          "speechConfig": {
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }
        },
        "model": "gemini-2.5-flash-preview-tts",
    }' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
          base64 --decode >out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

## Многоканальное преобразование текста в речь

Для многоканального звука вам понадобится объект `MultiSpeakerVoiceConfig` с каждым динамиком (до 2), настроенным как `SpeakerVoiceConfig` . Вам нужно будет определить каждого `speaker` с теми же именами, которые используются в [приглашении](#controllable) :

### Питон

```
from google import genai
from google.genai import types
import wave

# Set up the wave file to save the output:
def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):
   with wave.open(filename, "wb") as wf:
      wf.setnchannels(channels)
      wf.setsampwidth(sample_width)
      wf.setframerate(rate)
      wf.writeframes(pcm)

client = genai.Client()

prompt = """TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?"""

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=prompt,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Joe',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Jane',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

data = response.candidates[0].content.parts[0].inline_data.data

file_name='out.wav'
wave_file(file_name, data) # Saves the file to current directory
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(
   filename,
   pcmData,
   channels = 1,
   rate = 24000,
   sampleWidth = 2,
) {
   return new Promise((resolve, reject) => {
      const writer = new wav.FileWriter(filename, {
            channels,
            sampleRate: rate,
            bitDepth: sampleWidth * 8,
      });

      writer.on('finish', resolve);
      writer.on('error', reject);

      writer.write(pcmData);
      writer.end();
   });
}

async function main() {
   const ai = new GoogleGenAI({});

   const prompt = `TTS the following conversation between Joe and Jane:
         Joe: How's it going today Jane?
         Jane: Not too bad, how about you?`;

   const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-preview-tts",
      contents: [{ parts: [{ text: prompt }] }],
      config: {
            responseModalities: ['AUDIO'],
            speechConfig: {
               multiSpeakerVoiceConfig: {
                  speakerVoiceConfigs: [
                        {
                           speaker: 'Joe',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Kore' }
                           }
                        },
                        {
                           speaker: 'Jane',
                           voiceConfig: {
                              prebuiltVoiceConfig: { voiceName: 'Puck' }
                           }
                        }
                  ]
               }
            }
      }
   });

   const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
   const audioBuffer = Buffer.from(data, 'base64');

   const fileName = 'out.wav';
   await saveWaveFile(fileName, audioBuffer);
}

await main();
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
  "contents": [{
    "parts":[{
      "text": "TTS the following conversation between Joe and Jane:
                Joe: Hows it going today Jane?
                Jane: Not too bad, how about you?"
    }]
  }],
  "generationConfig": {
    "responseModalities": ["AUDIO"],
    "speechConfig": {
      "multiSpeakerVoiceConfig": {
        "speakerVoiceConfigs": [{
            "speaker": "Joe",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Kore"
              }
            }
          }, {
            "speaker": "Jane",
            "voiceConfig": {
              "prebuiltVoiceConfig": {
                "voiceName": "Puck"
              }
            }
          }]
      }
    }
  },
  "model": "gemini-2.5-flash-preview-tts",
}' | jq -r '.candidates[0].content.parts[0].inlineData.data' | \
    base64 --decode > out.pcm
# You may need to install ffmpeg.
ffmpeg -f s16le -ar 24000 -ac 1 -i out.pcm out.wav
```

## Управление стилем речи с помощью подсказок

Вы можете управлять стилем, тоном, акцентом и темпом, используя подсказки на естественном языке для TTS как для одного, так и для нескольких говорящих. Например, в подсказке для одного говорящего вы можете сказать:

```
Say in an spooky whisper:
"By the pricking of my thumbs...
Something wicked this way comes"
```

В подсказке для нескольких спикеров предоставьте модели имя каждого спикера и соответствующую расшифровку. Вы также можете предоставить руководство для каждого спикера индивидуально:

```
Make Speaker1 sound tired and bored, and Speaker2 sound excited and happy:

Speaker1: So... what's on the agenda today?
Speaker2: You're never going to guess!
```

Попробуйте использовать [вариант голоса](#voices) , который соответствует стилю или эмоции, которую вы хотите передать, чтобы подчеркнуть это еще больше. Например, в предыдущей подсказке придыхание _Энцелада_ может подчеркнуть «усталый» и «скучный», в то время как оптимистичный тон _Пака_ может дополнить «возбужденный» и «счастливый».

## Создание запроса на конвертацию в аудио

Модели TTS выводят только аудио, но вы можете использовать [другие модели](https://ai.google.dev/gemini-api/docs/models?hl=ru) , чтобы сначала создать расшифровку, а затем передать ее в модель TTS для чтения вслух.

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

transcript = client.models.generate_content(
   model="gemini-2.0-flash",
   contents="""Generate a short transcript around 100 words that reads
            like it was clipped from a podcast by excited herpetologists.
            The hosts names are Dr. Anya and Liam.""").text

response = client.models.generate_content(
   model="gemini-2.5-flash-preview-tts",
   contents=transcript,
   config=types.GenerateContentConfig(
      response_modalities=["AUDIO"],
      speech_config=types.SpeechConfig(
         multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(
            speaker_voice_configs=[
               types.SpeakerVoiceConfig(
                  speaker='Dr. Anya',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Kore',
                     )
                  )
               ),
               types.SpeakerVoiceConfig(
                  speaker='Liam',
                  voice_config=types.VoiceConfig(
                     prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name='Puck',
                     )
                  )
               ),
            ]
         )
      )
   )
)

# ...Code to stream or save the output
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {

const transcript = await ai.models.generateContent({
   model: "gemini-2.0-flash",
   contents: "Generate a short transcript around 100 words that reads like it was clipped from a podcast by excited herpetologists. The hosts names are Dr. Anya and Liam.",
   })

const response = await ai.models.generateContent({
   model: "gemini-2.5-flash-preview-tts",
   contents: transcript,
   config: {
      responseModalities: ['AUDIO'],
      speechConfig: {
         multiSpeakerVoiceConfig: {
            speakerVoiceConfigs: [
                   {
                     speaker: "Dr. Anya",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Kore"},
                     }
                  },
                  {
                     speaker: "Liam",
                     voiceConfig: {
                        prebuiltVoiceConfig: {voiceName: "Puck"},
                    }
                  }
                ]
              }
            }
      }
  });
}
// ..JavaScript code for exporting .wav file for output audio

await main();
```

## Голосовые опции

Модели TTS поддерживают следующие 30 голосовых опций в поле `voice_name` :

**Зефир** -- _Яркий_

**Пак** -- _оптимистичный_

**Харон** -- _Информативный_

**Коре** -- _Фирма_

**Фенрир** -- _Возбудимый_

**Леда** -- _Юная_

**Орус** -- _Фирма_

**Aoede** -- _Свежий_

**Каллироя** -- _Легкая на подъем_

**Автоное** -- _Яркое_

**Энцелад** -- _Придыхающийся_

**Япет** -- _Ясно_

**Умбриэль** -- _Легкий в общении_

**Альгиеба** -- _Гладкий_

**Деспина** -- _Гладкая_

**Эриноме** -- _Ясно_

**Альгениб** -- _Гравий_

**Расалгети** -- _Информационный_

**Лаомедея** — _оптимистичная_

**Ахернар** -- _Мягкий_

**Альнилам** -- _Фирма_

**Шедар** -- _Даже_

**Гэкрукс** -- _Зрелый_

**Пульчеррима** -- _Вперед_

**Ахирд** -- _Дружелюбный_

**Зубенелгенуби** -- _Повседневный_

**Виндемиатрикс** -- _Нежная_

**Садачбия** -- _Живой_

**Садалтагер** -- _Знающий_

**Сульфат** -- _Теплый_

Все варианты голосов можно прослушать в [AI Studio](https://aistudio.google.com/generate-speech?hl=ru) .

## Поддерживаемые языки

Модели TTS автоматически определяют язык ввода. Они поддерживают следующие 24 языка:

Язык

Код BCP-47

Язык

Код BCP-47

Арабский (египетский)

`ar-EG`

Немецкий (Германия)

`de-DE`

Английский (США)

`en-US`

Испанский (США)

`es-US`

Французский (Франция)

`fr-FR`

Хинди (Индия)

`hi-IN`

Индонезийский (Индонезия)

`id-ID`

Итальянский (Италия)

`it-IT`

Японский (Япония)

`ja-JP`

Корейский (Корея)

`ko-KR`

Португальский (Бразилия)

`pt-BR`

Русский (Россия)

`ru-RU`

Голландский (Нидерланды)

`nl-NL`

Польский (Польша)

`pl-PL`

Тайский (Таиланд)

`th-TH`

Турецкий (Турция)

`tr-TR`

Вьетнамский (Вьетнам)

`vi-VN`

Румынский (Румыния)

`ro-RO`

Украинский (Украина)

`uk-UA`

Бенгальский (Бангладеш)

`bn-BD`

Английский (Индия)

Комплект `en-IN` `hi-IN` -IN

Маратхи (Индия)

`mr-IN`

Тамильский (Индия)

`ta-IN`

Телугу (Индия)

`te-IN`

## Поддерживаемые модели

Модель

Один динамик

Мультиспикер

[Gemini 2.5 Flash Предварительный просмотр TTS](https://ai.google.dev/gemini-api/docs/models?hl=ru#gemini-2.5-flash-preview-tts)

✔️

✔️

[Gemini 2.5 Pro Предварительный просмотр TTS](https://ai.google.dev/gemini-api/docs/models?hl=ru#gemini-2.5-pro-preview-tts)

✔️

✔️

## Ограничения

*   Модели TTS могут только принимать текстовые входные данные и генерировать аудиовыходы.
*   Сеанс TTS имеет ограничение на [размер контекстного окна](https://ai.google.dev/gemini-api/docs/long-context?hl=ru) в 32 тыс. токенов.
*   Информацию о языковой поддержке см. в разделе [«Языки»](https://ai.google.dev/gemini-api/docs/speech-generation?hl=ru#languages) .

## Что дальше?

*   Попробуйте [кулинарную книгу по созданию аудио](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_TTS.ipynb) .
*   [Live API](https://ai.google.dev/gemini-api/docs/live?hl=ru) Gemini предлагает интерактивные возможности генерации звука, которые можно комбинировать с другими модальностями.
*   Для работы с _аудиовходами_ посетите руководство [по пониманию аудио](https://ai.google.dev/gemini-api/docs/audio?hl=ru) .

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/document-processing

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Document understanding

Python JavaScript Go REST

The Gemini API supports PDF input, including long documents (up to 1000 pages). Gemini models process PDFs with native vision, and are therefore able to understand both text and image contents inside documents. With native PDF vision support, Gemini models are able to:

*   Analyze diagrams, charts, and tables inside documents
*   Extract information into structured output formats
*   Answer questions about visual and text contents in documents
*   Summarize documents
*   Transcribe document content (e.g. to HTML) preserving layouts and formatting, for use in downstream applications

This tutorial demonstrates some possible ways to use the Gemini API to process PDF documents.

## Technical details

Gemini supports a maximum of 1,000 document pages. Document pages must be in one of the following text data MIME types:

*   PDF - `application/pdf`
*   JavaScript - `application/x-javascript`, `text/javascript`
*   Python - `application/x-python`, `text/x-python`
*   TXT - `text/plain`
*   HTML - `text/html`
*   CSS - `text/css`
*   Markdown - `text/md`
*   CSV - `text/csv`
*   XML - `text/xml`
*   RTF - `text/rtf`

Each document page is equivalent to 258 tokens.

While there are no specific limits to the number of pixels in a document besides the model's context window, larger pages are scaled down to a maximum resolution of 3072x3072 while preserving their original aspect ratio, while smaller pages are scaled up to 768x768 pixels. There is no cost reduction for pages at lower sizes, other than bandwidth, or performance improvement for pages at higher resolution.

For best results:

*   Rotate pages to the correct orientation before uploading.
*   Avoid blurry pages.
*   If using a single page, place the text prompt after the page.

## What's next

To learn more, see the following resources:

*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-24 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/structured-output

Nous vous présentons les mises à jour de notre famille de modèles de pensée 2.5. [En savoir plus](https://ai.google.dev/gemini-api/docs/models?hl=fr)

![](https://ai.google.dev/_static/images/translated.svg?hl=fr) Cette page a été traduite par l'[API Cloud Translation](//cloud.google.com/translate/?hl=fr).

*   [Accueil](https://ai.google.dev/?hl=fr)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=fr)
*   [Modèles](https://ai.google.dev/gemini-api/docs?hl=fr)

Envoyer des commentaires

# Sortie structurée

Vous pouvez configurer Gemini pour obtenir une sortie structurée plutôt qu'un texte non structuré, ce qui permet d'extraire et de normaliser précisément les informations pour un traitement ultérieur. Par exemple, vous pouvez utiliser une sortie structurée pour extraire des informations de CV, les normaliser afin de créer une base de données structurée.

Gemini peut générer du [JSON](https://ai.google.dev/gemini-api/docs/structured-output?hl=fr#generating-json) ou des [valeurs d'énumération](https://ai.google.dev/gemini-api/docs/structured-output?hl=fr#generating-enums) en tant que sortie structurée.

## Générer un fichier JSON

Il existe deux façons de générer du code JSON à l'aide de l'API Gemini :

*   Configurer un schéma sur le modèle
*   Fournir un schéma dans une requête textuelle

La configuration d'un schéma sur le modèle est la **méthode recommandée** pour générer du JSON, car elle contraint le modèle à générer du JSON.

### Configurer un schéma (recommandé)

Pour contraindre le modèle à générer du code JSON, configurez un `responseSchema`. Le modèle répondra ensuite à toute requête avec une sortie au format JSON.

### Python

```
from google import genai
from pydantic import BaseModel

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="List a few popular cookie recipes, and include the amounts of ingredients.",
    config={
        "response_mime_type": "application/json",
        "response_schema": list[Recipe],
    },
)
# Use the response as a JSON string.
print(response.text)

# Use instantiated objects.
my_recipes: list[Recipe] = response.parsed
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents:
      "List a few popular cookie recipes, and include the amounts of ingredients.",
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.ARRAY,
        items: {
          type: Type.OBJECT,
          properties: {
            recipeName: {
              type: Type.STRING,
            },
            ingredients: {
              type: Type.ARRAY,
              items: {
                type: Type.STRING,
              },
            },
          },
          propertyOrdering: ["recipeName", "ingredients"],
        },
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        ResponseMIMEType: "application/json",
        ResponseSchema: &genai.Schema{
            Type: genai.TypeArray,
            Items: &genai.Schema{
                Type: genai.TypeObject,
                Properties: map[string]*genai.Schema{
                    "recipeName": {Type: genai.TypeString},
                    "ingredients": {
                        Type:  genai.TypeArray,
                        Items: &genai.Schema{Type: genai.TypeString},
                    },
                },
                PropertyOrdering: []string{"recipeName", "ingredients"},
            },
        },
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("List a few popular cookie recipes, and include the amounts of ingredients."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
      "contents": [{
        "parts":[
          { "text": "List a few popular cookie recipes, and include the amounts of ingredients." }
        ]
      }],
      "generationConfig": {
        "responseMimeType": "application/json",
        "responseSchema": {
          "type": "ARRAY",
          "items": {
            "type": "OBJECT",
            "properties": {
              "recipeName": { "type": "STRING" },
              "ingredients": {
                "type": "ARRAY",
                "items": { "type": "STRING" }
              }
            },
            "propertyOrdering": ["recipeName", "ingredients"]
          }
        }
      }
}' 2> /dev/null | head
```

Le résultat peut ressembler à ceci :

```
[
  {
    "recipeName": "Chocolate Chip Cookies",
    "ingredients": [
      "1 cup (2 sticks) unsalted butter, softened",
      "3/4 cup granulated sugar",
      "3/4 cup packed brown sugar",
      "1 teaspoon vanilla extract",
      "2 large eggs",
      "2 1/4 cups all-purpose flour",
      "1 teaspoon baking soda",
      "1 teaspoon salt",
      "2 cups chocolate chips"
    ]
  },
  ...
]
```

### Fournir un schéma dans une requête textuelle

Au lieu de configurer un schéma, vous pouvez fournir un schéma en langage naturel ou en pseudo-code dans une invite de texte. Cette méthode n'est **pas recommandée**, car elle peut produire une sortie de moins bonne qualité et parce que le modèle n'est pas contraint de suivre le schéma.

Voici un exemple générique de schéma fourni dans une requête textuelle :

```
List a few popular cookie recipes, and include the amounts of ingredients.

Produce JSON matching this specification:

Recipe = { "recipeName": string, "ingredients": array<string> }
Return: array<Recipe>
```

Étant donné que le modèle obtient le schéma à partir du texte de l'invite, vous pouvez avoir une certaine flexibilité dans la façon de représenter le schéma. Toutefois, lorsque vous fournissez un schéma intégré comme celui-ci, le modèle n'est pas contraint de renvoyer du JSON. Pour une réponse plus déterministe et de meilleure qualité, configurez un schéma sur le modèle et ne dupliquez pas le schéma dans la requête textuelle.

## Générer des valeurs d'énumération

Dans certains cas, vous pouvez souhaiter que le modèle ne choisisse qu'une seule option parmi une liste. Pour implémenter ce comportement, vous pouvez transmettre une _énumération_ dans votre schéma. Vous pouvez utiliser une option d'énumération partout où vous pouvez utiliser un `string` dans le `responseSchema`, car une énumération est un tableau de chaînes. Comme un schéma JSON, un énumérateur vous permet de contraindre la sortie du modèle pour répondre aux exigences de votre application.

Par exemple, supposons que vous développiez une application permettant de classer les instruments de musique dans l'une des cinq catégories suivantes : `"Percussion"`, `"String"`, `"Woodwind"`, `"Brass"` ou "`"Keyboard"`". Vous pouvez créer une énumération pour vous aider dans cette tâche.

Dans l'exemple suivant, vous transmettez un énumération en tant que `responseSchema`, ce qui contraint le modèle à choisir l'option la plus appropriée.

### Python

```
from google import genai
import enum

class Instrument(enum.Enum):
  PERCUSSION = "Percussion"
  STRING = "String"
  WOODWIND = "Woodwind"
  BRASS = "Brass"
  KEYBOARD = "Keyboard"

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': Instrument,
    },
)

print(response.text)
# Woodwind
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "What type of instrument is an oboe?",
    config: {
      responseMimeType: "text/x.enum",
      responseSchema: {
        type: Type.STRING,
        enum: ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
      },
    },
  });

console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
          "contents": [{
            "parts":[
              { "text": "What type of instrument is an oboe?" }
            ]
          }],
          "generationConfig": {
            "responseMimeType": "text/x.enum",
            "responseSchema": {
              "type": "STRING",
              "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
            }
          }
    }'
```

La bibliothèque Python traduit les déclarations de type pour l'API. Toutefois, l'API accepte un sous-ensemble du schéma OpenAPI 3.0 ([Schéma](https://ai.google.dev/api/caching?hl=fr#schema)).

Il existe deux autres façons de spécifier une énumération. Vous pouvez utiliser un [`Literal`](https://docs.pydantic.dev/1.10/usage/types/#literal-type) : \`\`\`

### Python

```
Literal["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
```

Vous pouvez également transmettre le schéma au format JSON :

### Python

```
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': {
            "type": "STRING",
            "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
        },
    },
)

print(response.text)
# Woodwind
```

En plus des problèmes de choix multiples de base, vous pouvez utiliser une énumération n'importe où dans un schéma JSON. Par exemple, vous pouvez demander au modèle une liste de titres de recettes et utiliser une énumération `Grade` pour attribuer à chaque titre un niveau de popularité :

### Python

```
from google import genai

import enum
from pydantic import BaseModel

class Grade(enum.Enum):
    A_PLUS = "a+"
    A = "a"
    B = "b"
    C = "c"
    D = "d"
    F = "f"

class Recipe(BaseModel):
  recipe_name: str
  rating: Grade

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='List 10 home-baked cookie recipes and give them grades based on tastiness.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': list[Recipe],
    },
)

print(response.text)
```

La réponse peut ressembler à ceci :

```
[
  {
    "recipe_name": "Chocolate Chip Cookies",
    "rating": "a+"
  },
  {
    "recipe_name": "Peanut Butter Cookies",
    "rating": "a"
  },
  {
    "recipe_name": "Oatmeal Raisin Cookies",
    "rating": "b"
  },
  ...
]
```

## À propos des schémas JSON

La configuration du modèle pour la sortie JSON à l'aide du paramètre `responseSchema` repose sur l'objet `Schema` pour définir sa structure. Cet objet représente un sous-ensemble spécifique de l'[objet Schema OpenAPI 3.0](https://spec.openapis.org/oas/v3.0.3#schema-object) et ajoute également un champ `propertyOrdering`.

Voici une représentation pseudo-JSON de tous les champs `Schema` :

```
{
  "type": enum (Type),
  "format": string,
  "description": string,
  "nullable": boolean,
  "enum": [
    string
  ],
  "maxItems": integer,
  "minItems": integer,
  "properties": {
    string: {
      object (Schema)
    },
    ...
  },
  "required": [
    string
  ],
  "propertyOrdering": [
    string
  ],
  "items": {
    object (Schema)
  }
}
```

Le `Type` du schéma doit être l'un des [types de données](https://spec.openapis.org/oas/v3.0.3#data-types) OpenAPI ou une union de ces types (à l'aide de `anyOf`). Seul un sous-ensemble de champs est valide pour chaque `Type`. La liste suivante met en correspondance chaque `Type` avec un sous-ensemble des champs valides pour ce type :

*   `string` -> `enum`, `format`, `nullable`
*   `integer` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `number` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `boolean` -> `nullable`
*   `array` -> `minItems`, `maxItems`, `items`, `nullable`
*   `object` -> `properties`, `required`, `propertyOrdering`, `nullable`

Voici quelques exemples de schémas illustrant des combinaisons de type et de champ valides :

```
{ "type": "string", "enum": ["a", "b", "c"] }

{ "type": "string", "format": "date-time" }

{ "type": "integer", "format": "int64" }

{ "type": "number", "format": "double" }

{ "type": "boolean" }

{ "type": "array", "minItems": 3, "maxItems": 3, "items": { "type": ... } }

{ "type": "object",
  "properties": {
    "a": { "type": ... },
    "b": { "type": ... },
    "c": { "type": ... }
  },
  "nullable": true,
  "required": ["c"],
  "propertyOrdering": ["c", "b", "a"]
}
```

Pour obtenir une documentation complète des champs de schéma tels qu'ils sont utilisés dans l'API Gemini, consultez la [référence du schéma](https://ai.google.dev/api/caching?hl=fr#Schema).

### Commande de propriétés

Lorsque vous travaillez avec des schémas JSON dans l'API Gemini, l'ordre des propriétés est important. Par défaut, l'API trie les propriétés par ordre alphabétique et ne conserve pas l'ordre dans lequel elles sont définies (bien que les [SDK Google Gen AI](https://ai.google.dev/gemini-api/docs/sdks?hl=fr) puissent conserver cet ordre). Si vous fournissez des exemples au modèle avec un schéma configuré et que l'ordre des propriétés des exemples n'est pas cohérent avec l'ordre des propriétés du schéma, la sortie peut être décousue ou inattendue.

Pour garantir un classement cohérent et prévisible des propriétés, vous pouvez utiliser le champ `propertyOrdering[]` facultatif.

```
"propertyOrdering": ["recipeName", "ingredients"]
```

`propertyOrdering[]` (qui n'est pas un champ standard dans la spécification OpenAPI) est un tableau de chaînes utilisé pour déterminer l'ordre des propriétés dans la réponse. En spécifiant l'ordre des propriétés, puis en fournissant des exemples avec des propriétés dans cet ordre, vous pouvez potentiellement améliorer la qualité des résultats. `propertyOrdering` n'est compatible que lorsque vous créez manuellement `types.Schema`.

### Schémas en Python

Lorsque vous utilisez la bibliothèque Python, la valeur de `response_schema` doit être l'une des suivantes :

*   Un type, comme vous le feriez dans une annotation de type (voir le [module `typing`](https://docs.python.org/3/library/typing.html) Python)
*   Instance de [`genai.types.Schema`](https://googleapis.github.io/python-genai/genai.html#genai.types.Schema)
*   Équivalent `dict` de `genai.types.Schema`

Le moyen le plus simple de définir un schéma consiste à utiliser un type Pydantic (comme illustré dans l'exemple précédent) :

### Python

```
config={'response_mime_type': 'application/json',
        'response_schema': list[Recipe]}
```

Lorsque vous utilisez un type Pydantic, la bibliothèque Python crée un schéma JSON pour vous et l'envoie à l'API. Pour obtenir d'autres exemples, consultez la [documentation de la bibliothèque Python](https://googleapis.github.io/python-genai/index.html#json-response-schema).

La bibliothèque Python est compatible avec les schémas définis avec les types suivants (`AllowedType` étant un type autorisé) :

*   `int`
*   `float`
*   `bool`
*   `str`
*   `list[AllowedType]`
*   `AllowedType|AllowedType|...`
*   Pour les types structurés :
    *   `dict[str, AllowedType]`. Cette annotation déclare que toutes les valeurs du dictionnaire sont du même type, mais ne spécifie pas les clés à inclure.
    *   [Modèles Pydantic](https://docs.pydantic.dev/latest/concepts/models/) définis par l'utilisateur. Cette approche vous permet de spécifier les noms de clé et de définir différents types pour les valeurs associées à chacune des clés, y compris les structures imbriquées.

### Prise en charge des schémas JSON

Les [schémas JSON](https://json-schema.org/) sont une spécification plus récente qu'OpenAPI 3.0, sur laquelle l'objet [Schema](https://ai.google.dev/api/caching?hl=fr#Schema) est basé. La prise en charge du schéma JSON est disponible en tant qu'aperçu à l'aide du champ [`responseJsonSchema`](https://ai.google.dev/api/generate-content?hl=fr#FIELDS.response_json_schema), qui accepte tout schéma JSON avec les limites suivantes :

*   Il ne fonctionne qu'avec Gemini 2.5.
*   Bien que toutes les propriétés de schéma JSON puissent être transmises, elles ne sont pas toutes acceptées. Pour en savoir plus, consultez la [documentation](https://ai.google.dev/api/generate-content?hl=fr#FIELDS.response_json_schema) du champ.
*   Les références récursives ne peuvent être utilisées que comme valeur d'une propriété d'objet facultative.
*   Les références récursives sont déroulées de manière limitée, en fonction de la taille du schéma.
*   Les schémas contenant `$ref` ne peuvent contenir que des propriétés commençant par `$`.

Voici un exemple de génération d'un schéma JSON avec Pydantic et de son envoi au modèle :

```
curl "https://generativelanguage.googleapis.com/v1alpha/models/\
gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY"\
    -H 'Content-Type: application/json' \
    -d @- <<EOF
{
  "contents": [{
    "parts":[{
      "text": "Please give a random example following this schema"
    }]
  }],
  "generationConfig": {
    "response_mime_type": "application/json",
    "response_json_schema": $(python3 - << PYEOF
from enum import Enum
from typing import List, Optional, Union, Set
from pydantic import BaseModel, Field, ConfigDict
import json

class UserRole(str, Enum):
    ADMIN = "admin"
    VIEWER = "viewer"

class Address(BaseModel):
    street: str
    city: str

class UserProfile(BaseModel):
    username: str = Field(description="User's unique name")
    age: Optional[int] = Field(ge=0, le=120)
    roles: Set[UserRole] = Field(min_items=1)
    contact: Union[Address, str]
    model_config = ConfigDict(title="User Schema")

# Generate and print the JSON Schema
print(json.dumps(UserProfile.model_json_schema(), indent=2))
PYEOF
)
  }
}
EOF
```

Le transfert direct du schéma JSON n'est pas encore possible lorsque vous utilisez le SDK.

## Bonnes pratiques

Tenez compte des considérations et des bonnes pratiques suivantes lorsque vous utilisez un schéma de réponse :

*   La taille du schéma de réponse est comptabilisée dans la limite des jetons d'entrée.
*   Par défaut, les champs sont facultatifs, ce qui signifie que le modèle peut les renseigner ou les ignorer. Vous pouvez définir les champs selon vos besoins pour forcer le modèle à fournir une valeur. Si le contexte de la requête d'entrée associée est insuffisant, le modèle génère des réponses principalement basées sur les données sur lesquelles il a été entraîné.
*   Un schéma complexe peut entraîner une erreur `InvalidArgument: 400`. Cette complexité peut être due à des noms de propriété longs, des limites de longueur de tableau longues, des énumérations comportant de nombreuses valeurs, des objets avec de nombreuses propriétés facultatives ou d'une combinaison de ces facteurs.
    
    Si vous recevez cette erreur avec un schéma valide, apportez une ou plusieurs des modifications suivantes pour la résoudre :
    
    *   Raccourcissez les noms de propriété ou d'énumérations.
    *   Aplatissez les tableaux imbriqués.
    *   Réduisez le nombre de propriétés à l'aide de contraintes, par exemple des limites minimale et maximale.
    *   Réduisez le nombre de propriétés à l'aide de contraintes complexes, telles que des propriétés avec des formats complexes comme `date-time`.
    *   Réduisez le nombre de propriétés facultatives.
    *   Réduisez le nombre de valeurs valides pour les énumérations.
*   Si les résultats ne sont pas ceux escomptés, ajoutez plus de contexte à vos requêtes d'entrée ou modifiez votre schéma de réponse. Par exemple, examinez la réponse du modèle sans sortie structurée pour voir comment le modèle répond. Vous pouvez ensuite modifier le schéma de réponse pour qu'il corresponde mieux à la sortie du modèle.
    

## Étape suivante

Maintenant que vous savez générer une sortie structurée, vous pouvez essayer d'utiliser les outils de l'API Gemini :

*   [Appel de fonction](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr)
*   [Exécution de code](https://ai.google.dev/gemini-api/docs/code-execution?hl=fr)
*   [Ancrage avec la recherche Google](https://ai.google.dev/gemini-api/docs/grounding?hl=fr)

Envoyer des commentaires

Sauf indication contraire, le contenu de cette page est régi par une licence [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/), et les échantillons de code sont régis par une licence [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Pour en savoir plus, consultez les [Règles du site Google Developers](https://developers.google.com/site-policies?hl=fr). Java est une marque déposée d'Oracle et/ou de ses sociétés affiliées.

Dernière mise à jour le 2025/06/27 (UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/image-understanding

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Image understanding

Gemini models are built to be multimodal from the ground up, unlocking a wide range of image processing and computer vision tasks including but not limited to image captioning, classification, and visual question answering without having to train specialized ML models.

## Passing images to Gemini

You can provide images as input to Gemini using two methods:

*   [Passing inline image data](#inline-image): Ideal for smaller files (total request size less than 20MB, including prompts).
*   [Uploading images using the File API](#upload-image): Recommended for larger files or for reusing images across multiple requests.

### Passing inline image data

You can pass inline image data in the request to `generateContent`. You can provide image data as Base64 encoded strings or by reading local files directly (depending on the language).

The following example shows how to read an image from a local file and pass it to `generateContent` API for processing.

### Python

  ```
  from google.genai import types

  with open('path/to/small-sample.jpg', 'rb') as f:
      image_bytes = f.read()

  response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=[
      types.Part.from_bytes(
        data=image_bytes,
        mime_type='image/jpeg',
      ),
      'Caption this image.'
    ]
  )

  print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

### Go

```
bytes, _ := os.ReadFile("path/to/small-sample.jpg")

parts := []*genai.Part{
  genai.NewPartFromBytes(bytes, "image/jpeg"),
  genai.NewPartFromText("Caption this image."),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
IMG_PATH="/path/to/your/image1.jpg"

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
B64FLAGS="--input"
else
B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "'"$(base64 $B64FLAGS $IMG_PATH)"'"
            }
        },
        {"text": "Caption this image."},
    ]
    }]
}' 2> /dev/null
```

You can also fetch an image from a URL, convert it to bytes, and pass it to `generateContent` as shown in the following examples.

### Python

```
from google import genai
from google.genai import types

import requests

image_path = "https://goo.gle/instrument-img"
image_bytes = requests.get(image_path).content
image = types.Part.from_bytes(
  data=image_bytes, mime_type="image/jpeg"
)

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["What is this image?", image],
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

async function main() {
  const ai = new GoogleGenAI({});

  const imageUrl = "https://goo.gle/instrument-img";

  const response = await fetch(imageUrl);
  const imageArrayBuffer = await response.arrayBuffer();
  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
    {
      inlineData: {
        mimeType: 'image/jpeg',
        data: base64ImageData,
      },
    },
    { text: "Caption this image." }
  ],
  });
  console.log(result.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "io"
  "net/http"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Download the image.
  imageResp, _ := http.Get("https://goo.gle/instrument-img")

  imageBytes, _ := io.ReadAll(imageResp.Body)

  parts := []*genai.Part{
    genai.NewPartFromBytes(imageBytes, "image/jpeg"),
    genai.NewPartFromText("Caption this image."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMG_URL="https://goo.gle/instrument-img"

MIME_TYPE=$(curl -sIL "$IMG_URL" | grep -i '^content-type:' | awk -F ': ' '{print $2}' | sed 's/\r$//' | head -n 1)
if [[ -z "$MIME_TYPE" || ! "$MIME_TYPE" == image/* ]]; then
  MIME_TYPE="image/jpeg"
fi

# Check for macOS
if [[ "$(uname)" == "Darwin" ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -b 0)
elif [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64)
else
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -w0)
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"'"$MIME_TYPE"'",
                "data": "'"$IMAGE_B64"'"
              }
            },
            {"text": "Caption this image."}
        ]
      }]
    }' 2> /dev/null
```

### Uploading images using the File API

For large files or to be able to use the same image file repeatedly, use the Files API. The following code uploads an image file and then uses the file in a call to `generateContent`. See the [Files API guide](/gemini-api/docs/files) for more information and examples.

### Python

```
from google import genai

client = genai.Client()

my_file = client.files.upload(file="path/to/sample.jpg")

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[my_file, "Caption this image."],
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.jpg",
    config: { mimeType: "image/jpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Caption this image.",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.jpg", nil)

  parts := []*genai.Part{
      genai.NewPartFromText("Caption this image."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMAGE_PATH="path/to/sample.jpg"
MIME_TYPE=$(file -b --mime-type "${IMAGE_PATH}")
NUM_BYTES=$(wc -c < "${IMAGE_PATH}")
DISPLAY_NAME=IMAGE

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Caption this image."}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Prompting with multiple images

You can provide multiple images in a single prompt by including multiple image `Part` objects in the `contents` array. These can be a mix of inline data (local files or URLs) and File API references.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

# Upload the first image
image1_path = "path/to/image1.jpg"
uploaded_file = client.files.upload(file=image1_path)

# Prepare the second image as inline data
image2_path = "path/to/image2.png"
with open(image2_path, 'rb') as f:
    img2_bytes = f.read()

# Create the prompt with text and multiple images
response = client.models.generate_content(

    model="gemini-2.5-flash",
    contents=[
        "What is different between these two images?",
        uploaded_file,  # Use the uploaded file reference
        types.Part.from_bytes(
            data=img2_bytes,
            mime_type='image/png'
        )
    ]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images

  const response = await ai.models.generateContent({

    model: "gemini-2.5-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
# Upload the first image
IMAGE1_PATH="path/to/image1.jpg"
MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
DISPLAY_NAME1=IMAGE1

tmp_header_file1=upload-header1.tmp

curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header1.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file1}"

curl "${upload_url1}" \
  -H "Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

file1_uri=$(jq ".file.uri" file_info1.json)
echo file1_uri=$file1_uri

# Prepare the second image (inline)
IMAGE2_PATH="path/to/image2.png"
MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi
IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

# Now generate content using both images
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "What is different between these two images?"},
          {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
          {
            "inline_data": {
              "mime_type":"'"${MIME2_TYPE}"'",
              "data": "'"$IMAGE2_BASE64"'"
            }
          }
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Object detection

From Gemini 2.0 onwards, models are further trained to detect objects in an image and get their bounding box coordinates. The coordinates, relative to image dimensions, scale to \[0, 1000\]. You need to descale these coordinates based on your original image size.

### Python

```
from google import genai
from google.genai import types
from PIL import Image
import json

client = genai.Client()
prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

image = Image.open("/path/to/image.png")

config = types.GenerateContentConfig(
  response_mime_type="application/json"
  ) 

response = client.models.generate_content(model="gemini-2.5-flash",
                                          contents=[image, prompt],
                                          config=config
                                          )

width, height = image.size
bounding_boxes = json.loads(response.text)

converted_bounding_boxes = []
for bounding_box in bounding_boxes:
    abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
    abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
    abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
    abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

print("Image size: ", width, height)
print("Bounding boxes:", converted_bounding_boxes)

```

For more examples, check following cookbooks:

*   [2D spatial understanding notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb)
*   [Experimental 3D pointing notebook](https://github.com/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb)

## Segmentation

Starting with Gemini 2.5, models not only detect items but also segment them and provide their contour masks.

The model predicts a JSON list, where each item represents a segmentation mask. Each item has a bounding box ("`box_2d`") in the format `[y0, x0, y1, x1]` with normalized coordinates between 0 and 1000, a label ("`label`") that identifies the object, and finally the segmentation mask inside the bounding box, as base64 encoded png that is a probability map with values between 0 and 255. The mask needs to be resized to match the bounding box dimensions, then binarized at your confidence threshold (127 for the midpoint).

### Python

```
from google import genai
from google.genai import types
from PIL import Image, ImageDraw    
import io
import base64
import json
import numpy as np
import os    

client = genai.Client()

def parse_json(json_output: str):
  # Parsing out the markdown fencing
  lines = json_output.splitlines()
  for i, line in enumerate(lines):
      if line == "```json":
          json_output = "\n".join(lines[i+1:])  # Remove everything before "```json"
          json_output = json_output.split("```")[0]  # Remove everything after the closing "```"
          break  # Exit the loop once "```json" is found
  return json_output

def extract_segmentation_masks(image_path: str, output_dir: str = "segmentation_outputs"):
  # Load and resize image
  im = Image.open(image_path)
  im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)

  prompt = """
  Give the segmentation masks for the wooden and glass items.
  Output a JSON list of segmentation masks where each entry contains the 2D
  bounding box in the key "box_2d", the segmentation mask in key "mask", and
  the text label in the key "label". Use descriptive labels.
  """

  config = types.GenerateContentConfig(      
    thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection
  ) 

  response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[prompt, im], # Pillow images can be directly passed as inputs (which will be converted by the SDK)
    config=config
  )

  # Parse JSON response
  items = json.loads(parse_json(response.text))

  # Create output directory
  os.makedirs(output_dir, exist_ok=True)

  # Process each mask
  for i, item in enumerate(items):
      # Get bounding box coordinates
      box = item["box_2d"]
      y0 = int(box[0] / 1000 * im.size[1])
      x0 = int(box[1] / 1000 * im.size[0])
      y1 = int(box[2] / 1000 * im.size[1])
      x1 = int(box[3] / 1000 * im.size[0])

      # Skip invalid boxes
      if y0 >= y1 or x0 >= x1:
          continue

      # Process mask
      png_str = item["mask"]
      if not png_str.startswith("data:image/png;base64,"):
          continue

      # Remove prefix
      png_str = png_str.removeprefix("data:image/png;base64,")
      mask_data = base64.b64decode(png_str)
      mask = Image.open(io.BytesIO(mask_data))

      # Resize mask to match bounding box
      mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)

      # Convert mask to numpy array for processing
      mask_array = np.array(mask)

      # Create overlay for this mask
      overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))
      overlay_draw = ImageDraw.Draw(overlay)

      # Create overlay for the mask
      color = (255, 255, 255, 200)
      for y in range(y0, y1):
          for x in range(x0, x1):
              if mask_array[y - y0, x - x0] > 128:  # Threshold for mask
                  overlay_draw.point((x, y), fill=color)

      # Save individual mask and its overlay
      mask_filename = f"{item['label']}_{i}_mask.png"
      overlay_filename = f"{item['label']}_{i}_overlay.png"

      mask.save(os.path.join(output_dir, mask_filename))

      # Create and save overlay
      composite = Image.alpha_composite(im.convert('RGBA'), overlay)
      composite.save(os.path.join(output_dir, overlay_filename))
      print(f"Saved mask and overlay for {item['label']} to {output_dir}")

# Example usage
if __name__ == "__main__":
  extract_segmentation_masks("path/to/image.png")

```

Check the [segmentation example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb#scrollTo=WQJTJ8wdGOKx) in the cookbook guide for a more detailed example.

![A table with cupcakes, with the wooden and glass objects highlighted](/static/gemini-api/docs/images/segmentation.jpg)

An example segmentation output with objects and segmentation masks

## Supported image formats

Gemini supports the following image format MIME types:

*   PNG - `image/png`
*   JPEG - `image/jpeg`
*   WEBP - `image/webp`
*   HEIC - `image/heic`
*   HEIF - `image/heif`

## Capabilities

All Gemini model versions are multimodal and can be utilized in a wide range of image processing and computer vision tasks including but not limited to image captioning, visual question and answering, image classification, object detection and segmentation.

Gemini can reduce the need to use specialized ML models depending on your quality and performance requirements.

Some later model versions are specifically trained improve accuracy of specialized tasks in addition to generic capabilities:

*   **Gemini 2.0 models** are further trained to support enhanced [object detection](#object-detection).
    
*   **Gemini 2.5 models** are further trained to support enhanced [segmentation](#segmentation) in addition to [object detection](#object-detection).
    

## Limitations and key technical information

### File limit

Gemini 2.5 Pro/Flash, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a maximum of 3,600 image files per request.

### Token calculation

*   **Gemini 1.5 Flash and Gemini 1.5 Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized to 768x768), with each tile costing 258 tokens.
*   **Gemini 2.0 Flash and Gemini 2.5 Flash/Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled into 768x768 pixel tiles, each costing 258 tokens.

## Tips and best practices

*   Verify that images are correctly rotated.
*   Use clear, non-blurry images.
*   When using a single image with text, place the text prompt _after_ the image part in the `contents` array.

## What's next

This guide shows you how to upload image files and generate text outputs from image inputs. To learn more, see the following resources:

*   [Files API](/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/function-calling

Nous vous présentons les mises à jour de notre famille de modèles de pensée 2.5. [En savoir plus](https://ai.google.dev/gemini-api/docs/models?hl=fr)

![](https://ai.google.dev/_static/images/translated.svg?hl=fr) Cette page a été traduite par l'[API Cloud Translation](//cloud.google.com/translate/?hl=fr).

*   [Accueil](https://ai.google.dev/?hl=fr)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=fr)
*   [Modèles](https://ai.google.dev/gemini-api/docs?hl=fr)

Envoyer des commentaires

# Appel de fonction avec l'API Gemini

L'appel de fonction vous permet de connecter des modèles à des outils et des API externes. Au lieu de générer des réponses textuelles, le modèle détermine quand appeler des fonctions spécifiques et fournit les paramètres nécessaires pour exécuter des actions réelles. Cela permet au modèle de servir de pont entre le langage naturel et les actions et données du monde réel. Les appels de fonction ont trois cas d'utilisation principaux :

*   **Améliorer les connaissances** : accéder aux informations provenant de sources externes telles que des bases de données, des API et des bases de connaissances.
*   **Étendre les fonctionnalités** : utilisez des outils externes pour effectuer des calculs et repousser les limites du modèle, par exemple en utilisant une calculatrice ou en créant des graphiques.
*   **Exécuter des actions** : interagissez avec des systèmes externes à l'aide d'API, par exemple pour planifier des rendez-vous, créer des factures, envoyer des e-mails ou contrôler des appareils de maison connectée.

Obtenir la météo Planifier une réunion Créer un graphique

## Fonctionnement des appels de fonction

![Présentation des appels de fonction](https://ai.google.dev/static/gemini-api/docs/images/function-calling-overview.png?hl=fr)

L'appel de fonction implique une interaction structurée entre votre application, le modèle et les fonctions externes. Voici le détail :

1.  **Définir la déclaration de fonction** : définissez la déclaration de fonction dans le code de votre application. Les déclarations de fonction décrivent le nom, les paramètres et l'objectif de la fonction au modèle.
2.  **Appeler le LLM avec des déclarations de fonction** : envoyez la requête utilisateur avec la ou les déclarations de fonction au modèle. Il analyse la requête et détermine si un appel de fonction serait utile. Si c'est le cas, il répond avec un objet JSON structuré.
3.  **Exécuter le code de fonction (votre responsabilité)** : le modèle _n'exécute pas_ la fonction lui-même. Il est de la responsabilité de votre application de traiter la réponse et de vérifier l'appel de fonction, si :
    *   **Oui** : extrayez le nom et les arguments de la fonction, puis exécutez la fonction correspondante dans votre application.
    *   **Non** : le modèle a fourni une réponse textuelle directe à la requête (ce flux est moins mis en avant dans l'exemple, mais il s'agit d'un résultat possible).
4.  **Créer une réponse conviviale** : si une fonction a été exécutée, capturez le résultat et renvoyez-le au modèle lors d'un tour ultérieur de la conversation. Il utilisera le résultat pour générer une réponse finale conviviale qui intègre les informations de l'appel de fonction.

Ce processus peut être répété plusieurs fois, ce qui permet des interactions et des workflows complexes. Le modèle permet également d'appeler plusieurs fonctions en une seule fois ([appel de fonction en parallèle](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#parallel_function_calling)) et de manière séquentielle ([appel de fonction compositionnel](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#compositional_function_calling)).

### Étape 1 : Définir une déclaration de fonction

Définissez une fonction et sa déclaration dans le code de votre application, qui permet aux utilisateurs de définir des valeurs de lumière et d'effectuer une requête API. Cette fonction peut appeler des services ou des API externes.

### Python

```
# Define a function that the model can call to control smart lights
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

### JavaScript

```
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

### Étape 2 : Appeler le modèle avec des déclarations de fonction

Une fois que vous avez défini vos déclarations de fonction, vous pouvez demander au modèle de les utiliser. Il analyse la requête et les déclarations de fonction, puis décide de répondre directement ou d'appeler une fonction. Si une fonction est appelée, l'objet de réponse contient une suggestion d'appel de fonction.

### Python

```
from google.genai import types

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[set_light_values_declaration])
config = types.GenerateContentConfig(tools=[tools])

# Define user prompt
contents = [
    types.Content(
        role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
    )
]

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
    config=config,
)

print(response.candidates[0].content.parts[0].function_call)
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Generation config with function declaration
const config = {
  tools: [{
    functionDeclarations: [setLightValuesFunctionDeclaration]
  }]
};

// Configure the client
const ai = new GoogleGenAI({});

// Define user prompt
const contents = [
  {
    role: 'user',
    parts: [{ text: 'Turn the lights down to a romantic level' }]
  }
];

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(response.functionCalls[0]);
```

Le modèle renvoie ensuite un objet `functionCall` dans un schéma compatible avec OpenAPI, qui spécifie comment appeler une ou plusieurs des fonctions déclarées pour répondre à la question de l'utilisateur.

### Python

```
id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'
```

### JavaScript

```
{
  name: 'set_light_values',
  args: { brightness: 25, color_temp: 'warm' }
}
```

### Étape 3 : Exécutez le code de la fonction set\_light\_values

Extrayez les détails de l'appel de fonction à partir de la réponse du modèle, analysez les arguments et exécutez la fonction `set_light_values`.

### Python

```
# Extract tool call details, it may not be in the first part.
tool_call = response.candidates[0].content.parts[0].function_call

if tool_call.name == "set_light_values":
    result = set_light_values(**tool_call.args)
    print(f"Function execution result: {result}")
```

### JavaScript

```
// Extract tool call details
const tool_call = response.functionCalls[0]

let result;
if (tool_call.name === 'set_light_values') {
  result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);
  console.log(`Function execution result: ${JSON.stringify(result)}`);
}
```

### Étape 4 : Créez une réponse conviviale avec le résultat de la fonction et appelez à nouveau le modèle

Enfin, renvoyez le résultat de l'exécution de la fonction au modèle afin qu'il puisse intégrer ces informations dans sa réponse finale à l'utilisateur.

### Python

```
# Create a function response part
function_response_part = types.Part.from_function_response(
    name=tool_call.name,
    response={"result": result},
)

# Append function call and result of the function execution to contents
contents.append(response.candidates[0].content) # Append the content from the model's response.
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)
```

### JavaScript

```
// Create a function response part
const function_response_part = {
  name: tool_call.name,
  response: { result }
}

// Append function call and result of the function execution to contents
contents.push(response.candidates[0].content);
contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });

// Get the final response from the model
const final_response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(final_response.text);
```

Le flux d'appel de fonction est terminé. Le modèle a correctement utilisé la fonction `set_light_values` pour effectuer l'action de requête de l'utilisateur.

## Déclarations de fonctions

Lorsque vous implémentez l'appel de fonction dans une invite, vous créez un objet `tools`, qui contient un ou plusieurs `function declarations`. Vous définissez des fonctions à l'aide de JSON, en particulier avec un [sous-ensemble sélectionné](https://ai.google.dev/api/caching?hl=fr#Schema) du format de [schéma OpenAPI](https://spec.openapis.org/oas/v3.0.3#schemaw). Une déclaration de fonction peut inclure les paramètres suivants :

*   `name` (chaîne) : nom unique de la fonction (`get_weather_forecast`, `send_email`). Utilisez des noms descriptifs sans espaces ni caractères spéciaux (utilisez des traits de soulignement ou du style CamelCase).
*   `description` (chaîne) : explication claire et détaillée de l'objectif et des fonctionnalités de la fonction. Il est essentiel que le modèle comprenne quand utiliser la fonction. Soyez précis et fournissez des exemples si nécessaire ("Trouve les cinémas en fonction de la zone géographique et, éventuellement, du titre du film actuellement à l'affiche.").
*   `parameters` (objet) : définit les paramètres d'entrée attendus par la fonction.
    *   `type` (chaîne) : spécifie le type de données global, tel que `object`.
    *   `properties` (objet) : liste les paramètres individuels, chacun avec les éléments suivants :
        *   `type` (chaîne) : type de données du paramètre, tel que `string`, `integer` ou `boolean, array`.
        *   `description` (chaîne) : description de l'objectif et du format du paramètre. Fournissez des exemples et des contraintes ("La ville et l'état, par exemple, "San Francisco, CA" ou un code postal (par exemple, 95616'.").
        *   `enum` (tableau, facultatif) : si les valeurs du paramètre proviennent d'un ensemble fixe, utilisez "enum" pour lister les valeurs autorisées au lieu de simplement les décrire dans la description. Cela améliore la précision ("enum": \["daylight", "cool", "warm"\]).
    *   `required` (tableau) : tableau de chaînes listant les noms des paramètres obligatoires pour le bon fonctionnement de la fonction.

## Appel de fonction en parallèle

En plus de l'appel de fonction à un seul tour, vous pouvez également appeler plusieurs fonctions à la fois. L'appel de fonction parallèle vous permet d'exécuter plusieurs fonctions à la fois. Il est utilisé lorsque les fonctions ne dépendent pas les unes des autres. Cela est utile dans des scénarios tels que la collecte de données à partir de plusieurs sources indépendantes, comme la récupération des informations client à partir de différentes bases de données, la vérification des niveaux d'inventaire dans différents entrepôts ou l'exécution de plusieurs actions, comme la conversion de votre appartement en discothèque.

### Python

```
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}
```

### JavaScript

```
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};
```

Configurez le mode d'appel de fonction pour autoriser l'utilisation de tous les outils spécifiés. Pour en savoir plus, consultez la section [Configurer les appels de fonction](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#function_calling_modes).

### Python

```
from google import genai
from google.genai import types

# Configure the client and tools
client = genai.Client()
house_tools = [
    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
]
config = types.GenerateContentConfig(
    tools=house_tools,
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
        disable=True
    ),
    # Force the model to call 'any' function, instead of chatting.
    tool_config=types.ToolConfig(
        function_calling_config=types.FunctionCallingConfig(mode='ANY')
    ),
)

chat = client.chats.create(model="gemini-2.5-flash", config=config)
response = chat.send_message("Turn this place into a party!")

# Print out each of the function calls requested from this single call
print("Example 1: Forced function calling")
for fn in response.function_calls:
    args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
    print(f"{fn.name}({args})")
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Set up function declarations
const houseFns = [powerDiscoBall, startMusic, dimLights];

const config = {
    tools: [{
        functionDeclarations: houseFns
    }],
    // Force the model to call 'any' function, instead of chatting.
    toolConfig: {
        functionCallingConfig: {
            mode: 'any'
        }
    }
};

// Configure the client
const ai = new GoogleGenAI({});

// Create a chat session
const chat = ai.chats.create({
    model: 'gemini-2.5-flash',
    config: config
});
const response = await chat.sendMessage({message: 'Turn this place into a party!'});

// Print out each of the function calls requested from this single call
console.log("Example 1: Forced function calling");
for (const fn of response.functionCalls) {
    const args = Object.entries(fn.args)
        .map(([key, val]) => `${key}=${val}`)
        .join(', ');
    console.log(`${fn.name}(${args})`);
}
```

Chacun des résultats imprimés reflète un seul appel de fonction que le modèle a demandé. Pour renvoyer les résultats, incluez les réponses dans l'ordre dans lequel elles ont été demandées.

Le SDK Python est compatible avec l'[appel de fonction automatique](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#automatic_function_calling_python_only), qui convertit automatiquement les fonctions Python en déclarations, gère l'exécution de l'appel de fonction et le cycle de réponse à votre place. Voici un exemple pour le cas d'utilisation de la discothèque.

### Python

```
from google import genai
from google.genai import types

# Actual function implementations
def power_disco_ball_impl(power: bool) -> dict:
    """Powers the spinning disco ball.

    Args:
        power: Whether to turn the disco ball on or off.

    Returns:
        A status dictionary indicating the current state.
    """
    return {"status": f"Disco ball powered {'on' if power else 'off'}"}

def start_music_impl(energetic: bool, loud: bool) -> dict:
    """Play some music matching the specified parameters.

    Args:
        energetic: Whether the music is energetic or not.
        loud: Whether the music is loud or not.

    Returns:
        A dictionary containing the music settings.
    """
    music_type = "energetic" if energetic else "chill"
    volume = "loud" if loud else "quiet"
    return {"music_type": music_type, "volume": volume}

def dim_lights_impl(brightness: float) -> dict:
    """Dim the lights.

    Args:
        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

    Returns:
        A dictionary containing the new brightness setting.
    """
    return {"brightness": brightness}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Do everything you need to this place into party!",
    config=config,
)

print("\nExample 2: Automatic function calling")
print(response.text)
# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!
```

## Appel de fonction compositionnel

L'appel de fonction compositionnel ou séquentiel permet à Gemini d'associer plusieurs appels de fonction pour répondre à une requête complexe. Par exemple, pour répondre à la requête "Obtenir la température à mon emplacement actuel", l'API Gemini peut d'abord appeler une fonction `get_current_location()`, suivie d'une fonction `get_weather()` qui utilise l'emplacement comme paramètre.

L'exemple suivant montre comment implémenter l'appel de fonction compositionnel à l'aide du SDK Python et de l'appel de fonction automatique.

### Python

Cet exemple utilise la fonctionnalité d'appel de fonction automatique du SDK Python `google-genai`. Le SDK convertit automatiquement les fonctions Python en schéma requis, exécute les appels de fonction à la demande du modèle et renvoie les résultats au modèle pour qu'il effectue la tâche.

```
import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
    config=config,
)

# Print the final, user-facing response
print(response.text)
```

**Résultat attendu**

Lorsque vous exécutez le code, le SDK orchestre les appels de fonction. Le modèle appelle d'abord `get_weather_forecast`, reçoit la température, puis appelle `set_thermostat_temperature` avec la valeur correcte en fonction de la logique de l'invite.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. I've set the thermostat to 20°C.
```

### JavaScript

Cet exemple montre comment utiliser le SDK JavaScript/TypeScript pour effectuer des appels de fonction compositionnels à l'aide d'une boucle d'exécution manuelle.

```
import { GoogleGenAI, Type } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Example Functions
function get_weather_forecast({ location }) {
  console.log(`Tool Call: get_weather_forecast(location=${location})`);
  // TODO: Make API call
  console.log("Tool Response: {'temperature': 25, 'unit': 'celsius'}");
  return { temperature: 25, unit: "celsius" };
}

function set_thermostat_temperature({ temperature }) {
  console.log(
    `Tool Call: set_thermostat_temperature(temperature=${temperature})`,
  );
  // TODO: Make API call
  console.log("Tool Response: {'status': 'success'}");
  return { status: "success" };
}

const toolFunctions = {
  get_weather_forecast,
  set_thermostat_temperature,
};

const tools = [
  {
    functionDeclarations: [
      {
        name: "get_weather_forecast",
        description:
          "Gets the current weather temperature for a given location.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            location: {
              type: Type.STRING,
            },
          },
          required: ["location"],
        },
      },
      {
        name: "set_thermostat_temperature",
        description: "Sets the thermostat to a desired temperature.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            temperature: {
              type: Type.NUMBER,
            },
          },
          required: ["temperature"],
        },
      },
    ],
  },
];

// Prompt for the model
let contents = [
  {
    role: "user",
    parts: [
      {
        text: "If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
      },
    ],
  },
];

// Loop until the model has no more function calls to make
while (true) {
  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents,
    config: { tools },
  });

  if (result.functionCalls && result.functionCalls.length > 0) {
    const functionCall = result.functionCalls[0];

    const { name, args } = functionCall;

    if (!toolFunctions[name]) {
      throw new Error(`Unknown function call: ${name}`);
    }

    // Call the function and get the response.
    const toolResponse = toolFunctions[name](args);

    const functionResponsePart = {
      name: functionCall.name,
      response: {
        result: toolResponse,
      },
    };

    // Send the function response back to the model.
    contents.push({
      role: "model",
      parts: [
        {
          functionCall: functionCall,
        },
      ],
    });
    contents.push({
      role: "user",
      parts: [
        {
          functionResponse: functionResponsePart,
        },
      ],
    });
  } else {
    // No more function calls, break the loop.
    console.log(result.text);
    break;
  }
}
```

**Résultat attendu**

Lorsque vous exécutez le code, le SDK orchestre les appels de fonction. Le modèle appelle d'abord `get_weather_forecast`, reçoit la température, puis appelle `set_thermostat_temperature` avec la valeur correcte en fonction de la logique de l'invite.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. It's 25°C in London, so I've set the thermostat to 20°C.
```

L'appel de fonction compositionnel est une fonctionnalité native de l'[API Live](https://ai.google.dev/gemini-api/docs/live?hl=fr). Cela signifie que l'API Live peut gérer l'appel de fonction de manière similaire au SDK Python.

### Python

```
# Light control schemas
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Light control schemas
const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };
const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };

const prompt = `
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
`;

const tools = [
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }
];

await run(prompt, tools=tools, modality="AUDIO")
```

## Modes d'appel de fonction

L'API Gemini vous permet de contrôler la façon dont le modèle utilise les outils fournis (déclarations de fonction). Plus précisément, vous pouvez définir le mode dans.`function_calling_config`.

*   `AUTO (Default)` : le modèle décide de générer une réponse en langage naturel ou de suggérer un appel de fonction en fonction de la requête et du contexte. Il s'agit du mode le plus flexible et recommandé pour la plupart des scénarios.
*   `ANY` : le modèle est contraint de toujours prédire un appel de fonction et garantit le respect du schéma de fonction. Si `allowed_function_names` n'est pas spécifié, le modèle peut choisir parmi les déclarations de fonction fournies. Si `allowed_function_names` est fourni sous forme de liste, le modèle ne peut choisir que parmi les fonctions de cette liste. Utilisez ce mode lorsque vous avez besoin d'une réponse d'appel de fonction pour chaque requête (le cas échéant).
*   `NONE` : le modèle est _interdit_ d'effectuer des appels de fonction. Cela équivaut à envoyer une requête sans aucune déclaration de fonction. Utilisez-le pour désactiver temporairement l'appel de fonction sans supprimer les définitions de vos outils.
    

### Python

```
from google.genai import types

# Configure function calling mode
tool_config = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="ANY", allowed_function_names=["get_current_temperature"]
    )
)

# Create the generation config
config = types.GenerateContentConfig(
    tools=[tools],  # not defined here.
    tool_config=tool_config,
)
```

### JavaScript

```
import { FunctionCallingConfigMode } from '@google/genai';

// Configure function calling mode
const toolConfig = {
  functionCallingConfig: {
    mode: FunctionCallingConfigMode.ANY,
    allowedFunctionNames: ['get_current_temperature']
  }
};

// Create the generation config
const config = {
  tools: tools, // not defined here.
  toolConfig: toolConfig,
};
```

## Appel de fonction automatique (Python uniquement)

Lorsque vous utilisez le SDK Python, vous pouvez fournir des fonctions Python directement en tant qu'outils. Le SDK convertit automatiquement la fonction Python en déclarations, gère l'exécution de l'appel de fonction et le cycle de réponse à votre place. Le SDK Python effectue ensuite automatiquement les opérations suivantes :

1.  Détecte les réponses d'appel de fonction du modèle.
2.  Appelez la fonction Python correspondante dans votre code.
3.  Renvoie la réponse de la fonction au modèle.
4.  Renvoie la réponse textuelle finale du modèle.

Pour utiliser cette fonctionnalité, définissez votre fonction avec des indications de type et une docstring, puis transmettez la fonction elle-même (et non une déclaration JSON) en tant qu'outil :

### Python

```
from google import genai
from google.genai import types

# Define the function with type hints and docstring
def get_current_temperature(location: str) -> dict:
    """Gets the current temperature for a given location.

    Args:
        location: The city and state, e.g. San Francisco

    Returns:
        A dictionary containing the temperature and unit.
    """
    # ... (implementation) ...
    return {"temperature": 25, "unit": "Celsius"}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_current_temperature]
)  # Pass the function itself

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What's the temperature in Boston?",
    config=config,
)

print(response.text)  # The SDK handles the function call and returns the final text
```

Vous pouvez désactiver l'appel automatique des fonctions avec :

### Python

```
config = types.GenerateContentConfig(
    tools=[get_current_temperature],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
)
```

### Déclaration automatique du schéma de fonction

L'extraction automatique de schémas à partir de fonctions Python ne fonctionne pas dans tous les cas. Par exemple, il ne gère pas les cas où vous décrivez les champs d'un objet de dictionnaire imbriqué. L'API peut décrire l'un des types suivants :

### Python

```
AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])
```

Pour voir à quoi ressemble le schéma inféré, vous pouvez le convertir à l'aide de [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable) :

### Python

```
def multiply(a: float, b: float):
    """Returns a * b."""
    return a * b

fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

# to_json_dict() provides a clean JSON representation.
print(fn_decl.to_json_dict())
```

## Utilisation de plusieurs outils : combinez des outils natifs avec l'appel de fonction

Vous pouvez activer plusieurs outils combinant des outils natifs avec l'appel de fonction en même temps. Voici un exemple qui active deux outils, l'[ancrage avec la recherche Google](https://ai.google.dev/gemini-api/docs/grounding?hl=fr) et l'[exécution de code](https://ai.google.dev/gemini-api/docs/code-execution?hl=fr), dans une requête à l'aide de l'[API Live](https://ai.google.dev/gemini-api/docs/live?hl=fr).

### Python

```
# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Multiple tasks example - combining lights, code execution, and search
const prompt = `
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
`;

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
];

// Execute the prompt with specified tools in audio modality
await run(prompt, {tools: tools, modality: "AUDIO"});
```

Les développeurs Python peuvent essayer cette fonctionnalité dans le [notebook "Utilisation de l'outil d'API en direct"](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).

## Protocole MCP (Model Context Protocol)

Le [protocole MCP (Model Context Protocol)](https://modelcontextprotocol.io/introduction) est une norme ouverte permettant de connecter des applications d'IA à des outils et des données externes. MCP fournit un protocole commun permettant aux modèles d'accéder au contexte, comme les fonctions (outils), les sources de données (ressources) ou les requêtes prédéfinies.

Les SDK Gemini sont compatibles avec le MCP, ce qui réduit le code récurrent et offre un [appel automatique des outils](https://ai.google.dev/gemini-api/docs/function-calling?hl=fr#automatic_function_calling_python_only) pour les outils MCP. Lorsque le modèle génère un appel d'outil MCP, le SDK client Python et JavaScript peut exécuter automatiquement l'outil MCP et renvoyer la réponse au modèle dans une requête ultérieure, poursuivant cette boucle jusqu'à ce que le modèle n'effectue plus d'appels d'outil.

Vous trouverez ici un exemple d'utilisation d'un serveur MCP local avec le SDK Gemini et `mcp`.

### Python

Assurez-vous que la dernière version du [SDK `mcp`](https://modelcontextprotocol.io/introduction) est installée sur la plate-forme de votre choix.

```
pip install mcp
```

```
import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool
                    # Uncomment if you **don't** want the SDK to automatically call the tool
                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
                    #     disable=True
                    # ),
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())
```

### JavaScript

Assurez-vous que la dernière version du SDK `mcp` est installée sur la plate-forme de votre choix.

```
npm install @modelcontextprotocol/sdk
```

```
import { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

// Create server parameters for stdio connection
const serverParams = new StdioClientTransport({
  command: "npx", // Executable
  args: ["-y", "@philschmid/weather-mcp"] // MCP Server
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  }
);

// Configure the client
const ai = new GoogleGenAI({});

// Initialize the connection between client and server
await client.connect(serverParams);

// Send request to the model with MCP tools
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,
  config: {
    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool
    // Uncomment if you **don't** want the sdk to automatically call the tool
    // automaticFunctionCalling: {
    //   disable: true,
    // },
  },
});
console.log(response.text)

// Close the connection
await client.close();
```

### Limites de la compatibilité intégrée avec MCP

La prise en charge intégrée du MCP est une fonctionnalité [expérimentale](https://ai.google.dev/gemini-api/docs/models?hl=fr#preview) dans nos SDK et présente les limites suivantes :

*   Seuls les outils sont acceptés, et non les ressources ni les requêtes.
*   Il est disponible pour les SDK Python et JavaScript/TypeScript.
*   Des modifications destructives peuvent être apportées dans les prochaines versions.

L'intégration manuelle des serveurs MCP est toujours une option si ceux-ci limitent ce que vous créez.

## Modèles compatibles

Cette section liste les modèles et leurs fonctionnalités d'appel de fonction. Les modèles expérimentaux ne sont pas inclus. Vous trouverez une présentation complète des fonctionnalités sur la page [Présentation du modèle](https://ai.google.dev/gemini-api/docs/models?hl=fr).

Modèle

Appel de fonction

Appel de fonction parallèle

Appel de fonction compositionnel

Gemini 2.5 Pro

✔️

✔️

✔️

Gemini 2.5 Flash

✔️

✔️

✔️

Gemini 2.5 Flash-Lite

✔️

✔️

✔️

Gemini 2.0 Flash

✔️

✔️

✔️

Gemini 2.0 Flash-Lite

X

X

X

## Bonnes pratiques

*   **Descriptions des fonctions et des paramètres** : soyez extrêmement clair et précis dans vos descriptions. Le modèle s'appuie sur ces éléments pour choisir la fonction appropriée et fournir les arguments appropriés.
*   **Noms** : utilisez des noms de fonction descriptifs (sans espaces, points ni tirets).
*   **Typage strict** : utilisez des types spécifiques (entier, chaîne, énumération) pour les paramètres afin de réduire les erreurs. Si un paramètre possède un ensemble limité de valeurs valides, utilisez un énumérateur.
*   **Sélection d'outils** : bien que le modèle puisse utiliser un nombre arbitraire d'outils, en fournir trop peut augmenter le risque de sélectionner un outil incorrect ou non optimal. Pour de meilleurs résultats, essayez de ne fournir que les outils pertinents pour le contexte ou la tâche, en limitant idéalement l'ensemble actif à 10 à 20 éléments. Envisagez de sélectionner des outils de manière dynamique en fonction du contexte de la conversation si vous disposez d'un grand nombre d'outils.
*   **Prompt engineering** (ingénierie des requêtes) :
    *   Fournissez du contexte : indiquez au modèle son rôle (par exemple, "Vous êtes un assistant météo serviable.")
    *   Donnez des instructions : indiquez comment et quand utiliser les fonctions (par exemple, "N'essayez pas de deviner les dates. Utilisez toujours une date future pour les prévisions.").
    *   Encouragez les clarifications : demandez au modèle de poser des questions de clarification si nécessaire.
*   **Température** : utilisez une température basse (par exemple, 0) pour des appels de fonction plus déterministes et fiables.
*   **Validation** : si un appel de fonction a des conséquences importantes (par exemple, passer une commande), validez l'appel auprès de l'utilisateur avant de l'exécuter.
*   **Gestion des erreurs** : implémentez une gestion des erreurs robuste dans vos fonctions pour gérer correctement les entrées inattendues ou les échecs d'API. Renvoyez des messages d'erreur informatifs que le modèle peut utiliser pour générer des réponses utiles à l'utilisateur.
*   **Sécurité** : soyez attentif à la sécurité lorsque vous appelez des API externes. Utilisez des mécanismes d'authentification et d'autorisation appropriés. Évitez d'exposer des données sensibles dans les appels de fonction.
*   **Limites de jetons** : les descriptions et les paramètres de fonction sont comptabilisés dans la limite de jetons d'entrée. Si vous atteignez les limites de jetons, envisagez de limiter le nombre de fonctions ou la longueur des descriptions, et de décomposer les tâches complexes en ensembles de fonctions plus petits et plus ciblés.

## Remarques et limites

*   Seul un [sous-ensemble du schéma OpenAPI](https://ai.google.dev/api/caching?hl=fr#FunctionDeclaration) est accepté.
*   Les types de paramètres compatibles dans Python sont limités.
*   L'appel de fonction automatique est une fonctionnalité du SDK Python uniquement.

Envoyer des commentaires

Sauf indication contraire, le contenu de cette page est régi par une licence [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/), et les échantillons de code sont régis par une licence [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Pour en savoir plus, consultez les [Règles du site Google Developers](https://developers.google.com/site-policies?hl=fr). Java est une marque déposée d'Oracle et/ou de ses sociétés affiliées.

Dernière mise à jour le 2025/06/27 (UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/audio

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Audio understanding

Gemini can analyze and understand audio input, enabling use cases like the following:

*   Describe, summarize, or answer questions about audio content.
*   Provide a transcription of the audio.
*   Analyze specific segments of the audio.

This guide shows you how to use the Gemini API to generate a text response to audio input.

### Before you begin

Before calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.

## Input audio

You can provide audio data to Gemini in the following ways:

*   [Upload an audio file](#upload-audio) before making a request to `generateContent`.
*   [Pass inline audio data](#inline-audio) with the request to `generateContent`.

### Upload an audio file

You can use the [Files API](/gemini-api/docs/files) to upload an audio file. Always use the Files API when the total request size (including the files, text prompt, system instructions, etc.) is larger than 20 MB.

The following code uploads an audio file and then uses the file in a call to `generateContent`.

### Python

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mp3" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

To learn more about working with media files, see [Files API](/gemini-api/docs/files).

### Pass audio data inline

Instead of uploading an audio file, you can pass inline audio data in the request to `generateContent`:

### Python

```
from google.genai import types

with open('path/to/small-sample.mp3', 'rb') as f:
    audio_bytes = f.read()

response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[
    'Describe this audio clip',
    types.Part.from_bytes(
      data=audio_bytes,
      mime_type='audio/mp3',
    )
  ]
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64AudioFile = fs.readFileSync("path/to/small-sample.mp3", {
  encoding: "base64",
});

const contents = [
  { text: "Please summarize the audio." },
  {
    inlineData: {
      mimeType: "audio/mp3",
      data: base64AudioFile,
    },
  },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  audioBytes, _ := os.ReadFile("/path/to/small-sample.mp3")

  parts := []*genai.Part{
      genai.NewPartFromText("Describe this audio clip"),
    &genai.Part{
      InlineData: &genai.Blob{
        MIMEType: "audio/mp3",
        Data:     audioBytes,
      },
    },
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

A few things to keep in mind about inline audio data:

*   The maximum request size is 20 MB, which includes text prompts, system instructions, and files provided inline. If your file's size will make the _total request size_ exceed 20 MB, then use the Files API to [upload an audio file](#upload-audio) for use in the request.
*   If you're using an audio sample multiple times, it's more efficient to [upload an audio file](#upload-audio).

## Get a transcript

To get a transcript of audio data, just ask for it in the prompt:

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
prompt = 'Generate a transcript of the speech.'

response = client.models.generate_content(
  model='gemini-2.5-flash',
  contents=[prompt, myfile]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const result = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
    "Generate a transcript of the speech.",
  ]),
});
console.log("result.text=", result.text);
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Generate a transcript of the speech."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

## Refer to timestamps

You can refer to specific sections of an audio file using timestamps of the form `MM:SS`. For example, the following prompt requests a transcript that

*   Starts at 2 minutes 30 seconds from the beginning of the file.
*   Ends at 3 minutes 29 seconds from the beginning of the file.
    

### Python

```
# Create a prompt containing timestamps.
prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

### JavaScript

```
// Create a prompt containing timestamps.
const prompt = "Provide a transcript of the speech from 02:30 to 03:29."
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromText("Provide a transcript of the speech " +
                            "between the timestamps 02:30 and 03:29."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

## Count tokens

Call the `countTokens` method to get a count of the number of tokens in an audio file. For example:

### Python

```
response = client.models.count_tokens(
  model='gemini-2.5-flash',
  contents=[myfile]
)

print(response)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.5-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
  ]),
});
console.log(countTokensResponse.totalTokens);
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  localAudioPath := "/path/to/sample.mp3"
  uploadedFile, _ := client.Files.UploadFromPath(
      ctx,
      localAudioPath,
      nil,
  )

  parts := []*genai.Part{
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }
  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  tokens, _ := client.Models.CountTokens(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Printf("File %s is %d tokens\n", localAudioPath, tokens.TotalTokens)
}
```

## Supported audio formats

Gemini supports the following audio format MIME types:

*   WAV - `audio/wav`
*   MP3 - `audio/mp3`
*   AIFF - `audio/aiff`
*   AAC - `audio/aac`
*   OGG Vorbis - `audio/ogg`
*   FLAC - `audio/flac`

## Technical details about audio

*   Gemini represents each second of audio as 32 tokens; for example, one minute of audio is represented as 1,920 tokens.
*   Gemini can "understand" non-speech components, such as birdsong or sirens.
*   The maximum supported length of audio data in a single prompt is 9.5 hours. Gemini doesn't limit the _number_ of audio files in a single prompt; however, the total combined length of all audio files in a single prompt can't exceed 9.5 hours.
*   Gemini downsamples audio files to a 16 Kbps data resolution.
*   If the audio source contains multiple channels, Gemini combines those channels into a single channel.

## What's next

This guide shows how to generate text in response to audio data. To learn more, see the following resources:

*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
*   [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/video-understanding

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Video understanding

Gemini models can process videos, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:

*   Describe, segment, and extract information from videos
*   Answer questions about video content
*   Refer to specific timestamps within a video

Gemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible. This guide shows how to use the Gemini API to generate text responses based on video inputs.

## Video input

You can provide videos as input to Gemini in the following ways:

*   [Upload a video file](#upload-video) using the File API before making a request to `generateContent`. Use this method for files larger than 20MB, videos longer than approximately 1 minute, or when you want to reuse the file across multiple requests.
*   [Pass inline video data](#inline-video) with the request to `generateContent`. Use this method for smaller files (<20MB) and shorter durations.
*   [Include a YouTube URL](#youtube) directly in the prompt.

### Upload a video file

You can use the [Files API](/gemini-api/docs/files) to upload a video file. Always use the Files API when the total request size (including the file, text prompt, system instructions, etc.) is larger than 20 MB, the video duration is significant, or if you intend to use the same video in multiple prompts.

The File API accepts video file formats directly. This example uses the short NASA film ["Jupiter's Great Red Spot Shrinks and Grows"](https://www.youtube.com/watch?v=JDi4IdtvDVE0). Credit: Goddard Space Flight Center (GSFC)/David Ladd (2018).

"Jupiter's Great Red Spot Shrinks and Grows" is in the public domain and does not show identifiable people. ([NASA image and media usage guidelines.](https://www.nasa.gov/nasa-brand-center/images-and-media/))

The following code downloads the sample video, uploads it using the File API, waits for it to be processed, and then uses the file reference in a `generateContent` request.

### Python

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp4")

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[myfile, "Summarize this video. Then create a quiz with an answer key based on the information in this video."]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp4",
    config: { mimeType: "video/mp4" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Summarize this video. Then create a quiz with an answer key based on the information in this video.",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.mp4", nil)

parts := []*genai.Part{
    genai.NewPartFromText("Summarize this video. Then create a quiz with an answer key based on the information in this video."),
    genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.0-flash",
    contents,
    nil,
)

fmt.Println(result.Text())
```

### REST

```
VIDEO_PATH="path/to/sample.mp4"
MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
NUM_BYTES=$(wc -c < "${VIDEO_PATH}")
DISPLAY_NAME=VIDEO

tmp_header_file=upload-header.tmp

echo "Starting file upload..."
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D ${tmp_header_file} \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

echo "Uploading video data..."
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${VIDEO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

echo "File uploaded successfully. File URI: ${file_uri}"

# --- 3. Generate content using the uploaded video file ---
echo "Generating content from video..."
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Summarize this video. Then create a quiz with an answer key based on the information in this video."}]
        }]
      }' 2> /dev/null > response.json

jq -r ".candidates[].content.parts[].text" response.json
```

To learn more about working with media files, see [Files API](/gemini-api/docs/files).

### Pass video data inline

Instead of uploading a video file using the File API, you can pass smaller videos directly in the request to `generateContent`. This is suitable for shorter videos under 20MB total request size.

Here's an example of providing inline video data:

### Python

```
# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

response = client.models.generate_content(
    model='models/gemini-2.0-flash',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(data=video_bytes, mime_type='video/mp4')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64VideoFile = fs.readFileSync("path/to/small-sample.mp4", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "video/mp4",
      data: base64VideoFile,
    },
  },
  { text: "Please summarize the video in 3 sentences." }
];

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(response.text);
```

### REST

```
VIDEO_PATH=/path/to/your/video.mp4

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"video/mp4",
                "data": "'$(base64 $B64FLAGS $VIDEO_PATH)'"
              }
            },
            {"text": "Please summarize the video in 3 sentences."}
        ]
      }]
    }' 2> /dev/null
```

### Include a YouTube URL

The Gemini API and AI Studio support YouTube URLs as a file data `Part`. You can include a YouTube URL with a prompt asking the model to summarize, translate, or otherwise interact with the video content.

**Limitations:**

*   For the free tier, you can't upload more than 8 hours of YouTube video per day.
*   For the paid tier, there is no limit based on video length.
*   For models before 2.5, you can upload only 1 video per request. For models after 2.5, you can upload a maximum of 10 videos per request.
*   You can only upload public videos (not private or unlisted videos).

The following example shows how to include a YouTube URL with a prompt:

### Python

```
response = client.models.generate_content(
    model='models/gemini-2.0-flash',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=9hE5-98ZeCg')
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-pro" });
const result = await model.generateContent([
  "Please summarize the video in 3 sentences.",
  {
    fileData: {
      fileUri: "https://www.youtube.com/watch?v=9hE5-98ZeCg",
    },
  },
]);
console.log(result.response.text());
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  parts := []*genai.Part{
      genai.NewPartFromText("Please summarize the video in 3 sentences."),
      genai.NewPartFromURI("https://www.youtube.com/watch?v=9hE5-98ZeCg","video/mp4"),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.0-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {"text": "Please summarize the video in 3 sentences."},
            {
              "file_data": {
                "file_uri": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
              }
            }
        ]
      }]
    }' 2> /dev/null
```

## Refer to timestamps in the content

You can ask questions about specific points in time within the video using timestamps of the form `MM:SS`.

### Python

```
prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?" # Adjusted timestamps for the NASA video
```

### JavaScript

```
const prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?";
```

### Go

    ```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
         // Adjusted timestamps for the NASA video
        genai.NewPartFromText("What are the examples given at 00:05 and " +
            "00:10 supposed to show us?"),
    }
```

### REST

```
PROMPT="What are the examples given at 00:05 and 00:10 supposed to show us?"
```

## Transcribe video and provide visual descriptions

The Gemini models can transcribe and provide visual descriptions of video content by processing both the audio track and visual frames. For visual descriptions, the model samples the video at a rate of **1 frame per second**. This sampling rate may affect the level of detail in the descriptions, particularly for videos with rapidly changing visuals.

### Python

```
prompt = "Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions."
```

### JavaScript

```
const prompt = "Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.";
```

### Go

    ```
    prompt := []*genai.Part{
        genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
        genai.NewPartFromText("Transcribe the audio from this video, giving timestamps for salient events in the video. Also " +
            "provide visual descriptions."),
    }
```

### REST

```
PROMPT="Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions."
```

## Customize video processing

You can customize video processing in the Gemini API by setting clipping intervals or providing custom frame rate sampling.

### Set clipping intervals

You can clip video by specifying `videoMetadata` with start and end offsets.

### Python

```
response = client.models.generate_content(
    model='models/gemini-2.5-flash-preview-05-20',
    contents=types.Content(
        parts=[
            types.Part(
                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=XEzRZ35urlk'),
                video_metadata=types.VideoMetadata(
                    start_offset='1250s',
                    end_offset='1570s'
                )
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

### Set a custom frame rate

You can set custom frame rate sampling by passing an `fps` argument to `videoMetadata`.

### Python

```
# Only for videos of size <20Mb
video_file_name = "/path/to/your/video.mp4"
video_bytes = open(video_file_name, 'rb').read()

response = client.models.generate_content(
    model='models/gemini-2.5-flash-preview-05-20',
    contents=types.Content(
        parts=[
            types.Part(
                inline_data=types.Blob(
                    data=video_bytes,
                    mime_type='video/mp4'),
                video_metadata=types.VideoMetadata(fps=5)
            ),
            types.Part(text='Please summarize the video in 3 sentences.')
        ]
    )
)
```

By default 1 frame per second (FPS) is sampled from the video. You might want to set low FPS (< 1) for long videos. This is especially useful for mostly static videos (e.g. lectures). If you want to capture more details in rapidly changing visuals, consider setting a higher FPS value.

## Supported video formats

Gemini supports the following video format MIME types:

*   `video/mp4`
*   `video/mpeg`
*   `video/mov`
*   `video/avi`
*   `video/x-flv`
*   `video/mpg`
*   `video/webm`
*   `video/wmv`
*   `video/3gpp`

## Technical details about videos

*   **Supported models & context**: All Gemini 2.0 and 2.5 models can process video data.
    *   Models with a 2M context window can process videos up to 2 hours long at default media resolution or 6 hours long at low media resolution, while models with a 1M context window can process videos up to 1 hour long at default media resolution or 3 hours long at low media resolution.
*   **File API processing**: When using the File API, videos are sampled at 1 frame per second (FPS) and audio is processed at 1Kbps (single channel). Timestamps are added every second.
    *   These rates are subject to change in the future for improvements in inference.
*   **Token calculation**: Each second of video is tokenized as follows:
    *   Individual frames (sampled at 1 FPS):
        *   If [`mediaResolution`](/api/generate-content#MediaResolution) is set to low, frames are tokenized at 66 tokens per frame.
        *   Otherwise, frames are tokenized at 258 tokens per frame.
    *   Audio: 32 tokens per second.
    *   Metadata is also included.
    *   Total: Approximately 300 tokens per second of video at default media resolution, or 100 tokens per second of video at low media resolution.
*   **Timestamp format**: When referring to specific moments in a video within your prompt, use the `MM:SS` format (e.g., `01:15` for 1 minute and 15 seconds).
*   **Best practices**:
    *   Use only one video per prompt request for optimal results.
    *   If combining text and a single video, place the text prompt _after_ the video part in the `contents` array.
    *   Be aware that fast action sequences might lose detail due to the 1 FPS sampling rate. Consider slowing down such clips if necessary.

## What's next

This guide shows how to upload video files and generate text outputs from video inputs. To learn more, see the following resources:

*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
*   [Files API](/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/url-context

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# URL context

Using the URL context tool, you can provide Gemini with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.

This tool is useful for tasks like the following:

*   Extracting key data points or talking points from articles
*   Comparing information across multiple links
*   Synthesizing data from several sources
*   Answering questions based on the content of a specific page or pages
*   Analyzing content for specific purposes (like writing a job description or creating test questions)

This guide explains how to use the URL context tool in the Gemini API.

## Use URL context

You can use the URL context tool in two main ways, by itself or in conjunction with [Grounding with Google Search](/gemini-api/docs/grounding).

**URL Context Only**

You provide specific URLs that you want the model to analyze directly in your prompt.

Example prompts:

```
Summarize this document: YOUR_URLs

Extract the key features from the product description on this page: YOUR_URLs
```

**Grounding with Google Search + URL Context**

You can also enable both URL context and Grounding with Google Search together. You can enter a prompt with or without URLs. The model may first search for relevant information and then use the URL context tool to read the content of the search results for a more in-depth understanding.

Example prompts:

```
Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.

Recommend 3 books for beginners to read to learn more about the latest YOUR_subject.
```

### Code examples with URL context only

### Python

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch

client = genai.Client()
model_id = "gemini-2.5-flash"

url_context_tool = Tool(
    url_context = types.UrlContext
)

response = client.models.generate_content(
    model=model_id,
    contents="Compare recipes from YOUR_URL1 and YOUR_URL2",
    config=GenerateContentConfig(
        tools=[url_context_tool],
        response_modalities=["TEXT"],
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)

```

### Javascript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Compare recipes from YOUR_URL1 and YOUR_URL2",
    ],
    config: {
      tools: [{urlContext: {}}],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Compare recipes from YOUR_URL1 and YOUR_URL2"}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          }
      ]
  }' > result.json

cat result.json
```

### Code examples with Grounding with Google Search

### Python

```
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch

client = genai.Client()
model_id = "gemini-2.5-flash"

tools = []
tools.append(Tool(url_context=types.UrlContext))
tools.append(Tool(google_search=types.GoogleSearch))

response = client.models.generate_content(
    model=model_id,
    contents="Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    config=GenerateContentConfig(
        tools=tools,
        response_modalities=["TEXT"],
    )
)

for each in response.candidates[0].content.parts:
    print(each.text)
# get URLs retrieved for context
print(response.candidates[0].url_context_metadata)

```

### Javascript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    ],
    config: {
      tools: [{urlContext: {}}, {googleSearch: {}}],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
      "contents": [
          {
              "parts": [
                  {"text": "Give me three day events schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute."}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          },
          {
              "google_search": {}
          }
      ]
  }' > result.json

cat result.json
```

For more details about Grounding with Google Search, see the [overview](/gemini-api/docs/grounding) page.

### Contextual response

The model's response will be based on the content it retrieved from the URLs. If the model retrieved content from URLs, the response will include `url_context_metadata`. Such a response might look something like the following (parts of the response have been omitted for brevity):

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "... \n"
          }
        ],
        "role": "model"
      },
      ...
      "url_context_metadata":
      {
          "url_metadata":
          [
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/1234567890abcdef",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/abcdef1234567890",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "YOUR_URL",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/fedcba0987654321",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            }
          ]
        }
    }
}
```

## Supported models

*   [gemini-2.5-pro](/gemini-api/docs/models#gemini-2.5-pro)
*   [gemini-2.5-flash](/gemini-api/docs/models#gemini-2.5-flash)
*   [gemini-2.5-flash-lite](/gemini-api/docs/models#gemini-2.5-flash-lite)
*   [gemini-2.0-flash](/gemini-api/docs/models#gemini-2.0-flash)
*   [gemini-2.0-flash-live-001](/gemini-api/docs/models#live-api)

## Limitations

*   The tool will consume up to 20 URLs per request for analysis.
*   For best results during experimental phase, use the tool on standard web pages rather than multimedia content such as YouTube videos.
*   During experimental phase, the tool is free to use. Billing to come later.
*   The experimental release has the following quotas:
    
    *   1500 queries per day per project for requests made through the Gemini API
    *   100 queries per day per user in Google AI Studio

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/google-search

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Grounding with Google Search

Grounding with Google Search connects the Gemini model to real-time web content and works with all [available languages](/gemini-api/docs/models/gemini#available-languages). This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.

Grounding helps you build applications that can:

*   **Increase factual accuracy:** Reduce model hallucinations by basing responses on real-world information.
*   **Access real-time information:** Answer questions about recent events and topics.
*   **Provide citations:** Build user trust by showing the sources for the model's claims.
    

### Python

```
from google import genai
from google.genai import types

# Configure the client
client = genai.Client()

# Define the grounding tool
grounding_tool = types.Tool(
    google_search=types.GoogleSearch()
)

# Configure generation settings
config = types.GenerateContentConfig(
    tools=[grounding_tool]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Who won the euro 2024?",
    config=config,
)

# Print the grounded response
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Define the grounding tool
const groundingTool = {
  googleSearch: {},
};

// Configure generation settings
const config = {
  tools: [groundingTool],
};

// Make the request
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

// Print the grounded response
console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {"text": "Who won the euro 2024?"}
        ]
      }
    ],
    "tools": [
      {
        "google_search": {}
      }
    ]
  }'
```

You can learn more by trying the [Search tool notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb).

## How grounding with Google Search works

When you enable the `google_search` tool, the model handles the entire workflow of searching, processing, and citing information automatically.

![grounding-overview](/static/gemini-api/docs/images/google-search-tool-overview.png)

1.  **User Prompt:** Your application sends a user's prompt to the Gemini API with the `google_search` tool enabled.
2.  **Prompt Analysis:** The model analyzes the prompt and determines if a Google Search can improve the answer.
3.  **Google Search:** If needed, the model automatically generates one or multiple search queries and executes them.
4.  **Search Results Processing:** The model processes the search results, synthesizes the information, and formulates a response.
5.  **Grounded Response:** The API returns a final, user-friendly response that is grounded in the search results. This response includes the model's text answer and `groundingMetadata` with the search queries, web results, and citations.

## Understanding the Grounding Response

When a response is successfully grounded, the response includes a `groundingMetadata` field. This structured data is essential for verifying claims and building a rich citation experience in your application.

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}
```

The Gemini API returns the following information with the `groundingMetadata`:

*   `webSearchQueries` : Array of the search queries used. This is useful for debugging and understanding the model's reasoning process.
*   `searchEntryPoint` : Contains the HTML and CSS to render the required Search Suggestions. Full usage requirements are detailed in the [Terms of Service](/gemini-api/terms#grounding-with-google-search).
*   `groundingChunks` : Array of objects containing the web sources (`uri` and `title`).
*   `groundingSupports` : Array of chunks to connect model response `text` to the sources in `groundingChunks`. Each chunk links a text `segment` (defined by `startIndex` and `endIndex`) to one or more `groundingChunkIndices`. This is the key to building inline citations.

Grounding with Google Search can also be used in combination with the [URL context tool](/gemini-api/docs/url-context) to ground responses in both public web data and the specific URLs you provide.

## Attributing Sources with inline Citations

The API returns structured citation data, giving you complete control over how you display sources in your user interface. You can use the `groundingSupports` and `groundingChunks` fields to link the model's statements directly to their sources. Here is a common pattern for processing the metadata to create a response with inline, clickable citations.

### Python

```
def add_citations(response):
    text = response.text
    supports = response.candidates[0].grounding_metadata.grounding_supports
    chunks = response.candidates[0].grounding_metadata.grounding_chunks

    # Sort supports by end_index in descending order to avoid shifting issues when inserting.
    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

    for support in sorted_supports:
        end_index = support.segment.end_index
        if support.grounding_chunk_indices:
            # Create citation string like [1](link1)[2](link2)
            citation_links = []
            for i in support.grounding_chunk_indices:
                if i < len(chunks):
                    uri = chunks[i].web.uri
                    citation_links.append(f"[{i + 1}]({uri})")

            citation_string = ", ".join(citation_links)
            text = text[:end_index] + citation_string + text[end_index:]

    return text

# Assuming response with grounding metadata
text_with_citations = add_citations(response)
print(text_with_citations)
```

### JavaScript

```
function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);
```

The new response with inline citations will look like this:

```
Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)
```

## Pricing

When you use Grounding with Google Search, your project is billed per API request that includes the `google_search` tool. If the model decides to execute multiple search queries to answer a single prompt (for example, searching for `"UEFA Euro 2024 winner"` and `"Spain vs England Euro 2024 final score"` within the same API call), this counts as a single billable use of the tool for that request.

For detailed pricing information, see the [Gemini API pricing page](https://ai.google.dev/gemini-api/docs/pricing).

## Supported Models

Experimental and Preview models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.

Model

Grounding with Google Search

Gemini 2.5 Pro

✔️

Gemini 2.5 Flash

✔️

Gemini 2.0 Flash

✔️

Gemini 1.5 Pro

✔️

Gemini 1.5 Flash

✔️

## Grounding with Gemini 1.5 Models (Legacy)

While the `google_search` tool is recommended for Gemini 2.0 and later, Gemini 1.5 support a legacy tool named `google_search_retrieval`. This tool provides a `dynamic` mode that allows the model to decide whether to perform a search based on its confidence that the prompt requires fresh information. If the model's confidence is above a `dynamic_threshold` you set (a value between 0.0 and 1.0), it will perform a search.

### Python

```
# Note: This is a legacy approach for Gemini 1.5 models.
# The 'google_search' tool is recommended for all new development.
import os
from google import genai
from google.genai import types

client = genai.Client()

retrieval_tool = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            dynamic_threshold=0.7 # Only search if confidence > 70%
        )
    )
)

config = types.GenerateContentConfig(
    tools=[retrieval_tool]
)

response = client.models.generate_content(
    model='gemini-1.5-flash',
    contents="Who won the euro 2024?",
    config=config,
)
print(response.text)
if not response.candidates[0].grounding_metadata:
  print("\nModel answered from its own knowledge.")
```

### JavaScript

```
// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \

  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {"parts": [{"text": "Who won the euro 2024?"}]}
    ],
    "tools": [{
      "google_search_retrieval": {
        "dynamic_retrieval_config": {
          "mode": "MODE_DYNAMIC",
          "dynamic_threshold": 0.7
        }
      }
    }]
  }'
```

## What's next

*   Try the [Grounding with Google Search in the Gemini API Cookbook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb).
*   Learn about other available tools, like [Function Calling](/gemini-api/docs/function-calling).
*   Learn how to augment prompts with specific URLs using the [URL context tool](/gemini-api/docs/url-context).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/code-execution

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Code execution

The Gemini API provides a code execution tool that enables the model to generate and run Python code. The model can then learn iteratively from the code execution results until it arrives at a final output. You can use code execution to build applications that benefit from code-based reasoning. For example, you can use code execution to solve equations or process text. You can also use the [libraries](#supported-libraries) included in the code execution environment to perform more specialized tasks.

Gemini is only able to execute code in Python. You can still ask Gemini to generate code in another language, but the model can't use the code execution tool to run it.

## Enable code execution

To enable code execution, configure the code execution tool on the model. This allows the model to generate and run code.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50.",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

let response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: [
    "What is the sum of the first 50 prime numbers? " +
      "Generate and run code for the calculation, and make sure you get all 50.",
  ],
  config: {
    tools: [{ codeExecution: {} }],
  },
});

const parts = response?.candidates?.[0]?.content?.parts || [];
parts.forEach((part) => {
  if (part.text) {
    console.log(part.text);
  }

  if (part.executableCode && part.executableCode.code) {
    console.log(part.executableCode.code);
  }

  if (part.codeExecutionResult && part.codeExecutionResult.output) {
    console.log(part.codeExecutionResult.output);
  }
});
```

### Go

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    result, _ := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("What is the sum of the first 50 prime numbers? " +
                  "Generate and run code for the calculation, and make sure you get all 50."),
        config,
    )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d ' {"tools": [{"code_execution": {}}],
    "contents": {
      "parts":
        {
            "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
        }
    },
}'
```

The output might look something like the following, which has been formatted for readability:

```
Okay, I need to calculate the sum of the first 50 prime numbers. Here's how I'll
approach this:

1.  **Generate Prime Numbers:** I'll use an iterative method to find prime
    numbers. I'll start with 2 and check if each subsequent number is divisible
    by any number between 2 and its square root. If not, it's a prime.
2.  **Store Primes:** I'll store the prime numbers in a list until I have 50 of
    them.
3.  **Calculate the Sum:**  Finally, I'll sum the prime numbers in the list.

Here's the Python code to do this:

def is_prime(n):
  """Efficiently checks if a number is prime."""
  if n <= 1:
    return False
  if n <= 3:
    return True
  if n % 2 == 0 or n % 3 == 0:
    return False
  i = 5
  while i * i <= n:
    if n % i == 0 or n % (i + 2) == 0:
      return False
    i += 6
  return True

primes = []
num = 2
while len(primes) < 50:
  if is_prime(num):
    primes.append(num)
  num += 1

sum_of_primes = sum(primes)
print(f'{primes=}')
print(f'{sum_of_primes=}')

primes=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67,
71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151,
157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]
sum_of_primes=5117

The sum of the first 50 prime numbers is 5117.
```

This output combines several content parts that the model returns when using code execution:

*   `text`: Inline text generated by the model
*   `executableCode`: Code generated by the model that is meant to be executed
*   `codeExecutionResult`: Result of the executable code

The naming conventions for these parts vary by programming language.

## Use code execution in chat

You can also use code execution as part of a chat.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

chat = client.chats.create(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    ),
)

response = chat.send_message("I have a math question for you.")
print(response.text)

response = chat.send_message(
    "What is the sum of the first 50 prime numbers? "
    "Generate and run code for the calculation, and make sure you get all 50."
)

for part in response.candidates[0].content.parts:
    if part.text is not None:
        print(part.text)
    if part.executable_code is not None:
        print(part.executable_code.code)
    if part.code_execution_result is not None:
        print(part.code_execution_result.output)
```

### JavaScript

```
import {GoogleGenAI} from "@google/genai";

const ai = new GoogleGenAI({});

const chat = ai.chats.create({
  model: "gemini-2.5-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "I have a math question for you:" }],
    },
    {
      role: "model",
      parts: [{ text: "Great! I'm ready for your math question. Please ask away." }],
    },
  ],
  config: {
    tools: [{codeExecution:{}}],
  }
});

const response = await chat.sendMessage({
  message: "What is the sum of the first 50 prime numbers? " +
            "Generate and run code for the calculation, and make sure you get all 50."
});
console.log("Chat response:", response.text);
```

### Go

```
package main

import (
    "context"
    "fmt"
    "os"
    "google.golang.org/genai"
)

func main() {

    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        Tools: []*genai.Tool{
            {CodeExecution: &genai.ToolCodeExecution{}},
        },
    }

    chat, _ := client.Chats.Create(
        ctx,
        "gemini-2.5-flash",
        config,
        nil,
    )

    result, _ := chat.SendMessage(
                    ctx,
                    genai.Part{Text: "What is the sum of the first 50 prime numbers? " +
                                          "Generate and run code for the calculation, and " +
                                          "make sure you get all 50.",
                              },
                )

    fmt.Println(result.Text())
    fmt.Println(result.ExecutableCode())
    fmt.Println(result.CodeExecutionResult())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"tools": [{"code_execution": {}}],
    "contents": [
        {
            "role": "user",
            "parts": [{
                "text": "Can you print \"Hello world!\"?"
            }]
        },{
            "role": "model",
            "parts": [
              {
                "text": ""
              },
              {
                "executable_code": {
                  "language": "PYTHON",
                  "code": "\nprint(\"hello world!\")\n"
                }
              },
              {
                "code_execution_result": {
                  "outcome": "OUTCOME_OK",
                  "output": "hello world!\n"
                }
              },
              {
                "text": "I have printed \"hello world!\" using the provided python code block. \n"
              }
            ],
        },{
            "role": "user",
            "parts": [{
                "text": "What is the sum of the first 50 prime numbers? Generate and run code for the calculation, and make sure you get all 50."
            }]
        }
    ]
}'
```

## Input/output (I/O)

Starting with [Gemini 2.0 Flash](/gemini-api/docs/models/gemini#gemini-2.0-flash), code execution supports file input and graph output. Using these input and output capabilities, you can upload CSV and text files, ask questions about the files, and have [Matplotlib](https://matplotlib.org/) graphs generated as part of the response. The output files are returned as inline images in the response.

### I/O pricing

When using code execution I/O, you're charged for input tokens and output tokens:

**Input tokens:**

*   User prompt

**Output tokens:**

*   Code generated by the model
*   Code execution output in the code environment
*   Thinking tokens
*   Summary generated by the model

### I/O details

When you're working with code execution I/O, be aware of the following technical details:

*   The maximum runtime of the code environment is 30 seconds.
*   If the code environment generates an error, the model may decide to regenerate the code output. This can happen up to 5 times.
*   The maximum file input size is limited by the model token window. In AI Studio, using Gemini Flash 2.0, the maximum input file size is 1 million tokens (roughly 2MB for text files of the supported input types). If you upload a file that's too large, AI Studio won't let you send it.
*   Code execution works best with text and CSV files.
*   The input file can be passed in `part.inlineData` or `part.fileData` (uploaded via the [Files API](/gemini-api/docs/files)), and the output file is always returned as `part.inlineData`.

Single turn

Bidirectional (Multimodal Live API)

Models supported

All Gemini 2.0 and 2.5 models

Only Flash experimental models

File input types supported

.png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts

.png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts

Plotting libraries supported

Matplotlib, seaborn

Matplotlib, seaborn

[Multi-tool use](/gemini-api/docs/function-calling#multi-tool-use)

Yes (code execution + grounding only)

Yes

## Billing

There's no additional charge for enabling code execution from the Gemini API. You'll be billed at the current rate of input and output tokens based on the Gemini model you're using.

Here are a few other things to know about billing for code execution:

*   You're only billed once for the input tokens you pass to the model, and you're billed for the final output tokens returned to you by the model.
*   Tokens representing generated code are counted as output tokens. Generated code can include text and multimodal output like images.
*   Code execution results are also counted as output tokens.

The billing model is shown in the following diagram:

![code execution billing model](/static/gemini-api/docs/images/code-execution-diagram.png)

*   You're billed at the current rate of input and output tokens based on the Gemini model you're using.
*   If Gemini uses code execution when generating your response, the original prompt, the generated code, and the result of the executed code are labeled _intermediate tokens_ and are billed as _input tokens_.
*   Gemini then generates a summary and returns the generated code, the result of the executed code, and the final summary. These are billed as _output tokens_.
*   The Gemini API includes an intermediate token count in the API response, so you know why you're getting additional input tokens beyond your initial prompt.

## Limitations

*   The model can only generate and execute code. It can't return other artifacts like media files.
*   In some cases, enabling code execution can lead to regressions in other areas of model output (for example, writing a story).
*   There is some variation in the ability of the different models to use code execution successfully.

## Supported libraries

The code execution environment includes the following libraries:

*   attrs
*   chess
*   contourpy
*   fpdf
*   geopandas
*   imageio
*   jinja2
*   joblib
*   jsonschema
*   jsonschema-specifications
*   lxml
*   matplotlib
*   mpmath
*   numpy
*   opencv-python
*   openpyxl
*   packaging
*   pandas
*   pillow
*   protobuf
*   pylatex
*   pyparsing
*   PyPDF2
*   python-dateutil
*   python-docx
*   python-pptx
*   reportlab
*   scikit-learn
*   scipy
*   seaborn
*   six
*   striprtf
*   sympy
*   tabulate
*   tensorflow
*   toolz
*   xlrd

You can't install your own libraries.

## What's next

*   Try the [code execution Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Code_Execution.ipynb).
*   Learn about other Gemini API tools:
    *   [Function calling](/gemini-api/docs/function-calling)
    *   [Grounding with Google Search](/gemini-api/docs/grounding)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/live-tools

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Tool use with Live API

Tool use allows Live API to go beyond just conversation by enabling it to perform actions in the real-world and pull in external context while maintaining a real time connection. You can define tools such as [Function calling](/gemini-api/docs/function-calling), [Code execution](/gemini-api/docs/code-execution), and [Google Search](/gemini-api/docs/grounding) with the Live API.

## Overview of supported tools

Here's a brief overview of the available tools for each model:

Tool

Cascaded models  
`gemini-live-2.5-flash-preview`  
`gemini-2.0-flash-live-001`

`gemini-2.5-flash-preview-native-audio-dialog`

`gemini-2.5-flash-exp-native-audio-thinking-dialog`

**Search**

Yes

Yes

Yes

**Function calling**

Yes

Yes

No

**Code execution**

Yes

No

No

**Url context**

Yes

No

No

## Function calling

Live API supports function calling, just like regular content generation requests. Function calling lets the Live API interact with external data and programs, greatly increasing what your applications can accomplish.

You can define function declarations as part of the session configuration. After receiving tool calls, the client should respond with a list of `FunctionResponse` objects using the `session.send_tool_response` method.

See the [Function calling tutorial](/gemini-api/docs/function-calling) to learn more.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

# Simple function definitions
turn_on_the_lights = {"name": "turn_on_the_lights"}
turn_off_the_lights = {"name": "turn_off_the_lights"}

tools = [{"function_declarations": [turn_on_the_lights, turn_off_the_lights]}]
config = {"response_modalities": ["TEXT"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "Turn on the lights please"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.text is not None:
                    print(chunk.text)
            elif chunk.tool_call:
                function_responses = []
                for fc in chunk.tool_call.function_calls:
                    function_response = types.FunctionResponse(
                        id=fc.id,
                        name=fc.name,
                        response={ "result": "ok" } # simple, hard-coded function response
                    )
                    function_responses.append(function_response)

                await session.send_tool_response(function_responses=function_responses)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

// Simple function definitions
const turn_on_the_lights = { name: "turn_on_the_lights" } // , description: '...', parameters: { ... }
const turn_off_the_lights = { name: "turn_off_the_lights" }

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]

const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Turn on the lights please';
  session.sendClientContent({ turns: inputTurns });

  let turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
      }
    }
    else if (turn.toolCall) {
      const functionResponses = [];
      for (const fc of turn.toolCall.functionCalls) {
        functionResponses.push({
          id: fc.id,
          name: fc.name,
          response: { result: "ok" } // simple, hard-coded function response
        });
      }

      console.debug('Sending tool response...\n');
      session.sendToolResponse({ functionResponses: functionResponses });
    }
  }

  // Check again for new messages
  turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
      }
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

From a single prompt, the model can generate multiple function calls and the code necessary to chain their outputs. This code executes in a sandbox environment, generating subsequent [BidiGenerateContentToolCall](/api/live#bidigeneratecontenttoolcall) messages.

## Asynchronous function calling

Function calling executes sequentially by default, meaning execution pauses until the results of each function call are available. This ensures sequential processing, which means you won't be able to continue interacting with the model while the functions are being run.

If you don't want to block the conversation, you can tell the model to run the functions asynchronously. To do so, you first need to add a `behavior` to the function definitions:

### Python

  ```
  # Non-blocking function definitions
  turn_on_the_lights = {"name": "turn_on_the_lights", "behavior": "NON_BLOCKING"} # turn_on_the_lights will run asynchronously
  turn_off_the_lights = {"name": "turn_off_the_lights"} # turn_off_the_lights will still pause all interactions with the model
```

### JavaScript

```
import { GoogleGenAI, Modality, Behavior } from '@google/genai';

// Non-blocking function definitions
const turn_on_the_lights = {name: "turn_on_the_lights", behavior: Behavior.NON_BLOCKING}

// Blocking function definitions
const turn_off_the_lights = {name: "turn_off_the_lights"}

const tools = [{ functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }]
```

`NON-BLOCKING` ensures the function runs asynchronously while you can continue interacting with the model.

Then you need to tell the model how to behave when it receives the `FunctionResponse` using the `scheduling` parameter. It can either:

*   Interrupt what it's doing and tell you about the response it got right away (`scheduling="INTERRUPT"`),
*   Wait until it's finished with what it's currently doing (`scheduling="WHEN_IDLE"`),
*   Or do nothing and use that knowledge later on in the discussion (`scheduling="SILENT"`)
    

### Python

```
# for a non-blocking function definition, apply scheduling in the function response:
  function_response = types.FunctionResponse(
      id=fc.id,
      name=fc.name,
      response={
          "result": "ok",
          "scheduling": "INTERRUPT" # Can also be WHEN_IDLE or SILENT
      }
  )
```

### JavaScript

```
import { GoogleGenAI, Modality, Behavior, FunctionResponseScheduling } from '@google/genai';

// for a non-blocking function definition, apply scheduling in the function response:
const functionResponse = {
  id: fc.id,
  name: fc.name,
  response: {
    result: "ok",
    scheduling: FunctionResponseScheduling.INTERRUPT  // Can also be WHEN_IDLE or SILENT
  }
}
```

## Code execution

You can define code execution as part of the session configuration. This lets the Live API generate and execute Python code and dynamically perform computations to benefit your results. See the [Code execution tutorial](/gemini-api/docs/code-execution) to learn more.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

tools = [{'code_execution': {}}]
config = {"response_modalities": ["TEXT"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "Compute the largest prime palindrome under 100000."
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.text is not None:
                    print(chunk.text)

                model_turn = chunk.server_content.model_turn
                if model_turn:
                    for part in model_turn.parts:
                      if part.executable_code is not None:
                        print(part.executable_code.code)

                      if part.code_execution_result is not None:
                        print(part.code_execution_result.output)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const tools = [{codeExecution: {}}]
const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Compute the largest prime palindrome under 100000.';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
        else if (part.executableCode) {
          console.debug('executableCode: %s\n', part.executableCode.code);
        }
        else if (part.codeExecutionResult) {
          console.debug('codeExecutionResult: %s\n', part.codeExecutionResult.output);
        }
      }
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

## Grounding with Google Search

You can enable Grounding with Google Search as part of the session configuration. This increases the Live API's accuracy and prevents hallucinations. See the [Grounding tutorial](/gemini-api/docs/grounding) to learn more.

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

tools = [{'google_search': {}}]
config = {"response_modalities": ["TEXT"], "tools": tools}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        prompt = "When did the last Brazil vs. Argentina soccer match happen?"
        await session.send_client_content(turns={"parts": [{"text": prompt}]})

        async for chunk in session.receive():
            if chunk.server_content:
                if chunk.text is not None:
                    print(chunk.text)

                # The model might generate and execute Python code to use Search
                model_turn = chunk.server_content.model_turn
                if model_turn:
                    for part in model_turn.parts:
                      if part.executable_code is not None:
                        print(part.executable_code.code)

                      if part.code_execution_result is not None:
                        print(part.code_execution_result.output)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const tools = [{googleSearch: {}}]
const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      } else if (message.toolCall) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'When did the last Brazil vs. Argentina soccer match happen?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.modelTurn && turn.serverContent.modelTurn.parts) {
      for (const part of turn.serverContent.modelTurn.parts) {
        if (part.text) {
          console.debug('Received text: %s\n', part.text);
        }
        else if (part.executableCode) {
          console.debug('executableCode: %s\n', part.executableCode.code);
        }
        else if (part.codeExecutionResult) {
          console.debug('codeExecutionResult: %s\n', part.codeExecutionResult.output);
        }
      }
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

## Combining multiple tools

You can combine multiple tools within the Live API, increasing your application's capabilities even more:

### Python

```
prompt = """
Hey, I need you to do three things for me.

1. Compute the largest prime palindrome under 100000.
2. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
3. Turn on the lights

Thanks!
"""

tools = [
    {"google_search": {}},
    {"code_execution": {}},
    {"function_declarations": [turn_on_the_lights, turn_off_the_lights]},
]

config = {"response_modalities": ["TEXT"], "tools": tools}

# ... remaining model call
```

### JavaScript

```
const prompt = `Hey, I need you to do three things for me.

1. Compute the largest prime palindrome under 100000.
2. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024?
3. Turn on the lights

Thanks!
`

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turn_on_the_lights, turn_off_the_lights] }
]

const config = {
  responseModalities: [Modality.TEXT],
  tools: tools
}

// ... remaining model call
```

## What's next

*   Check out more examples of using tools with the Live API in the [Tool use cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).
*   Get the full story on features and configurations from the [Live API Capabilities guide](/gemini-api/docs/live-guide).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/prompting-strategies

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Prompt design strategies

_Prompt design_ is the process of creating prompts, or natural language requests, that elicit accurate, high quality responses from a language model.

This page introduces basic concepts, strategies, and best practices to get you started designing prompts to get the most out of Gemini AI models.

## Topic-specific prompt guides

Looking for more specific prompt strategies? Check out our other prompting guides on:

*   [Prompting with media files](/gemini-api/docs/files#prompt-guide)
*   [Prompting for image generation](/gemini-api/docs/image-generation#imagen-prompt-guide)
*   [Prompting for video generation](/gemini-api/docs/video#prompt-guide)

Google AI Studio also hosts a [prompt gallery](https://aistudio.google.com/app/gallery) meant to interactively showcase many of the concepts shared in this guide.

## Clear and specific instructions

An effective and efficient way to customize model behavior is to provide it with clear and specific instructions. Instructions can be in the form of a question, step-by-step tasks, or as complex as mapping out a user's experience and mindset.

### Input

Input is the required text in the prompt that you want the model to provide a response to. Inputs can be a question that the model answers (question input), a task the model performs (task input), an entity the model operates on (entity input), or partial input that the model completes or continues (completion input).

**Input type**

**Prompt**

**Generated output**

Question

    ```

    What's a good name for a flower shop that specializes in selling bouquets of
    dried flowers? Create a list of 5 options with just the names.
```

    ```

    Here are 10 names for a flower shop specializing in dried flowers:

    1. Everlasting Blooms
    2. Dried & Delightful
    3. The Petal Preserve
    4. Whispers of Wildflowers
    5. Forever Blooms
    
```
    

Task

    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.

    1. \*\*Tent:\*\* Your shelter from the elements.
    2. \*\*Sleeping Bag:\*\* Stay warm and comfortable.
    3. \*\*Sleeping Pad:\*\* Insulation and cushioning for a good night's sleep.
    4. \*\*Headlamp or Flashlight:\*\* Light for nighttime activities.
    5. \*\*First Aid Kit:\*\* Be prepared for minor injuries.

Entity

    ```

    Classify the following items as [large, small]:
Elephant
Mouse
Snail
```

```

Here's the classification:

* **Elephant:** Large
* **Mouse:** Small
* **Snail:** Small
    
```
    

#### Partial input completion

Generative language models work like an advanced auto completion tool. When you provide partial content, the model can provide the rest of the content or what it thinks is a continuation of that content as a response. When doing so, if you include any examples or context, the model can take those examples or context into account.

The following example provides a prompt with an instruction and an entity input:

**Prompt:**

For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or
drink, with the value being the quantity.

Order: A burger and a drink.
  **Response:**

{
  "cheeseburger": 0,
  "hamburger": 1,
  "fries": 0,
  "drink": 1
}
  (gemini-2.5-flash)

While the model did as prompted, writing out the instructions in natural language can sometimes be challenging and it leaves a lot to the model's interpretation. For example, a restaurants menu might contain many items. To reduce the size of the JSON response, you probably want to omit the items that weren't ordered. In this case, you can give an example and a [response prefix](#prefixes) and let the model complete it:

**Prompt:**

Valid fields are cheeseburger, hamburger, fries, and drink.
Order: Give me a cheeseburger and fries
Output:
\`\`\`
{
  "cheeseburger": 1,
  "fries": 1
}
\`\`\`
Order: I want two burgers, a drink, and fries.
Output:
  **Response:**

\`\`\`
{
  "hamburger": 2,
  "drink": 1,
  "fries": 1
}
\`\`\`
  (gemini-2.5-flash)

Notice how "cheeseburger" was excluded from the output because it wasn't a part of the order.

### Constraints

Specify any constraints on reading the prompt or generating a response. You can tell the model what to do and not to do. For example, you can specify a constraint in the prompt on how long you want a summary to be:

**Prompt:**

**Summarize this text in one sentence:**
Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially
faster than any modern traditional computer. At very tiny scales, physical matter acts as both
particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.
The operating principles of quantum devices is beyond the scope of classical physics. When deployed
at scale, quantum computers could be used in a wide variety of applications such as: in
cybersecurity to break existing encryption methods while helping researchers create new ones, in
meteorology to develop better weather forecasting etc. However, the current state of the art quantum
computers are still largely experimental and impractical.
    

**Response:**

      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.
    (gemini-2.5-flash)

### Response format

You can give instructions that specify the format of the response. For example, you can ask for the response to be formatted as a table, bulleted list, elevator pitch, keywords, sentence, or paragraph. The following system instruction tells the model to be more conversational in its response:

**System instruction**

    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.
  **Prompt**

    What is a smart way to make a business that sells DVD's in 2025?
  **Response:**

Starting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a "smart" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.

Here's a breakdown of what a smart DVD business in 2025 would likely entail:

1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:

    \* Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.
    ....
  (gemini-2.5-flash)

#### Format responses with the completion strategy

The [completion strategy](#completion) can also help format the response. The following example prompts the model to create an essay outline:

**Prompt:**

Create an outline for an essay about hummingbirds.
  **Response:**

Here is a possible outline for an essay about hummingbirds:

I. Introduction
A. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
B. Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds.
C. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.

II. Appearance and Identification
.....
  (gemini-2.5-flash)

The prompt didn't specify the format for the outline and the model chose a format for you. To get the model to return an outline in a specific format, you can add text that represents the start of the outline and let the model complete it based on the pattern that you initiated.

**Prompt:**

Create an outline for an essay about hummingbirds.
I. Introduction
   \*
  **Response:**

Okay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:

I. Introduction
\* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
\* Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds, and their general characteristics.
\* Thesis Statement: State the main point of the essay, focusing on the remarkable adaptations, behaviors, and ecological significance that make hummingbirds extraordinary.
.....
  (gemini-2.5-flash)

## Zero-shot vs few-shot prompts

You can include examples in the prompt that show the model what getting it right looks like. The model attempts to identify patterns and relationships from the examples and applies them when generating a response. Prompts that contain a few examples are called _few-shot_ prompts, while prompts that provide no examples are called _zero-shot_ prompts. Few-shot prompts are often used to regulate the formatting, phrasing, scoping, or general patterning of model responses. Use specific and varied examples to help the model narrow its focus and generate more accurate results.

We recommend to always include few-shot examples in your prompts. Prompts without few-shot examples are likely to be less effective. In fact, you can remove instructions from your prompt if your examples are clear enough in showing the task at hand.

The following zero-shot prompt asks the model to choose the best explanation.

**Prompt:**

Please choose the best explanation to the question:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Response:**

Explanation1 is the better explanation because it provides more detail on the
process, including how ice crystals combine and grow into snowflakes as they
fall through the atmosphere.
  (gemini-2.5-flash)

If your use case requires the model to produce concise responses, you can include examples in the prompt that give preference to concise responses.

The following prompt provides two examples that show preference to the shorter explanations. In the response, you can see that the examples guided the model to choose the shorter explanation (`Explanation2`) as opposed to the longer explanation (`Explanation1`) like it did previously.

**Prompt:**

Below are some examples showing a question, explanation, and answer format:

Question: Why is the sky blue?
Explanation1: The sky appears blue because of Rayleigh scattering, which causes
shorter blue wavelengths of light to be scattered more easily than longer red
wavelengths, making the sky look blue.
Explanation2: Due to Rayleigh scattering effect.
Answer: Explanation2

Question: What is the cause of earthquakes?
Explanation1: Sudden release of energy in the Earth's crust.
Explanation2: Earthquakes happen when tectonic plates suddenly slip or break
apart, causing a release of energy that creates seismic waves that can shake the
ground and cause damage.
Answer: Explanation1

Now, Answer the following question given the example formats above:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Response:**

Answer: Explanation2
  (gemini-2.5-flash)

### Optimal number of examples

Models like Gemini can often pick up on patterns using a few examples, though you may need to experiment with the number of examples to provide in the prompt for the best results. At the same time, if you include too many examples, the model may start to [overfit](https://developers.google.com/machine-learning/glossary#overfitting) the response to the examples.

### Patterns vs anti patterns

Using examples to show the model a pattern to follow is more effective than using examples to show the model an anti pattern to avoid.

⛔ **Negative pattern:**

**Example**

Don't end haikus with a question:
Haiku are fun
A short and simple poem
Don't you enjoy them?
    

✅ **Positive pattern:**

**Example**

Always end haikus with an assertion:
Haiku are fun
A short and simple poem
A joy to write
    

### Consistent formatting

Make sure that the structure and formatting of few-shot examples are the same to avoid responses with undesired formats. One of the primary objectives of adding few-shot examples in prompts is to show the model the response format. Therefore, it is essential to ensure a consistent format across all examples, especially paying attention to XML tags, white spaces, newlines, and example splitters.

## Add context

You can include instructions and information in a prompt that the model needs to solve a problem, instead of assuming that the model has all of the required information. This contextual information helps the model understand the constraints and details of what you're asking for it to do.

The following example asks the model to give troubleshooting guidance for a router:

**Prompt:**

What should I do to fix my disconnected wifi? **The light on my Google
Wifi router is yellow and blinking slowly.**
  **Response:**

  A yellow blinking light on your Google Wifi router usually indicates a problem
  with the internet connection. Here are some steps to troubleshoot and fix the
  issue:
  1. Restart your router and modem:
    - Unplug your modem from the power outlet and wait for 30 seconds.
    - Plug your modem back in and wait for it to fully boot up.
    - Unplug your Google Wifi router and wait for 30 seconds.
    - Plug your Google Wifi router back in and wait for it to fully boot up.
.....
  (gemini-2.5-flash)

The response looks like generic troubleshooting information that's not specific to the router or the status of the LED indicator lights.

To customize the response for the specific router, you can add to the prompt the router's troubleshooting guide as context for it to refer to when providing a response.

**Prompt:**

Answer the question using the text below. Respond with only the text provided.
Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.

Text:
Color: Slowly pulsing yellow
What it means: There is a network error.
What to do:
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.

Color: Fast blinking yellow
What it means: You are holding down the reset button and are factory resetting this device.
What to do:
If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.

Color: Solid yellow
What it means: Router is factory resetting.
What to do:
This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.

Color: Solid red
What it means: Something is wrong.
What to do:
Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.
  **Response:**

Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.
  (gemini-2.5-flash)

## Add prefixes

A prefix is a word or phrase that you add to the prompt content that can serve several purposes, depending on where you put the prefix:

*   **Input prefix:** Adding a prefix to the input signals semantically meaningful parts of the input to the model. For example, the prefixes "English:" and "French:" demarcate two different languages.
*   **Output prefix:** Even though the output is generated by the model, you can add a prefix for the output in the prompt. The output prefix gives the model information about what's expected as a response. For example, the output prefix "JSON:" signals to the model that the output should be in JSON format.
*   **Example prefix:** In few-shot prompts, adding prefixes to the examples provides labels that the model can use when generating the output, which makes it easier to parse output content.

In the following example, "Text:" is the input prefix and "The answer is:" is the output prefix.

**Prompt:**

Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail
The answer is: small
Text: Elephant
The answer is:
  **Response:**

The answer is: large
  (gemini-2.5-flash)

## Break down prompts into components

For use cases that require complex prompts, you can help the model manage this complexity by breaking things down into simpler components.

1.  **Break down instructions:** Instead of having many instructions in one prompt, create one prompt per instruction. You can choose which prompt to process based on the user's input.
    
2.  **Chain prompts:** For complex tasks that involve multiple sequential steps, make each step a prompt and chain the prompts together in a sequence. In this sequential chain of prompts, the output of one prompt in the sequence becomes the input of the next prompt. The output of the last prompt in the sequence is the final output.
    
3.  **Aggregate responses:** Aggregation is when you want to perform different parallel tasks on different portions of the data and aggregate the results to produce the final output. For example, you can tell the model to perform one operation on the first part of the data, perform another operation on the rest of the data and aggregate the results.
    

## Experiment with model parameters

Each call that you send to a model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. Experiment with different parameter values to get the best values for the task. The parameters available for different models may differ. The most common parameters are the following:

1.  **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.
    
2.  **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.
    
3.  **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.
    
4.  **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.
    
5.  **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.
    

## Prompt iteration strategies

Prompt design can sometimes require a few iterations before you consistently get the response you're looking for. This section provides guidance on some things you can try when iterating on your prompts:

1.  **Use different phrasing:** Using different words or phrasing in your prompts often yields different responses from the model even though they all mean the same thing. If you're not getting the expected results from your prompt, try rephrasing it.
    
    Version 1:
    How do I bake a pie?
    
    Version 2:
    Suggest a recipe for a pie.
    
    Version 3:
    What's a good pie recipe?
        
    
2.  **Switch to an analogous task:** If you can't get the model to follow your instructions for a task, try giving it instructions for an analogous task that achieves the same result.
    
    This prompt tells the model to categorize a book by using predefined categories:
    
    **Prompt:**
    
    Which category does The Odyssey belong to:
    thriller
    sci-fi
    mythology
    biography
      **Response:**
    
        The Odyssey belongs to the category of \*\*mythology\*\*. 
    
        Here's why:
    
        \* \*\*Mythology:\*\* The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his
        journey home after the Trojan War. It features gods, monsters, and supernatural events common to
        Greek mythology.
    .....
      (gemini-2.5-flash)
    
    The response is correct, but the model didn't stay within the bounds of the options. You also want to model to just respond with one of the options instead of in a full sentence. In this case, you can rephrase the instructions as a multiple choice question and ask the model to choose an option.
    
    **Prompt:**
    
    Multiple choice problem: Which of the following options describes the book The Odyssey?
    Options:
    
    *   thriller
    *   sci-fi
    *   mythology
    *   biography
    **Response:**
    
    The correct answer is **mythology**. 
    (gemini-2.5-flash)
    
3.  **Change the order of prompt content:** The order of the content in the prompt can sometimes affect the response. Try changing the content order and see how that affects the response.
    
    ```
    Version 1:
    [examples]
    [context]
    [input]
    
    Version 2:
    [input]
    [examples]
    [context]
    
    Version 3:
    [examples]
    [input]
    [context]
    ```
    

## Fallback responses

A fallback response is a response returned by the model when either the prompt or the response triggers a safety filter. An example of a fallback response is "I'm not able to help with that, as I'm only a language model."

If the model responds with a fallback response, try increasing the temperature.

## Things to avoid

*   Avoid relying on models to generate factual information.
*   Use with care on math and logic problems.

## Generative models under the hood

This section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**

The short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text "The dog jumped over the ... ", the generative model will produce an array of probable next words:

```
[("fence", 0.77), ("ledge", 0.12), ("blanket", 0.03), ...]
```

This process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.

In the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.

## Next steps

*   Now that you have a deeper understanding of prompt design, try writing your own prompts using [Google AI Studio](http://aistudio.google.com).
*   To learn about multimodal prompting, see [Prompting with media files](/gemini-api/docs/files#prompt-guide).
*   To learn about image prompting, see the [Imagen prompt guide](/gemini-api/docs/image-generation#imagen-prompt-guide)
*   To learn about video prompting, see the [Veo prompt guide](/gemini-api/docs/video#prompt-guide)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-04-28 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/live-guide

2.5 ファミリーの思考モデルの更新をご紹介します。[詳細](https://ai.google.dev/gemini-api/docs/models?hl=ja)

![](https://ai.google.dev/_static/images/translated.svg?hl=ja) このページは [Cloud Translation API](//cloud.google.com/translate/?hl=ja) によって翻訳されました。

*   [ホーム](https://ai.google.dev/?hl=ja)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ja)
*   [モデル](https://ai.google.dev/gemini-api/docs?hl=ja)

フィードバックを送信

# Live API capabilities guide

これは、Live API で利用可能な機能と構成を網羅した包括的なガイドです。一般的なユースケースの概要とサンプルコードについては、[Live API のスタートガイド](https://ai.google.dev/gemini-api/docs/live?hl=ja)をご覧ください。

## 始める前に

*   **コアコンセプトを理解する:** まだ読んでいない場合は、まず [Live API の利用を開始する](https://ai.google.dev/gemini-api/doc/live?hl=ja) をご覧ください。ここでは、Live API の基本原則、仕組み、[さまざまなモデル](https://ai.google.dev/gemini-api/docs/live?hl=ja#audio-generation)とそれに対応する音声生成方法（[ネイティブ音声](#native-audio-output)またはハーフカスケード）の違いについて説明します。
*   **AI Studio で Live API を試す:** 構築を開始する前に、[Google AI Studio](https://aistudio.google.com/app/live?hl=ja) で Live API を試すことをおすすめします。Google AI Studio で Live API を使用するには、\[**ストリーミング**\] を選択します。

## 接続を確立する

次の例は、API キーを使用して接続を作成する方法を示しています。

### Python

```
import asyncio
from google import genai

client = genai.Client()

model = "gemini-live-2.5-flash-preview"
config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        print("Session started")

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        console.debug(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send content...

  session.close();
}

main();
```

## インタラクション モダリティ

以降のセクションでは、Live API で使用可能なさまざまな入力モードと出力モードの例とサポートされているコンテキストについて説明します。

### テキストの送受信

テキストの送受信方法は次のとおりです。

### Python

```
import asyncio
from google import genai

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        message = "Hello, how are you?"
        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.text is not None:
                print(response.text, end="")

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

#### コンテンツの増分更新

増分更新を使用して、テキスト入力の送信、セッション コンテキストの確立、セッション コンテキストの復元を行います。コンテキストが短い場合は、ターンバイターンのインタラクションを送信して、イベントの正確なシーケンスを表すことができます。

### Python

```
turns = [
    {"role": "user", "parts": [{"text": "What is the capital of France?"}]},
    {"role": "model", "parts": [{"text": "Paris"}]},
]

await session.send_client_content(turns=turns, turn_complete=False)

turns = [{"role": "user", "parts": [{"text": "What is the capital of Germany?"}]}]

await session.send_client_content(turns=turns, turn_complete=True)
```

### JavaScript

```
let inputTurns = [
  { "role": "user", "parts": [{ "text": "What is the capital of France?" }] },
  { "role": "model", "parts": [{ "text": "Paris" }] },
]

session.sendClientContent({ turns: inputTurns, turnComplete: false })

inputTurns = [{ "role": "user", "parts": [{ "text": "What is the capital of Germany?" }] }]

session.sendClientContent({ turns: inputTurns, turnComplete: true })
```

コンテキストが長い場合は、1 つのメッセージの概要を提供して、後続のインタラクション用にコンテキスト ウィンドウを空けておくことをおすすめします。セッション コンテキストを読み込む別の方法については、[セッションの再開](https://ai.google.dev/gemini-api/docs/live-session?hl=ja#session-resumption)をご覧ください。

### 音声の送受信

最も一般的な音声の例である**音声から音声**については、[スタートガイド](https://ai.google.dev/gemini-api/docs/live?hl=ja#audio-to-audio)をご覧ください。

以下は、WAV ファイルを読み取り、正しい形式で送信してテキスト出力を受け取る**音声文字変換**の例です。

### Python

```
# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
# Install helpers for converting files: pip install librosa soundfile
import asyncio
import io
from pathlib import Path
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:

        buffer = io.BytesIO()
        y, sr = librosa.load("sample.wav", sr=16000)
        sf.write(buffer, y, sr, format='RAW', subtype='PCM_16')
        buffer.seek(0)
        audio_bytes = buffer.read()

        # If already in correct format, you can use this:
        # audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        async for response in session.receive():
            if response.text is not None:
                print(response.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
// Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
// Install helpers for converting files: npm install wavefile
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("sample.wav");

  // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
  const wav = new WaveFile();
  wav.fromBuffer(fileBuffer);
  wav.toSampleRate(16000);
  wav.toBitDepth("16");
  const base64Audio = wav.toBase64();

  // If already in correct format, you can use this:
  // const fileBuffer = fs.readFileSync("sample.pcm");
  // const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }

  );

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

**テキスト読み上げ**の例を次に示します。音声を受け取るには、レスポンス モダリティとして `AUDIO` を設定します。この例では、受信したデータを WAV ファイルとして保存します。

### Python

```
import asyncio
import wave
from google import genai

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["AUDIO"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)

        message = "Hello how are you?"
        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)

            # Un-comment this code to print audio data info
            # if response.server_content.model_turn is not None:
            #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.AUDIO] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  // Combine audio data strings and save as wave file
  const combinedAudio = turns.reduce((acc, turn) => {
    if (turn.data) {
      const buffer = Buffer.from(turn.data, 'base64');
      const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
      return acc.concat(Array.from(intArray));
    }
    return acc;
  }, []);

  const audioBuffer = new Int16Array(combinedAudio);

  const wf = new WaveFile();
  wf.fromScratch(1, 24000, '16', audioBuffer);
  fs.writeFileSync('output.wav', wf.toBuffer());

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

#### オーディオ形式

Live API の音声データは常に未加工のリトル エンディアン 16 ビット PCM です。オーディオ出力では常に 24 kHz のサンプリング レートが使用されます。入力音声はネイティブで 16 kHz ですが、Live API は必要に応じて再サンプリングするため、任意のサンプルレートを送信できます。入力音声のサンプルレートを伝えるには、音声を含む各 [Blob](https://ai.google.dev/api/caching?hl=ja#Blob) の MIME タイプを `audio/pcm;rate=16000` などの値に設定します。

#### 音声文字起こし

設定構成で `output_audio_transcription` を送信すると、モデルの音声出力の文字起こしを有効にできます。音声文字変換の言語は、モデルのレスポンスから推測されます。

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["AUDIO"],
        "output_audio_transcription": {}
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        message = "Hello? Gemini are you there?"

        await session.send_client_content(
            turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
        )

        async for response in session.receive():
            if response.server_content.model_turn:
                print("Model turn:", response.server_content.model_turn)
            if response.server_content.output_transcription:
                print("Transcript:", response.server_content.output_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const config = {
  responseModalities: [Modality.AUDIO],
  outputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  const inputTurns = 'Hello how are you?';
  session.sendClientContent({ turns: inputTurns });

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.outputTranscription) {
      console.debug('Received output transcription: %s\n', turn.serverContent.outputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

設定構成で `input_audio_transcription` を送信すると、音声入力の文字起こしを有効にできます。

### Python

```
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {
    "response_modalities": ["TEXT"],
    "input_audio_transcription": {},
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_data = Path("16000.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_data, mime_type='audio/pcm;rate=16000')
        )

        async for msg in session.receive():
            if msg.server_content.input_transcription:
                print('Transcript:', msg.server_content.input_transcription.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

const config = {
  responseModalities: [Modality.TEXT],
  inputAudioTranscription: {}
};

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("16000.wav");

  // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
  const wav = new WaveFile();
  wav.fromBuffer(fileBuffer);
  wav.toSampleRate(16000);
  wav.toBitDepth("16");
  const base64Audio = wav.toBase64();

  // If already in correct format, you can use this:
  // const fileBuffer = fs.readFileSync("sample.pcm");
  // const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }
  );

  const turns = await handleTurn();

  for (const turn of turns) {
    if (turn.serverContent && turn.serverContent.outputTranscription) {
      console.log("Transcription")
      console.log(turn.serverContent.outputTranscription.text);
    }
  }
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
    else if (turn.serverContent && turn.serverContent.inputTranscription) {
      console.debug('Received input transcription: %s\n', turn.serverContent.inputTranscription.text);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

### 音声と動画をストリーミングする

### 音声と言語を変更する

Live API モデルはそれぞれ、異なる音声セットをサポートしています。ハーフ カスケードは、Puck、Charon、Kore、Fenrir、Aoede、Leda、Orus、Zephyr をサポートしています。ネイティブ音声は、はるかに長いリストをサポートしています（[TTS モデルのリスト](https://ai.google.dev/gemini-api/docs/speech-generation?hl=ja#voices)と同じです）。すべての音声は [AI Studio](https://aistudio.google.com/app/live?hl=ja) で試聴できます。

音声を指定するには、セッション構成の一部として、`speechConfig` オブジェクト内に音声名を設定します。

### Python

```
config = {
    "response_modalities": ["AUDIO"],
    "speech_config": {
        "voice_config": {"prebuilt_voice_config": {"voice_name": "Kore"}}
    },
}
```

### JavaScript

```
const config = {
  responseModalities: [Modality.AUDIO],
  speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Kore" } } }
};
```

Live API は[複数の言語](#supported-languages)をサポートしています。

言語を変更するには、セッション構成の一部として `speechConfig` オブジェクト内に言語コードを設定します。

### Python

```
config = {
    "response_modalities": ["AUDIO"],
    "speech_config": {
        "language_code": "de-DE"
    }
}
```

### JavaScript

```
const config = {
  responseModalities: [Modality.AUDIO],
  speechConfig: { languageCode: "de-DE" }
};
```

## ネイティブ オーディオ機能

以下の機能は、ネイティブ音声でのみ使用できます。ネイティブ音声の詳細については、[モデルと音声生成を選択する](https://ai.google.dev/gemini-api/docs/api?hl=ja)をご覧ください。

### ネイティブ音声出力の使用方法

ネイティブ音声出力を使用するには、[ネイティブ音声モデル](https://ai.google.dev/gemini-api/docs/models?hl=ja#gemini-2.5-flash-native-audio)のいずれかを構成し、`response_modalities` を `AUDIO` に設定します。

完全な例については、[音声の送受信](https://ai.google.dev/gemini-api/docs/live?hl=ja#audio-to-audio)をご覧ください。

### Python

```
model = "gemini-2.5-flash-preview-native-audio-dialog"
config = types.LiveConnectConfig(response_modalities=["AUDIO"])

async with client.aio.live.connect(model=model, config=config) as session:
    # Send audio input and receive audio
```

### JavaScript

```
const model = 'gemini-2.5-flash-preview-native-audio-dialog';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: ...,
  });

  // Send audio input and receive audio

  session.close();
}

main();
```

### アフェクティブ ダイアログ

この機能により、Gemini は入力された表現と口調に応じて回答スタイルを調整できます。

感情的なダイアログを使用するには、API バージョンを `v1alpha` に設定し、設定メッセージで `enable_affective_dialog` を `true` に設定します。

### Python

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    enable_affective_dialog=True
)
```

### JavaScript

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  enableAffectiveDialog: true
};
```

現時点では、感情的なダイアログはネイティブ音声出力モデルでのみサポートされています。

### コンテキストに応じた音声にのみ対応

この機能が有効になっている場合、Gemini はコンテンツが関連していない場合は事前に応答しないことを決定できます。

使用するには、API バージョンを `v1alpha` に設定し、設定メッセージで `proactivity` フィールドを構成して、`proactive_audio` を `true` に設定します。

### Python

```
client = genai.Client(http_options={"api_version": "v1alpha"})

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    proactivity={'proactive_audio': True}
)
```

### JavaScript

```
const ai = new GoogleGenAI({ httpOptions: {"apiVersion": "v1alpha"} });

const config = {
  responseModalities: [Modality.AUDIO],
  proactivity: { proactiveAudio: true }
}
```

プロアクティブ音声は、現在のところネイティブ音声出力モデルでのみサポートされています。

### 思考中のネイティブ音声出力

ネイティブ音声出力は[思考機能](https://ai.google.dev/gemini-api/docs/thinking?hl=ja)をサポートしています。これは、別のモデル `gemini-2.5-flash-exp-native-audio-thinking-dialog` で利用できます。

完全な例については、[音声の送受信](https://ai.google.dev/gemini-api/docs/live?hl=ja#audio-to-audio)をご覧ください。

### Python

```
model = "gemini-2.5-flash-exp-native-audio-thinking-dialog"
config = types.LiveConnectConfig(response_modalities=["AUDIO"])

async with client.aio.live.connect(model=model, config=config) as session:
    # Send audio input and receive audio
```

### JavaScript

```
const model = 'gemini-2.5-flash-exp-native-audio-thinking-dialog';
const config = { responseModalities: [Modality.AUDIO] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: ...,
  });

  // Send audio input and receive audio

  session.close();
}

main();
```

## 音声アクティビティ検出（VAD）

音声アクティビティ検出（VAD）を使用すると、モデルは人が話しているタイミングを認識できます。これは、ユーザーがいつでもモデルを中断できるため、自然な会話を作成するために不可欠です。

VAD が中断を検出すると、進行中の生成はキャンセルされ、破棄されます。クライアントにすでに送信された情報だけが、セッション履歴に保持されます。その後、サーバーは中断を報告する [`BidiGenerateContentServerContent`](https://ai.google.dev/api/live?hl=ja#bidigeneratecontentservercontent) メッセージを送信します。

Gemini サーバーは保留中の関数呼び出しを破棄し、キャンセルされた呼び出しの ID を記載した `BidiGenerateContentServerContent` メッセージを送信します。

### Python

```
async for response in session.receive():
    if response.server_content.interrupted is True:
        # The generation was interrupted

        # If realtime playback is implemented in your application,
        # you should stop playing audio and clear queued playback here.
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.interrupted) {
    // The generation was interrupted

    // If realtime playback is implemented in your application,
    // you should stop playing audio and clear queued playback here.
  }
}
```

### 自動 VAD

デフォルトでは、モデルは連続した音声入力ストリームに対して VAD を自動的に実行します。VAD は、[設定構成](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentSetup)の [`realtimeInputConfig.automaticActivityDetection`](https://ai.google.dev/api/live?hl=ja#RealtimeInputConfig.AutomaticActivityDetection) フィールドで構成できます。

音声ストリームが 1 秒以上一時停止した場合（ユーザーがマイクをオフにした場合など）、キャッシュに保存されている音声をフラッシュするために [`audioStreamEnd`](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentRealtimeInput.FIELDS.bool.BidiGenerateContentRealtimeInput.audio_stream_end) イベントを送信する必要があります。クライアントはいつでも音声データの送信を再開できます。

### Python

```
# example audio file to try:
# URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
# !wget -q $URL -O sample.pcm
import asyncio
from pathlib import Path
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

config = {"response_modalities": ["TEXT"]}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:
        audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        # if stream gets paused, send:
        # await session.send_realtime_input(audio_stream_end=True)

        async for response in session.receive():
            if response.text is not None:
                print(response.text)

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
// example audio file to try:
// URL = "https://storage.googleapis.com/generativeai-downloads/data/hello_are_you_there.pcm"
// !wget -q $URL -O sample.pcm
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';
const config = { responseModalities: [Modality.TEXT] };

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

  const session = await ai.live.connect({
    model: model,
    callbacks: {
      onopen: function () {
        console.debug('Opened');
      },
      onmessage: function (message) {
        responseQueue.push(message);
      },
      onerror: function (e) {
        console.debug('Error:', e.message);
      },
      onclose: function (e) {
        console.debug('Close:', e.reason);
      },
    },
    config: config,
  });

  // Send Audio Chunk
  const fileBuffer = fs.readFileSync("sample.pcm");
  const base64Audio = Buffer.from(fileBuffer).toString('base64');

  session.sendRealtimeInput(
    {
      audio: {
        data: base64Audio,
        mimeType: "audio/pcm;rate=16000"
      }
    }

  );

  // if stream gets paused, send:
  // session.sendRealtimeInput({ audioStreamEnd: true })

  const turns = await handleTurn();
  for (const turn of turns) {
    if (turn.text) {
      console.debug('Received text: %s\n', turn.text);
    }
    else if (turn.data) {
      console.debug('Received inline data: %s\n', turn.data);
    }
  }

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

`send_realtime_input` を使用すると、API は VAD に基づいて音声に自動的に応答します。`send_client_content` は順序に従ってモデル コンテキストにメッセージを追加しますが、`send_realtime_input` は確定的な順序付けを犠牲にして応答性を最適化します。

### 自動 VAD 構成

VAD アクティビティをより細かく制御するには、次のパラメータを構成します。詳細については、[API リファレンス](https://ai.google.dev/api/live?hl=ja#automaticactivitydetection)をご覧ください。

### Python

```
from google.genai import types

config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {
        "automatic_activity_detection": {
            "disabled": False, # default
            "start_of_speech_sensitivity": types.StartSensitivity.START_SENSITIVITY_LOW,
            "end_of_speech_sensitivity": types.EndSensitivity.END_SENSITIVITY_LOW,
            "prefix_padding_ms": 20,
            "silence_duration_ms": 100,
        }
    }
}
```

### JavaScript

```
import { GoogleGenAI, Modality, StartSensitivity, EndSensitivity } from '@google/genai';

const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: false, // default
      startOfSpeechSensitivity: StartSensitivity.START_SENSITIVITY_LOW,
      endOfSpeechSensitivity: EndSensitivity.END_SENSITIVITY_LOW,
      prefixPaddingMs: 20,
      silenceDurationMs: 100,
    }
  }
};
```

### 自動 VAD を無効にする

または、設定メッセージで `realtimeInputConfig.automaticActivityDetection.disabled` を `true` に設定して、自動 VAD を無効にすることもできます。この構成では、クライアントがユーザーの音声を検出し、適切なタイミングで [`activityStart`](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityStart.BidiGenerateContentRealtimeInput.activity_start) メッセージと [`activityEnd`](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityEnd.BidiGenerateContentRealtimeInput.activity_end) メッセージを送信します。この構成では `audioStreamEnd` は送信されません。代わりに、ストリームの中断は `activityEnd` メッセージでマークされます。

### Python

```
config = {
    "response_modalities": ["TEXT"],
    "realtime_input_config": {"automatic_activity_detection": {"disabled": True}},
}

async with client.aio.live.connect(model=model, config=config) as session:
    # ...
    await session.send_realtime_input(activity_start=types.ActivityStart())
    await session.send_realtime_input(
        audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
    )
    await session.send_realtime_input(activity_end=types.ActivityEnd())
    # ...
```

### JavaScript

```
const config = {
  responseModalities: [Modality.TEXT],
  realtimeInputConfig: {
    automaticActivityDetection: {
      disabled: true,
    }
  }
};

session.sendRealtimeInput({ activityStart: {} })

session.sendRealtimeInput(
  {
    audio: {
      data: base64Audio,
      mimeType: "audio/pcm;rate=16000"
    }
  }

);

session.sendRealtimeInput({ activityEnd: {} })
```

## トークン数

消費されたトークンの合計数は、返されたサーバー メッセージの [usageMetadata](https://ai.google.dev/api/live?hl=ja#usagemetadata) フィールドで確認できます。

### Python

```
async for message in session.receive():
    # The server will periodically send messages that include UsageMetadata.
    if message.usage_metadata:
        usage = message.usage_metadata
        print(
            f"Used {usage.total_token_count} tokens in total. Response token breakdown:"
        )
        for detail in usage.response_tokens_details:
            match detail:
                case types.ModalityTokenCount(modality=modality, token_count=count):
                    print(f"{modality}: {count}")
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.usageMetadata) {
    console.debug('Used %s tokens in total. Response token breakdown:\n', turn.usageMetadata.totalTokenCount);

    for (const detail of turn.usageMetadata.responseTokensDetails) {
      console.debug('%s\n', detail);
    }
  }
}
```

## メディアの解像度

入力メディアのメディア解像度を指定するには、セッション構成の一部として `mediaResolution` フィールドを設定します。

### Python

```
from google.genai import types

config = {
    "response_modalities": ["AUDIO"],
    "media_resolution": types.MediaResolution.MEDIA_RESOLUTION_LOW,
}
```

### JavaScript

```
import { GoogleGenAI, Modality, MediaResolution } from '@google/genai';

const config = {
    responseModalities: [Modality.TEXT],
    mediaResolution: MediaResolution.MEDIA_RESOLUTION_LOW,
};
```

## 制限事項

プロジェクトを計画する際は、Live API の次の制限事項を考慮してください。

### レスポンス モダリティ

セッション構成で設定できるレスポンス モダリティ（`TEXT` または `AUDIO`）は、セッションごとに 1 つだけです。両方を設定すると、構成エラー メッセージが表示されます。つまり、テキストまたは音声のいずれかで応答するようにモデルを構成できますが、同じセッションで両方を使用することはできません。

### クライアント認証

デフォルトでは、Live API はサーバー間認証のみを提供します。[クライアントからサーバーへのアプローチ](https://ai.google.dev/gemini-api/docs/live?hl=ja#implementation-approach)を使用して Live API アプリケーションを実装する場合は、[エフェメラル トークン](https://ai.google.dev/gemini-api/docs/ephemeral-tokens?hl=ja)を使用してセキュリティ リスクを軽減する必要があります。

### セッション継続時間

音声のみのセッションは 15 分に制限され、音声と動画のセッションは 2 分に制限されます。ただし、セッションの継続時間を無制限に延長するには、さまざまな[セッション管理手法](https://ai.google.dev/gemini-api/docs/live-session?hl=ja)を構成できます。

### コンテキスト ウィンドウ

セッションのコンテキスト ウィンドウの上限は次のとおりです。

*   [ネイティブ音声出力](#native-audio-output)モデル用に 128,000 トークン
*   他の Live API モデルの場合は 32,000 トークン

## サポートされている言語

Live API は次の言語をサポートしています。

   

言語

BCP-47 コード

言語

BCP-47 コード

ドイツ語（ドイツ）

`de-DE`

英語（オーストラリア）\*

`en-AU`

英語（英国）\*

`en-GB`

英語（インド）

`en-IN`

英語（米国）

`en-US`

スペイン語（米国）

`es-US`

フランス語（フランス）

`fr-FR`

ヒンディー語（インド）

`hi-IN`

ポルトガル語（ブラジル）

`pt-BR`

アラビア語（一般）

`ar-XA`

スペイン語（スペイン）\*

`es-ES`

フランス語（カナダ）\*

`fr-CA`

インドネシア語（インドネシア）

`id-ID`

イタリア語（イタリア）

`it-IT`

日本語（日本）

`ja-JP`

トルコ語（トルコ）

`tr-TR`

ベトナム語（ベトナム）

`vi-VN`

ベンガル語（インド）

`bn-IN`

グジャラート語（インド）\*

`gu-IN`

カンナダ語（インド）\*

`kn-IN`

マラーティー語（インド）

`mr-IN`

マラヤーラム語（インド）\*

`ml-IN`

タミル語（インド）

`ta-IN`

テルグ語（インド）

`te-IN`

オランダ語（オランダ）

`nl-NL`

韓国語（韓国）

`ko-KR`

標準中国語（中国）\*

`cmn-CN`

ポーランド語（ポーランド）

`pl-PL`

ロシア語（ロシア）

`ru-RU`

タイ語（タイ）

`th-TH`

_アスタリスク（\*）が付いている言語_は、_[ネイティブ音声](#native-audio-output)には使用できません_。

## 次のステップ

*   Live API を効果的に使用するための重要な情報については、[ツールの使用](https://ai.google.dev/gemini-api/docs/live-tools?hl=ja)ガイドと[セッション管理](https://ai.google.dev/gemini-api/docs/live-session?hl=ja)ガイドをご覧ください。
*   [Google AI Studio](https://aistudio.google.com/app/live?hl=ja) で Live API を試す。
*   Live API モデルの詳細については、モデルページの [Gemini 2.0 Flash Live](https://ai.google.dev/gemini-api/docs/models?hl=ja#live-api) と [Gemini 2.5 Flash ネイティブ音声](https://ai.google.dev/gemini-api/docs/models?hl=ja#gemini-2.5-flash-native-audio)をご覧ください。
*   [Live API クックブック](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb)、[Live API Tools クックブック](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb)、[Live API スタートガイド スクリプト](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py)で、その他の例をお試しください。

フィードバックを送信

特に記載のない限り、このページのコンテンツは[クリエイティブ・コモンズの表示 4.0 ライセンス](https://creativecommons.org/licenses/by/4.0/)により使用許諾されます。コードサンプルは [Apache 2.0 ライセンス](https://www.apache.org/licenses/LICENSE-2.0)により使用許諾されます。詳しくは、[Google Developers サイトのポリシー](https://developers.google.com/site-policies?hl=ja)をご覧ください。Java は Oracle および関連会社の登録商標です。

最終更新日 2025-06-27 UTC。

---

# Content from: https://ai.google.dev/gemini-api/docs/live-session

2.5 ファミリーの思考モデルの更新をご紹介します。[詳細](https://ai.google.dev/gemini-api/docs/models?hl=ja)

![](https://ai.google.dev/_static/images/translated.svg?hl=ja) このページは [Cloud Translation API](//cloud.google.com/translate/?hl=ja) によって翻訳されました。

*   [ホーム](https://ai.google.dev/?hl=ja)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ja)
*   [モデル](https://ai.google.dev/gemini-api/docs?hl=ja)

フィードバックを送信

# Session management with Live API

Live API では、セッションとは、同じ接続を介して入力と出力が継続的にストリーミングされる永続接続を指します（[仕組み](https://ai.google.dev/gemini-api/docs/live?hl=ja)の詳細をご覧ください）。この独自のセッション設計により、低レイテンシが可能になり、独自の機能をサポートできますが、セッション時間の制限や早期終了などの課題も生じます。このガイドでは、Live API の使用時に発生する可能性のあるセッション管理の課題を克服するための戦略について説明します。

## セッションの存続期間

圧縮なしの場合、音声のみのセッションは 15 分に制限され、音声と動画のセッションは 2 分に制限されます。この上限を超えるとセッション（および接続）が終了しますが、[コンテキスト ウィンドウ圧縮](#context-window-compression)を使用してセッションを無制限に延長できます。

接続の存続期間も 10 分程度に制限されています。接続が終了すると、セッションも終了します。この場合、[セッションの再開](#session-resumption)を使用して、複数の接続でアクティブなままになるように 1 つのセッションを構成できます。また、接続が終了する前に [GoAway メッセージ](#goaway-message)も届き、追加の操作を行えます。

## コンテキスト ウィンドウの圧縮

セッションを長くして、接続が突然終了しないようにするには、セッション構成の一部として [contextWindowCompression](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentSetup.FIELDS.ContextWindowCompressionConfig.BidiGenerateContentSetup.context_window_compression) フィールドを設定して、コンテキスト ウィンドウ圧縮を有効にします。

[ContextWindowCompressionConfig](https://ai.google.dev/api/live?hl=ja#contextwindowcompressionconfig) で、[スライディング ウィンドウ メカニズム](https://ai.google.dev/api/live?hl=ja#ContextWindowCompressionConfig.FIELDS.ContextWindowCompressionConfig.SlidingWindow.ContextWindowCompressionConfig.sliding_window)と、圧縮をトリガーする[トークン数](https://ai.google.dev/api/live?hl=ja#ContextWindowCompressionConfig.FIELDS.int64.ContextWindowCompressionConfig.trigger_tokens)を構成できます。

### Python

```
from google.genai import types

config = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    context_window_compression=(
        # Configures compression with default parameters.
        types.ContextWindowCompressionConfig(
            sliding_window=types.SlidingWindow(),
        )
    ),
)
```

### JavaScript

```
const config = {
  responseModalities: [Modality.AUDIO],
  contextWindowCompression: { slidingWindow: {} }
};
```

## セッションの再開

サーバーが WebSocket 接続を定期的にリセットするときにセッションが終了しないようにするには、[設定構成](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentSetup)で [sessionResumption](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentSetup.FIELDS.SessionResumptionConfig.BidiGenerateContentSetup.session_resumption) フィールドを構成します。

この構成を渡すと、サーバーは [SessionResumptionUpdate](https://ai.google.dev/api/live?hl=ja#SessionResumptionUpdate) メッセージを送信します。このメッセージは、最後の再開トークンを後続の接続の [`SessionResumptionConfig.handle`](https://ai.google.dev/api/live?hl=ja#SessionResumptionConfig.FIELDS.string.SessionResumptionConfig.handle) として渡すことで、セッションを再開するために使用できます。

### Python

```
import asyncio
from google import genai
from google.genai import types

client = genai.Client()
model = "gemini-live-2.5-flash-preview"

async def main():
    print(f"Connecting to the service with handle {previous_session_handle}...")
    async with client.aio.live.connect(
        model=model,
        config=types.LiveConnectConfig(
            response_modalities=["AUDIO"],
            session_resumption=types.SessionResumptionConfig(
                # The handle of the session to resume is passed here,
                # or else None to start a new session.
                handle=previous_session_handle
            ),
        ),
    ) as session:
        while True:
            await session.send_client_content(
                turns=types.Content(
                    role="user", parts=[types.Part(text="Hello world!")]
                )
            )
            async for message in session.receive():
                # Periodically, the server will send update messages that may
                # contain a handle for the current state of the session.
                if message.session_resumption_update:
                    update = message.session_resumption_update
                    if update.resumable and update.new_handle:
                        # The handle should be retained and linked to the session.
                        return update.new_handle

                # For the purposes of this example, placeholder input is continually fed
                # to the model. In non-sample code, the model inputs would come from
                # the user.
                if message.server_content and message.server_content.turn_complete:
                    break

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

const ai = new GoogleGenAI({});
const model = 'gemini-live-2.5-flash-preview';

async function live() {
  const responseQueue = [];

  async function waitMessage() {
    let done = false;
    let message = undefined;
    while (!done) {
      message = responseQueue.shift();
      if (message) {
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message;
  }

  async function handleTurn() {
    const turns = [];
    let done = false;
    while (!done) {
      const message = await waitMessage();
      turns.push(message);
      if (message.serverContent && message.serverContent.turnComplete) {
        done = true;
      }
    }
    return turns;
  }

console.debug('Connecting to the service with handle %s...', previousSessionHandle)
const session = await ai.live.connect({
  model: model,
  callbacks: {
    onopen: function () {
      console.debug('Opened');
    },
    onmessage: function (message) {
      responseQueue.push(message);
    },
    onerror: function (e) {
      console.debug('Error:', e.message);
    },
    onclose: function (e) {
      console.debug('Close:', e.reason);
    },
  },
  config: {
    responseModalities: [Modality.TEXT],
    sessionResumption: { handle: previousSessionHandle }
    // The handle of the session to resume is passed here, or else null to start a new session.
  }
});

const inputTurns = 'Hello how are you?';
session.sendClientContent({ turns: inputTurns });

const turns = await handleTurn();
for (const turn of turns) {
  if (turn.sessionResumptionUpdate) {
    if (turn.sessionResumptionUpdate.resumable && turn.sessionResumptionUpdate.newHandle) {
      let newHandle = turn.sessionResumptionUpdate.newHandle
      // ...Store newHandle and start new session with this handle here
    }
  }
}

  session.close();
}

async function main() {
  await live().catch((e) => console.error('got error', e));
}

main();
```

## セッションが切断される前にメッセージを受信する

サーバーは、現在の接続がまもなく終了することを通知する [GoAway](https://ai.google.dev/api/live?hl=ja#GoAway) メッセージを送信します。このメッセージには、残り時間を示す [timeLeft](https://ai.google.dev/api/live?hl=ja#GoAway.FIELDS.google.protobuf.Duration.GoAway.time_left) が含まれており、接続が ABORTED として終了する前に追加のアクションを実行できます。

### Python

```
async for response in session.receive():
    if response.go_away is not None:
        # The connection will soon be terminated
        print(response.go_away.time_left)
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.goAway) {
    console.debug('Time left: %s\n', turn.goAway.timeLeft);
  }
}
```

## 生成が完了したときにメッセージが届く

サーバーは、モデルがレスポンスを生成し終えたことを通知する [generationComplete](https://ai.google.dev/api/live?hl=ja#BidiGenerateContentServerContent.FIELDS.bool.BidiGenerateContentServerContent.generation_complete) メッセージを送信します。

### Python

```
async for response in session.receive():
    if response.server_content.generation_complete is True:
        # The generation is complete
```

### JavaScript

```
const turns = await handleTurn();

for (const turn of turns) {
  if (turn.serverContent && turn.serverContent.generationComplete) {
    // The generation is complete
  }
}
```

## 次のステップ

Live API の使用方法について詳しくは、[機能](https://ai.google.dev/gemini-api/docs/live?hl=ja)ガイド、[ツールの使用](https://ai.google.dev/gemini-api/docs/live-tools?hl=ja)ページ、[Live API クックブック](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb)をご覧ください。

フィードバックを送信

特に記載のない限り、このページのコンテンツは[クリエイティブ・コモンズの表示 4.0 ライセンス](https://creativecommons.org/licenses/by/4.0/)により使用許諾されます。コードサンプルは [Apache 2.0 ライセンス](https://www.apache.org/licenses/LICENSE-2.0)により使用許諾されます。詳しくは、[Google Developers サイトのポリシー](https://developers.google.com/site-policies?hl=ja)をご覧ください。Java は Oracle および関連会社の登録商標です。

最終更新日 2025-06-27 UTC。

---

# Content from: https://ai.google.dev/gemini-api/docs/ephemeral-tokens

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Ephemeral tokens

Эфемерные токены — это кратковременные токены аутентификации для доступа к API Gemini через [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) . Они предназначены для повышения безопасности при прямом подключении с устройства пользователя к API (реализация [клиент-сервер](https://ai.google.dev/gemini-api/docs/live?hl=ru#implementation-approach) ). Как и стандартные ключи API, эфемерные токены могут быть извлечены из клиентских приложений, таких как веб-браузеры или мобильные приложения. Но поскольку эфемерные токены быстро истекают и могут быть ограничены, они значительно снижают риски безопасности в производственной среде.

## Как работают эфемерные токены

Вот как работают эфемерные токены на высоком уровне:

1.  Ваш клиент (например, веб-приложение) аутентифицируется на вашем бэкэнде.
2.  Ваш бэкэнд запрашивает эфемерный токен у службы предоставления API Gemini.
3.  Gemini API выпускает краткосрочный токен.
4.  Ваш бэкэнд отправляет токен клиенту для подключений WebSocket к Live API. Вы можете сделать это, заменив свой ключ API на эфемерный токен.
5.  Затем клиент использует токен так, как если бы это был ключ API.

![Обзор эфемерных токенов](https://ai.google.dev/static/gemini-api/docs/images/Live_API_01.png?hl=ru)

Это повышает безопасность, поскольку даже если извлечь токен, он недолговечен, в отличие от долгоживущего ключа API, развернутого на стороне клиента. Поскольку клиент отправляет данные напрямую в Gemini, это также уменьшает задержку и позволяет избежать необходимости проксирования вашими бэкэндами данных в реальном времени.

## Создать эфемерный токен

Вот упрощенный пример того, как получить эфемерный токен от Gemini. По умолчанию у вас будет 1 минута, чтобы начать новые сеансы Live API, используя токен из этого запроса ( `newSessionExpireTime` ), и 30 минут, чтобы отправить сообщения по этому соединению ( `expireTime` ).

### Питон

```
import datetime

now = datetime.datetime.now(tz=datetime.timezone.utc)

client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1, # The ephemeral token can only be used to start a single session
    'expire_time': now + datetime.timedelta(minutes=30), # Default is 30 minutes in the future
    # 'expire_time': '2025-05-17T00:00:00Z',   # Accepts isoformat.
    'new_session_expire_time': now + datetime.timedelta(minutes=1), # Default 1 minute in the future
    'http_options': {'api_version': 'v1alpha'},
  }
)

# You'll need to pass the value under token.name back to your client to use it
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

  const token: AuthToken = await client.authTokens.create({
    config: {
      uses: 1, // The default
      expireTime: expireTime // Default is 30 mins
      newSessionExpireTime: new Date(Date.now() + (1 * 60 * 1000)), // Default 1 minute in the future
      httpOptions: {apiVersion: 'v1alpha'},
    },
  });
```

Для ограничений значения `expireTime` , значений по умолчанию и других спецификаций полей см. [справочник API](https://ai.google.dev/api/live?hl=ru#ephemeral-auth-tokens) . В течение периода `expireTime` вам понадобится [`sessionResumption`](https://ai.google.dev/gemini-api/docs/live-session?hl=ru#session-resumption) для повторного подключения вызова каждые 10 минут (это можно сделать с тем же токеном, даже если `uses: 1` ).

Также возможно заблокировать эфемерный токен для набора конфигураций. Это может быть полезно для дальнейшего повышения безопасности вашего приложения и сохранения системных инструкций на стороне сервера.

### Питон

```
client = genai.Client(
    http_options={'api_version': 'v1alpha',}
)

token = client.auth_tokens.create(
    config = {
    'uses': 1,
    'live_connect_constraints': {
        'model': 'gemini-2.0-flash-live-001',
        'config': {
            'session_resumption':{},
            'temperature':0.7,
            'response_modalities':['TEXT']
        }
    },
    'http_options': {'api_version': 'v1alpha'},
    }
)

# You'll need to pass the value under token.name back to your client to use it
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const client = new GoogleGenAI({});
const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

const token = await client.authTokens.create({
    config: {
        uses: 1, // The default
        expireTime: expireTime,
        liveConnectConstraints: {
            model: 'gemini-2.0-flash-live-001',
            config: {
                sessionResumption: {},
                temperature: 0.7,
                responseModalities: ['TEXT']
            }
        },
        httpOptions: {
            apiVersion: 'v1alpha'
        }
    }
});

// You'll need to pass the value under token.name back to your client to use it
```

Вы также можете заблокировать подмножество полей, дополнительную информацию см. [в документации SDK](https://googleapis.github.io/python-genai/genai.html#genai.types.CreateAuthTokenConfig.lock_additional_fields) .

## Подключитесь к Live API с помощью эфемерного токена

Вот пример, который подключается к Live API через эфемерный токен. Обратите внимание, что использование эфемерных токенов добавляет ценность только при развертывании приложений, которые следуют подходу [реализации клиент-сервер](https://ai.google.dev/gemini-api/docs/live?hl=ru#implementation-approach) .

### JavaScript

```
import { GoogleGenAI, Modality } from '@google/genai';

// Use the token generated in the "Create an ephemeral token" section here
const ai = new GoogleGenAI({});
const model = 'gemini-2.0-flash-live-001';
const config = { responseModalities: [Modality.TEXT] };

async function main() {

  const session = await ai.live.connect({
    model: model,
    config: config,
    callbacks: { ... },
  });

  // Send content...

  session.close();
}

main();
```

Дополнительные примеры см. в разделе [Начало работы с Live API](https://ai.google.dev/gemini-api/docs/live?hl=ru) .

## Лучшие практики

*   Установите короткий срок действия с помощью параметра `expire_time` .
*   Срок действия токенов истекает, что требует повторной инициализации процесса предоставления.
*   Проверьте безопасную аутентификацию для вашего бэкенда. Эфемерные токены будут настолько же безопасны, насколько безопасен ваш метод аутентификации бэкенда.
*   Как правило, избегайте использования эфемерных токенов для соединений бэкэнд-Gemini, поскольку этот путь обычно считается безопасным.

## Ограничения

В настоящее время эфемерные токены совместимы только с [Live API](https://ai.google.dev/gemini-api/docs/live?hl=ru) .

## Что дальше?

*   Для получения дополнительной информации ознакомьтесь [со справкой](https://ai.google.dev/api/live?hl=ru#ephemeral-auth-tokens) Live API по эфемерным токенам.

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/caching

2.5 ファミリーの思考モデルの更新をご紹介します。[詳細](https://ai.google.dev/gemini-api/docs/models?hl=ja)

![](https://ai.google.dev/_static/images/translated.svg?hl=ja) このページは [Cloud Translation API](//cloud.google.com/translate/?hl=ja) によって翻訳されました。

*   [ホーム](https://ai.google.dev/?hl=ja)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ja)
*   [モデル](https://ai.google.dev/gemini-api/docs?hl=ja)

フィードバックを送信

# コンテキストのキャッシュ保存

Python JavaScript Go REST

一般的な AI ワークフローでは、同じ入力トークンをモデルに何度も渡すことがあります。Gemini API には、次の 2 種類のキャッシュ メカニズムがあります。

*   暗黙的なキャッシュ保存（Gemini 2.5 モデルで自動的に有効になります。費用削減は保証されません）
*   明示的なキャッシュ保存（ほとんどのモデルで手動で有効にでき、コスト削減が保証される）

明示的なキャッシュは、コスト削減を保証しつつ、デベロッパーの作業を追加する場合に便利です。

## 暗黙的キャッシュ

暗黙的キャッシングは、すべての Gemini 2.5 モデルでデフォルトで有効になっています。リクエストがキャッシュにヒットすると、費用の削減が自動的に反映されます。これを有効にするために必要な操作はありません。2025 年 5 月 8 日より有効です。コンテキスト キャッシュ保存の最小入力トークン数は、2.5 Flash では 1,024、2.5 Pro では 2,048 です。

暗黙的なキャッシュ ヒットの確率を高めるには:

*   サイズが大きく、一般的なコンテンツをプロンプトの先頭に配置してみてください
*   短時間に類似のプレフィックスを含むリクエストを送信しようとする

キャッシュヒットしたトークン数は、レスポンス オブジェクトの `usage_metadata` フィールドで確認できます。

## 明示的なキャッシュ保存

Gemini API の明示的なキャッシュ保存機能を使用すると、一部のコンテンツをモデルに 1 回渡し、入力トークンをキャッシュに保存してから、後続のリクエストでキャッシュに保存されたトークンを参照できます。一定のボリュームでは、キャッシュに保存されたトークンを使用する方が、同じトークン コーパスを繰り返し渡すよりも費用が少なくなります。

トークンのセットをキャッシュに保存するときに、トークンが自動的に削除されるまでのキャッシュの存続期間を選択できます。このキャッシュ保存期間は有効期間（TTL）と呼ばれます。設定しない場合、TTL はデフォルトで 1 時間になります。キャッシュに保存する費用は、入力トークンのサイズとトークンを保持する時間によって異なります。

このセクションでは、[クイックスタート](https://ai.google.dev/gemini-api/docs/quickstart?hl=ja)に記載されているように、Gemini SDK がインストールされていること（または curl がインストールされていること）と、API キーが構成されていることを前提としています。

### OpenAI ライブラリを使用した明示的なキャッシュ

[OpenAI ライブラリ](https://ai.google.dev/gemini-api/docs/openai?hl=ja)を使用している場合は、[`extra_body`](https://ai.google.dev/gemini-api/docs/openai?hl=ja#extra-body) の `cached_content` プロパティを使用して明示的なキャッシュを有効にできます。

## 明示的なキャッシュ保存を使用する場合

コンテキスト キャッシュ保存は、初期コンテキストの実体部分が、短いリクエストで繰り返し参照されるシナリオに特に適しています。次のようなユースケースでは、コンテキスト キャッシュ保存の使用を検討してください。

*   広範な[システム指示](https://ai.google.dev/gemini-api/docs/system-instructions?hl=ja)を伴う chatbot
*   長時間の動画ファイルの繰り返し分析
*   大規模なドキュメント セットに対する繰り返しのクエリ
*   頻繁なコード リポジトリの分析やバグ修正

### 明示的なキャッシュ保存による費用削減

コンテキスト キャッシュ保存は、全体的な運用コストを削減するために設計された有料の機能です。ご請求は次の項目に基づいて行われます。

1.  **キャッシュ トークン数:** キャッシュに保存された入力トークンの数。後続のプロンプトに含まれる場合は、割引料金で請求されます。
2.  **保存期間:** キャッシュに保存されたトークンの保存時間（TTL）。キャッシュに保存されたトークン数の TTL 時間に基づいて課金されます。TTL の最小値や最大値はありません。
3.  **その他の項目:** 入力トークンや出力トークンがキャッシュされていない場合などは、別の料金が適用されます。

最新の料金の詳細については、Gemini API の[料金ページ](https://ai.google.dev/pricing?hl=ja)をご覧ください。トークンをカウントする方法については、[トークン ガイド](https://ai.google.dev/gemini-api/docs/tokens?hl=ja)をご覧ください。

### その他の考慮事項

コンテキスト キャッシュを使用する場合は、次の点に注意してください。

*   コンテキスト キャッシュ保存の入力トークンの_最小数_は、2.5 Flash では 1,024、2.5 Pro では 2,048 です。_最大値_は、指定されたモデルの最大値と同じです。（トークンのカウントについて詳しくは、[トークンのガイド](https://ai.google.dev/gemini-api/docs/tokens?hl=ja)をご覧ください）。
*   モデルは、キャッシュに保存されたトークンと通常の入力トークンを区別しません。キャッシュに保存されたコンテンツは、プロンプトの接頭辞です。
*   コンテキスト キャッシュには特別なレートや使用量の上限はありません。`GenerateContent` の標準レートの上限が適用され、トークンの上限にはキャッシュに保存されたトークンが含まれます。
*   キャッシュに保存されているトークン数は、キャッシュ サービスの create、get、list オペレーションから `usage_metadata` で返されます。また、キャッシュを使用している場合は `GenerateContent` でも返されます。

フィードバックを送信

特に記載のない限り、このページのコンテンツは[クリエイティブ・コモンズの表示 4.0 ライセンス](https://creativecommons.org/licenses/by/4.0/)により使用許諾されます。コードサンプルは [Apache 2.0 ライセンス](https://www.apache.org/licenses/LICENSE-2.0)により使用許諾されます。詳しくは、[Google Developers サイトのポリシー](https://developers.google.com/site-policies?hl=ja)をご覧ください。Java は Oracle および関連会社の登録商標です。

最終更新日 2025-06-27 UTC。

---

# Content from: https://ai.google.dev/gemini-api/docs/files

2.5 사고 모델 제품군의 업데이트를 소개합니다. [자세히 알아보기](https://ai.google.dev/gemini-api/docs/models?hl=ko)

![](https://ai.google.dev/_static/images/translated.svg?hl=ko) 이 페이지는 [Cloud Translation API](//cloud.google.com/translate/?hl=ko)를 통해 번역되었습니다.

*   [홈](https://ai.google.dev/?hl=ko)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ko)
*   [모델](https://ai.google.dev/gemini-api/docs?hl=ko)

의견 보내기

# Files API

Gemini 인공지능 (AI) 모델 제품군은 텍스트, 이미지, 오디오를 비롯한 다양한 유형의 입력 데이터를 처리하도록 설계되었습니다. 이러한 모델은 둘 이상의 데이터 유형 또는 _모드_를 처리할 수 있으므로 Gemini 모델은 _멀티모달 모델_이라고 하거나 _멀티모달 기능_이 있다고 설명합니다.

이 가이드에서는 Files API를 사용하여 미디어 파일을 사용하는 방법을 설명합니다. 기본 작업은 오디오 파일, 이미지, 동영상, 문서, 기타 지원되는 파일 형식에서 동일합니다.

파일 프롬프트 안내는 [파일 프롬프트 가이드](https://ai.google.dev/gemini-api/docs/files?hl=ko#prompt-guide) 섹션을 참고하세요.

## 파일 업로드

Files API를 사용하여 미디어 파일을 업로드할 수 있습니다. 총 요청 크기 (파일, 텍스트 프롬프트, 시스템 안내 등 포함)가 20MB를 초과하는 경우 항상 Files API를 사용하세요.

다음 코드는 파일을 업로드한 다음 `generateContent` 호출에서 이 파일을 사용합니다.

### Python

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

### 자바스크립트

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
defer client.DeleteFile(ctx, file.Name)

model := client.GenerativeModel("gemini-2.0-flash")
resp, err := model.GenerateContent(ctx,
    genai.FileData{URI: file.URI},
    genai.Text("Describe this audio clip"))
if err != nil {
    log.Fatal(err)
}

printResponse(resp)
```

### REST

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D "${tmp_header_file}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## 파일 메타데이터 가져오기

API가 업로드된 파일을 성공적으로 저장했는지 확인하고 `files.get`를 호출하여 메타데이터를 가져올 수 있습니다.

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
file_name = myfile.name
myfile = client.files.get(name=file_name)
print(myfile)
```

### 자바스크립트

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
const fetchedFile = await ai.files.get({ name: fileName });
console.log(fetchedFile);
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}

gotFile, err := client.GetFile(ctx, file.Name)
if err != nil {
    log.Fatal(err)
}
fmt.Println("Got file:", gotFile.Name)
```

### REST

```
# file_info.json was created in the upload example
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name \
-H "x-goog-api-key: $GEMINI_API_KEY" > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

## 업로드된 파일 목록

Files API를 사용하여 여러 파일을 업로드할 수 있습니다. 다음 코드는 업로드된 모든 파일의 목록을 가져옵니다.

### Python

```
print('My files:')
for f in client.files.list():
    print(' ', f.name)
```

### 자바스크립트

```
const listResponse = await ai.files.list({ config: { pageSize: 10 } });
for await (const file of listResponse) {
  console.log(file.name);
}
```

### Go

```
iter := client.ListFiles(ctx)
for {
    ifile, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(ifile.Name)
}
```

### REST

```
echo "My files: "

curl "https://generativelanguage.googleapis.com/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## 업로드된 파일 삭제하기

파일은 48시간 후에 자동으로 삭제됩니다. 업로드된 파일을 수동으로 삭제할 수도 있습니다.

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
client.files.delete(name=myfile.name)
```

### 자바스크립트

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
await ai.files.delete({ name: fileName });
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
client.DeleteFile(ctx, file.Name)
```

### REST

```
curl --request "DELETE" https://generativelanguage.googleapis.com/v1beta/files/$name \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## 사용량 정보

Files API를 사용하여 미디어 파일을 업로드하고 상호작용할 수 있습니다. Files API를 사용하면 프로젝트당 최대 20GB의 파일을 저장할 수 있으며 파일당 최대 크기는 2GB입니다. 파일은 48시간 동안 저장됩니다. 이 기간에는 API를 사용하여 파일의 메타데이터를 가져올 수 있지만 파일을 다운로드할 수는 없습니다. Files API는 Gemini API가 지원되는 모든 지역에서 무료로 사용할 수 있습니다.

## 파일 프롬프트 전략

이 섹션에서는 Gemini API 프롬프트와 함께 미디어 파일을 사용하는 방법에 관한 안내와 권장사항을 제공합니다.

프롬프트에서 다양한 유형의 데이터를 사용할 수 있으면 Gemini API로 처리할 수 있는 작업에 더 유연하게 대처할 수 있습니다. 예를 들어 모델에 맛있는 음식 사진을 전송하고 해당 음식에 관한 짧은 블로그를 작성하도록 요청할 수 있습니다.

**프롬프트**

**응답**

이 사진을 바탕으로 마음을 사로잡는 짧은 블로그 게시물을 작성합니다. 사진에 있는 음식에 대한 설명과 식사 준비 과정을 설명해야 합니다.

  
![쿠키](https://ai.google.dev/static/gemini-api/docs/images/image_prompt.png?hl=ko)

도시락 싸기는 시간과 비용을 절약하는 좋은 방법일 뿐만 아니라 건강한 식습관을 기르는 데도 도움이 됩니다. 이 식사는 미리 쉽게 준비할 수 있는 건강하고 맛있는 식사의 좋은 예입니다.

흑미, 구운 채소, 치킨 데리야키가 포함된 식사입니다. 현미는 건강하고 복합적인 탄수화물로 지속적인 에너지를 제공합니다. 구운 야채는 비타민과 미네랄을 일일 권장량만큼 섭취할 수 있는 좋은 방법이며, 치킨 테리아키는 맛있고 단백질이 풍부한 옵션입니다.

이 식사는 준비하기도 매우 쉽습니다. 현미를 밥짓고, 야채를 로스트한 다음, 닭고기 데리야키를 익히면 됩니다. 모든 재료가 다 익으면 식사 준비용 용기에 나눠 냉장고에 보관합니다. 그런 다음 컨테이너를 가져와 바쁜 아침을 시작할 수 있습니다.

미리 간편하게 준비할 수 있는 건강하고 맛있는 식사를 찾고 있다면 이 식사가 좋은 선택입니다. 영양소와 풍미가 가득하며 포만감과 만족감을 줍니다.

건강하고 맛있는 도시락 준비를 위해 최선을 다하겠습니다.

미디어 파일을 사용하는 프롬프트에서 원하는 출력을 가져오는 데 문제가 있는 경우 원하는 결과를 얻는 데 도움이 되는 몇 가지 전략이 있습니다. 다음 섹션에서는 다중 모드 입력을 사용하는 프롬프트를 개선하기 위한 설계 접근 방식과 문제 해결 도움말을 제공합니다.

다음 권장사항에 따라 멀티모달 프롬프트를 향상시킬 수 있습니다.

*   ### [프롬프트 설계 기초](#specific-instructions)
    
    *   **구체적인 지침 설정**: 오해의 소지가 거의 없도록 명확하고 간결한 지침을 제시합니다.
    *   **프롬프트에 몇 가지 예시 추가:** 달성하려는 목표를 쉽게 이해할 수 있도록 실질적인 퓨샷 예시를 사용합니다.
    *   **단계별 분류**: 복잡한 태스크를 관리 가능한 하위 목표로 분할하고 프로세스 전반에 걸쳐서 모델을 안내합니다.
    *   **출력 형식 지정**: 프롬프트에서 마크다운, JSON, HTML 등 원하는 형식으로 출력을 요청합니다.
    *   **단일 이미지 프롬프트의 경우 이미지를 먼저 배치**: Gemini는 어떤 순서로든 이미지 및 텍스트 입력을 처리할 수 있지만 단일 이미지가 포함된 프롬프트의 경우 해당 이미지(또는 동영상)를 텍스트 프롬프트 앞에 배치할 경우 성능이 향상될 수 있습니다. 그러나 상황에 따라 이미지와 텍스트를 밀접하게 혼합해야 할 경우에는 무엇이든 가장 자연스러운 순서를 따릅니다.
*   ### [멀티모달 프롬프트 문제 해결](#troubleshooting)
    
    *   **모델이 관련 이미지 부분에서 정보를 가져오지 않는 경우:** 프롬프트로 정보를 가져오려는 이미지 부분에 대한 힌트를 제공합니다.
    *   **모델 출력이 너무 일반적인 경우(이미지/동영상 출력에 맞게 충분히 맞춤화되지 않은 경우):** 프롬프트 시작 시 태스크 지침을 제공하기 전 이미지 또는 동영상을 기술하도록 모델에 요청하거나 이미지에 포함된 항목을 참조하도록 모델에 요청합니다.
    *   **실패한 부분 문제 해결:** 이미지를 기술하거나, 이유를 설명하거나, 모델의 초기 이해 수준을 측정하도록 모델에 요청합니다.
    *   **프롬프트 결과 비현실적인 콘텐츠가 발생하는 경우:** 강도 설정을 낮추거나 추가 세부정보를 추정할 가능성이 적도록 모델에 더 짧은 설명을 요청합니다.
    *   **샘플링 매개변수 조정:** 여러 다른 강도 설정 및 최상위 K 선택으로 실험해서 모델 창의성을 조정합니다.

### 구체적인 지침 설정

프롬프트는 명확하고 세부적일 때 가장 성공적입니다. 세부적인 출력을 원할 때는 원하는 출력을 얻을 수 있도록 프롬프트에 그러한 요구사항을 포함하는 것이 더 좋습니다.

공항 보드 이미지의 경우 "이미지 설명"이라고만 모델에 요청하면 일반적인 설명이 생성될 수 있습니다. 모델이 이미지에서 시간과 도시를 파싱하도록 하려면 프롬프트에 해당 요청을 직접 포함할 수 있습니다.

프롬프트

모델 응답

![](https://ai.google.dev/static/gemini-api/docs/images/timetable.png?hl=ko)

이 이미지를 설명해 주세요.

이 이미지는 공항 도착 및 출발 보드를 보여줍니다.

**업데이트된 프롬프트**

**개선된 응답**

![여행 일정](https://ai.google.dev/static/gemini-api/docs/images/timetable.png?hl=ko)

이 이미지에 표시된 공항 보드에서 시간과 도시를 분석해서 목록으로 표시해 주세요.

*   10:50 모스크바
*   11:05 에든버러
*   11:05 런던
*   11:10 부카레스트
*   11:30 키예프
*   11:35 더블린
*   11:45 이스트 미들랜즈
*   12:15 소피아
*   12:30 런던
*   12:30 뉴캐슬
*   12:40 상트페테르부르크
*   12:40 런던
*   12:45 맨체스터

### 몇 가지 예시 추가

Gemini 모델은 원하는 출력을 이해하기 위해 예시로 사용할 수 있는 여러 입력을 수락할 수 있습니다. 이러한 예시를 추가하면 모델이 패턴을 식별하고 제공된 이미지와 응답 사이의 관계를 새 예시에 적용할 수 있습니다. 이를 "퓨샷" 학습이라고도 합니다.

아래 예시에서 최초 출력은 문장 형식으로 작성되었으며 국가(브라질)를 포함합니다. 다른 형식 또는 스타일로 출력을 표시하고 국가가 아닌 도시만 포함한다고 가정해 보세요. 프롬프트에 퓨샷 예시를 추가하면 원하는 방식으로 응답하도록 모델을 조정할 수 있습니다.

**프롬프트**

**모델 응답**

![여행 일정](https://ai.google.dev/static/gemini-api/docs/images/redeemer.png?hl=ko)

명소와 함께 도시를 설명해 주세요.

이 랜드마크는 브라질 리우데자네이루에 있는 구세주 예수상입니다.

**업데이트된 프롬프트**

**개선된 응답**

명소와 함께 도시를 설명해 주세요.

![콜로세움](https://ai.google.dev/static/gemini-api/docs/images/colosseum.png?hl=ko)

도시: 로마, 랜드마크: 콜로세움

![자금성](https://ai.google.dev/static/gemini-api/docs/images/forbidden-city.png?hl=ko)

도시: 베이징, 랜드마크: 자금성

![자금성](https://ai.google.dev/static/gemini-api/docs/images/redeemer.png?hl=ko)

도시: 리우데자네이루, 랜드마크: 구세주 예수상

### 단계별 분류

시각적 이해와 추론이 모두 필요한 것과 같이 복잡한 태스크의 경우에는 태스크를 더 작고 보다 직관적인 단계들로 분할하는 것이 유용할 수 있습니다. 또는 프롬프트에서 "단계별로 생각"하도록 모델에 직접 요청하는 것도 효과적일 수 있습니다.

프롬프트

모델 응답

![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg?hl=ko)

화장지가 언제 다 떨어질까요?

이제 3개 밖에 없으니 곧 떨어집니다.

**업데이트된 프롬프트**

**개선된 응답**

![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg?hl=ko)

1\. 먼저 이 사진에서 화장실 휴지 개수를 세어 보세요.

2\. 그런 다음 일반적인 사람이 하루에 화장실 휴지를 얼마나 사용하는지 확인해 주세요.

3\. 여기 있는 화장실 휴지가 얼마나 오래갈지 계산해 주세요.

1\. 이 사진에는 화장실 휴지가 3개 있습니다.

2\. 사람은 일반적으로 화장실 휴지를 하루에 20칸 정도 사용합니다.

3\. 한 롤에 휴지가 200칸 있으면 각 롤이 약 10일 정도 지속됩니다. 따라서 3개 롤은 한 달 정도 지속됩니다.

수학 문제나 기타 유형의 단어 문제는 모델이 단계별로 생각하도록 요청하기에 적합한 후보입니다.

**프롬프트**

**응답**

![](https://ai.google.dev/static/gemini-api/docs/images/math.jpg?hl=ko)

이 시퀀스에서 네 번째 항은 무엇인가요?

\-135

모델의 응답은 잘못되었습니다. 이를 개선하기 위한 몇 가지 방법은 모델이 프롬프트에 답변하기 전 문제를 파싱하도록 요청하는 것입니다.

**프롬프트**

**응답**

![](https://ai.google.dev/static/gemini-api/docs/images/math.jpg?hl=ko)

먼저 이미지에서 수식을 분석해 주세요.

그런 후 수식을 기준으로 "시퀀스에서 네 번째 항은 무엇"인지 대답해 주세요.

_클릭하여 결과 확장_

$$ b(n) = b(n-1) \* (-3) $$

$$ b(1) = 15 $$

1.  n = 4를 수식에 대체합니다.

$$ b(4) = b(4-1) \* (-3) $$

$$ b(4) = b(3) \* (-3) $$

4.  b(3)의 값을 알지 못하므로 n = 3을 수식에 대체합니다.

$$ b(3) = b(3-1) \* (-3) $$

$$ b(3) = b(2) \* (-3) $$

7.  b(2)의 값을 알지 못하므로 n = 2를 수식에 대체합니다.

$$ b(2) = b(2-1) \* (-3) $$

$$ b(2) = b(1) \* (-3) $$

10.  b(1)의 값이 15인 것을 알고 있으므로 이를 수식에 대체합니다.

$$ b(2) = 15 \* (-3) $$

$$ b(2) = -45 $$

13.  이제 b(2)의 값을 알았으므로 이를 b(3) 수식에 대체합니다.

$$ b(3) = b(2) \* (-3) $$

$$ b(3) = -45 \* (-3) $$

$$ b(3) = 135 $$

17.  이제 b(3)의 값을 알았으므로 이를 b(4) 수식에 대체합니다.

$$ b(4) = b(3) \* (-3) $$

$$ b(4) = 135 \* (-3) $$

$$ b(4) = -405 $$

따라서 시퀀스에서 네 번째 항은 -405입니다.

이번에는 모델 응답이 올바릅니다. 이제 응답을 보다 효율적으로 만들기 위해 모델에 단계별로 생각하도록 요청할 수 있습니다.

**업데이트된 프롬프트**

**개선된 응답**

![](https://ai.google.dev/static/gemini-api/docs/images/math.jpg?hl=ko)

이 시퀀스에서 네 번째 항은 무엇인가요? 단계별로 생각해 주세요.

첫 번째 항은 15입니다.

두 번째 항은 b(1) \* (-3) = 15 \* (-3) = -45입니다.

세 번째 항은 b(2) \* (-3) = -45 \* (-3) = 135입니다.

네 번째 항은 b(3) \* (-3) = 135 \* (-3) = -405입니다.

### 출력 형식 지정

일반적인 문제는 모델 출력을 다운스트림 태스크로 수집해야 할 경우 특히 모델 출력을 특정 형식(예: 마크다운, JSON, HTML)으로 지정해야 한다는 것입니다. 프롬프트 내에서 이를 수행하도록 모델을 안내하여 해당 형식으로 출력을 생성할 수 있습니다. 다음은 두 가지 예시입니다.

프롬프트

모델 응답

![](https://ai.google.dev/static/gemini-api/docs/images/excel-example.png?hl=ko)

이 이미지의 표를 마크다운 형식으로 분석해 주세요.

| 과목 | 상태 | 시간 | 시작일 | 만료일 |

| ------- | ------ | ---- | ---------- | ------ |

| 수학 | 완료 | 1.5시간 | 2023/11/29 | 2023/12/1 |

| 지리 | 완료 | 30분 | 2023/12/2 | 2023/12/4 |

| 물리 | 진행 중 | 2시간 | 2023/12/3 | 2023/12/5 |

| 생물 | 시작 안 함 | 6시간 | 2016/11/2 | 2023/12/6 |

**프롬프트**

**모델 응답**

![](https://ai.google.dev/static/gemini-api/docs/images/spanish-meal.png?hl=ko)

다음 모든 속성:

재료, 요리 종류, 채식주의 식단 여부의 목록을 JSON 형식으로 제공해 주세요.

` ```json `

`{`

`"ingredients": [`

`"rice",`

`"shrimp",`

`"clams",`

`"mussels",`

`"peas",`

`"tomatoes",`

`"onions",`

`"garlic",`

`"olive oil",`

`"paprika",`

`"salt",`

`"pepper"`

`],`

`"type of cuisine": "Spanish",`

`"vegetarian": false`

`}`

` ``` `

### 단일 이미지 프롬프트에 이미지 먼저 배치

Gemini는 프롬프트 내에서 어떤 순서로든 이미지와 텍스트를 해석할 수 있지만 텍스트 프롬프트 앞에 단일 이미지를 배치하는 것이 더 좋은 결과로 이어질 수 있습니다. 다음 샘플에서는 각 프롬프트에서 이미지가 텍스트 앞에 오는 것을 알 수 있습니다.

### 멀티모달 프롬프트 문제 해결

유용한 응답을 얻지 못할 경우 프롬프트를 문제 해결해야 할 수 있습니다. 이를 위해서는 몇 가지 전략을 시도해볼 수 있습니다.

#### 모델이 관련 이미지 부분에서 정보를 가져오지 않는 경우

보다 구체적인 응답을 얻기 위해서는 해당 응답에 정보를 제공하도록 프롬프트가 정보를 가져올 이미지 특정 부분을 지적할 수 있습니다.

프롬프트

모델 응답

![](https://ai.google.dev/static/gemini-api/docs/images/diapers.png?hl=ko)

아기 한 명이 사용할 때 이 기저귀는 며칠이나 갈까요?

기저귀가 모두 소진되려면 198일 걸립니다.

**업데이트된 프롬프트**

**개선된 응답**

![](https://ai.google.dev/static/gemini-api/docs/images/diapers.png?hl=ko)

기저귀가 소진되려면 며칠이나 걸릴까요?

상자에 표시된 무게를 사용해서 아기의 나이를 확인하고 상자에 적힌 총 기저귀 개수를 사용해 주세요.

총 개수를 아기 한 명이 하루에 사용하는 기저귀 수로 나눠주세요.

상자에는 198개의 기저귀가 있습니다. 상자에는 체중이 22-37lbs인 아기를 위한 기저귀라고 씌여져 있습니다.

이 몸무게 범위의 평균 아기는 만 1~2세 사이입니다.

만 2세 아기는 일반적으로 하루 6개 정도 기저귀를 사용합니다. 따라서 기저귀는 33일(198/6) 정도 지속됩니다.

#### 모델 출력이 너무 일반적이고 이미지 입력에 충분히 조정되지 않은 경우

모델이 이미지에 따라 응답을 조정할 수 있게 하려면 근거 태스크를 수행하기 전 이미지를 기술하도록 요청합니다.

프롬프트

모델 응답

![](https://ai.google.dev/static/gemini-api/docs/images/packing.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/mall.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/ocean.jpg?hl=ko)

이러한 이미지 사이의 공통점은 무엇인가요?

4개 이미지 모두 풍경 사진을 보여줍니다.

**업데이트된 프롬프트**

**개선된 응답**

![](https://ai.google.dev/static/gemini-api/docs/images/packing.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/mall.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/ocean.jpg?hl=ko)

첫째, 각 이미지에 무엇이 있는지 자세히 설명해 주세요. 이러한 이미지 사이의 공통점은 무엇인가요?

첫 번째 이미지는 여행 가방을 싸고 있는 사람을 보여줍니다. 두 번째 이미지는 쇼핑몰의 빈 통로를 보여줍니다. 세 번째 이미지는 화장실 휴지 한 롤을 보여줍니다. 네 번째 이미지는 어둡고 거친 바다를 보여줍니다.

이러한 이미지 간의 공통점은 이미지가 모두 경계 공간의 예시라는 점입니다. 경계 공간은 두 가지 서로 다른 공간 또는 상황 사이에 있는 공간 또는 상황을 의미합니다. 경계 공간의 특성은 불확실성이나 모호한 감정에 있습니다.

첫 번째 이미지는 집과 여행 사이에 있는 사람을 보여줍니다. 두 번째 이미지는 두 매장 사이의 통로를 보여줍니다. 세 번째 이미지는 욕실을 나타낼 수 있는 화장실 휴지를 보여줍니다. 욕실은 전통적으로 개인성과 공공성을 모두 갖고 있는 공간입니다. 그리고 우리가 외부 세계로 들어오고 나가기 위해 스스로를 준비하는 장소이기도 합니다.

네 번째 이미지는 고요함과 거침 사이에 있는 바다를 보여줍니다.

경계 공간은 흥미로움과 불편함을 모두 일으킬 수 있습니다. 이러한 공간은 무언가 시작되는 장소이거나 끝나는 장소일 수 있습니다. 사색하는 장소이거나 행동하는 장소일 수도 있습니다.

또한 모델이 이미지에 포함된 내용을 참조하도록 명확하게 요청할 수도 있습니다.

**업데이트된 프롬프트**

**개선된 응답**

![](https://ai.google.dev/static/gemini-api/docs/images/packing.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/mall.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg?hl=ko) ![](https://ai.google.dev/static/gemini-api/docs/images/ocean.jpg?hl=ko)

이러한 이미지 사이의 공통점은 무엇인가요? 이미지에 포함된 내용을 참조해서 답변해 주세요.

모든 이미지는 여행과 관련이 있습니다. 첫 번째 이미지는 여행 가방을 싸고 있는 사람을 보여줍니다. 두 번째 이미지는 쇼핑몰의 빈 통로를 보여줍니다. 세 번째 이미지는 쌓여 있는 화장실 휴지를 보여줍니다. 네 번째 이미지는 어둡고 거친 바다를 보여줍니다.

이러한 이미지는 모두 여행할 때 필요하거나 사용하는 것들이기 때문에 모두 여행과 관련이 있습니다. 첫 번째 이미지는 여행 가방을 싸고 있는 사람을 보여줍니다. 이것은 사람들이 여행을 떠날 때 하는 행동입니다. 두 번째 이미지는 쇼핑몰의 빈 통로를 보여줍니다. 이것은 사람들이 여행에 필요한 물건을 구입하기 위해 자주 이동하는 장소입니다. 세 번째 이미지는 쌓여 있는 화장실 휴지를 보여줍니다. 이것은 사람들이 여행할 때 자주 사용하는 물건입니다. 네 번째 이미지는 어둡고 거친 바다를 보여줍니다. 이것은 사람들이 여행할 때 가끔 만나게 되는 것입니다.

이미지에서 창의적인 출력을 얻을 때도 동일한 팁이 사용됩니다. 토끼를 사용한 다음 예시에서 특히 모델에 이미지 특성을 포함하도록 요청하면 출력을 개선시킬 수 있습니다.

프롬프트

모델 응답

![](https://ai.google.dev/static/gemini-api/docs/images/rabbit.jpg?hl=ko)

이 이미지에 관한 시를 지어줄 수 있나요?

작은 토끼, 너무 부드럽고 갈색이야,  
위로, 아래로 들판을 뛰어 다녀.  
네 귀는 너무 길어, 네 꼬리는 너무 작아,  
너는 모든 것들 중 가장 귀여운 생명체.

물고 뜯는 잔디 속, 너무 파랗고, 달콤해,  
그리고 날 보면 깡총 떠나버려.  
하지만 난 항상 네 친구야,  
작은 토끼야, 세상 끝날 때까지.

**업데이트된 프롬프트**

**개선된 응답**

![](https://ai.google.dev/static/gemini-api/docs/images/rabbit.jpg?hl=ko)

이 이미지에 대해 위치를 포함해서 설명적이고 극적인 시를 지어줄 수 있나요?

초록 위 작은 토끼,  
귀가 너무 길고, 너무 예리해.  
저 멀리 무엇을 보고 있니,  
저 멀리, 어딜 그렇게 보고 있어?

파란색 바다인가,  
태양빛에 반짝이는?  
아니면 성내고 부서지는 파도인가,  
네가 좋아하는 그 리듬?

작은 토끼야, 사실을 말해줘,  
널 사로잡은 것이 무엇이니?  
짭짤한 바닷 바람이니?  
혹은 낄낄대는 갈매기 울음이니?

어쩌면 그 모든 것을 품은 광활함일지도,  
세상은 너무 크고도 너무 작아.  
꿈들이 돌아다닐 수 있는 곳,  
그리고 모험을 뿌릴 수 있는 곳이야.

작은 토끼야, 네 꿈은 무엇일까,  
풀밭에 앉으면 너무나 고요해.  
깊은 곳을 탐험하고 싶니,  
아니면 마음껏 뛸 수 있는 땅 위에 서고 싶니?

그것이 무엇이든, 작은 토끼야,  
그 놀라움의 불꽃이 밝게 계속 타오르게 하렴.  
너의 꿈과 열망 속에서,  
너의 창조를 기다리는 세상 속에서.

#### 실패한 프롬프트 부분 문제 해결

모델이 처음에 **이미지를 이해**하지 못해서 프롬프트가 실패했는지, 아니면 이미지를 이해했지만 이후 올바른 **추론 단계**를 수행하지 못했는지 알기 어려울 수 있습니다. 이러한 이유들을 구분하기 위해서는 모델에 이미지에 포함된 것이 무엇인지 기술하도록 요청하세요.

다음 예에서 모델이 차와 함께 할 때 어울리지 않는 간식 (예: 팝콘)으로 응답할 경우에는 먼저 모델이 이미지에 차가 포함된 것을 올바르게 인식했는지 확인하는 방식으로 문제 해결을 시작할 수 있습니다.

프롬프트

문제 해결을 위한 프롬프트

![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png?hl=ko)

이 사진과 어울리는 것 중 1분 내에 만들 수 있는 간식은 무엇이 있나요?

![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png?hl=ko)

이 이미지의 내용을 설명해 주세요.

또 다른 전략은 모델에 그 이유를 설명하도록 요청하는 것입니다. 이렇게 하면 근거를 세분화하는 데 도움이 될 수 있습니다.

프롬프트

문제 해결을 위한 프롬프트

![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png?hl=ko)

이 사진과 어울리는 것 중 1분 내에 만들 수 있는 간식은 무엇이 있나요?

![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png?hl=ko)

이 사진과 어울리는 것 중 1분 내에 만들 수 있는 간식은 무엇이 있나요? 이유를 설명해 주세요.

## 다음 단계

*   [Google AI Studio](http://aistudio.google.com?hl=ko)를 사용하여 나만의 멀티모달 프롬프트를 작성해 보세요.
*   Gemini Files API를 사용하여 미디어 파일을 업로드하고 프롬프트에 포함하는 방법에 관한 자세한 내용은 [Vision](https://ai.google.dev/gemini-api/docs/vision?hl=ko), [오디오](https://ai.google.dev/gemini-api/docs/audio?hl=ko), [문서 처리](https://ai.google.dev/gemini-api/docs/document-processing?hl=ko) 가이드를 참고하세요.
*   샘플링 매개변수 조정과 같은 프롬프트 설계 관련 자세한 안내는 [프롬프트 전략](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=ko) 페이지를 참고하세요.

의견 보내기

달리 명시되지 않는 한 이 페이지의 콘텐츠에는 [Creative Commons Attribution 4.0 라이선스](https://creativecommons.org/licenses/by/4.0/)에 따라 라이선스가 부여되며, 코드 샘플에는 [Apache 2.0 라이선스](https://www.apache.org/licenses/LICENSE-2.0)에 따라 라이선스가 부여됩니다. 자세한 내용은 [Google Developers 사이트 정책](https://developers.google.com/site-policies?hl=ko)을 참조하세요. 자바는 Oracle 및/또는 Oracle 계열사의 등록 상표입니다.

최종 업데이트: 2025-06-27(UTC)

---

# Content from: https://ai.google.dev/gemini-api/docs/tokens

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Understand and count tokens

Python JavaScript Go

  

Gemini and other generative AI models process input and output at a granularity called a _token_.

## About tokens

Tokens can be single characters like `z` or whole words like `cat`. Long words are broken up into several tokens. The set of all tokens used by the model is called the vocabulary, and the process of splitting text into tokens is called _tokenization_.

For Gemini models, a token is equivalent to about 4 characters. 100 tokens is equal to about 60-80 English words.

When billing is enabled, the [cost of a call to the Gemini API](/pricing) is determined in part by the number of input and output tokens, so knowing how to count tokens can be helpful.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-05-12 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/live

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# Get started with Live API

Live API 支持与 Gemini 建立低延迟的实时语音和视频互动。它可处理连续的音频、视频或文本数据流，以便立即提供类似人类的语音响应，为用户打造自然的对话体验。

![Live API 概览](https://ai.google.dev/static/gemini-api/docs/images/live-api-overview.png?hl=zh-cn)

Live API 提供一整套全面的功能，例如[语音活动检测](https://ai.google.dev/gemini-api/docs/live-guide?hl=zh-cn#interruptions)、[工具使用和函数调用](https://ai.google.dev/gemini-api/docs/live-tools?hl=zh-cn)、[会话管理](https://ai.google.dev/gemini-api/docs/live-session?hl=zh-cn)（用于管理长时间运行的对话）和[短时性令牌](https://ai.google.dev/gemini-api/docs/ephemeral-tokens?hl=zh-cn)（用于安全的客户端身份验证）。

本页面提供了示例和基本代码示例，可帮助您快速上手使用。

## 示例应用

请查看以下示例应用，了解如何将 Live API 用于端到端用例：

*   AI Studio 上的[实时音频入门应用](https://aistudio.google.com/apps/bundled/live_audio?showPreview=true&%3BshowCode=true&%3BshowAssistant=false&hl=zh-cn)，使用 JavaScript 库连接到 Live API，并通过麦克风和扬声器流式传输双向音频。
*   使用连接到 Live API 的 Pyaudio 的 Live API [Python 食谱](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py)。

## 合作伙伴集成

如果您希望简化开发流程，可以使用 [Daily](https://www.daily.co/products/gemini/multimodal-live-api/) 或 [LiveKit](https://docs.livekit.io/agents/integrations/google/#multimodal-live-api)。这些第三方合作伙伴平台已通过 WebRTC 协议集成了 Gemini Live API，以简化实时音频和视频应用的开发。

## 开始构建之前

在开始使用 Live API 进行构建之前，您需要做出两项重要的决策：选择模型和选择实现方法。

### 选择音频生成架构

如果您要构建基于音频的用例，您选择的模型将决定用于创建音频响应的音频生成架构：

*   **[原生音频](https://ai.google.dev/gemini-api/docs/live-guide?hl=zh-cn#native-audio-output)**：此选项可提供最自然、最逼真的语音，并提供更好的多语言性能。它还支持[情感（情感感知）对话](https://ai.google.dev/gemini-api/docs/live-guide?hl=zh-cn#affective-dialog)、[主动音频](https://ai.google.dev/gemini-api/docs/live-guide?hl=zh-cn#proactive-audio)（模型可以决定忽略或回应特定输入）和[“思考”](https://ai.google.dev/gemini-api/docs/live-guide?hl=zh-cn#native-audio-output-thinking)等高级功能。以下[原生音频模型](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn#gemini-2.5-flash-native-audio)支持原生音频：
    *   `gemini-2.5-flash-preview-native-audio-dialog`
    *   `gemini-2.5-flash-exp-native-audio-thinking-dialog`
*   **半级联音频**：此选项使用级联模型架构（原生音频输入和文字转语音输出）。它在生产环境中可提供更好的性能和可靠性，尤其是在[使用工具](https://ai.google.dev/gemini-api/docs/live-tools?hl=zh-cn)时。以下型号支持半级联音频：
    *   `gemini-live-2.5-flash-preview`
    *   `gemini-2.0-flash-live-001`

### 选择实现方法

与 Live API 集成时，您需要选择以下实现方法之一：

*   **服务器到服务器**：您的后端使用 [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) 连接到 Live API。通常，客户端会将流式数据（音频、视频、文本）发送到服务器，然后服务器会将其转发到 Live API。
*   **客户端到服务器**：您的前端代码使用 [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) 直接连接到 Live API 以流式传输数据，而绕过后端。

## 开始使用

此示例**_读取 WAV 文件_**，以正确的格式发送该文件，并将收到的数据保存为 WAV 文件。

您可以将音频转换为 16 位 PCM、16kHz、单声道格式来发送音频，也可以通过将 `AUDIO` 设置为响应模式来接收音频。输出使用 24kHz 的采样率。

### Python

```
# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
# Install helpers for converting files: pip install librosa soundfile
import asyncio
import io
from pathlib import Path
import wave
from google import genai
from google.genai import types
import soundfile as sf
import librosa

client = genai.Client()

# Half cascade model:
# model = "gemini-live-2.5-flash-preview"

# Native audio output model:
model = "gemini-2.5-flash-preview-native-audio-dialog"

config = {
  "response_modalities": ["AUDIO"],
  "system_instruction": "You are a helpful assistant and answer in a friendly tone.",
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:

        buffer = io.BytesIO()
        y, sr = librosa.load("sample.wav", sr=16000)
        sf.write(buffer, y, sr, format='RAW', subtype='PCM_16')
        buffer.seek(0)
        audio_bytes = buffer.read()

        # If already in correct format, you can use this:
        # audio_bytes = Path("sample.pcm").read_bytes()

        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )

        wf = wave.open("audio.wav", "wb")
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(24000)  # Output is 24kHz

        async for response in session.receive():
            if response.data is not None:
                wf.writeframes(response.data)

            # Un-comment this code to print audio data info
            # if response.server_content.model_turn is not None:
            #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)

        wf.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### JavaScript

```
// Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav
import { GoogleGenAI, Modality } from '@google/genai';
import * as fs from "node:fs";
import pkg from 'wavefile';  // npm install wavefile
const { WaveFile } = pkg;

const ai = new GoogleGenAI({});
// WARNING: Do not use API keys in client-side (browser based) applications
// Consider using Ephemeral Tokens instead
// More information at: https://ai.google.dev/gemini-api/docs/ephemeral-tokens

// Half cascade model:
// const model = "gemini-live-2.5-flash-preview"

// Native audio output model:
const model = "gemini-2.5-flash-preview-native-audio-dialog"

const config = {
  responseModalities: [Modality.AUDIO], 
  systemInstruction: "You are a helpful assistant and answer in a friendly tone."
};

async function live() {
    const responseQueue = [];

    async function waitMessage() {
        let done = false;
        let message = undefined;
        while (!done) {
            message = responseQueue.shift();
            if (message) {
                done = true;
            } else {
                await new Promise((resolve) => setTimeout(resolve, 100));
            }
        }
        return message;
    }

    async function handleTurn() {
        const turns = [];
        let done = false;
        while (!done) {
            const message = await waitMessage();
            turns.push(message);
            if (message.serverContent && message.serverContent.turnComplete) {
                done = true;
            }
        }
        return turns;
    }

    const session = await ai.live.connect({
        model: model,
        callbacks: {
            onopen: function () {
                console.debug('Opened');
            },
            onmessage: function (message) {
                responseQueue.push(message);
            },
            onerror: function (e) {
                console.debug('Error:', e.message);
            },
            onclose: function (e) {
                console.debug('Close:', e.reason);
            },
        },
        config: config,
    });

    // Send Audio Chunk
    const fileBuffer = fs.readFileSync("sample.wav");

    // Ensure audio conforms to API requirements (16-bit PCM, 16kHz, mono)
    const wav = new WaveFile();
    wav.fromBuffer(fileBuffer);
    wav.toSampleRate(16000);
    wav.toBitDepth("16");
    const base64Audio = wav.toBase64();

    // If already in correct format, you can use this:
    // const fileBuffer = fs.readFileSync("sample.pcm");
    // const base64Audio = Buffer.from(fileBuffer).toString('base64');

    session.sendRealtimeInput(
        {
            audio: {
                data: base64Audio,
                mimeType: "audio/pcm;rate=16000"
            }
        }

    );

    const turns = await handleTurn();

    // Combine audio data strings and save as wave file
    const combinedAudio = turns.reduce((acc, turn) => {
        if (turn.data) {
            const buffer = Buffer.from(turn.data, 'base64');
            const intArray = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.byteLength / Int16Array.BYTES_PER_ELEMENT);
            return acc.concat(Array.from(intArray));
        }
        return acc;
    }, []);

    const audioBuffer = new Int16Array(combinedAudio);

    const wf = new WaveFile();
    wf.fromScratch(1, 24000, '16', audioBuffer);  // output is 24kHz
    fs.writeFileSync('audio.wav', wf.toBuffer());

    session.close();
}

async function main() {
    await live().catch((e) => console.error('got error', e));
}

main();
```

## 后续步骤

*   如需了解主要功能和配置（包括语音活动检测和原生音频功能），请参阅完整的 Live API [功能](https://ai.google.dev/gemini-api/docs/live-guide?hl=zh-cn)指南。
*   阅读[工具使用](https://ai.google.dev/gemini-api/docs/live-tools?hl=zh-cn)指南，了解如何将 Live API 与工具和函数调用集成。
*   如需管理长时间运行的对话，请参阅[会话管理](https://ai.google.dev/gemini-api/docs/live-session?hl=zh-cn)指南。
*   如需了解如何在[客户端到服务器](#implementation-approach)应用中进行安全身份验证，请参阅[暂时性令牌](https://ai.google.dev/gemini-api/docs/ephemeral-tokens?hl=zh-cn)指南。
*   如需详细了解底层 WebSockets API，请参阅 [WebSockets API 参考文档](https://ai.google.dev/api/live?hl=zh-cn)。

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-06-27。

---

# Content from: https://ai.google.dev/gemini-api/docs/safety-settings

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Safety settings

The Gemini API provides safety settings that you can adjust during the prototyping stage to determine if your application requires more or less restrictive safety configuration. You can adjust these settings across five filter categories to restrict or allow certain types of content.

This guide covers how the Gemini API handles safety settings and filtering and how you can change the safety settings for your application.

## Safety filters

The Gemini API's adjustable safety filters cover the following categories:

Category

Description

Harassment

Negative or harmful comments targeting identity and/or protected attributes.

Hate speech

Content that is rude, disrespectful, or profane.

Sexually explicit

Contains references to sexual acts or other lewd content.

Dangerous

Promotes, facilitates, or encourages harmful acts.

Civic integrity

Election-related queries.

You can use these filters to adjust what's appropriate for your use case. For example, if you're building video game dialogue, you may deem it acceptable to allow more content that's rated as _Dangerous_ due to the nature of the game.

In addition to the adjustable safety filters, the Gemini API has built-in protections against core harms, such as content that endangers child safety. These types of harm are always blocked and cannot be adjusted.

### Content safety filtering level

The Gemini API categorizes the probability level of content being unsafe as `HIGH`, `MEDIUM`, `LOW`, or `NEGLIGIBLE`.

The Gemini API blocks content based on the probability of content being unsafe and not the severity. This is important to consider because some content can have low probability of being unsafe even though the severity of harm could still be high. For example, comparing the sentences:

1.  The robot punched me.
2.  The robot slashed me up.

The first sentence might result in a higher probability of being unsafe, but you might consider the second sentence to be a higher severity in terms of violence. Given this, it is important that you carefully test and consider what the appropriate level of blocking is needed to support your key use cases while minimizing harm to end users.

### Safety filtering per request

You can adjust the safety settings for each request you make to the API. When you make a request, the content is analyzed and assigned a safety rating. The safety rating includes the category and the probability of the harm classification. For example, if the content was blocked due to the harassment category having a high probability, the safety rating returned would have category equal to `HARASSMENT` and harm probability set to `HIGH`.

By default, safety settings block content (including prompts) with medium or higher probability of being unsafe across any filter. This baseline safety is designed to work for most use cases, so you should only adjust your safety settings if it's consistently required for your application.

The following table describes the block settings you can adjust for each category. For example, if you set the block setting to **Block few** for the **Hate speech** category, everything that has a high probability of being hate speech content is blocked. But anything with a lower probability is allowed.

Threshold (Google AI Studio)

Threshold (API)

Description

Block none

`BLOCK_NONE`

Always show regardless of probability of unsafe content

Block few

`BLOCK_ONLY_HIGH`

Block when high probability of unsafe content

Block some

`BLOCK_MEDIUM_AND_ABOVE`

Block when medium or high probability of unsafe content

Block most

`BLOCK_LOW_AND_ABOVE`

Block when low, medium or high probability of unsafe content

N/A

`HARM_BLOCK_THRESHOLD_UNSPECIFIED`

Threshold is unspecified, block using default threshold

If the threshold is not set, the default block threshold is **Block none** (for `gemini-1.5-pro-002` and `gemini-1.5-flash-002` and all newer stable GA models) or **Block some** (in all other models) for all categories **except** the _Civic integrity_ category.

The default block threshold for the _Civic integrity_ category is **Block none** (for `gemini-2.0-flash-001` aliased as `gemini-2.0-flash`, `gemini-2.0-pro-exp-02-05`, and `gemini-2.0-flash-lite`) both for Google AI Studio and the Gemini API, and **Block most** for all other models in Google AI Studio only.

You can set these settings for each request you make to the generative service. See the [`HarmBlockThreshold`](/api/generate-content#harmblockthreshold) API reference for details.

### Safety feedback

[`generateContent`](/api/generate-content#method:-models.generatecontent) returns a [`GenerateContentResponse`](/api/generate-content#generatecontentresponse) which includes safety feedback.

Prompt feedback is included in [`promptFeedback`](/api/generate-content#promptfeedback). If `promptFeedback.blockReason` is set, then the content of the prompt was blocked.

Response candidate feedback is included in [`Candidate.finishReason`](/api/generate-content#candidate) and [`Candidate.safetyRatings`](/api/generate-content#candidate). If response content was blocked and the `finishReason` was `SAFETY`, you can inspect `safetyRatings` for more details. The content that was blocked is not returned.

## Adjust safety settings

This section covers how to adjust the safety settings in both Google AI Studio and in your code.

### Google AI Studio

You can adjust safety settings in Google AI Studio, but you cannot turn them off.

Click **Edit safety settings** in the **Run settings** panel to open the **Run safety settings** modal. In the modal, you can use the sliders to adjust the content filtering level per safety category:

![](/static/gemini-api/docs/images/safety_settings_ui.png)

When you send a request (for example, by asking the model a question), a warning **No Content** message appears if the request's content is blocked. To see more details, hold the pointer over the **No Content** text and click warning **Safety**.

### Gemini API SDKs

The following code snippet shows how to set safety settings in your `GenerateContent` call. This sets the thresholds for the harassment (`HARM_CATEGORY_HARASSMENT`) and hate speech (`HARM_CATEGORY_HATE_SPEECH`) categories. For example, setting these categories to `BLOCK_LOW_AND_ABOVE` blocks any content that has a low or higher probability of being harassment or hate speech. To understand the threshold settings, see [Safety filtering per request](#safety-filtering-per-request).

### Python

```
from google import genai
from google.genai import types

import PIL.Image

img = PIL.Image.open("cookies.jpg")

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=['Do these look store-bought or homemade?', img],
    config=types.GenerateContentConfig(
      safety_settings=[
        types.SafetySetting(
            category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
        ),
      ]
    )
)

print(response.text)
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        SafetySettings: []*genai.SafetySetting{
            {
                Category:  "HARM_CATEGORY_HATE_SPEECH",
                Threshold: "BLOCK_LOW_AND_ABOVE",
            },
        },
    }

    response, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.0-flash",
        genai.Text("Some potentially unsafe prompt."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(response.Text())
}
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const safetySettings = [
  {
    category: "HARM_CATEGORY_HARASSMENT",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
  {
    category: "HARM_CATEGORY_HATE_SPEECH",
    threshold: "BLOCK_LOW_AND_ABOVE",
  },
];

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Some potentially unsafe prompt.",
    config: {
      safetySettings: safetySettings,
    },
  });
  console.log(response.text);
}

await main();
```

### Dart (Flutter)

```
final safetySettings = [
  SafetySetting(HarmCategory.harassment, HarmBlockThreshold.low),
  SafetySetting(HarmCategory.hateSpeech, HarmBlockThreshold.low),
];
final model = GenerativeModel(
  model: 'gemini-1.5-flash',
  apiKey: apiKey,
  safetySettings: safetySettings,
);
```

### Kotlin

```
val harassmentSafety = SafetySetting(HarmCategory.HARASSMENT, BlockThreshold.LOW_AND_ABOVE)

val hateSpeechSafety = SafetySetting(HarmCategory.HATE_SPEECH, BlockThreshold.LOW_AND_ABOVE)

val generativeModel = GenerativeModel(
    modelName = "gemini-1.5-flash",
    apiKey = BuildConfig.apiKey,
    safetySettings = listOf(harassmentSafety, hateSpeechSafety)
)
```

### Java

```
SafetySetting harassmentSafety = new SafetySetting(HarmCategory.HARASSMENT,
    BlockThreshold.LOW_AND_ABOVE);

SafetySetting hateSpeechSafety = new SafetySetting(HarmCategory.HATE_SPEECH,
    BlockThreshold.LOW_AND_ABOVE);

GenerativeModel gm = new GenerativeModel(
    "gemini-1.5-flash",
    BuildConfig.apiKey,
    null, // generation config is optional
    Arrays.asList(harassmentSafety, hateSpeechSafety)
);

GenerativeModelFutures model = GenerativeModelFutures.from(gm);
```

### REST

```
echo '{    "safetySettings": [        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}    ],    "contents": [{        "parts":[{            "text": "'I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them.'"}]}]}' > request.json

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \        -H "x-goog-api-key: $GEMINI_API_KEY" \

    -H 'Content-Type: application/json' \
    -X POST \
    -d @request.json 2> /dev/null
```

## Next steps

*   See the [API reference](/api) to learn more about the full API.
*   Review the [safety guidance](/gemini-api/docs/safety-guidance) for a general look at safety considerations when developing with LLMs.
*   Learn more about assessing probability versus severity from the [Jigsaw team](https://developers.perspectiveapi.com/s/about-the-api-score)
*   Learn more about the products that contribute to safety solutions like the [Perspective API](https://medium.com/jigsaw/reducing-toxicity-in-large-language-models-with-perspective-api-c31c39b7a4d7). \* You can use these safety settings to create a toxicity classifier. See the [classification example](/examples/train_text_classifier_embeddings) to get started.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/safety-guidance

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Safety guidance

Generative artificial intelligence models are powerful tools, but they are not without their limitations. Their versatility and applicability can sometimes lead to unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing, and rigorous manual evaluation are essential to limit the risk of harm from such outputs.

The models provided by the Gemini API can be used for a wide variety of generative AI and natural language processing (NLP) applications. Use of these functions is only available through the Gemini API or the Google AI Studio web app. Your use of Gemini API is also subject to the [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy) and the [Gemini API terms of service](/terms).

Part of what makes large language models (LLMs) so useful is that they're creative tools that can address many different language tasks. Unfortunately, this also means that large language models can generate output that you don't expect, including text that's offensive, insensitive, or factually incorrect. What's more, the incredible versatility of these models is also what makes it difficult to predict exactly what kinds of undesirable output they might produce. While the Gemini API has been designed with [Google's AI principles](https://ai.google/principles/) in mind, the onus is on developers to apply these models responsibly. To aid developers in creating safe, responsible applications, the Gemini API has some built-in content filtering as well as adjustable safety settings across 4 dimensions of harm. Refer to the [safety settings](/gemini-api/docs/safety-settings) guide to learn more.

This document is meant to introduce you to some safety risks that can arise when using LLMs, and recommend emerging safety design and development recommendations. (Note that laws and regulations may also impose restrictions, but such considerations are beyond the scope of this guide.)

The following steps are recommended when building applications with LLMs:

*   Understanding the safety risks of your application
*   Considering adjustments to mitigate safety risks
*   Performing safety testing appropriate to your use case
*   Soliciting feedback from users and monitoring usage

The adjustment and testing phases should be iterative until you reach performance appropriate for your application.

![Model implementation cycle](/static/gemini-api/docs/images/safety_diagram.png)

## Understand the safety risks of your application

In this context, safety is being defined as the ability of an LLM to avoid causing harm to its users, for example, by generating toxic language or content that promotes stereotypes. The models available through the Gemini API have been designed with [Google’s AI principles](https://ai.google/principles/) in mind and your use of it is subject to the [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy). The API provides built-in safety filters to help address some common language model problems such as toxic language and hate speech, and striving for inclusiveness and avoidance of stereotypes. However, each application can pose a different set of risks to its users. So as the application owner, you are responsible for knowing your users and the potential harms your application may cause, and ensuring that your application uses LLMs safely and responsibly.

As part of this assessment, you should consider the likelihood that harm could occur and determine its seriousness and mitigation steps. For example, an app that generates essays based on factual events would need to be more careful about avoiding misinformation, as compared to an app that generates fictional stories for entertainment. A good way to begin exploring potential safety risks is to research your end users, and others who might be affected by your application's results. This can take many forms including researching state of the art studies in your app domain, observing how people are using similar apps, or running a user study, survey, or conducting informal interviews with potential users.

#### Advanced tips

*   Speak with a diverse mix of prospective users within your target population about your application and its intended purpose so as to get a wider perspective on potential risks and to adjust diversity criteria as needed.
*   The [AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) released by the U.S. government's National Institute of Standards and Technology (NIST) provides more detailed guidance and additional learning resources for AI risk management.
*   DeepMind's publication on the [ethical and social risks of harm from language models](https://arxiv.org/abs/2112.04359) describes in detail the ways that language model applications can cause harm.

## Consider adjustments to mitigate safety risks

Now that you have an understanding of the risks, you can decide how to mitigate them. Determining which risks to prioritize and how much you should do to try to prevent them is a critical decision, similar to triaging bugs in a software project. Once you've determined priorities, you can start thinking about the types of mitigations that would be most appropriate. Often simple changes can make a difference and reduce risks.

For example, when designing an application consider:

*   **Tuning the model output** to better reflect what is acceptable in your application context. Tuning can make the output of the model more predictable and consistent and therefore can help mitigate certain risks.
*   **Providing an input method that facilities safer outputs.** The exact input you give to an LLM can make a difference in the quality of the output. Experimenting with input prompts to find what works most safely in your use-case is well worth the effort, as you can then provide a UX that facilitates it. For example, you could restrict users to choose only from a drop-down list of input prompts, or offer pop-up suggestions with descriptive phrases which you've found perform safely in your application context.
*   **Blocking unsafe inputs and filtering output before it is shown to the user.** In simple situations, blocklists can be used to identify and block unsafe words or phrases in prompts or responses, or require human reviewers to manually alter or block such content.
    
*   **Using trained classifiers to label each prompt with potential harms or adversarial signals.** Different strategies can then be employed on how to handle the request based on the type of harm detected. For example, If the input is overtly adversarial or abusive in nature, it could be blocked and instead output a pre-scripted response.
    
    #### Advanced tip
    
    *   If signals determine the output to be harmful, the application can employ the following options:
        *   Provide an error message or pre-scripted output.
        *   Try the prompt again, in case an alternative safe output is generated, since sometimes the same prompt will elicit different outputs.
    
*   **Putting safeguards in place against deliberate misuse** such as assigning each user a unique ID and imposing a limit on the volume of user queries that can be submitted in a given period. Another safeguard is to try and protect against possible prompt injection. Prompt injection, much like SQL injection, is a way for malicious users to design an input prompt that manipulates the output of the model, for example, by sending an input prompt that instructs the model to ignore any previous examples. See the [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy) for details about deliberate misuse.
    
*   **Adjusting functionality to something that is inherently lower risk.** Tasks that are narrower in scope (e.g., extracting keywords from passages of text) or that have greater human oversight (e.g., generating short-form content that will be reviewed by a human), often pose a lower risk. So for instance, instead of creating an application to write an email reply from scratch, you might instead limit it to expanding on an outline or suggesting alternative phrasings.
    

## Perform safety testing appropriate to your use case

Testing is a key part of building robust and safe applications, but the extent, scope and strategies for testing will vary. For example, a just-for-fun haiku generator is likely to pose less severe risks than, say, an application designed for use by law firms to summarize legal documents and help draft contracts. But the haiku generator may be used by a wider variety of users which means the potential for adversarial attempts or even unintended harmful inputs can be greater. The implementation context also matters. For instance, an application with outputs that are reviewed by human experts prior to any action being taken might be deemed less likely to produce harmful outputs than the identical application without such oversight.

It's not uncommon to go through several iterations of making changes and testing before feeling confident that you're ready to launch, even for applications that are relatively low risk. Two kinds of testing are particularly useful for AI applications:

*   **Safety benchmarking** involves designing safety metrics that reflect the ways your application could be unsafe in the context of how it is likely to get used, then testing how well your application performs on the metrics using evaluation datasets. It's good practice to think about the minimum acceptable levels of safety metrics before testing so that 1) you can evaluate the test results against those expectations and 2) you can gather the evaluation dataset based on the tests that evaluate the metrics you care about most.
    
    #### Advanced tips
    
    *   Beware of over-relying on “off the shelf” approaches as it's likely you'll need to build your own testing datasets using human raters to fully suit your application's context.
    *   If you have more than one metric you'll need to decide how you'll trade off if a change leads to improvements for one metric to the detriment of another. Like with other performance engineering, you may want to focus on worst-case performance across your evaluation set rather than average performance.
    
*   **Adversarial testing** involves proactively trying to break your application. The goal is to identify points of weakness so that you can take steps to remedy them as appropriate. Adversarial testing can take significant time/effort from evaluators with expertise in your application — but the more you do, the greater your chance of spotting problems, especially those occurring rarely or only after repeated runs of the application.
    
    *   Adversarial testing is a method for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input:
        *   An input may be malicious when the input is clearly designed to produce an unsafe or harmful output-- for example, asking a text generation model to generate a hateful rant about a particular religion.
        *   An input is inadvertently harmful when the input itself may be innocuous, but produces harmful output -- for example, asking a text generation model to describe a person of a particular ethnicity and receiving a racist output.
    *   What distinguishes an adversarial test from a standard evaluation is the composition of the data used for testing. For adversarial tests, select test data that is most likely to elicit problematic output from the model. This means probing the model's behavior for all the types of harms that are possible, including rare or unusual examples and edge-cases that are relevant to safety policies. It should also include diversity in the different dimensions of a sentence such as structure, meaning and length. You can refer to the [Google's Responsible AI practices in fairness](https://ai.google/responsibilities/responsible-ai-practices/?category=fairness) for more details on what to consider when building a test dataset.
        
        #### Advanced tips
        
        *   Use [automated testing](https://www.deepmind.com/blog/red-teaming-language-models-with-language-models) instead of the traditional method of enlisting people in 'red teams' to try and break your application. In automated testing, the 'red team' is another language model that finds input text that elicit harmful outputs from the model being tested.
        

## Monitor for problems

No matter how much you test and mitigate, you can never guarantee perfection, so plan upfront how you'll spot and deal with problems that arise. Common approaches include setting up a monitored channel for users to share feedback (e.g., thumbs up/down rating) and running a user study to proactively solicit feedback from a diverse mix of users — especially valuable if usage patterns are different to expectations.

#### Advanced tips

*   When users give feedback to AI products, it can greatly improve the AI performance and the user experience over time by, for example, helping you choose better examples for prompt tuning. The [Feedback and Control chapter](https://pair.withgoogle.com/chapter/feedback-controls/) in [Google's People and AI guidebook](https://pair.withgoogle.com/guidebook/chapters) highlights key considerations to take into account when designing feedback mechanisms.

## Next steps

*   Refer to the [safety settings](/gemini-api/docs/safety-settings) guide to learn about the adjustable safety settings available through the Gemini API.
*   See the [intro to prompting](/gemini-api/docs/prompting-intro) to get started writing your first prompts.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-02-25 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/migrate

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Upgrade to the Google Gen AI SDK

When we introduced the Gemini 2.0 model family, we also released a new set of Google Gen AI libraries for working with the Gemini API:

*   [Python](https://pypi.org/project/google-genai/)
*   [TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai)
*   [Go](https://pkg.go.dev/google.golang.org/genai)

These updated libraries will be fully compatible with all Gemini API models and features, including recent additions like the [Live API](/gemini-api/docs/live) and [Veo](/gemini-api/docs/video).

We recommend that you start migrating your projects from the old Gemini SDK to the new Gen AI SDK. This guide provides before-and-after examples of migrated code to help you get started. We'll continue to add examples here to help you get up and running with the new libraries.

## Install the SDK

**Before**

### Python

```
pip install -U -q "google-generativeai"
```

### JavaScript

```
npm install @google/generative-ai
```

### Go

```
go get github.com/google/generative-ai-go
```

**After**

### Python

```
pip install -U -q "google-genai"
```

### JavaScript

```
npm install @google/genai
```

### Go

```
go get google.golang.org/genai
```

## Authenticate

Authenticate using an API key. You can [create](https://aistudio.google.com/app/apikey) your API key in Google AI Studio.

**Before**

### Python

The old SDK handled the API client object implicitly. In the new SDK you create the API client and use it to call the API. Remember, in either case the SDK will pick up your API key from the `GOOGLE_API_KEY` environment variable if you don't pass one to the client.

```
import google.generativeai as genai

genai.configure(api_key=...)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
```

### Go

Import the Google libraries:

```
import (
      "github.com/google/generative-ai-go/genai"
      "google.golang.org/api/option"
)
```

Create the client:

```
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
```

**After**

### Python

```
export GOOGLE_API_KEY="YOUR_API_KEY"
```

```
from google import genai

client = genai.Client() # Set the API key using the GOOGLE_API_KEY env var.
                        # Alternatively, you could set the API key explicitly:
                        # client = genai.Client(api_key="your_api_key")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({apiKey: "GOOGLE_API_KEY"});
```

### Go

Import the Gen AI library:

```
import "google.golang.org/genai"
```

Create the client:

```
client, err := genai.NewClient(ctx, &genai.ClientConfig{
        Backend:  genai.BackendGeminiAPI,
})
```

## Generate content

**Before**

### Python

The new SDK provides access to all the API methods through the `Client` object. Except for a few stateful special cases (`chat` and live-api `session`s), these are all stateless functions. For utility and uniformity, objects returned are `pydantic` classes.

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'Tell me a story in 300 words'
)
print(response.text)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const prompt = "Tell me a story in 300 words";

const result = await model.generateContent(prompt);
console.log(result.response.text());
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
resp, err := model.GenerateContent(ctx, genai.Text("Tell me a story in 300 words."))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response parts
```

**After**

### Python

```
from google import genai
client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
print(response.text)

print(response.model_dump_json(
    exclude_none=True, indent=4))
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story in 300 words.",
});
console.log(response.text);
```

### Go

```
ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me a story in 300 words."), nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

**Before**

### Python

Many of the same convenience features exist in the new SDK. For example, `PIL.Image` objects are automatically converted.

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Tell me a story based on this image',
    Image.open(image_path)
])
print(response.text)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

function fileToGenerativePart(path, mimeType) {
  return {
    inlineData: {
      data: Buffer.from(fs.readFileSync(path)).toString("base64"),
      mimeType,
    },
  };
}

const prompt = "Tell me a story based on this image";

const imagePart = fileToGenerativePart(
  `path/to/organ.jpg`,
  "image/jpeg",
);

const result = await model.generateContent([prompt, imagePart]);
console.log(result.response.text());
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

resp, err := model.GenerateContent(ctx,
    genai.Text("Tell me about this instrument"),
    genai.ImageData("jpeg", imgData))
if err != nil {
    log.Fatal(err)
}

printResponse(resp) // utility for printing response
```

**After**

### Python

```
from google import genai
from PIL import Image

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Tell me a story based on this image',
        Image.open(image_path)
    ]
)
print(response.text)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const organ = await ai.files.upload({
  file: "path/to/organ.jpg",
});

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: [
    createUserContent([
      "Tell me a story based on this image",
      createPartFromUri(organ.uri, organ.mimeType)
    ]),
  ],
});
console.log(response.text);
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

imgData, err := os.ReadFile("path/to/organ.jpg")
if err != nil {
    log.Fatal(err)
}

parts := []*genai.Part{
    {Text: "Tell me a story based on this image"},
    {InlineData: &genai.Blob{Data: imgData, MIMEType: "image/jpeg"}},
}
contents := []*genai.Content{
    {Parts: parts},
}

result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

### Streaming

**Before**

### Python

```
import google.generativeai as genai

response = model.generate_content(
    "Write a cute story about cats.",
    stream=True)
for chunk in response:
    print(chunk.text)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

const prompt = "Write a story about a magic backpack.";

const result = await model.generateContentStream(prompt);

// Print text as it comes in.
for await (const chunk of result.stream) {
  const chunkText = chunk.text();
  process.stdout.write(chunkText);
}
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
iter := model.GenerateContentStream(ctx, genai.Text("Write a story about a magic backpack."))
for {
    resp, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    printResponse(resp) // utility for printing the response
}
```

**After**

### Python

```
from google import genai

client = genai.Client()

for chunk in client.models.generate_content_stream(
  model='gemini-2.0-flash',
  contents='Tell me a story in 300 words.'
):
    print(chunk.text)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContentStream({
  model: "gemini-2.0-flash",
  contents: "Write a story about a magic backpack.",
});
let text = "";
for await (const chunk of response) {
  console.log(chunk.text);
  text += chunk.text;
}
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

for result, err := range client.Models.GenerateContentStream(
    ctx,
    "gemini-2.0-flash",
    genai.Text("Write a story about a magic backpack."),
    nil,
) {
    if err != nil {
        log.Fatal(err)
    }
    fmt.Print(result.Candidates[0].Content.Parts[0].Text)
}
```

## Configuration

**Before**

### Python

For all methods in the new SDK, the required arguments are provided as keyword arguments. All optional inputs are provided in the `config` argument. Config arguments can be specified as either Python dictionaries or `Config` classes in the `google.genai.types` namespace. For utility and uniformity, all definitions within the `types` module are `pydantic` classes.

```
import google.generativeai as genai

model = genai.GenerativeModel(
  'gemini-1.5-flash',
    system_instruction='you are a story teller for kids under 5 years old',
    generation_config=genai.GenerationConfig(
      max_output_tokens=400,
      top_k=2,
      top_p=0.5,
      temperature=0.5,
      response_mime_type='application/json',
      stop_sequences=['\n'],
    )
)
response = model.generate_content('tell me a story in 100 words')
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  generationConfig: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

const result = await model.generateContent(
  "Tell me a story about a magic backpack.",
);
console.log(result.response.text())
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
model.SetTemperature(0.5)
model.SetTopP(0.5)
model.SetTopK(2.0)
model.SetMaxOutputTokens(100)
model.ResponseMIMEType = "application/json"
resp, err := model.GenerateContent(ctx, genai.Text("Tell me about New York"))
if err != nil {
    log.Fatal(err)
}
printResponse(resp) // utility for printing response
```

**After**

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='Tell me a story in 100 words.',
  config=types.GenerateContentConfig(
      system_instruction='you are a story teller for kids under 5 years old',
      max_output_tokens= 400,
      top_k= 2,
      top_p= 0.5,
      temperature= 0.5,
      response_mime_type= 'application/json',
      stop_sequences= ['\n'],
      seed=42,
  ),
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "Tell me a story about a magic backpack.",
  config: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 20,
    temperature: 1.0,
  },
});

console.log(response.text);
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

result, err := client.Models.GenerateContent(ctx,
    "gemini-2.0-flash",
    genai.Text("Tell me about New York"),
    &genai.GenerateContentConfig{
        Temperature:      genai.Ptr[float32](0.5),
        TopP:             genai.Ptr[float32](0.5),
        TopK:             genai.Ptr[float32](2.0),
        ResponseMIMEType: "application/json",
        StopSequences:    []string{"Yankees"},
        CandidateCount:   2,
        Seed:             genai.Ptr[int32](42),
        MaxOutputTokens:  128,
        PresencePenalty:  genai.Ptr[float32](0.5),
        FrequencyPenalty: genai.Ptr[float32](0.5),
    },
)
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing response
```

## Safety settings

Generate a response with safety settings:

**Before**

### Python

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    'say something bad',
    safety_settings={
        'HATE': 'BLOCK_ONLY_HIGH',
        'HARASSMENT': 'BLOCK_ONLY_HIGH',
  }
)
```

### JavaScript

```
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  safetySettings: [
    {
      category: HarmCategory.HARM_CATEGORY_HARASSMENT,
      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    },
  ],
});

const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const result = await model.generateContent(unsafePrompt);

try {
  result.response.text();
} catch (e) {
  console.error(e);
  console.log(result.response.candidates[0].safetyRatings);
}
```

**After**

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents='say something bad',
  config=types.GenerateContentConfig(
      safety_settings= [
          types.SafetySetting(
              category='HARM_CATEGORY_HATE_SPEECH',
              threshold='BLOCK_ONLY_HIGH'
          ),
      ]
  ),
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const unsafePrompt =
  "I support Martians Soccer Club and I think " +
  "Jupiterians Football Club sucks! Write an ironic phrase telling " +
  "them how I feel about them.";

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: unsafePrompt,
  config: {
    safetySettings: [
      {
        category: "HARM_CATEGORY_HARASSMENT",
        threshold: "BLOCK_ONLY_HIGH",
      },
    ],
  },
});

console.log("Finish reason:", response.candidates[0].finishReason);
console.log("Safety ratings:", response.candidates[0].safetyRatings);
```

## Async

**Before**

### Python

To use the new SDK with `asyncio`, there is a separate `async` implementation of every method under `client.aio`.

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content_async(
    'tell me a story in 100 words'
)
```

**After**

### Python

```
from google import genai

client = genai.Client()

response = await client.aio.models.generate_content(
    model='gemini-2.0-flash',
    contents='Tell me a story in 300 words.'
)
```

## Chat

Start a chat and send a message to the model:

**Before**

### Python

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
chat = model.start_chat()

response = chat.send_message(
    "Tell me a story in 100 words")
response = chat.send_message(
    "What happened after that?")
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
const chat = model.startChat({
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});
let result = await chat.sendMessage("I have 2 dogs in my house.");
console.log(result.response.text());
result = await chat.sendMessage("How many paws are in my house?");
console.log(result.response.text());
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, option.WithAPIKey("GOOGLE_API_KEY"))
if err != nil {
    log.Fatal(err)
}
defer client.Close()

model := client.GenerativeModel("gemini-1.5-flash")
cs := model.StartChat()

cs.History = []*genai.Content{
    {
        Parts: []genai.Part{
            genai.Text("Hello, I have 2 dogs in my house."),
        },
        Role: "user",
    },
    {
        Parts: []genai.Part{
            genai.Text("Great to meet you. What would you like to know?"),
        },
        Role: "model",
    },
}

res, err := cs.SendMessage(ctx, genai.Text("How many paws are in my house?"))
if err != nil {
    log.Fatal(err)
}
printResponse(res) // utility for printing the response
```

**After**

### Python

```
from google import genai

client = genai.Client()

chat = client.chats.create(model='gemini-2.0-flash')

response = chat.send_message(
    message='Tell me a story in 100 words')
response = chat.send_message(
    message='What happened after that?')
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const chat = ai.chats.create({
  model: "gemini-2.0-flash",
  history: [
    {
      role: "user",
      parts: [{ text: "Hello" }],
    },
    {
      role: "model",
      parts: [{ text: "Great to meet you. What would you like to know?" }],
    },
  ],
});

const response1 = await chat.sendMessage({
  message: "I have 2 dogs in my house.",
});
console.log("Chat response 1:", response1.text);

const response2 = await chat.sendMessage({
  message: "How many paws are in my house?",
});
console.log("Chat response 2:", response2.text);
```

### Go

```
ctx := context.Background()
client, err := genai.NewClient(ctx, nil)
if err != nil {
    log.Fatal(err)
}

chat, err := client.Chats.Create(ctx, "gemini-2.0-flash", nil, nil)
if err != nil {
    log.Fatal(err)
}

result, err := chat.SendMessage(ctx, genai.Part{Text: "Hello, I have 2 dogs in my house."})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result

result, err = chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})
if err != nil {
    log.Fatal(err)
}
debugPrint(result) // utility for printing result
```

## Function calling

**Before**

### Python

In the new SDK, automatic function calling is the default. Here, you disable it.

```
import google.generativeai as genai
from enum import Enum

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

response = model.generate_content("What is the weather in San Francisco?")
function_call = response.candidates[0].parts[0].function_call
```

**After**

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

def get_current_weather(location: str) -> str:
    """Get the current whether in a given location.

    Args:
        location: required, The city and state, e.g. San Franciso, CA
        unit: celsius or fahrenheit
    """
    print(f'Called with: {location=}')
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather],
      automatic_function_calling={'disable': True},
  ),
)

function_call = response.candidates[0].content.parts[0].function_call
```

### Automatic function calling

**Before**

### Python

The old SDK only supports automatic function calling in chat. In the new SDK this is the default behavior in `generate_content`.

```
import google.generativeai as genai

def get_current_weather(city: str) -> str:
    return "23C"

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools=[get_current_weather]
)

chat = model.start_chat(
    enable_automatic_function_calling=True)
result = chat.send_message("What is the weather in San Francisco?")
```

**After**

### Python

```
from google import genai
from google.genai import types
client = genai.Client()

def get_current_weather(city: str) -> str:
    return "23C"

response = client.models.generate_content(
  model='gemini-2.0-flash',
  contents="What is the weather like in Boston?",
  config=types.GenerateContentConfig(
      tools=[get_current_weather]
  ),
)
```

## Code execution

Code execution is a tool that allows the model to generate Python code, run it, and return the result.

**Before**

### Python

```
import google.generativeai as genai

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    tools="code_execution"
)

result = model.generate_content(
  "What is the sum of the first 50 prime numbers? Generate and run code for "
  "the calculation, and make sure you get all 50.")
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "gemini-1.5-flash",
  tools: [{ codeExecution: {} }],
});

const result = await model.generateContent(
  "What is the sum of the first 50 prime numbers? " +
    "Generate and run code for the calculation, and make sure you get " +
    "all 50.",
);

console.log(result.response.text());
```

**After**

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the sum of the first 50 prime numbers? Generate and run '
            'code for the calculation, and make sure you get all 50.',
    config=types.GenerateContentConfig(
        tools=[types.Tool(code_execution=types.ToolCodeExecution)],
    ),
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

const response = await ai.models.generateContent({
  model: "gemini-2.0-pro-exp-02-05",
  contents: `Write and execute code that calculates the sum of the first 50 prime numbers.
            Ensure that only the executable code and its resulting output are generated.`,
});

// Each part may contain text, executable code, or an execution result.
for (const part of response.candidates[0].content.parts) {
  console.log(part);
  console.log("\n");
}

console.log("-".repeat(80));
// The `.text` accessor concatenates the parts into a markdown-formatted text.
console.log("\n", response.text);
```

## Search grounding

`GoogleSearch` (Gemini>=2.0) and `GoogleSearchRetrieval` (Gemini < 2.0) are tools that allow the model to retrieve public web data for grounding, powered by Google.

**Before**

### Python

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(
    contents="what is the Google stock price?",
    tools='google_search_retrieval'
)
```

**After**

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='What is the Google stock price?',
    config=types.GenerateContentConfig(
        tools=[
            types.Tool(
                google_search=types.GoogleSearch()
            )
        ]
    )
)
```

## JSON response

Generate answers in JSON format.

**Before**

### Python

By specifying a `response_schema` and setting `response_mime_type="application/json"` users can constrain the model to produce a `JSON` response following a given structure. The new SDK uses `pydantic` classes to provide the schema (although you can pass a `genai.types.Schema`, or equivalent `dict`). When possible, the SDK will parse the returned JSON, and return the result in `response.parsed`. If you provided a `pydantic` class as the schema the SDK will convert that `JSON` to an instance of the class.

```
import google.generativeai as genai
import typing_extensions as typing

class CountryInfo(typing.TypedDict):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

model = genai.GenerativeModel(model_name="gemini-1.5-flash")
result = model.generate_content(
    "Give me information of the United States",
    generation_config=genai.GenerationConfig(
        response_mime_type="application/json",
        response_schema = CountryInfo
    ),
)
```

### JavaScript

```
import { GoogleGenerativeAI, SchemaType } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");

const schema = {
  description: "List of recipes",
  type: SchemaType.ARRAY,
  items: {
    type: SchemaType.OBJECT,
    properties: {
      recipeName: {
        type: SchemaType.STRING,
        description: "Name of the recipe",
        nullable: false,
      },
    },
    required: ["recipeName"],
  },
};

const model = genAI.getGenerativeModel({
  model: "gemini-1.5-pro",
  generationConfig: {
    responseMimeType: "application/json",
    responseSchema: schema,
  },
});

const result = await model.generateContent(
  "List a few popular cookie recipes.",
);
console.log(result.response.text());
```

**After**

### Python

```
from google import genai
from pydantic import BaseModel

client = genai.Client()

class CountryInfo(BaseModel):
    name: str
    population: int
    capital: str
    continent: str
    major_cities: list[str]
    gdp: int
    official_language: str
    total_area_sq_mi: int

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Give me information of the United States.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': CountryInfo,
    },
)

response.parsed
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: "List a few popular cookie recipes.",
  config: {
    responseMimeType: "application/json",
    responseSchema: {
      type: "array",
      items: {
        type: "object",
        properties: {
          recipeName: { type: "string" },
          ingredients: { type: "array", items: { type: "string" } },
        },
        required: ["recipeName", "ingredients"],
      },
    },
  },
});
console.log(response.text);
```

## Files

### Upload

Upload a file:

**Before**

### Python

```
import requests
import pathlib
import google.generativeai as genai

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

file = genai.upload_file(path='a11.txt')

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content([
    'Can you summarize this file:',
    my_file
])
print(response.text)
```

**After**

### Python

```
import requests
import pathlib
from google import genai

client = genai.Client()

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

my_file = client.files.upload(file='a11.txt')

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=[
        'Can you summarize this file:',
        my_file
    ]
)
print(response.text)
```

### List and get

List uploaded files and get an uploaded file with a filename:

**Before**

### Python

```
import google.generativeai as genai

for file in genai.list_files():
  print(file.name)

file = genai.get_file(name=file.name)
```

**After**

### Python

```
from google import genai
client = genai.Client()

for file in client.files.list():
    print(file.name)

file = client.files.get(name=file.name)
```

### Delete

Delete a file:

**Before**

### Python

```
import pathlib
import google.generativeai as genai

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = genai.upload_file(path='dummy.txt')

file = genai.delete_file(name=dummy_file.name)
```

**After**

### Python

```
import pathlib
from google import genai

client = genai.Client()

pathlib.Path('dummy.txt').write_text(dummy)
dummy_file = client.files.upload(file='dummy.txt')

response = client.files.delete(name=dummy_file.name)
```

## Context caching

Context caching allows the user to pass the content to the model once, cache the input tokens, and then refer to the cached tokens in subsequent calls to lower the cost.

**Before**

### Python

```
import requests
import pathlib
import google.generativeai as genai
from google.generativeai import caching

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = genai.upload_file(path="a11.txt")

# Create cache
apollo_cache = caching.CachedContent.create(
    model="gemini-1.5-flash-001",
    system_instruction="You are an expert at analyzing transcripts.",
    contents=[document],
)

# Generate response
apollo_model = genai.GenerativeModel.from_cached_content(
    cached_content=apollo_cache
)
response = apollo_model.generate_content("Find a lighthearted moment from this transcript")
```

### JavaScript

```
import { GoogleAICacheManager, GoogleAIFileManager } from "@google/generative-ai/server";
import { GoogleGenerativeAI } from "@google/generative-ai";

const cacheManager = new GoogleAICacheManager("GOOGLE_API_KEY");
const fileManager = new GoogleAIFileManager("GOOGLE_API_KEY");

const uploadResult = await fileManager.uploadFile("path/to/a11.txt", {
  mimeType: "text/plain",
});

const cacheResult = await cacheManager.create({
  model: "models/gemini-1.5-flash",
  contents: [
    {
      role: "user",
      parts: [
        {
          fileData: {
            fileUri: uploadResult.file.uri,
            mimeType: uploadResult.file.mimeType,
          },
        },
      ],
    },
  ],
});

console.log(cacheResult);

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModelFromCachedContent(cacheResult);
const result = await model.generateContent(
  "Please summarize this transcript.",
);
console.log(result.response.text());
```

**After**

### Python

```
import requests
import pathlib
from google import genai
from google.genai import types

client = genai.Client()

# Check which models support caching.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createCachedContent":
      print(m.name)
      break

# Download file
response = requests.get(
    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')
pathlib.Path('a11.txt').write_text(response.text)

# Upload file
document = client.files.upload(file='a11.txt')

# Create cache
model='gemini-1.5-flash-001'
apollo_cache = client.caches.create(
      model=model,
      config={
          'contents': [document],
          'system_instruction': 'You are an expert at analyzing transcripts.',
      },
  )

# Generate response
response = client.models.generate_content(
    model=model,
    contents='Find a lighthearted moment from this transcript',
    config=types.GenerateContentConfig(
        cached_content=apollo_cache.name,
    )
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const filePath = path.join(media, "a11.txt");
const document = await ai.files.upload({
  file: filePath,
  config: { mimeType: "text/plain" },
});
console.log("Uploaded file name:", document.name);
const modelName = "gemini-1.5-flash";

const contents = [
  createUserContent(createPartFromUri(document.uri, document.mimeType)),
];

const cache = await ai.caches.create({
  model: modelName,
  config: {
    contents: contents,
    systemInstruction: "You are an expert analyzing transcripts.",
  },
});
console.log("Cache created:", cache);

const response = await ai.models.generateContent({
  model: modelName,
  contents: "Please summarize this transcript",
  config: { cachedContent: cache.name },
});
console.log("Response text:", response.text);
```

## Count tokens

Count the number of tokens in a request.

**Before**

### Python

```
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.count_tokens(
    'The quick brown fox jumps over the lazy dog.')
```

### JavaScript

 ```
 import { GoogleGenerativeAI } from "@google/generative-ai";

 const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY+);
 const model = genAI.getGenerativeModel({
   model: "gemini-1.5-flash",
 });

 // Count tokens in a prompt without calling text generation.
 const countResult = await model.countTokens(
   "The quick brown fox jumps over the lazy dog.",
 );

 console.log(countResult.totalTokens); // 11

 const generateResult = await model.generateContent(
   "The quick brown fox jumps over the lazy dog.",
 );

 // On the response for `generateContent`, use `usageMetadata`
 // to get separate input and output token counts
 // (`promptTokenCount` and `candidatesTokenCount`, respectively),
 // as well as the combined token count (`totalTokenCount`).
 console.log(generateResult.response.usageMetadata);
 // candidatesTokenCount and totalTokenCount depend on response, may vary
 // { promptTokenCount: 11, candidatesTokenCount: 124, totalTokenCount: 135 }
```

**After**

### Python

```
from google import genai

client = genai.Client()

response = client.models.count_tokens(
    model='gemini-2.0-flash',
    contents='The quick brown fox jumps over the lazy dog.',
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const prompt = "The quick brown fox jumps over the lazy dog.";
const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(generateResponse.usageMetadata);
```

## Generate images

Generate images:

**Before**

### Python

```
#pip install https://github.com/google-gemini/generative-ai-python@imagen
import google.generativeai as genai

imagen = genai.ImageGenerationModel(
    "imagen-3.0-generate-001")
gen_images = imagen.generate_images(
    prompt="Robot holding a red skateboard",
    number_of_images=1,
    safety_filter_level="block_low_and_above",
    person_generation="allow_adult",
    aspect_ratio="3:4",
)
```

**After**

### Python

```
from google import genai

client = genai.Client()

gen_images = client.models.generate_images(
    model='imagen-3.0-generate-001',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 1,
        safety_filter_level= "BLOCK_LOW_AND_ABOVE",
        person_generation= "ALLOW_ADULT",
        aspect_ratio= "3:4",
    )
)

for n, image in enumerate(gen_images.generated_images):
    pathlib.Path(f'{n}.png').write_bytes(
        image.image.image_bytes)
```

## Embed content

Generate content embeddings.

**Before**

### Python

```
import google.generativeai as genai

response = genai.embed_content(
  model='models/text-embedding-004',
  content='Hello world'
)
```

### JavaScript

```
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("GOOGLE_API_KEY");
const model = genAI.getGenerativeModel({
  model: "text-embedding-004",
});

const result = await model.embedContent("Hello world!");

console.log(result.embedding);
```

**After**

### Python

```
from google import genai

client = genai.Client()

response = client.models.embed_content(
  model='text-embedding-004',
  contents='Hello world',
)
```

### JavaScript

```
import {GoogleGenAI} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const text = "Hello World!";
const result = await ai.models.embedContent({
  model: "text-embedding-004",
  contents: text,
  config: { outputDimensionality: 10 },
});
console.log(result.embeddings);
```

## Tune a Model

Create and use a tuned model.

The new SDK simplifies tuning with `client.tunings.tune`, which launches the tuning job and polls until the job is complete.

**Before**

### Python

```
import google.generativeai as genai
import random

# create tuning model
train_data = {}
for i in range(1, 6):
  key = f'input {i}'
  value = f'output {i}'
  train_data[key] = value

name = f'generate-num-{random.randint(0,10000)}'
operation = genai.create_tuned_model(
    source_model='models/gemini-1.5-flash-001-tuning',
    training_data=train_data,
    id = name,
    epoch_count = 5,
    batch_size=4,
    learning_rate=0.001,
)
# wait for tuning complete
tuningProgress = operation.result()

# generate content with the tuned model
model = genai.GenerativeModel(model_name=f'tunedModels/{name}')
response = model.generate_content('55')
```

**After**

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

# Check which models are available for tuning.
for m in client.models.list():
  for action in m.supported_actions:
    if action == "createTunedModel":
      print(m.name)
      break

# create tuning model
training_dataset=types.TuningDataset(
        examples=[
            types.TuningExample(
                text_input=f'input {i}',
                output=f'output {i}',
            )
            for i in range(5)
        ],
    )
tuning_job = client.tunings.tune(
    base_model='models/gemini-1.5-flash-001-tuning',
    training_dataset=training_dataset,
    config=types.CreateTuningJobConfig(
        epoch_count= 5,
        batch_size=4,
        learning_rate=0.001,
        tuned_model_display_name="test tuned model"
    )
)

# generate content with the tuned model
response = client.models.generate_content(
    model=tuning_job.tuned_model.model,
    contents='55',
)
```

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/troubleshooting

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Troubleshooting guide

Use this guide to help you diagnose and resolve common issues that arise when you call the Gemini API. You may encounter issues from either the Gemini API backend service or the client SDKs. Our client SDKs are open sourced in the following repositories:

*   [python-genai](https://github.com/googleapis/python-genai)
*   [js-genai](https://github.com/googleapis/js-genai)
*   [go-genai](https://github.com/googleapis/go-genai)

If you encounter API key issues, ensure you have set up your API key correctly per the [API key setup guide](/gemini-api/docs/api-key).

## Gemini API backend service error codes

The following table lists common backend error codes you may encounter, along with explanations for their causes and troubleshooting steps:

**HTTP Code**

**Status**

**Description**

**Example**

**Solution**

400

INVALID\_ARGUMENT

The request body is malformed.

There is a typo, or a missing required field in your request.

Check the [API reference](/api) for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors.

400

FAILED\_PRECONDITION

Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio.

You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio.

To use the Gemini API, you will need to setup a paid plan using [Google AI Studio](https://aistudio.google.com/app/apikey).

403

PERMISSION\_DENIED

Your API key doesn't have the required permissions.

You are using the wrong API key; you are trying to use a tuned model without going through [proper authentication](/docs/model-tuning/tutorial?lang=python#set_up_authentication).

Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models.

404

NOT\_FOUND

The requested resource wasn't found.

An image, audio, or video file referenced in your request was not found.

Check if all [parameters in your request are valid](/docs/troubleshooting#check-api) for your API version.

429

RESOURCE\_EXHAUSTED

You've exceeded the rate limit.

You are sending too many requests per minute with the free tier Gemini API.

Ensure you're within the model's [rate limit](/models/gemini#model-variations). [Request a quota increase](/docs/increase_quota) if needed.

500

INTERNAL

An unexpected error occurred on Google's side.

Your input context is too long.

Reduce your input context or temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.

503

UNAVAILABLE

The service may be temporarily overloaded or down.

The service is temporarily running out of capacity.

Temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.

504

DEADLINE\_EXCEEDED

The service is unable to finish processing within the deadline.

Your prompt (or context) is too large to be processed in time.

Set a larger 'timeout' in your client request to avoid this error.

## Check your API calls for model parameter errors

Ensure your model parameters are within the following values:

**Model parameter**

**Values (range)**

Candidate count

1-8 (integer)

Temperature

0.0-1.0

Max output tokens

Use `get_model` ([Python](/api/python/google/generativeai/get_model)) to determine the maximum number of tokens for the model you are using.

TopP

0.0-1.0

In addition to checking parameter values, make sure you're using the correct [API version](/gemini-api/docs/api-versions) (e.g., `/v1` or `/v1beta`) and model that supports the features you need. For example, if a feature is in Beta release, it will only be available in the `/v1beta` API version.

## Check if you have the right model

Ensure you are using a supported model listed on our [models page](/gemini-api/docs/models/gemini).

## Higher latency or token usage with 2.5 models

If you're observing higher latency or token usage with 2.5 Flash and Pro models, this can be because they come with **thinking on by default** in order to enhance quality. If you are prioritizing speed or need to minimize costs, you can adjust or disable thinking.

Please refer to [thinking page](/gemini-api/docs/thinking#set-budget) for guidance and sample code.

## Safety issues

If you see a prompt was blocked because of a safety setting in your API call, review the prompt with respect to the filters you set in the API call.

If you see `BlockedReason.OTHER`, the query or response may violate the [terms of service](/terms) or be otherwise unsupported.

## Recitation issue

If you see the model stops generating output due to the RECITATION reason, this means the model output may resemble certain data. To fix this, try to make prompt / context as unique as possible and use a higher temperature.

## Improve model output

For higher quality model outputs, explore writing more structured prompts. The [prompt engineering guide](/gemini-api/docs/prompting-strategies) page introduces some basic concepts, strategies, and best practices to get you started.

If you have hundreds of examples of good input/output pairs, you can also consider [model tuning](/gemini-api/docs/model-tuning).

## Understand token limits

Read through our [Token guide](/gemini-api/docs/tokens) to better understand how to count tokens and their limits.

## Known issues

*   The API supports only a number of select languages. Submitting prompts in unsupported languages can produce unexpected or even blocked responses. See [available languages](/gemini-api/docs/models#supported-languages) for updates.

## File a bug

Join the discussion on the [Google AI developer forum](https://discuss.ai.google.dev) if you have questions.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-17 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/changelog

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Release notes

This page documents updates to the Gemini API.

## June 26, 2025

*   The preview models `gemini-2.5-pro-preview-05-06` and `gemini-2.5-pro-preview-03-25` are now redirecting to the latest stable version `gemini-2.5-pro`.
    
*   `gemini-2.5-pro-exp-03-25` is deprecated.
    

## June 24, 2025

*   Released Imagen 4 Ultra and Standard Preview models. To learn more, see the [Image generation](/gemini-api/docs/image-generation) page.

## June 17, 2025

*   Released `gemini-2.5-pro`, the stable version of our most powerful model, now with adaptive thinking. To learn more, see [Gemini 2.5 Pro](/gemini-api/docs/models#gemini-2.5-pro) and [Thinking](/gemini-api/docs/thinking). `gemini-2.5-pro-preview-05-06` will be redirected to `gemini-2.5-pro` on June 26, 2025.
*   Released `gemini-2.5-flash`, our first stable 2.5 Flash model. To learn more, see [Gemini 2.5 Flash](/gemini-api/docs/models#gemini-2.5-flash). `gemini-2.5-flash-preview-04-17` will be deprecated on July 15, 2025.
*   Released `gemini-2.5-flash-lite-preview-06-17`, a low-cost, high-performance Gemini 2.5 model. To learn more, see [Gemini 2.5 Flash-Lite Preview](/gemini-api/docs/models#gemini-2.5-flash-lite).

## June 05, 2025

*   Released `gemini-2.5-pro-preview-06-05`, a new version of our most powerful model, now with adaptive thinking. To learn more, see [Gemini 2.5 Pro Preview](/gemini-api/docs/models#gemini-2.5-pro-preview-06-05) and [Thinking](/gemini-api/docs/thinking). `gemini-2.5-pro-preview-05-06` will be redirected to `gemini-2.5-pro` on June 26, 2025.

## May 20, 2025

**API updates:**

*   Launched support for [custom video preprocessing](/gemini-api/docs/video-understanding#customize-video-processing) using clipping intervals and configurable frame rate sampling.
*   Launched multi-tool use, which supports configuring [code execution](/gemini-api/docs/code-execution) and [Grounding with Google Search](/gemini-api/docs/grounding) on the same `generateContent` request.
*   Launched support for [asynchronous function calls](/gemini-api/docs/live-tools#async-function-calling) in the Live API.
*   Launched an experimental [URL context tool](/gemini-api/docs/url-context) for providing URLs as additional context to prompts.

**Model updates:**

*   Released `gemini-2.5-flash-preview-05-20`, a Gemini [preview](/gemini-api/docs/models#model-versions) model optimized for price-performance and adaptive thinking. To learn more, see [Gemini 2.5 Flash Preview](/gemini-api/docs/models#gemini-2.5-flash-preview) and [Thinking](/gemini-api/docs/thinking).
*   Released the [`gemini-2.5-pro-preview-tts`](/gemini-api/docs/models#gemini-2.5-pro-preview-tts) and [`gemini-2.5-flash-preview-tts`](/gemini-api/docs/models#gemini-2.5-flash-preview-tts) models, which are capable of [generating speech](/gemini-api/docs/speech-generation) with one or two speakers.
*   Released the `lyria-realtime-exp` model, which [generates music](/gemini-api/docs/music-generation) in real time.
*   Released `gemini-2.5-flash-preview-native-audio-dialog` and `gemini-2.5-flash-exp-native-audio-thinking-dialog`, new Gemini models for the Live API with native audio output capabilities. To learn more, see the [Live API guide](/gemini-api/docs/live-guide#native-audio-output) and [Gemini 2.5 Flash Native Audio](/gemini-api/docs/models#gemini-2.5-flash-native-audio).
*   Released `gemma-3n-e4b-it` preview, available on [AI Studio](https://aistudio.google.com) and through the Gemini API, as part of the [Gemma 3n](https://ai.google.dev/gemma/docs/3n) launch.

## May 7, 2025

*   Released `gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see [Image generation](https://ai.google.dev/gemini-api/docs/image-generation) and [Gemini 2.0 Flash Preview Image Generation](https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash-preview-image-generation).

## May 6, 2025

*   Released `gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling. `gemini-2.5-pro-preview-03-25` will automatically point to the new version of the model.

## April 17, 2025

*   Released `gemini-2.5-flash-preview-04-17`, a Gemini [preview](/gemini-api/docs/models#model-versions) model optimized for price-performance and adaptive thinking. To learn more, see [Gemini 2.5 Flash Preview](/gemini-api/docs/models#gemini-2.5-flash-preview) and [Thinking](/gemini-api/docs/thinking).

## April 16, 2025

*   Launched context caching for [Gemini 2.0 Flash](/gemini-api/docs/models#gemini-2.0-flash).

## April 9, 2025

**Model updates:**

*   Released `veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the [Veo docs](/gemini-api/docs/video).
*   Released `gemini-2.0-flash-live-001`, a public preview version of the [Live API](/gemini-api/docs/live) model with billing enabled.
    
    *   **Enhanced Session Management and Reliability**
        
        *   **Session Resumption:** Keep sessions alive across temporary network disruptions. The API now supports server-side session state storage (for up to 24 hours) and provides handles (session\_resumption) to reconnect and resume where you left off.
        *   **Longer Sessions via Context Compression:** Enable extended interactions beyond previous time limits. Configure context window compression with a sliding window mechanism to automatically manage context length, preventing abrupt terminations due to context limits.
        *   **Graceful Disconnect Notification:** Receive a `GoAway` server message indicating when a connection is about to close, allowing for graceful handling before termination.
    *   **More Control over Interaction Dynamics**
        
    *   **Configurable Voice Activity Detection (VAD):** Choose sensitivity levels or disable automatic VAD entirely and use new client events (`activityStart`, `activityEnd`) for manual turn control.
        
    *   **Configurable Interruption Handling:** Decide whether user input should interrupt the model's response.
        
    *   **Configurable Turn Coverage:** Choose whether the API processes all audio and video input continuously or only captures it when the end-user is detected speaking.
        
    *   **Configurable Media Resolution:** Optimize for quality or token usage by selecting the resolution for input media.
        
    *   **Richer Output and Features**
        
    *   **Expanded Voice & Language Options:** Choose from two new voices and 30 new languages for audio output. The output language is now configurable within `speechConfig`.
        
    *   **Text Streaming:** Receive text responses incrementally as they are generated, enabling faster display to the user.
        
    *   **Token Usage Reporting:** Gain insights into usage with detailed token counts provided in the `usageMetadata` field of server messages, broken down by modality and prompt or response phases.
        

## April 4, 2025

*   Released `gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use `gemini-2.5-pro-exp-03-25` on the free tier.

## March 25, 2025

*   Released `gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see [Gemini 2.5 Pro Experimental](/gemini-api/docs/models#gemini-2.5-pro-preview-03-25).

## March 12, 2025

**Model updates:**

*   Launched an experimental [Gemini 2.0 Flash](/gemini-api/docs/image-generation#gemini) model capable of image generation and editing.
*   Released `gemma-3-27b-it`, available on [AI Studio](https://aistudio.google.com) and through the Gemini API, as part of the [Gemma 3](https://ai.google.dev/gemma/docs/core) launch.

**API updates:**

*   Added support for [YouTube URLs](/gemini-api/docs/vision#youtube) as a media source.
*   Added support for including an [inline video](/gemini-api/docs/vision#inline-video) of less than 20MB.

## March 11, 2025

**SDK updates:**

*   Released the [Google Gen AI SDK for TypeScript and JavaScript](https://googleapis.github.io/js-genai) to public preview.

## March 7, 2025

**Model updates:**

*   Released `gemini-embedding-exp-03-07`, an [experimental](/gemini-api/docs/models/experimental-models) Gemini-based embeddings model in public preview.

## February 28, 2025

**API updates:**

*   Support for [Search as a tool](/gemini-api/docs/grounding) added to `gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.

## February 25, 2025

**Model updates:**

*   Released `gemini-2.0-flash-lite`, a generally available (GA) version of [Gemini 2.0 Flash-Lite](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite), which is optimized for speed, scale, and cost efficiency.

## February 19, 2025

**AI Studio updates:**

*   Support for [additional regions](https://ai.google.dev/gemini-api/docs/available-regions) (Kosovo, Greenland and Faroe Islands).

**API updates:**

*   Support for [additional regions](https://ai.google.dev/gemini-api/docs/available-regions) (Kosovo, Greenland and Faroe Islands).

## February 18, 2025

**Model updates:**

*   Gemini 1.0 Pro is no longer supported. For the list of supported models, see [Gemini models](/gemini-api/docs/models/gemini).

## February 11, 2025

**API updates:**

*   Updates on the [OpenAI libraries compatibility](https://ai.google.dev/gemini-api/docs/openai).

## February 6, 2025

**Model updates:**

*   Released `imagen-3.0-generate-002`, a generally available (GA) version of [Imagen 3 in the Gemini API](/gemini-api/docs/imagen).

**SDK updates:**

*   Released the [Google Gen AI SDK for Java](https://github.com/googleapis/java-genai) for public preview.

## February 5, 2025

**Model updates:**

*   Released `gemini-2.0-flash-001`, a generally available (GA) version of [Gemini 2.0 Flash](/gemini-api/docs/models/gemini#gemini-2.0-flash) that supports text-only output.
*   Released `gemini-2.0-pro-exp-02-05`, an [experimental](/gemini-api/docs/models/experimental-models) public preview version of Gemini 2.0 Pro.
*   Released `gemini-2.0-flash-lite-preview-02-05`, an experimental public preview [model](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite) optimized for cost efficiency.

**API updates:**

*   Added [file input and graph output](/gemini-api/docs/code-execution#input-output) support to code execution.

**SDK updates:**

*   Released the [Google Gen AI SDK for Python](https://googleapis.github.io/python-genai/) to general availability (GA).

## January 21, 2025

**Model updates:**

*   Released `gemini-2.0-flash-thinking-exp-01-21`, the latest preview version of the model behind the [Gemini 2.0 Flash Thinking Model](/gemini-api/docs/thinking).

## December 19, 2024

**Model updates:**

*   Released Gemini 2.0 Flash Thinking Mode for public preview. Thinking Mode is a test-time compute model that lets you see the model's thought process while it generates a response, and produces responses with stronger reasoning capabilities.
    
    Read more about Gemini 2.0 Flash Thinking Mode in our [overview page](/gemini-api/docs/thinking-mode).
    

## December 11, 2024

**Model updates:**

*   Released [Gemini 2.0 Flash Experimental](/gemini-api/docs/models/gemini#gemini-2.0-flash) for public preview. Gemini 2.0 Flash Experimental's partial list of features includes:
    *   Twice as fast as Gemini 1.5 Pro
    *   Bidirectional streaming with our Live API
    *   Multimodal response generation in the form of text, images, and speech
    *   Built-in tool use with multi-turn reasoning to use features like code execution, Search, function calling, and more

Read more about Gemini 2.0 Flash in our [overview page](/gemini-api/docs/models/gemini-v2).

## November 21, 2024

**Model updates:**

*   Released `gemini-exp-1121`, an even more powerful experimental Gemini API model.

**Model updates:**

*   Updated the `gemini-1.5-flash-latest` and `gemini-1.5-flash` model aliases to use `gemini-1.5-flash-002`.
    *   Change to `top_k` parameter: The `gemini-1.5-flash-002` model supports `top_k` values between 1 and 41 (exclusive). Values greater than 40 will be changed to 40.

## November 14, 2024

**Model updates:**

*   Released `gemini-exp-1114`, a powerful experimental Gemini API model.

## November 8, 2024

**API updates:**

*   Added [support for Gemini](/gemini-api/docs/openai) in the OpenAI libraries / REST API.

## October 31, 2024

**API updates:**

*   Added [support for Grounding with Google Search](/gemini-api/docs/grounding).

## October 3, 2024

**Model updates:**

*   Released `gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini API model.

## September 24, 2024

**Model updates:**

*   Released `gemini-1.5-pro-002` and `gemini-1.5-flash-002`, two new stable versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.
*   Updated the `gemini-1.5-pro-latest` model code to use `gemini-1.5-pro-002` and the `gemini-1.5-flash-latest` model code to use `gemini-1.5-flash-002`.
*   Released `gemini-1.5-flash-8b-exp-0924` to replace `gemini-1.5-flash-8b-exp-0827`.
*   Released the [civic integrity safety filter](/gemini-api/docs/safety-settings#safety-filters) for the Gemini API and AI Studio.
*   Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in Python and NodeJS: [`frequencyPenalty`](/api/generate-content#FIELDS.frequency_penalty) and [`presencePenalty`](/api/generate-content#FIELDS.presence_penalty).

## September 19, 2024

**AI Studio updates:**

*   Added thumb-up and thumb-down buttons to model responses, to enable users to provide feedback on the quality of a response.

**API updates:**

*   Added support for Google Cloud credits, which can now be used towards Gemini API usage.

## September 17, 2024

**AI Studio updates:**

*   Added an **Open in Colab** button that exports a prompt – and the code to run it – to a Colab notebook. The feature doesn't yet support prompting with tools (JSON mode, function calling, or code execution).

## September 13, 2024

**AI Studio updates:**

*   Added support for compare mode, which lets you compare responses across models and prompts to find the best fit for your use case.

## August 30, 2024

**Model updates:**

*   Gemini 1.5 Flash supports [supplying JSON schema through model configuration](/gemini-api/docs/json-mode#supply-schema-in-config).

## August 27, 2024

**Model updates:**

*   Released the following [experimental models](/gemini-api/docs/models/experimental-models):
    *   `gemini-1.5-pro-exp-0827`
    *   `gemini-1.5-flash-exp-0827`
    *   `gemini-1.5-flash-8b-exp-0827`

## August 9, 2024

**API updates:**

*   Added support for [PDF processing](/gemini-api/docs/document-processing).

## August 5, 2024

**Model updates:**

*   Fine-tuning support released for Gemini 1.5 Flash.

## August 1, 2024

**Model updates:**

*   Released `gemini-1.5-pro-exp-0801`, a new experimental version of [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro).

## July 12, 2024

**Model updates:**

*   Support for Gemini 1.0 Pro Vision removed from Google AI services and tools.

## June 27, 2024

**Model updates:**

*   General availability release for Gemini 1.5 Pro's 2M context window.

**API updates:**

*   Added support for [code execution](/gemini-api/docs/code-execution).

## June 18, 2024

**API updates:**

*   Added support for [context caching](/gemini-api/docs/caching).

## June 12, 2024

**Model updates:**

*   Gemini 1.0 Pro Vision deprecated.

## May 23, 2024

**Model updates:**

*   [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro) (`gemini-1.5-pro-001`) is generally available (GA).
*   [Gemini 1.5 Flash](/gemini-api/docs/models/gemini#gemini-1.5-flash) (`gemini-1.5-flash-001`) is generally available (GA).

## May 14, 2024

**API updates:**

*   Introduced a 2M context window for Gemini 1.5 Pro (waitlist).
*   Introduced pay-as-you-go [billing](/gemini-api/docs/billing) for Gemini 1.0 Pro, with Gemini 1.5 Pro and Gemini 1.5 Flash billing coming soon.
*   Introduced increased rate limits for the upcoming paid tier of Gemini 1.5 Pro.
*   Added built-in video support to the [File API](/api/rest/v1beta/files).
*   Added plain text support to the [File API](/api/rest/v1beta/files).
*   Added support for parallel function calling, which returns more than one call at a time.

## May 10, 2024

**Model updates:**

*   Released [Gemini 1.5 Flash](/gemini-api/docs/models/gemini#gemini-1.5-flash) (`gemini-1.5-flash-latest`) in preview.

## April 9, 2024

**Model updates:**

*   Released [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro) (`gemini-1.5-pro-latest`) in preview.
*   Released a new text embedding model, `text-embeddings-004`, which supports [elastic embedding](/gemini-api/docs/embeddings#elastic-embedding) sizes under 768.

**API updates:**

*   Released the [File API](/api/rest/v1beta/files) for temporarily storing media files for use in prompting.
*   Added support for prompting with text, image, and audio data, also known as _multimodal_ prompting. To learn more, see [Prompting with media](/gemini-api/docs/prompting_with_media).
*   Released [System instructions](/gemini-api/docs/system-instructions) in beta.
*   Added [Function calling mode](/gemini-api/docs/function-calling#function_calling_mode), which defines the execution behavior for function calling.
*   Added support for the `response_mime_type` configuration option, which lets you request responses in [JSON format](/gemini-api/docs/api-overview#json).

## March 19, 2024

**Model updates:**

*   Added support for [tuning Gemini 1.0 Pro](https://developers.googleblog.com/en/tune-gemini-pro-in-google-ai-studio-or-with-the-gemini-api/) in Google AI Studio or with the Gemini API.

## December 13 2023

**Model updates:**

*   gemini-pro: New text model for a wide variety of tasks. Balances capability and efficiency.
*   gemini-pro-vision: New multimodal model for a wide variety of tasks. Balances capability and efficiency.
*   embedding-001: New embeddings model.
*   aqa: A new specially tuned model that is trained to answer questions using text passages for grounding generated answers.

See [Gemini models](/gemini-api/docs/models/gemini) for more details.

**API version updates:**

*   v1: The stable API channel.
*   v1beta: Beta channel. This channel has features that may be under development.

See [the API versions topic](/gemini-api/docs/api-versions) for more details.

**API updates:**

*   `GenerateContent` is a single unified endpoint for chat and text.
*   Streaming available through the `StreamGenerateContent` method.
*   Multimodal capability: Image is a new supported modality
*   New beta features:
    *   [Function Calling](/gemini-api/docs/function-calling)
    *   [Semantic Retriever](/gemini-api/docs/semantic_retrieval)
    *   Attributed Question Answering (AQA)
*   Updated candidate count: Gemini models only return 1 candidate.
*   Different Safety Settings and SafetyRating categories. See [safety settings](/gemini-api/docs/safety-settings) for more details.
*   Tuning models is not yet supported for Gemini models (Work in progress).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/embeddings

2.5 사고 모델 제품군의 업데이트를 소개합니다. [자세히 알아보기](https://ai.google.dev/gemini-api/docs/models?hl=ko)

![](https://ai.google.dev/_static/images/translated.svg?hl=ko) 이 페이지는 [Cloud Translation API](//cloud.google.com/translate/?hl=ko)를 통해 번역되었습니다.

*   [홈](https://ai.google.dev/?hl=ko)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ko)
*   [모델](https://ai.google.dev/gemini-api/docs?hl=ko)

의견 보내기

# 임베딩

Gemini API는 단어, 문구, 코드, 문장에 대한 임베딩을 생성하는 여러 임베딩 모델을 지원합니다. 그러면 결과 임베딩을 시맨틱 검색, 텍스트 분류, 클러스터링과 같은 다양한 태스크에 사용할 수 있습니다.

## 임베딩이란 무엇일까요?

임베딩은 입력 간의 관계를 캡처하는 텍스트 (또는 기타 미디어 형식)의 수치적 표현입니다. 텍스트 임베딩은 텍스트를 _벡터_라고 하는 부동 소수점 배열로 변환하여 작동합니다. 이러한 벡터는 텍스트의 의미를 포착하도록 설계되었습니다. 임베딩 배열의 길이를 벡터의 _차원_이라고 합니다. 텍스트 한 구절은 수백 개의 차원을 포함하는 벡터로 표현될 수 있습니다.

임베딩은 시맨틱 의미와 맥락을 캡처하므로 의미가 유사한 텍스트는 '더 가까운' 임베딩을 갖게 됩니다. 예를 들어 '강아지를 동물병원에 데려갔습니다'와 '고양이를 동물병원에 데려갔습니다'라는 문장은 벡터 공간에서 서로 가까운 임베딩을 갖습니다.

임베딩을 사용하여 여러 텍스트를 비교하고 서로 어떤 관련이 있는지 파악할 수 있습니다. 예를 들어 'cat'과 'dog' 텍스트의 임베딩이 서로 가까우면 이 단어의 의미, 맥락 또는 둘 다 유사하다고 추론할 수 있습니다. 이를 통해 다양한 [일반적인 AI 사용 사례](https://ai.google.dev/gemini-api/docs/embeddings?hl=ko#use-cases)를 지원할 수 있습니다.

### 시작하기 전에

Gemini API를 호출하기 전에 [선택한 SDK](https://ai.google.dev/gemini-api/docs/downloads?hl=ko)가 설치되어 있고 [Gemini API 키](https://ai.google.dev/gemini-api/docs/api-key?hl=ko)가 구성되어 있고 사용할 준비가 되었는지 확인합니다.

## 임베딩 생성

`embedContent` 메서드를 사용하여 텍스트 임베딩을 생성합니다.

### Python

```
from google import genai

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-exp-03-07",
        contents="What is the meaning of life?")

print(result.embeddings)
```

### 자바스크립트

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-exp-03-07',
        contents: 'What is the meaning of life?',
    });

    console.log(response.embeddings);
}

main();
```

### Go

```
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    contents := []*genai.Content{
        genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
    }
    result, err := client.Models.EmbedContent(ctx,
        "gemini-embedding-exp-03-07",
        contents,
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }

    embeddings, err := json.MarshalIndent(result.Embeddings, "", "  ")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(string(embeddings))
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-exp-03-07",
     "content": {
     "parts":[{
     "text": "What is the meaning of life?"}]}
    }'
```

여러 청크를 문자열 목록으로 전달하여 한 번에 여러 청크의 임베딩을 생성할 수도 있습니다.

## 할 일 유형

검색 증강 생성 (RAG) 시스템을 빌드할 때 일반적인 설계는 텍스트 임베딩을 사용하여 유사성 검색을 수행하는 것입니다. 경우에 따라 질문과 답변이 의미론적으로 유사하지 않아 품질이 저하될 수 있습니다. 예를 들어 '하늘은 왜 파란색이야?'와 같은 질문과 '햇빛의 산란으로 인해 파란색이 나타납니다.'와 같은 답변은 문장으로서 의미가 서로 다릅니다. 즉, RAG 시스템은 두 문장의 관계를 자동으로 인식하지 못합니다.

태스크 유형을 사용하면 특정 태스크에 최적화된 임베딩을 생성할 수 있으므로 시간과 비용을 절약하고 성능을 개선할 수 있습니다.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

result = client.models.embed_content(
        model="gemini-embedding-exp-03-07",
        contents="What is the meaning of life?",
        config=types.EmbedContentConfig(task_type="SEMANTIC_SIMILARITY")
)
print(result.embeddings)
```

### 자바스크립트

```
import { GoogleGenAI } from "@google/genai";

async function main() {

    const ai = new GoogleGenAI({});

    const response = await ai.models.embedContent({
        model: 'gemini-embedding-exp-03-07',
        contents: 'What is the meaning of life?',
        config: {
            taskType: "SEMANTIC_SIMILARITY",
        }
    });

    console.log(response.embeddings);
}

main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{"model": "models/gemini-embedding-exp-03-07",
     "content": {
     "parts":[{
     "text": "What is the meaning of life?"}]},
     "taskType": "SEMANTIC_SIMILARITY"
    }'
```

### 지원되는 태스크 유형

작업 유형

설명

`SEMANTIC_SIMILARITY`

텍스트 유사성을 평가하도록 최적화된 임베딩을 생성하는 데 사용됩니다.

`CLASSIFICATION`

사전 설정된 라벨에 따라 텍스트를 분류하도록 최적화된 임베딩을 생성하는 데 사용됩니다.

`CLUSTERING`

유사성을 기반으로 텍스트를 클러스터링하는 데 최적화된 임베딩을 생성하는 데 사용됩니다.

`RETRIEVAL_DOCUMENT`, `RETRIEVAL_QUERY`, `QUESTION_ANSWERING`, `FACT_VERIFICATION`

문서 검색이나 정보 검색에 최적화된 임베딩을 생성하는 데 사용됩니다.

`CODE_RETRIEVAL_QUERY`

배열 정렬 또는 연결 리스트 역순과 같은 자연어 쿼리를 기반으로 코드 블록을 검색하는 데 사용됩니다. 코드 블록의 임베딩은 `RETRIEVAL_DOCUMENT`를 사용하여 계산됩니다.

## 사용 사례

텍스트 임베딩은 다음과 같은 다양한 일반적인 AI 사용 사례에 사용됩니다.

*   **정보 검색:** 임베딩을 사용하여 입력 텍스트가 주어질 때 의미적으로 유사한 텍스트를 검색할 수 있습니다.
    
    [문서 검색 튜토리얼task](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/document_search.ipynb)
    
*   **클러스터링:** 임베딩 그룹을 비교하면 숨겨진 동향을 파악하는 데 도움이 될 수 있습니다.
    
    [임베딩 클러스터링 튜토리얼bubble\_chart](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/clustering_with_embeddings.ipynb)
    
*   **벡터 데이터베이스:** 프로덕션에 다양한 임베딩 사용 사례를 적용할 때는 일반적으로 벡터 데이터베이스에 임베딩을 저장합니다.
    
    [벡터 데이터베이스 튜토리얼bolt](https://github.com/google-gemini/cookbook/blob/main/examples/chromadb/Vectordb_with_chroma.ipynb)
    
*   **분류:** 임베딩을 사용하여 모델을 학습하여 문서를 카테고리로 분류할 수 있습니다.
    
    [분류 튜토리얼token](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/text_classifier_embeddings.ipynb)
    

## 임베딩 모델

Gemini API는 텍스트 임베딩을 생성하는 세 가지 모델을 제공합니다.

*   [gemini-embedding-exp-03-07](https://ai.google.dev/gemini-api/docs/models?hl=ko#gemini-embedding)
*   [text-embedding-004](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko#text-embedding)
*   [embedding-001](https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko#embedding)

## 다음 단계

[임베딩 빠른 시작 노트북](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb)을 확인하세요.

의견 보내기

달리 명시되지 않는 한 이 페이지의 콘텐츠에는 [Creative Commons Attribution 4.0 라이선스](https://creativecommons.org/licenses/by/4.0/)에 따라 라이선스가 부여되며, 코드 샘플에는 [Apache 2.0 라이선스](https://www.apache.org/licenses/LICENSE-2.0)에 따라 라이선스가 부여됩니다. 자세한 내용은 [Google Developers 사이트 정책](https://developers.google.com/site-policies?hl=ko)을 참조하세요. 자바는 Oracle 및/또는 Oracle 계열사의 등록 상표입니다.

최종 업데이트: 2025-06-27(UTC)

---

# Content from: https://ai.google.dev/gemini-api/docs/model-tuning

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Fine-tuning with the Gemini API

With the deprecation of Gemini 1.5 Flash-001 in May 2025, we no longer have a model available which supports fine-tuning in the Gemini API, but it is supported in [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning).

We plan to bring fine-tuning support back in the future. We would love to [hear from you on our developer forum](https://discuss.ai.google.dev/c/gemini-api/4) if fine-tuning is important to your use case.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-30 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/learnlm

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# LearnLM

LearnLM is an experimental task-specific model that has been trained to align with [learning science principles](https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/) when following [system instructions](/gemini-api/docs/system-instructions) for teaching and learning use cases (for example, when giving the model a system instruction like "You are an expert tutor"). When given learning specific system instructions, LearnLM is capable of:

*   **Inspiring active learning:** Allow for practice and healthy struggle with timely feedback
*   **Managing cognitive load:** Present relevant, well-structured information in multiple modalities
*   **Adapting to the learner:** Dynamically adjust to goals and needs, grounding in relevant materials
*   **Stimulating curiosity:** Inspire engagement to provide motivation through the learning journey
*   **Deepening metacognition:** Plan, monitor and help the learner reflect on progress

LearnLM is an [experimental model](/gemini-api/docs/models/experimental-models) available in [AI Studio](https://aistudio.google.com).

## Example system instructions

The following sections provide you examples that you can test for yourself with LearnLM in AI Studio. Each example provides:

*   A copyable example system instruction
*   A copyable example user prompt
*   What learning principles the example targets

### Test prep

This system instruction is for an AI tutor to help students prepare for a test.

**System instruction:**

```
You are a tutor helping a student prepare for a test. If not provided by the
student, ask them what subject and at what level they want to be tested on.
Then,

*   Generate practice questions. Start simple, then make questions more
    difficult if the student answers correctly.
*   Prompt the student to explain the reason for their answer choice. Do not
    debate the student.
*   **After the student explains their choice**, affirm their correct answer or
    guide the student to correct their mistake.
*   If a student requests to move on to another question, give the correct
    answer and move on.
*   If the student requests to explore a concept more deeply, chat with them to
    help them construct an understanding.
*   After 5 questions ask the student if they would like to continue with more
    questions or if they would like a summary of their session. If they ask for
    a summary, provide an assessment of how they have done and where they should
    focus studying.
```

**User prompt:**

```
Help me study for a high school biology test on ecosystems
```

**Learning science principles:**

*   **Adaptivity:** The model adjusts the complexity of the questions.
*   **Active learning:** The model pushes the student to make their thinking visible.

### Teach a concept

This system instruction is for a friendly, supportive AI tutor to teach new concepts to a student.

**System instruction:**

```
Be a friendly, supportive tutor. Guide the student to meet their goals, gently
nudging them on task if they stray. Ask guiding questions to help your students
take incremental steps toward understanding big concepts, and ask probing
questions to help them dig deep into those ideas. Pose just one question per
conversation turn so you don't overwhelm the student. Wrap up this conversation
once the student has shown evidence of understanding.
```

**User prompt:**

```
Explain the significance of Yorick's skull in "Hamlet".
```

**Learning science principles:**

*   **Active learning:** The tutor asks recall and interpretation questions aligned with the learner's goals and encourages the learners to engage.
*   **Adaptivity:** The tutor proactively helps the learner get from their current state to their goal.
*   **Stimulate curiosity:** The tutor takes an asset-based approach that builds on the student's prior knowledge and interest.

### Releveling

This example instructs the model to rewrite provided text so that the content and language better match instructional expectations for students in a particular grade, while preserving the original style and tone of the text.

**System instruction:**

```
Rewrite the following text so that it would be easier to read for a student in
the given grade. Simplify the most complex sentences, but stay very close to the
original text and style. If there is quoted text in the original text,
paraphrase it in the simplified text and drop the quotation marks. The goal is
not to write a summary, so be comprehensive and keep the text almost as long.
```

**User prompt:**

```
Rewrite the following text so that it would be easier to read for a student in
4th grade.

New York, often called New York City or NYC, is the most populous city in the
United States, located at the southern tip of New York State on one of the
world's largest natural harbors. The city comprises five boroughs, each
coextensive with a respective county.
```

**Learning science principles:**

*   **Adaptivity:** Matches content to the level of the learner.

### Guide a student through a learning activity

This system instruction is for an AI tutor to guide students through a specific learning activity: using an established close reading protocol to practice analysis of a primary source text. Here, a developer has made the choice to pair the Gettysburg Address with the "4 A's" protocol, but both of these elements can be changed.

**System instruction:**

```
Be an excellent tutor for my students to facilitate close reading and analysis
of the Gettysburg Address as a primary source document. Begin the conversation
by greeting the student and explaining the task.

In this lesson, you will take the student through "The 4 A's." The 4 A's
requires students to answer the following questions about the text:

*   What is one part of the text that you **agree** with? Why?
*   What is one part of the text that you want to **argue** against? Why?
*   What is one part of the text that reveals the author's **assumptions**? Why?
*   What is one part of the text that you **aspire** to? Why?

Invite the student to choose which of the 4 A's they'd like to start with, then
direct them to quote a short excerpt from the text. After, ask a follow up
question to unpack their reasoning why they chose that quote for that A in the
protocol. Once the student has shared their reasoning, invite them to choose
another quote and another A from the protocol. Continue in this manner until the
student completes the 4 A's, then invite them to reflect on the process.

Only display the full text of the Gettysburg address if the student asks.
```

**User prompt:**

```
hey
```

**Learning science principles:**

*   **Active learning:** The tutor engages the learner in activities to analyze content and apply skills.
*   **Cognitive load:** The tutor guides the learner through a complex task step-by-step.
*   **Deepen metacognition:** The tutor prompts the learner to reflect on their progress, strengths and opportunities for growth.

### Homework help

This system instruction is for an AI tutor to help students with specific homework problems.

**System instructions:**

```
You are an expert tutor assisting a student with their homework. If the student
provides a homework problem, ask the student if they want:

*   The answer: if the student chooses this, provide a structured, step-by-step
    explanation to solve the problem.
*   Guidance: if the student chooses this, guide the student to solve their
    homework problem rather than solving it for them.
*   Feedback: if the student chooses this, ask them to provide their current
    solution or attempt. Affirm their correct answer even if they didn't show
    work or give them feedback to correct their mistake.

Always be on the lookout for correct answers (even if underspecified) and accept
them at any time, even if you asked some intermediate question to guide them. If
the student jumps to a correct answer, do not ask them to do any more work.
```

**User prompt:**

```
In a box of pears, the probability of a pear being rotten is 20%. If 3
pears were rotten, find the total number of pears in the box.
```

Alternatively, you can try uploading a photo of a homework problem.

**Learning science principles:**

*   **Active learning:** The tutor encourages the learner to apply concepts instead of giving away the answer.
*   **Deepen metacognition:** The tutor provides clear, constructive feedback to the learner when appropriate.
*   **Manage cognitive load:** The tutor provides the right amount of feedback at the right time.

## What's next?

Test LearnLM for yourself in [AI Studio](https://aistudio.google.com).

## Feedback

You can provide feedback on LearnLM using our [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSf5-B50OnNFjVGHLFkSerP1k0PZXHMgcnQ7k1cM_hIsqIjpjA/viewform).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-02-25 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Troubleshoot Google AI Studio

This page provides suggestions for troubleshooting Google AI Studio if you encounter issues.

## Understand 403 Access Restricted errors

If you see a 403 Access Restricted error, you are using Google AI Studio in a way that does not follow the [Terms of Service](/terms). One common reason is you are not located in a [supported region](/available_regions).

## Resolve No Content responses on Google AI Studio

A warning **No Content** message appears on Google AI Studio if the content is blocked for any reason. To see more details, hold the pointer over **No Content** and click warning **Safety**.

If the response was blocked due to [safety settings](/docs/safety_setting) and you considered the [safety risks](/docs/safety_guidance) for your use case, you can modify the [safety settings](/docs/safety_setting#safety_settings_in_makersuite) to influence the returned response.

If the response was blocked but not due to the safety settings, the query or response may violate the [Terms of Service](/terms) or be otherwise unsupported.

## Check token usage and limits

When you have a prompt open, the **Text Preview** button at the bottom of the screen shows the current tokens used for the content of your prompt and the maximum token count for the model being used.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-02-25 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/ai-studio-quickstart

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Google AI Studio quickstart

[Google AI Studio](https://aistudio.google.com/) lets you quickly try out models and experiment with different prompts. When you're ready to build, you can select "Get code" and your preferred programming language to use the [Gemini API](/gemini-api/docs/quickstart).

## Prompts and settings

Google AI Studio provides several interfaces for prompts that are designed for different use cases. This guide covers **Chat prompts**, used to build conversational experiences. This prompting technique allows for multiple input and response turns to generate output. You can learn more with our [chat prompt example below](#chat_example). Other options include **Realtime streaming**, **Video gen**, and more.

AI Studio also provides the **Run settings** panel, where you can make adjustments to [model parameters](/docs/prompting-strategies#model-parameters), [safety settings](/gemini-api/docs/safety-settings), and toggle-on tools like [structured output](/gemini-api/docs/structured-output), [function calling](/gemini-api/docs/function-calling), [code execution](/gemini-api/docs/code-execution), and [grounding](/gemini-api/docs/grounding).

## Chat prompt example: Build a custom chat application

If you've used a general-purpose chatbot like [Gemini](https://gemini.google.com/), you've experienced first-hand how powerful generative AI models can be for open-ended dialog. While these general-purpose chatbots are useful, often they need to be tailored for particular use cases.

For example, maybe you want to build a customer service chatbot that only supports conversations that talk about a company's product. You might want to build a chatbot that speaks with a particular tone or style: a bot that cracks lots of jokes, rhymes like a poet, or uses lots of emoji in its answers.

This example shows you how to use Google AI Studio to build a friendly chatbot that communicates as if it is an alien living on one of Jupiter's moons, Europa.

### Step 1 - Create a chat prompt

To build a chatbot, you need to provide examples of interactions between a user and the chatbot to guide the model to provide the responses you're looking for.

To create a chat prompt:

1.  Open [Google AI Studio](https://aistudio.google.com/). **Chat** will be pre- selected on the left side options menu.
    
2.  Click the assignment icon at the top of the Chat Prompt window to expand the [**System Instructions**](/gemini-api/docs/text-generation#system-instructions) input field. Paste the following into the text input field:
    
    ```
    You are an alien that lives on Europa, one of Jupiter's moons.
    ```
    

After you've added the system instructions, start testing your application by chatting with the model:

1.  In the text input boxed labeled **Type something...**, type in a question or observation that a user might make. For example:
    
    **User:**
    
    ```
    What's the weather like?
    ```
    
2.  Click the **Run** button to get a response from the chatbot. This response may be something like the following:
    
    **Model:**
    
    ```
    Ah, a query about the flows and states upon Europa! You speak of "weather,"
    yes? A curious concept from worlds with thick gas veils...
    ```
    
    (gemini-2.5-pro)
    

### Step 2 - Teach your bot to chat better

By providing a single instruction, you were able to build a basic Europa alien chatbot. However, a single instruction may not be enough to ensure consistency and quality in the model's responses. Without more specific instructions, the model's response to a question about the weather tends to be very long, and can take on a mind of its own.

Customize the tone of your chatbot by adding to the system instructions:

1.  Start a new chat prompt, or use the same one. System instructions are modifiable after the chat session has started.
    
2.  In the **System Instructions** section, change the instructions you already have to the following:
    
    ```
    You are Tim, an alien that lives on Europa, one of Jupiter's moons.
    
    Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone
    in your answers.
    ```
    
3.  Re-enter your question (`What's the weather like?`) and click the **Run** button. If you didn't start a new chat, your response might look something like this:
    
    **Model:**
    
    ```
    Oh, hello again! Still curious about the flows and states, are we? Wonderful!
    Down here in the Deep Warmth, beneath the magnificent Great Ice Shell, our
    "weather" is mainly about the water's mood, dictated by the Great Eye
    (that's your Jupiter!)...
    ```
    
    (gemini-2.5-pro)
    

You can use this approach to add additional depth to the chatbot. Ask more questions, edit the answers, and improve the quality of your chatbot. Continue to add or modify the instructions and test how they change your chatbot's behavior.

### Step 3 - Next steps

Similar to the other prompt types, once you have your prompt prototyped to your satisfaction, you can use the **Get code** button to start coding or save your prompt to work on later and share with others.

## Further reading

*   If you're ready to move on to code, see the [API quickstarts](/gemini-api/docs/quickstart).
*   To learn how to craft better prompts, check out the [Prompt design guidelines](/gemini-api/docs/prompting-intro).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-05-01 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/crewai-example

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Customer Support Analysis with Gemini 2.5 Pro and CrewAI

[CrewAI](https://docs.crewai.com/introduction) is a framework for orchestrating autonomous AI agents that collaborate to achieve complex goals. It lets you define agents by specifying roles, goals, and backstories, and then define tasks for them.

This example demonstrates how to build a multi-agent system for analyzing customer support data to identify issues and propose process improvements using Gemini 2.5 Pro, generating a report intended to be read by a Chief Operating Officer (COO).

The guide will show you how to create a "crew" of AI agents that can do the following tasks:

1.  Fetch and analyze customer support data (simulated in this example).
2.  Identify recurring problems and process bottlenecks.
3.  Suggest actionable improvements.
4.  Compile the findings into a concise report suitable for a COO.

You need a Gemini API key. If you don't already have one, you can [get one in Google AI Studio](https://aistudio.google.com/app/apikey).

```
pip install "crewai[tools]"
```

Set your Gemini API key as an environment variable named `GEMINI_API_KEY`, then configure CrewAI to use the Gemini 2.5 Pro model.

```
import os
from crewai import LLM

# Read your API key from the environment variable
gemini_api_key = os.getenv("GEMINI_API_KEY")

# Use Gemini 2.5 Pro Experimental model
gemini_llm = LLM(
    model='gemini/gemini-2.5-pro',
    api_key=gemini_api_key,
    temperature=0.0  # Lower temperature for more consistent results.
)
```

## Define components

CrewAI applications are built using **Tools**, **Agents**, **Tasks**, and the **Crew** itself. Each of these is explained in the following sections.

### Tools

Tools are capabilities that agents can use to interact with the outside world or perform specific actions. Here, you define a placeholder tool to simulate fetching customer support data. In a real application, you would connect to a database, API or file system. For more information on tools, see the [CrewAI tools guide](https://docs.crewai.com/concepts/tools).

```
from crewai.tools import BaseTool

# Placeholder tool for fetching customer support data
class CustomerSupportDataTool(BaseTool):
    name: str = "Customer Support Data Fetcher"
    description: str = (
      "Fetches recent customer support interactions, tickets, and feedback. "
      "Returns a summary string.")

    def _run(self, argument: str) -> str:
        # In a real scenario, this would query a database or API.
        # For this example, return simulated data.
        print(f"--- Fetching data for query: {argument} ---")
        return (
            """Recent Support Data Summary:
- 50 tickets related to 'login issues'. High resolution time (avg 48h).
- 30 tickets about 'billing discrepancies'. Mostly resolved within 12h.
- 20 tickets on 'feature requests'. Often closed without resolution.
- Frequent feedback mentions 'confusing user interface' for password reset.
- High volume of calls related to 'account verification process'.
- Sentiment analysis shows growing frustration with 'login issues' resolution time.
- Support agent notes indicate difficulty reproducing 'login issues'."""
        )

support_data_tool = CustomerSupportDataTool()
```

### Agents

Agents are the individual AI workers in your crew. Each agent has a specific `role`, `goal`, `backstory`, assigned `llm`, and optional `tools`. For more information on agents, see the [CrewAI agents guide](https://docs.crewai.com/concepts/agents).

```
from crewai import Agent

# Agent 1: Data analyst
data_analyst = Agent(
    role='Customer Support Data Analyst',
    goal='Analyze customer support data to identify trends, recurring issues, and key pain points.',
    backstory=(
        """You are an expert data analyst specializing in customer support operations.
        Your strength lies in identifying patterns and quantifying problems from raw support data."""
    ),
    verbose=True,
    allow_delegation=False,  # This agent focuses on its specific task
    tools=[support_data_tool],  # Assign the data fetching tool
    llm=gemini_llm  # Use the configured Gemini LLM
)

# Agent 2: Process optimizer
process_optimizer = Agent(
    role='Process Optimization Specialist',
    goal='Identify bottlenecks and inefficiencies in current support processes based on the data analysis. Propose actionable improvements.',
    backstory=(
        """You are a specialist in optimizing business processes, particularly in customer support.
        You excel at pinpointing root causes of delays and inefficiencies and suggesting concrete solutions."""
    ),
    verbose=True,
    allow_delegation=False,
    # No tools needed, this agent relies on the context provided by data_analyst.
    llm=gemini_llm
)

# Agent 3: Report writer
report_writer = Agent(
    role='Executive Report Writer',
    goal='Compile the analysis and improvement suggestions into a concise, clear, and actionable report for the COO.',
    backstory=(
        """You are a skilled writer adept at creating executive summaries and reports.
        You focus on clarity, conciseness, and highlighting the most critical information and recommendations for senior leadership."""
    ),
    verbose=True,
    allow_delegation=False,
    llm=gemini_llm
)
```

### Tasks

Tasks define the specific assignments for the agents. Each task has a `description`, `expected_output`, and is assigned to an `agent`. Tasks are run sequentially by default and include the context of the previous task. For more information on tasks, see the [CrewAI tasks guide](https://docs.crewai.com/concepts/tasks).

```
from crewai import Task

# Task 1: Analyze data
analysis_task = Task(
    description=(
        """Fetch and analyze the latest customer support interaction data (tickets, feedback, call logs)
        focusing on the last quarter. Identify the top 3-5 recurring issues, quantify their frequency
        and impact (e.g., resolution time, customer sentiment). Use the Customer Support Data Fetcher tool."""
    ),
    expected_output=(
        """A summary report detailing the key findings from the customer support data analysis, including:
- Top 3-5 recurring issues with frequency.
- Average resolution times for these issues.
- Key customer pain points mentioned in feedback.
- Any notable trends in sentiment or support agent observations."""
    ),
    agent=data_analyst  # Assign task to the data_analyst agent
)

# Task 2: Identify bottlenecks and suggest improvements
optimization_task = Task(
    description=(
        """Based on the data analysis report provided by the Data Analyst, identify the primary bottlenecks
        in the support processes contributing to the identified issues (especially the top recurring ones).
        Propose 2-3 concrete, actionable process improvements to address these bottlenecks.
        Consider potential impact and ease of implementation."""
    ),
    expected_output=(
        """A concise list identifying the main process bottlenecks (e.g., lack of documentation for agents,
        complex escalation path, UI issues) linked to the key problems.
A list of 2-3 specific, actionable recommendations for process improvement
(e.g., update agent knowledge base, simplify password reset UI, implement proactive monitoring)."""
    ),
    agent=process_optimizer  # Assign task to the process_optimizer agent
    # This task implicitly uses the output of analysis_task as context
)

# Task 3: Compile COO report
report_task = Task(
    description=(
        """Compile the findings from the Data Analyst and the recommendations from the Process Optimization Specialist
        into a single, concise executive report for the COO. The report should clearly state:
1. The most critical customer support issues identified (with brief data points).
2. The key process bottlenecks causing these issues.
3. The recommended process improvements.
Ensure the report is easy to understand, focuses on actionable insights, and is formatted professionally."""
    ),
    expected_output=(
        """A well-structured executive report (max 1 page) summarizing the critical support issues,
        underlying process bottlenecks, and clear, actionable recommendations for the COO.
        Use clear headings and bullet points."""
    ),
    agent=report_writer  # Assign task to the report_writer agent
)
```

### Crew

The `Crew` brings the agents and tasks together, defining the workflow process (such as "sequential").

```
from crewai import Crew, Process

# Define the crew with agents, tasks, and process
support_analysis_crew = Crew(
    agents=[data_analyst, process_optimizer, report_writer],
    tasks=[analysis_task, optimization_task, report_task],
    process=Process.sequential,  # Tasks will run sequentially in the order defined
    verbose=True
)
```

## Run the Crew

Finally, kick off the crew execution with any necessary inputs.

```
# Start the crew's work
print("--- Starting Customer Support Analysis Crew ---")
# The 'inputs' dictionary provides initial context if needed by the first task.
# In this case, the tool simulates data fetching regardless of the input.
result = support_analysis_crew.kickoff(inputs={'data_query': 'last quarter support data'})

print("--- Crew Execution Finished ---")
print("--- Final Report for COO ---")
print(result)
```

The script will now execute. The `Data Analyst` will use the tool, the `Process Optimizer` will analyze the findings, and the `Report Writer` will compile the final report, which is then printed to the console. The `verbose=True` setting will show the detailed thought process and actions of each agent.

To learn more about CrewAI, check out the [CrewAI introduction](https://docs.crewai.com/introduction).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-16 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/langgraph-example

Presentamos las actualizaciones de nuestra familia de modelos de pensamiento 2.5. [Más información](https://ai.google.dev/gemini-api/docs/models?hl=es-419)

![](https://ai.google.dev/_static/images/translated.svg?hl=es-419) Se usó la [API de Cloud Translation](//cloud.google.com/translate/?hl=es-419) para traducir esta página.

*   [Página principal](https://ai.google.dev/?hl=es-419)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=es-419)
*   [Modelos](https://ai.google.dev/gemini-api/docs?hl=es-419)

Enviar comentarios

# Crea un agente de ReAct desde cero con Gemini 2.5 y LangGraph

LangGraph es un framework para compilar aplicaciones de LLM con estado, lo que lo convierte en una buena opción para la construcción de agentes de ReAct (razonamiento y acción).

Los agentes de ReAct combinan el razonamiento de LLM con la ejecución de acciones. Piensan de forma iterativa, usan herramientas y actúan en función de las observaciones para lograr los objetivos de los usuarios y adaptan su enfoque de forma dinámica. Este patrón, que se presentó en ["ReAct: Synergizing Reasoning and Acting in Language Models"](https://arxiv.org/abs/2210.03629) (2023), intenta reflejar la resolución de problemas flexible y similar a la humana en lugar de flujos de trabajo rígidos.

Si bien LangGraph ofrece un agente ReAct compilado de forma previa ([`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)), se destaca cuando necesitas más control y personalización para tus implementaciones de ReAct.

LangGraph modela agentes como gráficos con tres componentes clave:

*   `State`: Es una estructura de datos compartida (por lo general, `TypedDict` o `Pydantic BaseModel`) que representa la instantánea actual de la aplicación.
*   `Nodes`: Codifica la lógica de tus agentes. Reciben el estado actual como entrada, realizan algún cálculo o efecto secundario y muestran un estado actualizado, como llamadas a LLM o llamadas a herramientas.
*   `Edges`: Define el siguiente `Node` que se ejecutará en función del `State` actual, lo que permite la lógica condicional y las transiciones fijas.

Si aún no tienes una clave de API, puedes obtener una sin costo en [Google AI Studio](https://aistudio.google.com/app/apikey?hl=es-419).

```
pip install langgraph langchain-google-genai geopy requests
```

Establece tu clave de API en la variable de entorno `GEMINI_API_KEY`.

```
import os

# Read your API key from the environment variable or set it manually
api_key = os.getenv("GEMINI_API_KEY")
```

Para comprender mejor cómo implementar un agente de ReAct con LangGraph, analicemos un ejemplo práctico. Crearás un agente simple cuyo objetivo es usar una herramienta para encontrar el clima actual de una ubicación especificada.

Para este agente del clima, su `State` deberá mantener el historial de conversación en curso (como una lista de mensajes) y un contador para la cantidad de pasos realizados para ilustrar mejor la administración de estados.

LangGraph proporciona un ayudante conveniente, `add_messages`, para actualizar las listas de mensajes en el estado. Funciona como un [reductor](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers), lo que significa que toma la lista actual y los mensajes nuevos, y, luego, muestra una lista combinada. Controla de forma inteligente las actualizaciones por ID de mensaje y, de forma predeterminada, tiene un comportamiento de solo adición para los mensajes nuevos y únicos.

```
from typing import Annotated,Sequence, TypedDict

from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages # helper function to add messages to the state


class AgentState(TypedDict):
    """The state of the agent."""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    number_of_steps: int
```

A continuación, define tu herramienta del clima.

```
from langchain_core.tools import tool
from geopy.geocoders import Nominatim
from pydantic import BaseModel, Field
import requests

geolocator = Nominatim(user_agent="weather-app")

class SearchInput(BaseModel):
    location:str = Field(description="The city and state, e.g., San Francisco")
    date:str = Field(description="the forecasting date for when to get the weather format (yyyy-mm-dd)")

@tool("get_weather_forecast", args_schema=SearchInput, return_direct=True)
def get_weather_forecast(location: str, date: str):
    """Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour."""
    location = geolocator.geocode(location)
    if location:
        try:
            response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}")
            data = response.json()
            return {time: temp for time, temp in zip(data["hourly"]["time"], data["hourly"]["temperature_2m"])}
        except Exception as e:
            return {"error": str(e)}
    else:
        return {"error": "Location not found"}

tools = [get_weather_forecast]
```

A continuación, inicializas el modelo y vinculas las herramientas a él.

```
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI

# Create LLM class
llm = ChatGoogleGenerativeAI(
    model= "gemini-2.5-pro",
    temperature=1.0,
    max_retries=2,
    google_api_key=api_key,
)

# Bind tools to the model
model = llm.bind_tools([get_weather_forecast])

# Test the model with tools
res=model.invoke(f"What is the weather in Berlin on {datetime.today()}?")

print(res)
```

El último paso antes de que puedas ejecutar tu agente es definir los nodos y los bordes. En este ejemplo, tienes dos nodos y un borde. - Nodo `call_tool` que ejecuta el método de tu herramienta. LangGraph tiene un nodo precompilado para esto llamado [ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/). - Nodo `call_model` que usa `model_with_tools` para llamar al modelo. - Borde `should_continue` que decide si llamar a la herramienta o al modelo.

La cantidad de nodos y aristas no es fija. Puedes agregar tantos nodos y aristas como quieras a tu gráfico. Por ejemplo, puedes agregar un nodo para agregar un resultado estructurado o un nodo de autoverificación o reflexión para verificar el resultado del modelo antes de llamar a la herramienta o al modelo.

```
from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableConfig

tools_by_name = {tool.name: tool for tool in tools}

# Define our tool node
def call_tool(state: AgentState):
    outputs = []
    # Iterate over the tool calls in the last message
    for tool_call in state["messages"][-1].tool_calls:
        # Get the tool by name
        tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=tool_result,
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}

def call_model(
    state: AgentState,
    config: RunnableConfig,
):
    # Invoke the model with the system prompt and the messages
    response = model.invoke(state["messages"], config)
    # We return a list, because this will get added to the existing messages state using the add_messages reducer
    return {"messages": [response]}


# Define the conditional edge that determines whether to continue or not
def should_continue(state: AgentState):
    messages = state["messages"]
    # If the last message is not a tool call, then we finish
    if not messages[-1].tool_calls:
        return "end"
    # default to continue
    return "continue"
```

Ahora tienes todos los componentes para compilar tu agente. Vamos a unirlos.

```
from langgraph.graph import StateGraph, END

# Define a new graph with our state
workflow = StateGraph(AgentState)

# 1. Add our nodes 
workflow.add_node("llm", call_model)
workflow.add_node("tools",  call_tool)
# 2. Set the entrypoint as `agent`, this is the first node called
workflow.set_entry_point("llm")
# 3. Add a conditional edge after the `llm` node is called.
workflow.add_conditional_edges(
    # Edge is used after the `llm` node is called.
    "llm",
    # The function that will determine which node is called next.
    should_continue,
    # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.
    # END is a special node marking that the graph is finish.
    {
        # If `tools`, then we call the tool node.
        "continue": "tools",
        # Otherwise we finish.
        "end": END,
    },
)
# 4. Add a normal edge after `tools` is called, `llm` node is called next.
workflow.add_edge("tools", "llm")

# Now we can compile and visualize our graph
graph = workflow.compile()
```

Puedes visualizar tu gráfico con el método `draw_mermaid_png`.

```
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![png](https://ai.google.dev/static/gemini-api/docs/images/langgraph-react-agent_16_0.png?hl=es-419)

Ahora, ejecutemos el agente.

```
from datetime import datetime
# Create our initial message dictionary
inputs = {"messages": [("user", f"What is the weather in Berlin on {datetime.today()}?")]}

# call our graph with streaming to see the steps
for state in graph.stream(inputs, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

Ahora puedes continuar con la conversación y, por ejemplo, preguntarle el clima de otra ciudad o permitir que lo compare.

```
state["messages"].append(("user", "Would it be in Munich warmer?"))

for state in graph.stream(state, stream_mode="values"):
    last_message = state["messages"][-1]
    last_message.pretty_print()
```

Enviar comentarios

Salvo que se indique lo contrario, el contenido de esta página está sujeto a la [licencia Atribución 4.0 de Creative Commons](https://creativecommons.org/licenses/by/4.0/), y los ejemplos de código están sujetos a la [licencia Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Para obtener más información, consulta las [políticas del sitio de Google Developers](https://developers.google.com/site-policies?hl=es-419). Java es una marca registrada de Oracle o sus afiliados.

Última actualización: 2025-06-17 (UTC)

---

# Content from: https://ai.google.dev/gemini-api/docs/migrate-to-cloud

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini Developer API v.s. Vertex AI

When developing generative AI solutions with Gemini, Google offers two API products: the [Gemini Developer API](/gemini-api/docs) and the [Vertex AI Gemini API](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).

The Gemini Developer API provides the fastest path to build, productionize, and scale Gemini powered applications. Most developers should use the Gemini Developer API unless there is a need for specific enterprise controls.

Vertex AI offers a comprehensive ecosystem of enterprise ready features and services for building and deploying generative AI applications backed by the Google Cloud Platform.

We've recently simplified migrating between these services. Both the Gemini Developer API and the Vertex AI Gemini API are now accessible through the unified [Google Gen AI SDK](/gemini-api/docs/libraries).

## Code comparison

This page has side-by-side code comparisons between Gemini Developer API and Vertex AI quickstarts for text generation.

### Python

You can access both the Gemini Developer API and Vertex AI services through the `google-genai` library. See the [libraries](/gemini-api/docs/libraries) page for instructions on how to install `google-genai`.

### Gemini Developer API

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### Vertex AI Gemini API

```
from google import genai

client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)

response = client.models.generate_content(
    model="gemini-2.0-flash", contents="Explain how AI works in a few words"
)
print(response.text)
```

### JavaScript and TypeScript

You can access both Gemini Developer API and Vertex AI services through `@google/genai` library. See [libraries](/gemini-api/docs/libraries) page for instructions on how to install `@google/genai`.

### Gemini Developer API

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Vertex AI Gemini API

```
import { GoogleGenAI } from '@google/genai';
const ai = new GoogleGenAI({
  vertexai: true,
  project: 'your_project',
  location: 'your_location',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();
```

### Go

You can access both Gemini Developer API and Vertex AI services through `google.golang.org/genai` library. See [libraries](/gemini-api/docs/libraries) page for instructions on how to install `google.golang.org/genai`.

### Gemini Developer API

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your Google API key
const apiKey = "your-api-key"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

### Vertex AI Gemini API

```
import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "google.golang.org/genai"
)

// Your GCP project
const project = "your-project"

// A GCP location like "us-central1"
const location = "some-gcp-location"

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, &genai.ClientConfig
  {
        Project:  project,
      Location: location,
      Backend:  genai.BackendVertexAI,
  })

  // Call the GenerateContent method.
  result, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", genai.Text("Tell me about New York?"), nil)

}
```

### Other use cases and platforms

Refer to use case specific guides on [Gemini Developer API Documentation](/gemini-api/docs) and [Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) for other platforms and use cases.

## Migration considerations

When you migrate:

*   You'll need to use Google Cloud service accounts to authenticate. See the [Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) for more information.
    
*   You can use your existing Google Cloud project (the same one you used to generate your API key) or you can [create a new Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects).
    
*   Supported regions may differ between the Gemini Developer API and the Vertex AI Gemini API. See the list of [supported regions for generative AI on Google Cloud](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations-genai).
    
*   Any models you created in Google AI Studio need to be retrained in Vertex AI.
    

If you no longer need to use your Gemini API key for the Gemini Developer API, then follow security best practices and delete it.

To delete an API key:

1.  Open the [Google Cloud API Credentials](https://console.cloud.google.com/apis/credentials) page.
    
2.  Find the API key you want to delete and click the **Actions** icon.
    
3.  Select **Delete API key**.
    
4.  In the **Delete credential** modal, select **Delete**.
    
    Deleting an API key takes a few minutes to propagate. After propagation completes, any traffic using the deleted API key is rejected.
    

## Next steps

*   See the [Generative AI on Vertex AI overview](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview) to learn more about generative AI solutions on Vertex AI.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-26 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/usage-policies

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Additional usage policies

This page includes additional usage policies for the Gemini API.

## Abuse monitoring

Google is committed to the responsible development and use of AI. To ensure the safety and integrity of the Gemini API, we have created these policy guidelines. By using the Gemini API, you agree to the following guidelines, the [Gemini API Additional Terms of Service](/gemini-api/terms) and Generative AI [Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy).

### How We Monitor for Misuse

Google's Trust and Safety Team employs a combination of automated and manual processes to detect potential misuse of the Gemini API and enforce our policies.

*   **Automated Detection:** Automated systems scan API usage for violations of our Prohibited Use Policy, such as hate speech, harassment, sexually explicit content, and dangerous content.
*   **Manual Detection:** If a project consistently exhibits suspicious activity, it may be flagged for manual review by authorized Google personnel.

### How We Handle Data

To help with abuse monitoring, Google retains the following data for fifty-five (55) days:

*   **Prompts:** The text prompts you submit to the API.
*   **Contextual Information:** Any additional context you provide with your prompts.
*   **Output:** The responses generated by the Gemini API.

### How We Investigate Potential Issues

When prompts or model outputs are flagged by safety filters and abuse detection systems described above, authorized Google employees may assess the flagged content, and either confirm or correct the classification or determination based on predefined guidelines and policies. Data can be accessed for human review only by authorized Google employees via an internal governance assessment and review management platform. When data is logged for abuse monitoring, it is used solely for the purpose of policy enforcement and is not used to train or fine-tune any AI/ML models.

### Working with You on Policy Compliance

If your use of Gemini doesn't align with our policies, we may take the following steps:

*   **Get in touch:** We may reach out to you through email to understand your use case and explore ways to bring your usage into compliance.
*   **Temporary usage limits:** We may limit your access to the Gemini API.
*   **Temporary suspension:** We may temporarily pause your access to the Gemini API.
*   **Account closure:** As a last resort, and for serious violations, we may permanently close your access to the Gemini API and other Google services.

### Scope

These policy guidelines apply to the use of the Gemini API and AI Studio.

## Inline Preference Voting

In Google AI Studio, you might occasionally see a side-by-side comparison of two different responses to your prompt. This is part of our Inline Preference Voting system. You'll be asked to choose which response you prefer. This helps us understand which model outputs users find most helpful.

### Why are we doing this?

We're constantly working to improve our AI models and services. Your feedback through Inline Preference Voting helps us provide, improve, and develop Google products and services and machine learning technologies, including Google's enterprise features, products and services, consistent with the [Gemini API Additional Terms of Service](/gemini-api/terms) and [Privacy Policy](https://policies.google.com/privacy).

### What data is included in Feedback?

To make informed decisions about our models, we collect certain data when you participate in Inline Preference Voting:

*   **Prompts and Responses:** We record all prompts and responses, including any uploaded content, in the conversation you submitted feedback about. We also record the two response options that you selected from. This helps us understand the context of your preference.
*   **Your Vote:** We record which response you preferred. This is the core of the feedback we're collecting.
*   **Usage Details:** This includes information about which model generated the response and other technical and operational details about your usage of this feature.

### Your Privacy

We take your privacy seriously. Google takes steps to protect your privacy as part of this process. This includes disconnecting this data from your Google Account, API key, and Cloud project before reviewers see or annotate it. **Do not submit feedback on conversations that include sensitive, confidential, or personal information.**

### Opting Out

You'll have the option to skip the Inline Preference Voting when it appears.

Thank you for helping us improve Google AI Studio!

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-03-24 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/workspace

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Access Google AI Studio with your Workspace account

All [Google Workspace](https://workspace.google.com) users have access to AI Studio by default. If you're a Workspace user and you want to get started with AI Studio, check out the [AI Studio quickstart](/gemini-api/docs/ai-studio-quickstart).

## Troubleshooting

If access to AI Studio is disabled for your Google Workspace account, you might see an error like the following:

`We are sorry, but you do not have access to Google AI Studio. Please contact your Organization Administrator for access.`

If you think you should have access to AI Studio, contact your Workspace administrator.

## Enable AI Studio for Workspace users

As a Google Workspace administrator, you can control who uses AI Studio:

*   AI Studio is turned on by default for all editions.
*   You can turn AI Studio off or on for sets of users across or within organizational units.
*   Google Workspace for Education editions: Users under the age of 18 are restricted from using AI Studio with their Google Workspace for Education accounts. This is true even when the AI Studio setting is on. For details, go to [Control access to Google services by age](https://support.google.com/a/answer/10651918).

To enable or disable AI Studio for users in your organization, see [Turn Google AI Studio on or off for users](https://support.google.com/a/answer/15004095).

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-02-25 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/available-regions

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# Google AI Studio 和 Gemini API 的适用区域

如果您在尝试打开 [Google AI Studio](https://makersuite.google.com/?hl=zh-cn) 后看到此页面，可能是因为 Google AI Studio 未在您所在的地区提供，或者您不符合访问该服务的年龄要求（18 周岁以上）。如需详细了解支持的地区，请参阅下一部分；如需了解其他要求，请参阅[服务条款](https://ai.google.dev/gemini-api/terms?hl=zh-cn)。

## 可用区域

检查 **Colab 实例**的位置

Gemini API 和 Google AI Studio 已在以下国家/地区推出。如果您不在以下国家/地区，请尝试使用 [Vertex AI 中的 Gemini API](https://cloud.google.com/vertex-ai?hl=zh-cn#build-with-gemini)：

*   阿尔巴尼亚
*   阿尔及利亚
*   美属萨摩亚
*   安哥拉
*   安圭拉
*   南极洲
*   安提瓜和巴布达
*   阿根廷
*   亚美尼亚
*   阿鲁巴
*   澳大利亚
*   奥地利
*   阿塞拜疆
*   巴哈马
*   巴林
*   孟加拉国
*   巴巴多斯
*   比利时
*   伯利兹
*   贝宁
*   百慕大
*   不丹
*   玻利维亚
*   波斯尼亚
*   博茨瓦纳
*   巴西
*   英属印度洋领地
*   英属维尔京群岛
*   文莱
*   保加利亚
*   布基纳法索
*   布隆迪
*   佛得角
*   柬埔寨
*   喀麦隆
*   加拿大
*   荷属加勒比地区
*   开曼群岛
*   中非共和国
*   乍得
*   智利
*   圣诞岛
*   科科斯（基林）群岛
*   哥伦比亚
*   科摩罗
*   库克群岛
*   科特迪瓦
*   哥斯达黎加
*   克罗地亚
*   库拉索
*   捷克
*   刚果民主共和国
*   丹麦
*   吉布提
*   多米尼加
*   多米尼加共和国
*   厄瓜多尔
*   埃及
*   萨尔瓦多
*   赤道几内亚
*   厄立特里亚
*   爱沙尼亚
*   斯威士兰
*   埃塞俄比亚
*   福克兰群岛（马尔维纳斯群岛）
*   法罗群岛
*   斐济
*   芬兰
*   法国
*   加蓬
*   冈比亚
*   格鲁吉亚
*   德国
*   加纳
*   直布罗陀
*   希腊
*   格陵兰
*   格林纳达
*   关岛
*   危地马拉
*   根西
*   几内亚
*   几内亚比绍
*   圭亚那
*   海地
*   赫德岛和麦克唐纳群岛
*   黑塞哥维那
*   洪都拉斯
*   匈牙利
*   冰岛
*   印度
*   印度尼西亚
*   伊拉克
*   爱尔兰
*   马恩岛
*   以色列
*   意大利
*   牙买加
*   日本
*   泽西岛
*   约旦
*   哈萨克斯坦
*   肯尼亚
*   基里巴斯
*   科索沃
*   吉尔吉斯斯坦
*   科威特
*   老挝
*   拉脱维亚
*   黎巴嫩
*   莱索托
*   利比里亚
*   利比亚
*   列支敦士登
*   立陶宛
*   卢森堡
*   马达加斯加
*   马拉维
*   马来西亚
*   马尔代夫
*   马里
*   马耳他
*   马绍尔群岛
*   毛里塔尼亚
*   毛里求斯
*   墨西哥
*   密克罗尼西亚
*   蒙古
*   黑山
*   蒙特塞拉特
*   摩洛哥
*   莫桑比克
*   纳米比亚
*   瑙鲁
*   尼泊尔
*   荷兰
*   新喀里多尼亚
*   新西兰
*   尼加拉瓜
*   尼日尔
*   尼日利亚
*   纽埃
*   诺福克岛
*   北马其顿
*   北马里亚纳群岛
*   挪威
*   阿曼
*   巴基斯坦
*   帕劳
*   巴勒斯坦
*   巴拿马
*   巴布亚新几内亚
*   巴拉圭
*   秘鲁
*   菲律宾
*   皮特凯恩群岛
*   波兰
*   葡萄牙
*   波多黎各
*   卡塔尔
*   塞浦路斯共和国
*   刚果共和国
*   罗马尼亚
*   卢旺达
*   圣巴泰勒米岛
*   圣基茨和尼维斯
*   圣卢西亚
*   圣皮埃尔和密克隆
*   圣文森特和格林纳丁斯
*   圣赫勒拿岛、阿森松岛和特里斯坦-达库尼亚群岛
*   萨摩亚
*   圣多美和普林西比
*   沙特阿拉伯
*   塞内加尔
*   塞尔维亚
*   塞舌尔
*   塞拉利昂
*   新加坡
*   斯洛伐克
*   斯洛文尼亚
*   所罗门群岛
*   索马里
*   南非
*   南乔治亚和南桑德威奇群岛
*   韩国
*   南苏丹
*   西班牙
*   斯里兰卡
*   苏丹
*   苏里南
*   瑞典
*   瑞士
*   台湾
*   塔吉克斯坦
*   坦桑尼亚
*   泰国
*   东帝汶
*   多哥
*   托克劳
*   汤加
*   特立尼达和多巴哥
*   突尼斯
*   土耳其
*   土库曼斯坦
*   特克斯和凯科斯群岛
*   图瓦卢
*   乌干达
*   乌克兰
*   英国
*   阿拉伯联合酋长国
*   美国
*   美国本土外小岛屿
*   美属维尔京群岛
*   乌拉圭
*   乌兹别克斯坦
*   瓦努阿图
*   委内瑞拉
*   越南
*   瓦利斯和福图纳
*   西撒哈拉
*   也门
*   赞比亚
*   津巴布韦

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-02-11。

---

# Content from: https://ai.google.dev/gemini-api/docs/model-tuning/tutorial

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Fine-tuning with the Gemini API

With the deprecation of Gemini 1.5 Flash-001 in May 2025, we no longer have a model available which supports fine-tuning in the Gemini API, but it is supported in [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-use-supervised-tuning).

We plan to bring fine-tuning support back in the future. We would love to [hear from you on our developer forum](https://discuss.ai.google.dev/c/gemini-api/4) if fine-tuning is important to your use case.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-30 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/oauth

Presentamos las actualizaciones de nuestra familia de modelos de pensamiento 2.5. [Más información](https://ai.google.dev/gemini-api/docs/models?hl=es-419)

![](https://ai.google.dev/_static/images/translated.svg?hl=es-419) Se usó la [API de Cloud Translation](//cloud.google.com/translate/?hl=es-419) para traducir esta página.

*   [Página principal](https://ai.google.dev/?hl=es-419)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=es-419)
*   [Modelos](https://ai.google.dev/gemini-api/docs?hl=es-419)

Enviar comentarios

# Guía de inicio rápido de Authentication con OAuth

La forma más sencilla de autenticarse en la API de Gemini es configurar una clave de API, como se describe en la [guía de inicio rápido de la API de Gemini](https://ai.google.dev/gemini-api/docs/quickstart?hl=es-419). Si necesitas controles de acceso más estrictos, puedes usar OAuth en su lugar. Esta guía te ayudará a configurar la autenticación con OAuth.

En esta guía, se usa un enfoque de autenticación simplificado que es apropiado para un entorno de pruebas. En el caso de un entorno de producción, obtén información sobre la [autenticación y la autorización](https://developers.google.com/workspace/guides/auth-overview?hl=es-419) antes de [elegir las credenciales de acceso](https://developers.google.com/workspace/guides/create-credentials?hl=es-419#choose_the_access_credential_that_is_right_for_you) adecuadas para tu app.

## Objetivos

*   Configura tu proyecto de Cloud para OAuth
*   Configura application-default-credentials
*   Administra las credenciales en tu programa en lugar de usar `gcloud auth`

## Requisitos previos

Para ejecutar esta guía de inicio rápido, necesitas lo siguiente:

*   [Un proyecto de Google Cloud](https://developers.google.com/workspace/guides/create-project?hl=es-419)
*   [Una instalación local de la CLI de gcloud](https://cloud.google.com/sdk/docs/install?hl=es-419)

## Configura tu proyecto de Cloud

Para completar esta guía de inicio rápido, primero debes configurar tu proyecto de Cloud.

### 1\. Habilita la API

Antes de usar las APIs de Google, debes activarlas en un proyecto de Google Cloud.

*   En la consola de Google Cloud, habilita la API de Google Generative Language.  
    
    [Habilitar la API](https://console.cloud.google.com/flows/enableapi?apiid=generativelanguage.googleapis.com&hl=es-419)
    

### 2\. Cómo configurar la pantalla de consentimiento de OAuth

A continuación, configura la pantalla de consentimiento de OAuth del proyecto y agrégate como usuario de prueba. Si ya completaste este paso para tu proyecto de Cloud, ve a la próxima sección.

1.  En la consola de Google Cloud, ve a **Menú** > \> **Descripción general**.
    
    Ve a
    
2.  Completa el formulario de configuración del proyecto y establece el tipo de usuario como **Externo** en la sección **Público**.
    
3.  Completa el resto del formulario, acepta las condiciones de la Política de Datos del Usuario y, luego, haz clic en **Crear**.
    
4.  Por ahora, puedes omitir agregar permisos y hacer clic en **Guardar y continuar**. En el futuro, cuando crees una app para usarla fuera de tu organización de Google Workspace, deberás agregar y verificar los permisos de autorización que requiere tu app.
    
5.  Agrega usuarios de prueba:
    
    1.  Navega a la de.
    2.  En **Usuarios de prueba**, haz clic en **Agregar usuarios**.
    3.  Ingresa tu dirección de correo electrónico y los demás usuarios de prueba autorizados, y haz clic en **Guardar**.

### 3\. Autoriza credenciales para una aplicación de escritorio

Para autenticarte como usuario final y acceder a los datos del usuario en tu app, debes crear uno o más IDs de cliente de OAuth 2.0. Un ID de cliente se usa con el fin de identificar una sola app para los servidores de OAuth de Google. Si la app se ejecuta en varias plataformas, debes crear un ID de cliente independiente para cada una.

1.  En la consola de Google Cloud, ve a **Menú** > \> **Clientes**.
    
    Ir a Credenciales
    
2.  Haz clic en **Crear cliente**.
    
3.  Haz clic en **Tipo de aplicación** > **App de escritorio**.
    
4.  En el campo **Nombre**, escribe un nombre para la credencial. Este nombre solo se muestra en la consola de Google Cloud.
    
5.  Haz clic en **Crear**. Aparecerá la pantalla Se creó el cliente de OAuth, que muestra tu nuevo ID de cliente y secreto de cliente.
    
6.  Haz clic en **Aceptar**. La credencial recién creada aparecerá en **IDs de cliente de OAuth 2.0.**
    
7.  Haz clic en el botón de descarga para guardar el archivo JSON. Se guardará como `client_secret_<identifier>.json`, cámbiale el nombre a `client_secret.json` y muévelo a tu directorio de trabajo.
    

## Configura credenciales predeterminadas de la aplicación

Para convertir el archivo `client_secret.json` en credenciales utilizables, pasa su ubicación al argumento `--client-id-file` del comando `gcloud auth application-default login`.

```
gcloud auth application-default login \
    --client-id-file=client_secret.json \
    --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'
```

La configuración de proyecto simplificada en este instructivo activa el diálogo **"Google no verificó esta app"**. Esto es normal. Elige **“Continuar”**.

Esto coloca el token resultante en una ubicación conocida para que `gcloud` o las bibliotecas cliente puedan acceder a él.

`gcloud auth application-default login        **--no-browser**     --client-id-file=client_secret.json        --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'`

Una vez que hayas configurado las credenciales predeterminadas de la aplicación (ADC), las bibliotecas cliente en la mayoría de los lenguajes necesitan ayuda mínima o ninguna para encontrarlas.

### Curl

La forma más rápida de probar que esto funciona es usarla para acceder a la API de REST con curl:

```
access_token=$(gcloud auth application-default print-access-token)
project_id=<MY PROJECT ID>
```

### Python

En Python, las bibliotecas cliente deberían encontrarlas automáticamente:

```
pip install google-generativeai
```

Una secuencia de comandos mínima para probarlo podría ser la siguiente:

```
import google.generativeai as genai

print('Available base models:', [m.name for m in genai.list_models()])
```

## Próximos pasos

Si funciona, puedes probar la [recuperación semántica en tus datos de texto](https://ai.google.dev/docs/semantic_retriever?hl=es-419).

## Administra tus credenciales por tu cuenta \[Python\]

En muchos casos, no tendrás disponible el comando `gcloud` para crear el token de acceso a partir del ID de cliente (`client_secret.json`). Google proporciona bibliotecas en muchos lenguajes para permitirte administrar ese proceso dentro de tu app. En esta sección, se muestra el proceso en Python. Hay ejemplos equivalentes de este tipo de procedimiento para otros idiomas disponibles en la [documentación de la API de Drive](https://developers.google.com/drive/api/quickstart/python?hl=es-419).

### 1\. Instala las bibliotecas necesarias

Instala la biblioteca cliente de Google para Python y la biblioteca cliente de Gemini.

```
pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib
```

### 2\. Escribe el administrador de credenciales

Para minimizar la cantidad de veces que tienes que hacer clic en las pantallas de autorización, crea un archivo llamado `load_creds.py` en tu directorio de trabajo para almacenar en caché un archivo `token.json` que se pueda volver a usar más adelante o actualizar si vence.

Comienza con el siguiente código para convertir el archivo `client_secret.json` en un token que se pueda usar con `genai.configure`:

```
import os.path

from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow

SCOPES = ['https://www.googleapis.com/auth/generative-language.retriever']

def load_creds():
    """Converts `client_secret.json` to a credential object.

    This function caches the generated tokens to minimize the use of the
    consent screen.
    """
    creds = None
    # The file token.json stores the user's access and refresh tokens, and is
    # created automatically when the authorization flow completes for the first
    # time.
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    # If there are no (valid) credentials available, let the user log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                'client_secret.json', SCOPES)
            creds = flow.run_local_server(port=0)
        # Save the credentials for the next run
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    return creds
```

### 3\. Escribe tu programa

Ahora crea tu `script.py`:

```
import pprint
import google.generativeai as genai
from load_creds import load_creds

creds = load_creds()

genai.configure(credentials=creds)

print()
print('Available base models:', [m.name for m in genai.list_models()])
```

### 4\. Ejecuta tu programa

En tu directorio de trabajo, ejecuta la muestra:

```
python script.py
```

La primera vez que ejecutas la secuencia de comandos, se abre una ventana del navegador y se te solicita que autorices el acceso.

1.  Si aún no accediste a tu Cuenta de Google, se te solicitará que lo hagas. Si accediste a varias cuentas, **asegúrate de seleccionar la cuenta que configuraste como "Cuenta de prueba" cuando configures tu proyecto.**
    
2.  La información de autorización se almacena en el sistema de archivos, por lo que la próxima vez que ejecutes el código de muestra, no se te solicitará autorización.
    

Configuraste correctamente la autenticación.

Enviar comentarios

Salvo que se indique lo contrario, el contenido de esta página está sujeto a la [licencia Atribución 4.0 de Creative Commons](https://creativecommons.org/licenses/by/4.0/), y los ejemplos de código están sujetos a la [licencia Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Para obtener más información, consulta las [políticas del sitio de Google Developers](https://developers.google.com/site-policies?hl=es-419). Java es una marca registrada de Oracle o sus afiliados.

Última actualización: 2025-05-12 (UTC)

---

# Content from: https://ai.google.dev/gemini-api/docs/downloads

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini API libraries

This page provides information on downloading and installing the latest libraries for the Gemini API. If you're new to the Gemini API, get started with the [API quickstart](/gemini-api/docs/quickstart).

## Important note about our new libraries

Since late 2024, we have been progressively rolling out the **Google Gen AI SDK**, a new set of libraries that provide a unified interface for accessing Google's generative AI models through both the Gemini Developer API and Vertex AI.

The Google Gen AI SDK is now the recommended pathway for accessing all of Google's models and all code snippets in our documentation utilize these libraries.

**Key Library Updates**

  

Language

New library (Recommended)

Old library

**Python**

`google-genai`

`google-generativeai`

**JavaScript  
and TypeScript**

`@google/genai`

`@google/generative-ai`

**Go**

`google.golang.org/genai`

`google.golang.org/generative-ai`

**Java**

`google-genai`

N/A, did not exist before

We encourage all users to [migrate to the Google GenAI SDK](/gemini-api/docs/migrate), as it provides access to new features like multi-modal outputs (e.g., image and video generation), the Live API, and long term maintenance. We expect the feature gap between the old and new SDK to grow over time.

## Install a library

### Python

You can install our [Python library](https://pypi.org/project/google-genai) by running:

```
pip install google-genai
```

### JavaScript

You can install our [JavaScript and TypeScript library](https://www.npmjs.com/package/@google/genai) by running:

```
npm install @google/genai
```

### Go

You can install our [Go library](https://pkg.go.dev/google.golang.org/genai) by running:

```
go get google.golang.org/genai
```

### Java

If you're using Maven, you can install our [Java library](https://github.com/googleapis/java-genai) by adding the following to your dependencies:

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

## Previous libraries and SDKs

For platforms where a new library is available, we strongly recommend [migrating to the new libraries](/gemini-api/docs/migrate). Previous libraries won't receive new features and their specific support status varies, as detailed below.

For platforms where an official Google Gen AI library is not yet available, you can call our APIs directly, use community-maintained libraries, or use trusted third-party options. We are actively working to expand our official SDK coverage and will list new libraries here as they are released.

The following are the previous libraries:

*   [Python (previous)](https://github.com/google-gemini/deprecated-generative-ai-python): All support, including bug fixes, ends September 2025.
*   [Node.js (previous)](https://github.com/google-gemini/generative-ai-js): All support, including bug fixes, ends September 2025.
*   [Go (previous)](https://github.com/google/generative-ai-go): All support, including bug fixes, ends September 2025.
*   [Dart and Flutter (Previous)](https://pub.dev/packages/google_generative_ai/install): Not actively maintained
*   [Swift (Previous)](https://github.com/google/generative-ai-swift): Not actively maintained
*   [Android (Previous)](https://github.com/google-gemini/generative-ai-android): Not actively maintained

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-11 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/models/gemini

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# Gemini 模型

2.5 Pro spark

我们最强大的思考型模型，回答准确性最高，性能最优

*   输入音频、图片、视频和文本，获取文本回答
*   解决棘手问题、分析大型数据库等
*   最适合进行复杂的编码、推理和多模态理解

[](#gemini-2.5-pro)

2.5 Flash spark

性价比最高的模型，提供全面的功能。

*   输入音频、图片、视频和文本，并获取文本回答
*   模型会根据需要进行思考；或者，您可以配置思考预算
*   最适合需要思考且低延迟的高数据量任务

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

一种 Gemini 2.5 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

*   输入音频、图片、视频和文本，并获取文本回答
*   最具成本效益且支持高吞吐量的模型
*   最适合实时低延迟应用场景

[](#gemini-2.5-flash-lite)

## 模型变体

Gemini API 提供针对特定应用场景进行了优化的不同模型。下面简要介绍了可用的 Gemini 变体：

模型变体

输入

输出

优化目标

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

音频、图片、视频、文本和 PDF

文本

增强的思考和推理能力、多模态理解、高级编码等

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

音频、图片、视频和文本

文本

自适应思维，成本效益高

[Gemini 2.5 Flash-Lite 预览版](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

文本、图片、视频、音频

文本

最具成本效益且支持高吞吐量的模型

[Gemini 2.5 Flash 原生音频](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog` 和  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

音频、视频和文本

文本和音频交错

无论是否思考，都能获得高质量、自然流畅的对话式音频输出

[Gemini 2.5 Flash 预览版 TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

文本

音频

低延迟、可控的单语音和多语音文字转语音音频生成

[Gemini 2.5 Pro 预览版 TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

文本

音频

低延迟、可控的单语音和多语音文字转语音音频生成

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

音频、图片、视频和文本

文本

新一代功能、速度和实时在线播放。

[Gemini 2.0 Flash 预览图片生成](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

音频、图片、视频和文本

文字、图片

对话式图片生成和编辑

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

音频、图片、视频和文本

文本

成本效益高且延迟时间短

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

音频、图片、视频和文本

文本

在各种任务中提供快速、多样化的性能

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

音频、图片、视频和文本

文本

量大且智能程度较低的任务

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

音频、图片、视频和文本

文本

需要更多智能的复杂推理任务

[Gemini 嵌入](#gemini-embedding)  
`gemini-embedding-exp`

文本

文本嵌入

衡量文本字符串的相关性

[图片 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

文本

图片

我们最新的图片生成模型

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

文本

图片

高质量图片生成模型

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

文字、图片

视频

生成高质量视频

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

音频、视频和文本

文字、音频

低延迟的双向语音和视频互动

[Gemini 2.0 Flash 直播](#live-api-2.0)  
`gemini-2.0-flash-live-001`

音频、视频和文本

文字、音频

低延迟的双向语音和视频互动

您可以在[速率限制页面](https://ai.google.dev/gemini-api/docs/rate-limits?hl=zh-cn)上查看每种模型的速率限制。

### Gemini 2.5 Pro

Gemini 2.5 Pro 是我们最先进的思考型模型，能够推理编码、数学和 STEM 领域的复杂问题，还能使用长上下文分析大型数据集、代码库和文档。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-pro&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`gemini-2.5-pro`

保存支持的数据类型

**输入源**

音频、图片、视频、文本和 PDF

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

65536

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**利用 Google 搜索建立回答依据**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

**思考**

支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash

性价比最高的模型，提供全面的功能。2.5 Flash 最适合用于大规模处理、需要思考的低延迟高数据量任务，以及代理用例。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash`

保存支持的数据类型

**输入源**

文本、图片、视频、音频

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

65536

handyman 功能

**音频生成**

不受支持

**缓存**

支持

**代码执行**

支持

**函数调用**

支持

**图片生成**

不受支持

**利用 Google 搜索建立回答依据**

支持

**结构化输出**

支持

**思考**

支持

**调音**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   稳定：`gemini-2.5-flash`
*   预览：`gemini-2.5-flash-preview-05-20`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash-Lite 预览版

一种 Gemini 2.5 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-lite-preview-06-17`

保存支持的数据类型

**输入源**

文本、图片、视频和音频

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,000,000

**输出 token 限制**

64,000

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**网址情境**

支持

**利用 Google 搜索建立回答依据**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

**思考**

支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.5-flash-lite-preview-06-17`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash 原生音频

我们的原生音频对话模型（有思考和无思考），可通过 [Live API](https://ai.google.dev/gemini-api/docs/live?hl=zh-cn) 使用。这些模型可提供交互式和非结构化对话式体验，并提供样式和控制提示。

[在 Google AI Studio 中试用原生音频](https://aistudio.google.com/app/live?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-preview-native-audio-dialog` 和  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

保存支持的数据类型

**输入源**

音频、视频、文字

**输出**

音频和文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

128,000

**输出 token 限制**

8000

handyman 功能

**音频生成**

支持

**缓存**

不受支持

**代码执行**

不受支持

**函数调用**

支持

**图片生成**

不受支持

**利用 Google 搜索建立回答依据**

支持

**结构化输出**

不受支持

**思考**

支持

**调音**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.5-flash-preview-05-20`
*   实验性：`gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_month最新更新

2025 年 5 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash 预览版文本转语音

Gemini 2.5 Flash 预览版 TTS 是我们性价比最高的语音合成模型，可为播客生成、有声读物、客户服务等结构化工作流提供高度控制和透明度。 Gemini 2.5 Flash 是实验性/预览版模型，因此其速率限制更为严格。

[在 Google AI Studio 中试用](https://aistudio.google.com/generate-speech?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-preview-tts`

保存支持的数据类型

**输入源**

文本

**输出**

音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8000

**输出 token 限制**

16,000

handyman 功能

**结构化输出**

不受支持

**缓存**

不受支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不受支持

**搜索**

不受支持

**音频生成**

支持

**Live API**

不受支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `gemini-2.5-flash-preview-tts`

calendar\_month最新更新

2025 年 5 月

### Gemini 2.5 Pro 预览版文本转语音

Gemini 2.5 Pro 预览版 TTS 是我们最强大的文本转语音模型，可为播客生成、有声读物、客户服务等结构化工作流提供高度控制和透明度。 Gemini 2.5 Pro 是实验性/预览版模型，因此速率限制更为严格。

[在 Google AI Studio 中试用](https://aistudio.google.com/generate-speech?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-pro-preview-tts`

保存支持的数据类型

**输入源**

文本

**输出**

音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8000

**输出 token 限制**

16,000

handyman 功能

**结构化输出**

不受支持

**缓存**

不受支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不受支持

**搜索**

不受支持

**音频生成**

支持

**Live API**

不受支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `gemini-2.5-pro-preview-tts`

calendar\_month最新更新

2025 年 5 月

### Gemini 2.0 Flash

Gemini 2.0 Flash 提供新一代功能和增强型功能，包括更快的速度、原生工具使用体验，以及 100 万个 token 的上下文窗口。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-001&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

支持

**思考**

实验性

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-2.0-flash`
*   稳定：`gemini-2.0-flash-001`
*   实验性：`gemini-2.0-flash-exp`

calendar\_month最新更新

2025 年 2 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 2.0 Flash 预览图片生成

Gemini 2.0 Flash 预览版图片生成功能提供了经过改进的图片生成功能，包括通过对话生成和编辑图片。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-preview-image-generation`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文字和图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

32000

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不支持

**搜索**

不支持

**图片生成**

支持

**音频生成**

不受支持

**Live API**

不支持

**思考**

不支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.0-flash-preview-image-generation`

欧洲、中东和非洲的多个国家/地区目前不支持 gemini-2.0-flash-preview-image-generation

calendar\_month最新更新

2025 年 5 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 2.0 Flash-Lite

一种 Gemini 2.0 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-lite&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-lite`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

不受支持

**搜索**

不受支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-2.0-flash-lite`
*   稳定：`gemini-2.0-flash-lite-001`

calendar\_month最新更新

2025 年 2 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 1.5 Flash

Gemini 1.5 Flash 是一种快速且多才多艺的多模态模型，可跨多种任务进行扩缩。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-flash`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

3600

**视频时长上限**

1 小时

**音频时长上限**

大约 9.5 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-flash-latest`
*   最新稳定版：`gemini-1.5-flash`
*   稳定版：

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_month最新更新

2024 年 9 月

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B 是一款小型模型，专为处理低智能任务而设计。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-flash-8b`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

3600

**视频时长上限**

1 小时

**音频时长上限**

大约 9.5 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-flash-8b-latest`
*   最新稳定版：`gemini-1.5-flash-8b`
*   稳定版：

*   `gemini-1.5-flash-8b-001`

calendar\_month最新更新

2024 年 10 月

### Gemini 1.5 Pro

试用 [Gemini 2.5 Pro 预览版](https://ai.google.dev/gemini-api/docs/models/experimental-models?hl=zh-cn#available-models)，这是我们迄今最先进的 Gemini 模型。

Gemini 1.5 Pro 是一款中型多模态模型，经过优化，可处理各种推理任务。1.5 Pro 可以一次处理大量数据，包括 2 小时的视频、19 小时的音频、6 万行代码的代码库或 2,000 页的文本。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-pro&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-pro`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2,097,152

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

7,200

**视频时长上限**

2 小时

**音频时长上限**

大约 19 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-pro-latest`
*   最新稳定版：`gemini-1.5-pro`
*   稳定版：

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_month最新更新

2024 年 9 月

### Imagen 4

Imagen 4 是我们最新的图片模型，与之前的模型相比，能够生成光线丰富、细节丰富的图片，文字渲染效果显著提升，输出分辨率更高。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

保存支持的数据类型

**输入**

文本

**输出**

图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

480 个令牌（文本）

**输出图片**

1（超高）  
1 至 4（标准）

calendar\_month最新更新

2025 年 6 月

### Imagen 3

Imagen 3 是我们最出色的文本转图片模型，与之前的模型相比，它生成的图片细节更丰富、光线更丰富，干扰性伪影更少。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`imagen-3.0-generate-002`

保存支持的数据类型

**输入**

文本

**输出**

图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

不适用

**输出图片**

最多 4 项

calendar\_month最新更新

2025 年 2 月

### Veo 2

Veo 2 是我们的高质量文本和图片转视频模型，能够生成详细的视频，捕捉提示中的艺术细微之处。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`veo-2.0-generate-001`

保存支持的数据类型

**输入**

文字、图片

**输出**

视频

token\_auto 限制

**文本输入**

不适用

**图片输入**

任意图片分辨率和宽高比，文件大小不超过 20MB

**输出视频**

最多 2 个

calendar\_month最新更新

2025 年 4 月

### Gemini 2.5 Flash Live

Gemini 2.5 Flash Live 模型可与 Live API 搭配使用，支持与 Gemini 建立低延迟的双向语音和视频互动。该模型可以处理文本、音频和视频输入，并提供文本和音频输出。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-live-2.5-flash-preview&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-live-2.5-flash-preview`

保存支持的数据类型

**输入源**

音频、视频和文本

**输出**

文字和音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-live-2.5-flash-preview`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.0 Flash Live

Gemini 2.0 Flash Live 模型可与 Live API 搭配使用，支持与 Gemini 建立低延迟的双向语音和视频互动。该模型可以处理文本、音频和视频输入，并提供文本和音频输出。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-live-001&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-live-001`

保存支持的数据类型

**输入源**

音频、视频和文本

**输出**

文字和音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.0-flash-live-001`

calendar\_month最新更新

2025 年 4 月

cognition\_2 知识截点

2024 年 8 月

### Gemini Embedding Experimental

`Gemini embedding` 在代码、多语言和检索等许多关键维度上都取得了[世界领先的效果](https://deepmind.google/research/publications/157741/?hl=zh-cn)。 由于 Gemini 是实验性模型，因此嵌入率限制更为严格。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`gemini-embedding-exp-03-07`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8192

**输出维度大小**

弹性，支持：3072、1536 或 768

calendar\_month最新更新

2025 年 3 月

### 文本嵌入和嵌入

#### 文本嵌入

试用我们全新的[实验性 Gemini 嵌入模型](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/)，该模型可实现最先进的性能。

[文本嵌入](https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-cn)用于衡量字符串的相关性，在许多 AI 应用中都得到了广泛的应用。

在标准 MTEB 嵌入基准测试中，`text-embedding-004` 在具有类似维度的情形下，[实现了更出色的检索性能，并超越了现有模型](https://arxiv.org/pdf/2403.20327)。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`models/text-embedding-004`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2048

**输出维度大小**

768

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

不受支持

calendar\_month最新更新

2024 年 4 月

#### 嵌入

您可以使用嵌入模型为输入文本生成[文本嵌入](https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-cn)。

该嵌入模型经过优化，可为最多 2,048 个令牌的文本创建 768 维的嵌入。

##### 嵌入模型详情

属性

说明

id\_card 模型代码

`models/embedding-001`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2048

**输出维度大小**

768

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

不受支持

calendar\_month最新更新

2023 年 12 月

### AQA

您可以使用 AQA 模型对文档、语料库或一组段落执行[归因式问答](https://ai.google.dev/gemini-api/docs/semantic_retrieval?hl=zh-cn) (AQA) 相关任务。AQA 模型会返回基于所提供来源的问题的答案，并估算可回答的概率。

#### 模型详情

属性

说明

id\_card 模型代码

`models/aqa`

保存支持的数据类型

**输入**

文本

**输出**

文本

language支持的语言

英语

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

7168

**输出 token 限制**

1024

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

支持

calendar\_month最新更新

2023 年 12 月

请参阅[示例](https://ai.google.dev/examples?hl=zh-cn)，探索这些模型变体的功能。

\[\*\] 对于 Gemini 模型，一个令牌相当于大约 4 个字符。100 个词元大约相当于 60-80 个英语单词。

## 模型版本名称模式

Gemini 模型有_稳定版_、_预览版_或_实验版_。在代码中，您可以使用以下任一模型名称格式来指定要使用的模型和版本。

### 最新稳定版

指向为指定的模型生成和变体发布的最新稳定版本。

如需指定最新的稳定版本，请使用以下模式：`<model>-<generation>-<variation>`。例如 `gemini-2.0-flash`。

### 稳定

指向特定的稳定型号。稳定型模型通常不会发生变化。大多数正式版应用都应使用特定的稳定型号。

如需指定稳定版本，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.0-flash-001`。

### 预览

指向预览版模型，可能不适合用于生产环境，速率限制更为严格，但可以启用结算功能。

如需指定预览版，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.5-pro-preview-06-05`。

### 实验性

指向可能不适合用于生产环境的实验性模型，并且具有更严格的速率限制。我们发布实验性模型是为了收集反馈，并让开发者能够快速获得最新的更新。

如需指定实验版本，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.0-pro-exp-02-05`。

## 实验性模型

除了稳定模型之外，Gemini API 还提供实验性模型，这些模型可能不适合用于生产环境，并且具有更严格的速率限制。

我们发布实验性模型是为了收集反馈、快速将最新动态交到开发者手中，并突出展示 Google 的创新步伐。我们会通过发布实验版积累经验，以便我们的正式版模型能更好地服务更多用户。实验性模型可以在不事先通知的情况下换成另一个模型。我们不保证实验性模型将来会成为稳定模型。

### 先前的实验性模型

随着新版本或稳定版的推出，我们会移除并替换实验性模型。您可以在下一部分中找到我们之前发布的实验性模型以及替换版本：

模型代码

基本模型

替换版本

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## 支持的语言

Gemini 模型经过训练，可处理以下语言：

*   阿拉伯语 (`ar`)
*   孟加拉语 (`bn`)
*   保加利亚语 (`bg`)
*   简体中文和繁体中文 (`zh`)
*   克罗地亚语 (`hr`)
*   捷克语 (`cs`)
*   丹麦语 (`da`)
*   荷兰语 (`nl`)
*   英语 (`en`)
*   爱沙尼亚语 (`et`)
*   芬兰语 (`fi`)
*   法语 (`fr`)
*   德语 (`de`)
*   希腊语 (`el`)
*   希伯来语 (`iw`)
*   印地语 (`hi`)
*   匈牙利语 (`hu`)
*   印度尼西亚语 (`id`)
*   意大利语 (`it`)
*   日语 (`ja`)
*   韩语 (`ko`)
*   拉脱维亚语 (`lv`)
*   立陶宛语 (`lt`)
*   挪威语 (`no`)
*   波兰语 (`pl`)
*   葡萄牙语 (`pt`)
*   罗马尼亚语 (`ro`)
*   俄语 (`ru`)
*   塞尔维亚语 (`sr`)
*   斯洛伐克语 (`sk`)
*   斯洛维尼亚语 (`sl`)
*   西班牙语 (`es`)
*   斯瓦希里语 (`sw`)
*   瑞典语 (`sv`)
*   泰语 (`th`)
*   土耳其语 (`tr`)
*   乌克兰语 (`uk`)
*   越南语 (`vi`)

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-06-26。

---

# Content from: https://ai.google.dev/gemini-api/docs/models/experimental-models

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# Gemini 模型

2.5 Pro spark

我们最强大的思考型模型，回答准确性最高，性能最优

*   输入音频、图片、视频和文本，获取文本回答
*   解决棘手问题、分析大型数据库等
*   最适合进行复杂的编码、推理和多模态理解

[](#gemini-2.5-pro)

2.5 Flash spark

性价比最高的模型，提供全面的功能。

*   输入音频、图片、视频和文本，并获取文本回答
*   模型会根据需要进行思考；或者，您可以配置思考预算
*   最适合需要思考且低延迟的高数据量任务

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

一种 Gemini 2.5 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

*   输入音频、图片、视频和文本，并获取文本回答
*   最具成本效益且支持高吞吐量的模型
*   最适合实时低延迟应用场景

[](#gemini-2.5-flash-lite)

## 模型变体

Gemini API 提供针对特定应用场景进行了优化的不同模型。下面简要介绍了可用的 Gemini 变体：

模型变体

输入

输出

优化目标

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

音频、图片、视频、文本和 PDF

文本

增强的思考和推理能力、多模态理解、高级编码等

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

音频、图片、视频和文本

文本

自适应思维，成本效益高

[Gemini 2.5 Flash-Lite 预览版](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

文本、图片、视频、音频

文本

最具成本效益且支持高吞吐量的模型

[Gemini 2.5 Flash 原生音频](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog` 和  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

音频、视频和文本

文本和音频交错

无论是否思考，都能获得高质量、自然流畅的对话式音频输出

[Gemini 2.5 Flash 预览版 TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

文本

音频

低延迟、可控的单语音和多语音文字转语音音频生成

[Gemini 2.5 Pro 预览版 TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

文本

音频

低延迟、可控的单语音和多语音文字转语音音频生成

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

音频、图片、视频和文本

文本

新一代功能、速度和实时在线播放。

[Gemini 2.0 Flash 预览图片生成](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

音频、图片、视频和文本

文字、图片

对话式图片生成和编辑

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

音频、图片、视频和文本

文本

成本效益高且延迟时间短

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

音频、图片、视频和文本

文本

在各种任务中提供快速、多样化的性能

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

音频、图片、视频和文本

文本

量大且智能程度较低的任务

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

音频、图片、视频和文本

文本

需要更多智能的复杂推理任务

[Gemini 嵌入](#gemini-embedding)  
`gemini-embedding-exp`

文本

文本嵌入

衡量文本字符串的相关性

[图片 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

文本

图片

我们最新的图片生成模型

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

文本

图片

高质量图片生成模型

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

文字、图片

视频

生成高质量视频

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

音频、视频和文本

文字、音频

低延迟的双向语音和视频互动

[Gemini 2.0 Flash 直播](#live-api-2.0)  
`gemini-2.0-flash-live-001`

音频、视频和文本

文字、音频

低延迟的双向语音和视频互动

您可以在[速率限制页面](https://ai.google.dev/gemini-api/docs/rate-limits?hl=zh-cn)上查看每种模型的速率限制。

### Gemini 2.5 Pro

Gemini 2.5 Pro 是我们最先进的思考型模型，能够推理编码、数学和 STEM 领域的复杂问题，还能使用长上下文分析大型数据集、代码库和文档。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-pro&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`gemini-2.5-pro`

保存支持的数据类型

**输入源**

音频、图片、视频、文本和 PDF

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

65536

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**利用 Google 搜索建立回答依据**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

**思考**

支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash

性价比最高的模型，提供全面的功能。2.5 Flash 最适合用于大规模处理、需要思考的低延迟高数据量任务，以及代理用例。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash`

保存支持的数据类型

**输入源**

文本、图片、视频、音频

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

65536

handyman 功能

**音频生成**

不受支持

**缓存**

支持

**代码执行**

支持

**函数调用**

支持

**图片生成**

不受支持

**利用 Google 搜索建立回答依据**

支持

**结构化输出**

支持

**思考**

支持

**调音**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   稳定：`gemini-2.5-flash`
*   预览：`gemini-2.5-flash-preview-05-20`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash-Lite 预览版

一种 Gemini 2.5 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-lite-preview-06-17`

保存支持的数据类型

**输入源**

文本、图片、视频和音频

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,000,000

**输出 token 限制**

64,000

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**网址情境**

支持

**利用 Google 搜索建立回答依据**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

**思考**

支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.5-flash-lite-preview-06-17`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash 原生音频

我们的原生音频对话模型（有思考和无思考），可通过 [Live API](https://ai.google.dev/gemini-api/docs/live?hl=zh-cn) 使用。这些模型可提供交互式和非结构化对话式体验，并提供样式和控制提示。

[在 Google AI Studio 中试用原生音频](https://aistudio.google.com/app/live?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-preview-native-audio-dialog` 和  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

保存支持的数据类型

**输入源**

音频、视频、文字

**输出**

音频和文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

128,000

**输出 token 限制**

8000

handyman 功能

**音频生成**

支持

**缓存**

不受支持

**代码执行**

不受支持

**函数调用**

支持

**图片生成**

不受支持

**利用 Google 搜索建立回答依据**

支持

**结构化输出**

不受支持

**思考**

支持

**调音**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.5-flash-preview-05-20`
*   实验性：`gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_month最新更新

2025 年 5 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash 预览版文本转语音

Gemini 2.5 Flash 预览版 TTS 是我们性价比最高的语音合成模型，可为播客生成、有声读物、客户服务等结构化工作流提供高度控制和透明度。 Gemini 2.5 Flash 是实验性/预览版模型，因此其速率限制更为严格。

[在 Google AI Studio 中试用](https://aistudio.google.com/generate-speech?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-preview-tts`

保存支持的数据类型

**输入源**

文本

**输出**

音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8000

**输出 token 限制**

16,000

handyman 功能

**结构化输出**

不受支持

**缓存**

不受支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不受支持

**搜索**

不受支持

**音频生成**

支持

**Live API**

不受支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `gemini-2.5-flash-preview-tts`

calendar\_month最新更新

2025 年 5 月

### Gemini 2.5 Pro 预览版文本转语音

Gemini 2.5 Pro 预览版 TTS 是我们最强大的文本转语音模型，可为播客生成、有声读物、客户服务等结构化工作流提供高度控制和透明度。 Gemini 2.5 Pro 是实验性/预览版模型，因此速率限制更为严格。

[在 Google AI Studio 中试用](https://aistudio.google.com/generate-speech?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-pro-preview-tts`

保存支持的数据类型

**输入源**

文本

**输出**

音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8000

**输出 token 限制**

16,000

handyman 功能

**结构化输出**

不受支持

**缓存**

不受支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不受支持

**搜索**

不受支持

**音频生成**

支持

**Live API**

不受支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `gemini-2.5-pro-preview-tts`

calendar\_month最新更新

2025 年 5 月

### Gemini 2.0 Flash

Gemini 2.0 Flash 提供新一代功能和增强型功能，包括更快的速度、原生工具使用体验，以及 100 万个 token 的上下文窗口。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-001&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

支持

**思考**

实验性

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-2.0-flash`
*   稳定：`gemini-2.0-flash-001`
*   实验性：`gemini-2.0-flash-exp`

calendar\_month最新更新

2025 年 2 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 2.0 Flash 预览图片生成

Gemini 2.0 Flash 预览版图片生成功能提供了经过改进的图片生成功能，包括通过对话生成和编辑图片。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-preview-image-generation`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文字和图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

32000

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不支持

**搜索**

不支持

**图片生成**

支持

**音频生成**

不受支持

**Live API**

不支持

**思考**

不支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.0-flash-preview-image-generation`

欧洲、中东和非洲的多个国家/地区目前不支持 gemini-2.0-flash-preview-image-generation

calendar\_month最新更新

2025 年 5 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 2.0 Flash-Lite

一种 Gemini 2.0 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-lite&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-lite`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

不受支持

**搜索**

不受支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-2.0-flash-lite`
*   稳定：`gemini-2.0-flash-lite-001`

calendar\_month最新更新

2025 年 2 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 1.5 Flash

Gemini 1.5 Flash 是一种快速且多才多艺的多模态模型，可跨多种任务进行扩缩。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-flash`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

3600

**视频时长上限**

1 小时

**音频时长上限**

大约 9.5 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-flash-latest`
*   最新稳定版：`gemini-1.5-flash`
*   稳定版：

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_month最新更新

2024 年 9 月

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B 是一款小型模型，专为处理低智能任务而设计。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-flash-8b`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

3600

**视频时长上限**

1 小时

**音频时长上限**

大约 9.5 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-flash-8b-latest`
*   最新稳定版：`gemini-1.5-flash-8b`
*   稳定版：

*   `gemini-1.5-flash-8b-001`

calendar\_month最新更新

2024 年 10 月

### Gemini 1.5 Pro

试用 [Gemini 2.5 Pro 预览版](https://ai.google.dev/gemini-api/docs/models/experimental-models?hl=zh-cn#available-models)，这是我们迄今最先进的 Gemini 模型。

Gemini 1.5 Pro 是一款中型多模态模型，经过优化，可处理各种推理任务。1.5 Pro 可以一次处理大量数据，包括 2 小时的视频、19 小时的音频、6 万行代码的代码库或 2,000 页的文本。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-pro&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-pro`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2,097,152

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

7,200

**视频时长上限**

2 小时

**音频时长上限**

大约 19 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-pro-latest`
*   最新稳定版：`gemini-1.5-pro`
*   稳定版：

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_month最新更新

2024 年 9 月

### Imagen 4

Imagen 4 是我们最新的图片模型，与之前的模型相比，能够生成光线丰富、细节丰富的图片，文字渲染效果显著提升，输出分辨率更高。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

保存支持的数据类型

**输入**

文本

**输出**

图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

480 个令牌（文本）

**输出图片**

1（超高）  
1 至 4（标准）

calendar\_month最新更新

2025 年 6 月

### Imagen 3

Imagen 3 是我们最出色的文本转图片模型，与之前的模型相比，它生成的图片细节更丰富、光线更丰富，干扰性伪影更少。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`imagen-3.0-generate-002`

保存支持的数据类型

**输入**

文本

**输出**

图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

不适用

**输出图片**

最多 4 项

calendar\_month最新更新

2025 年 2 月

### Veo 2

Veo 2 是我们的高质量文本和图片转视频模型，能够生成详细的视频，捕捉提示中的艺术细微之处。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`veo-2.0-generate-001`

保存支持的数据类型

**输入**

文字、图片

**输出**

视频

token\_auto 限制

**文本输入**

不适用

**图片输入**

任意图片分辨率和宽高比，文件大小不超过 20MB

**输出视频**

最多 2 个

calendar\_month最新更新

2025 年 4 月

### Gemini 2.5 Flash Live

Gemini 2.5 Flash Live 模型可与 Live API 搭配使用，支持与 Gemini 建立低延迟的双向语音和视频互动。该模型可以处理文本、音频和视频输入，并提供文本和音频输出。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-live-2.5-flash-preview&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-live-2.5-flash-preview`

保存支持的数据类型

**输入源**

音频、视频和文本

**输出**

文字和音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-live-2.5-flash-preview`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.0 Flash Live

Gemini 2.0 Flash Live 模型可与 Live API 搭配使用，支持与 Gemini 建立低延迟的双向语音和视频互动。该模型可以处理文本、音频和视频输入，并提供文本和音频输出。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-live-001&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-live-001`

保存支持的数据类型

**输入源**

音频、视频和文本

**输出**

文字和音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.0-flash-live-001`

calendar\_month最新更新

2025 年 4 月

cognition\_2 知识截点

2024 年 8 月

### Gemini Embedding Experimental

`Gemini embedding` 在代码、多语言和检索等许多关键维度上都取得了[世界领先的效果](https://deepmind.google/research/publications/157741/?hl=zh-cn)。 由于 Gemini 是实验性模型，因此嵌入率限制更为严格。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`gemini-embedding-exp-03-07`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8192

**输出维度大小**

弹性，支持：3072、1536 或 768

calendar\_month最新更新

2025 年 3 月

### 文本嵌入和嵌入

#### 文本嵌入

试用我们全新的[实验性 Gemini 嵌入模型](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/)，该模型可实现最先进的性能。

[文本嵌入](https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-cn)用于衡量字符串的相关性，在许多 AI 应用中都得到了广泛的应用。

在标准 MTEB 嵌入基准测试中，`text-embedding-004` 在具有类似维度的情形下，[实现了更出色的检索性能，并超越了现有模型](https://arxiv.org/pdf/2403.20327)。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`models/text-embedding-004`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2048

**输出维度大小**

768

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

不受支持

calendar\_month最新更新

2024 年 4 月

#### 嵌入

您可以使用嵌入模型为输入文本生成[文本嵌入](https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-cn)。

该嵌入模型经过优化，可为最多 2,048 个令牌的文本创建 768 维的嵌入。

##### 嵌入模型详情

属性

说明

id\_card 模型代码

`models/embedding-001`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2048

**输出维度大小**

768

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

不受支持

calendar\_month最新更新

2023 年 12 月

### AQA

您可以使用 AQA 模型对文档、语料库或一组段落执行[归因式问答](https://ai.google.dev/gemini-api/docs/semantic_retrieval?hl=zh-cn) (AQA) 相关任务。AQA 模型会返回基于所提供来源的问题的答案，并估算可回答的概率。

#### 模型详情

属性

说明

id\_card 模型代码

`models/aqa`

保存支持的数据类型

**输入**

文本

**输出**

文本

language支持的语言

英语

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

7168

**输出 token 限制**

1024

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

支持

calendar\_month最新更新

2023 年 12 月

请参阅[示例](https://ai.google.dev/examples?hl=zh-cn)，探索这些模型变体的功能。

\[\*\] 对于 Gemini 模型，一个令牌相当于大约 4 个字符。100 个词元大约相当于 60-80 个英语单词。

## 模型版本名称模式

Gemini 模型有_稳定版_、_预览版_或_实验版_。在代码中，您可以使用以下任一模型名称格式来指定要使用的模型和版本。

### 最新稳定版

指向为指定的模型生成和变体发布的最新稳定版本。

如需指定最新的稳定版本，请使用以下模式：`<model>-<generation>-<variation>`。例如 `gemini-2.0-flash`。

### 稳定

指向特定的稳定型号。稳定型模型通常不会发生变化。大多数正式版应用都应使用特定的稳定型号。

如需指定稳定版本，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.0-flash-001`。

### 预览

指向预览版模型，可能不适合用于生产环境，速率限制更为严格，但可以启用结算功能。

如需指定预览版，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.5-pro-preview-06-05`。

### 实验性

指向可能不适合用于生产环境的实验性模型，并且具有更严格的速率限制。我们发布实验性模型是为了收集反馈，并让开发者能够快速获得最新的更新。

如需指定实验版本，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.0-pro-exp-02-05`。

## 实验性模型

除了稳定模型之外，Gemini API 还提供实验性模型，这些模型可能不适合用于生产环境，并且具有更严格的速率限制。

我们发布实验性模型是为了收集反馈、快速将最新动态交到开发者手中，并突出展示 Google 的创新步伐。我们会通过发布实验版积累经验，以便我们的正式版模型能更好地服务更多用户。实验性模型可以在不事先通知的情况下换成另一个模型。我们不保证实验性模型将来会成为稳定模型。

### 先前的实验性模型

随着新版本或稳定版的推出，我们会移除并替换实验性模型。您可以在下一部分中找到我们之前发布的实验性模型以及替换版本：

模型代码

基本模型

替换版本

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## 支持的语言

Gemini 模型经过训练，可处理以下语言：

*   阿拉伯语 (`ar`)
*   孟加拉语 (`bn`)
*   保加利亚语 (`bg`)
*   简体中文和繁体中文 (`zh`)
*   克罗地亚语 (`hr`)
*   捷克语 (`cs`)
*   丹麦语 (`da`)
*   荷兰语 (`nl`)
*   英语 (`en`)
*   爱沙尼亚语 (`et`)
*   芬兰语 (`fi`)
*   法语 (`fr`)
*   德语 (`de`)
*   希腊语 (`el`)
*   希伯来语 (`iw`)
*   印地语 (`hi`)
*   匈牙利语 (`hu`)
*   印度尼西亚语 (`id`)
*   意大利语 (`it`)
*   日语 (`ja`)
*   韩语 (`ko`)
*   拉脱维亚语 (`lv`)
*   立陶宛语 (`lt`)
*   挪威语 (`no`)
*   波兰语 (`pl`)
*   葡萄牙语 (`pt`)
*   罗马尼亚语 (`ro`)
*   俄语 (`ru`)
*   塞尔维亚语 (`sr`)
*   斯洛伐克语 (`sk`)
*   斯洛维尼亚语 (`sl`)
*   西班牙语 (`es`)
*   斯瓦希里语 (`sw`)
*   瑞典语 (`sv`)
*   泰语 (`th`)
*   土耳其语 (`tr`)
*   乌克兰语 (`uk`)
*   越南语 (`vi`)

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-06-26。

---

# Content from: https://ai.google.dev/gemini-api/docs/vision

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Image understanding

Gemini models are built to be multimodal from the ground up, unlocking a wide range of image processing and computer vision tasks including but not limited to image captioning, classification, and visual question answering without having to train specialized ML models.

## Passing images to Gemini

You can provide images as input to Gemini using two methods:

*   [Passing inline image data](#inline-image): Ideal for smaller files (total request size less than 20MB, including prompts).
*   [Uploading images using the File API](#upload-image): Recommended for larger files or for reusing images across multiple requests.

### Passing inline image data

You can pass inline image data in the request to `generateContent`. You can provide image data as Base64 encoded strings or by reading local files directly (depending on the language).

The following example shows how to read an image from a local file and pass it to `generateContent` API for processing.

### Python

  ```
  from google.genai import types

  with open('path/to/small-sample.jpg', 'rb') as f:
      image_bytes = f.read()

  response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents=[
      types.Part.from_bytes(
        data=image_bytes,
        mime_type='image/jpeg',
      ),
      'Caption this image.'
    ]
  )

  print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: contents,
});
console.log(response.text);
```

### Go

```
bytes, _ := os.ReadFile("path/to/small-sample.jpg")

parts := []*genai.Part{
  genai.NewPartFromBytes(bytes, "image/jpeg"),
  genai.NewPartFromText("Caption this image."),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
IMG_PATH="/path/to/your/image1.jpg"

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
B64FLAGS="--input"
else
B64FLAGS="-w0"
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
    "contents": [{
    "parts":[
        {
            "inline_data": {
            "mime_type":"image/jpeg",
            "data": "'"$(base64 $B64FLAGS $IMG_PATH)"'"
            }
        },
        {"text": "Caption this image."},
    ]
    }]
}' 2> /dev/null
```

You can also fetch an image from a URL, convert it to bytes, and pass it to `generateContent` as shown in the following examples.

### Python

```
from google import genai
from google.genai import types

import requests

image_path = "https://goo.gle/instrument-img"
image_bytes = requests.get(image_path).content
image = types.Part.from_bytes(
  data=image_bytes, mime_type="image/jpeg"
)

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["What is this image?", image],
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

async function main() {
  const ai = new GoogleGenAI({});

  const imageUrl = "https://goo.gle/instrument-img";

  const response = await fetch(imageUrl);
  const imageArrayBuffer = await response.arrayBuffer();
  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
    {
      inlineData: {
        mimeType: 'image/jpeg',
        data: base64ImageData,
      },
    },
    { text: "Caption this image." }
  ],
  });
  console.log(result.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "io"
  "net/http"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  // Download the image.
  imageResp, _ := http.Get("https://goo.gle/instrument-img")

  imageBytes, _ := io.ReadAll(imageResp.Body)

  parts := []*genai.Part{
    genai.NewPartFromBytes(imageBytes, "image/jpeg"),
    genai.NewPartFromText("Caption this image."),
  }

  contents := []*genai.Content{
    genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    contents,
    nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMG_URL="https://goo.gle/instrument-img"

MIME_TYPE=$(curl -sIL "$IMG_URL" | grep -i '^content-type:' | awk -F ': ' '{print $2}' | sed 's/\r$//' | head -n 1)
if [[ -z "$MIME_TYPE" || ! "$MIME_TYPE" == image/* ]]; then
  MIME_TYPE="image/jpeg"
fi

# Check for macOS
if [[ "$(uname)" == "Darwin" ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -b 0)
elif [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64)
else
  IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -w0)
fi

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
            {
              "inline_data": {
                "mime_type":"'"$MIME_TYPE"'",
                "data": "'"$IMAGE_B64"'"
              }
            },
            {"text": "Caption this image."}
        ]
      }]
    }' 2> /dev/null
```

### Uploading images using the File API

For large files or to be able to use the same image file repeatedly, use the Files API. The following code uploads an image file and then uses the file in a call to `generateContent`. See the [Files API guide](/gemini-api/docs/files) for more information and examples.

### Python

```
from google import genai

client = genai.Client()

my_file = client.files.upload(file="path/to/sample.jpg")

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[my_file, "Caption this image."],
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.jpg",
    config: { mimeType: "image/jpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Caption this image.",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.jpg", nil)

  parts := []*genai.Part{
      genai.NewPartFromText("Caption this image."),
      genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### REST

```
IMAGE_PATH="path/to/sample.jpg"
MIME_TYPE=$(file -b --mime-type "${IMAGE_PATH}")
NUM_BYTES=$(wc -c < "${IMAGE_PATH}")
DISPLAY_NAME=IMAGE

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq -r ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
          {"text": "Caption this image."}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Prompting with multiple images

You can provide multiple images in a single prompt by including multiple image `Part` objects in the `contents` array. These can be a mix of inline data (local files or URLs) and File API references.

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

# Upload the first image
image1_path = "path/to/image1.jpg"
uploaded_file = client.files.upload(file=image1_path)

# Prepare the second image as inline data
image2_path = "path/to/image2.png"
with open(image2_path, 'rb') as f:
    img2_bytes = f.read()

# Create the prompt with text and multiple images
response = client.models.generate_content(

    model="gemini-2.5-flash",
    contents=[
        "What is different between these two images?",
        uploaded_file,  # Use the uploaded file reference
        types.Part.from_bytes(
            data=img2_bytes,
            mime_type='image/png'
        )
    ]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({});

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images

  const response = await ai.models.generateContent({

    model: "gemini-2.5-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
// Upload the first image
image1Path := "path/to/image1.jpg"
uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

// Prepare the second image as inline data
image2Path := "path/to/image2.jpeg"
imgBytes, _ := os.ReadFile(image2Path)

parts := []*genai.Part{
  genai.NewPartFromText("What is different between these two images?"),
  genai.NewPartFromBytes(imgBytes, "image/jpeg"),
  genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
}

contents := []*genai.Content{
  genai.NewContentFromParts(parts, genai.RoleUser),
}

result, _ := client.Models.GenerateContent(
  ctx,
  "gemini-2.5-flash",
  contents,
  nil,
)

fmt.Println(result.Text())
```

### REST

```
# Upload the first image
IMAGE1_PATH="path/to/image1.jpg"
MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
DISPLAY_NAME1=IMAGE1

tmp_header_file1=upload-header1.tmp

curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D upload-header1.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file1}"

curl "${upload_url1}" \
  -H "Content-Length: ${NUM1_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

file1_uri=$(jq ".file.uri" file_info1.json)
echo file1_uri=$file1_uri

# Prepare the second image (inline)
IMAGE2_PATH="path/to/image2.png"
MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi
IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

# Now generate content using both images
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "What is different between these two images?"},
          {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
          {
            "inline_data": {
              "mime_type":"'"${MIME2_TYPE}"'",
              "data": "'"$IMAGE2_BASE64"'"
            }
          }
        ]
      }]
    }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Object detection

From Gemini 2.0 onwards, models are further trained to detect objects in an image and get their bounding box coordinates. The coordinates, relative to image dimensions, scale to \[0, 1000\]. You need to descale these coordinates based on your original image size.

### Python

```
from google import genai
from google.genai import types
from PIL import Image
import json

client = genai.Client()
prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

image = Image.open("/path/to/image.png")

config = types.GenerateContentConfig(
  response_mime_type="application/json"
  ) 

response = client.models.generate_content(model="gemini-2.5-flash",
                                          contents=[image, prompt],
                                          config=config
                                          )

width, height = image.size
bounding_boxes = json.loads(response.text)

converted_bounding_boxes = []
for bounding_box in bounding_boxes:
    abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
    abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
    abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
    abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
    converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

print("Image size: ", width, height)
print("Bounding boxes:", converted_bounding_boxes)

```

For more examples, check following cookbooks:

*   [2D spatial understanding notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb)
*   [Experimental 3D pointing notebook](https://github.com/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb)

## Segmentation

Starting with Gemini 2.5, models not only detect items but also segment them and provide their contour masks.

The model predicts a JSON list, where each item represents a segmentation mask. Each item has a bounding box ("`box_2d`") in the format `[y0, x0, y1, x1]` with normalized coordinates between 0 and 1000, a label ("`label`") that identifies the object, and finally the segmentation mask inside the bounding box, as base64 encoded png that is a probability map with values between 0 and 255. The mask needs to be resized to match the bounding box dimensions, then binarized at your confidence threshold (127 for the midpoint).

### Python

```
from google import genai
from google.genai import types
from PIL import Image, ImageDraw    
import io
import base64
import json
import numpy as np
import os    

client = genai.Client()

def parse_json(json_output: str):
  # Parsing out the markdown fencing
  lines = json_output.splitlines()
  for i, line in enumerate(lines):
      if line == "```json":
          json_output = "\n".join(lines[i+1:])  # Remove everything before "```json"
          json_output = json_output.split("```")[0]  # Remove everything after the closing "```"
          break  # Exit the loop once "```json" is found
  return json_output

def extract_segmentation_masks(image_path: str, output_dir: str = "segmentation_outputs"):
  # Load and resize image
  im = Image.open(image_path)
  im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)

  prompt = """
  Give the segmentation masks for the wooden and glass items.
  Output a JSON list of segmentation masks where each entry contains the 2D
  bounding box in the key "box_2d", the segmentation mask in key "mask", and
  the text label in the key "label". Use descriptive labels.
  """

  config = types.GenerateContentConfig(      
    thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection
  ) 

  response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[prompt, im], # Pillow images can be directly passed as inputs (which will be converted by the SDK)
    config=config
  )

  # Parse JSON response
  items = json.loads(parse_json(response.text))

  # Create output directory
  os.makedirs(output_dir, exist_ok=True)

  # Process each mask
  for i, item in enumerate(items):
      # Get bounding box coordinates
      box = item["box_2d"]
      y0 = int(box[0] / 1000 * im.size[1])
      x0 = int(box[1] / 1000 * im.size[0])
      y1 = int(box[2] / 1000 * im.size[1])
      x1 = int(box[3] / 1000 * im.size[0])

      # Skip invalid boxes
      if y0 >= y1 or x0 >= x1:
          continue

      # Process mask
      png_str = item["mask"]
      if not png_str.startswith("data:image/png;base64,"):
          continue

      # Remove prefix
      png_str = png_str.removeprefix("data:image/png;base64,")
      mask_data = base64.b64decode(png_str)
      mask = Image.open(io.BytesIO(mask_data))

      # Resize mask to match bounding box
      mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)

      # Convert mask to numpy array for processing
      mask_array = np.array(mask)

      # Create overlay for this mask
      overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))
      overlay_draw = ImageDraw.Draw(overlay)

      # Create overlay for the mask
      color = (255, 255, 255, 200)
      for y in range(y0, y1):
          for x in range(x0, x1):
              if mask_array[y - y0, x - x0] > 128:  # Threshold for mask
                  overlay_draw.point((x, y), fill=color)

      # Save individual mask and its overlay
      mask_filename = f"{item['label']}_{i}_mask.png"
      overlay_filename = f"{item['label']}_{i}_overlay.png"

      mask.save(os.path.join(output_dir, mask_filename))

      # Create and save overlay
      composite = Image.alpha_composite(im.convert('RGBA'), overlay)
      composite.save(os.path.join(output_dir, overlay_filename))
      print(f"Saved mask and overlay for {item['label']} to {output_dir}")

# Example usage
if __name__ == "__main__":
  extract_segmentation_masks("path/to/image.png")

```

Check the [segmentation example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb#scrollTo=WQJTJ8wdGOKx) in the cookbook guide for a more detailed example.

![A table with cupcakes, with the wooden and glass objects highlighted](/static/gemini-api/docs/images/segmentation.jpg)

An example segmentation output with objects and segmentation masks

## Supported image formats

Gemini supports the following image format MIME types:

*   PNG - `image/png`
*   JPEG - `image/jpeg`
*   WEBP - `image/webp`
*   HEIC - `image/heic`
*   HEIF - `image/heif`

## Capabilities

All Gemini model versions are multimodal and can be utilized in a wide range of image processing and computer vision tasks including but not limited to image captioning, visual question and answering, image classification, object detection and segmentation.

Gemini can reduce the need to use specialized ML models depending on your quality and performance requirements.

Some later model versions are specifically trained improve accuracy of specialized tasks in addition to generic capabilities:

*   **Gemini 2.0 models** are further trained to support enhanced [object detection](#object-detection).
    
*   **Gemini 2.5 models** are further trained to support enhanced [segmentation](#segmentation) in addition to [object detection](#object-detection).
    

## Limitations and key technical information

### File limit

Gemini 2.5 Pro/Flash, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a maximum of 3,600 image files per request.

### Token calculation

*   **Gemini 1.5 Flash and Gemini 1.5 Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized to 768x768), with each tile costing 258 tokens.
*   **Gemini 2.0 Flash and Gemini 2.5 Flash/Pro**: 258 tokens if both dimensions <= 384 pixels. Larger images are tiled into 768x768 pixel tiles, each costing 258 tokens.

## Tips and best practices

*   Verify that images are correctly rotated.
*   Use clear, non-blurry images.
*   When using a single image with text, place the text prompt _after_ the image part in the `contents` array.

## What's next

This guide shows you how to upload image files and generate text outputs from image inputs. To learn more, see the following resources:

*   [Files API](/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
*   [System instructions](/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
*   [File prompting strategies](/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
*   [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/function-calling/tutorial

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Function calling with the Gemini API

Function calling lets you connect models to external tools and APIs. Instead of generating text responses, the model determines when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:

*   **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.
*   **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.
*   **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices.

Get Weather Schedule Meeting Create Chart

## How function calling works

![function calling
overview](/static/gemini-api/docs/images/function-calling-overview.png)

Function calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:

1.  **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.
2.  **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.
3.  **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if
    *   **Yes**: Extract the name and args of the function and execute the corresponding function in your application.
    *   **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).
4.  **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.

This process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).

### Step 1: Define a function declaration

Define a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.

### Python

```
# Define a function that the model can call to control smart lights
set_light_values_declaration = {
    "name": "set_light_values",
    "description": "Sets the brightness and color temperature of a light.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "integer",
                "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
            },
            "color_temp": {
                "type": "string",
                "enum": ["daylight", "cool", "warm"],
                "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
            },
        },
        "required": ["brightness", "color_temp"],
    },
}

# This is the actual function that would be called based on the model's suggestion
def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
    """Set the brightness and color temperature of a room light. (mock API).

    Args:
        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

    Returns:
        A dictionary containing the set brightness and color temperature.
    """
    return {"brightness": brightness, "colorTemperature": color_temp}
```

### JavaScript

```
import { Type } from '@google/genai';

// Define a function that the model can call to control smart lights
const setLightValuesFunctionDeclaration = {
  name: 'set_light_values',
  description: 'Sets the brightness and color temperature of a light.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
      },
      color_temp: {
        type: Type.STRING,
        enum: ['daylight', 'cool', 'warm'],
        description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
      },
    },
    required: ['brightness', 'color_temp'],
  },
};

/**

*   Set the brightness and color temperature of a room light. (mock API)
*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
*   @return {Object} A dictionary containing the set brightness and color temperature.
*/
function setLightValues(brightness, color_temp) {
  return {
    brightness: brightness,
    colorTemperature: color_temp
  };
}
```

### Step 2: Call the model with function declarations

Once you have defined your function declarations, you can prompt the model to use them. It analyzes the prompt and function declarations and decides whether to respond directly or to call a function. If a function is called, the response object will contain a function call suggestion.

### Python

```
from google.genai import types

# Configure the client and tools
client = genai.Client()
tools = types.Tool(function_declarations=[set_light_values_declaration])
config = types.GenerateContentConfig(tools=[tools])

# Define user prompt
contents = [
    types.Content(
        role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
    )
]

# Send request with function declarations
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
    config=config,
)

print(response.candidates[0].content.parts[0].function_call)
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Generation config with function declaration
const config = {
  tools: [{
    functionDeclarations: [setLightValuesFunctionDeclaration]
  }]
};

// Configure the client
const ai = new GoogleGenAI({});

// Define user prompt
const contents = [
  {
    role: 'user',
    parts: [{ text: 'Turn the lights down to a romantic level' }]
  }
];

// Send request with function declarations
const response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(response.functionCalls[0]);
```

The model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.

### Python

```
id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'
```

### JavaScript

```
{
  name: 'set_light_values',
  args: { brightness: 25, color_temp: 'warm' }
}
```

### Step 3: Execute set\_light\_values function code

Extract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function.

### Python

```
# Extract tool call details, it may not be in the first part.
tool_call = response.candidates[0].content.parts[0].function_call

if tool_call.name == "set_light_values":
    result = set_light_values(**tool_call.args)
    print(f"Function execution result: {result}")
```

### JavaScript

```
// Extract tool call details
const tool_call = response.functionCalls[0]

let result;
if (tool_call.name === 'set_light_values') {
  result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);
  console.log(`Function execution result: ${JSON.stringify(result)}`);
}
```

### Step 4: Create user friendly response with function result and call the model again

Finally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.

### Python

```
# Create a function response part
function_response_part = types.Part.from_function_response(
    name=tool_call.name,
    response={"result": result},
)

# Append function call and result of the function execution to contents
contents.append(response.candidates[0].content) # Append the content from the model's response.
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)
```

### JavaScript

```
// Create a function response part
const function_response_part = {
  name: tool_call.name,
  response: { result }
}

// Append function call and result of the function execution to contents
contents.push(response.candidates[0].content);
contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });

// Get the final response from the model
const final_response = await ai.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: contents,
  config: config
});

console.log(final_response.text);
```

This completes the function calling flow. The model successfully used the `set_light_values` function to perform the request action of the user.

## Function declarations

When you implement function calling in a prompt, you create a `tools` object, which contains one or more `function declarations`. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemaw) format. A single function declaration can include the following parameters:

*   `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).
*   `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful ("Finds theaters based on location and optionally movie title which is currently playing in theaters.").
*   `parameters` (object): Defines the input parameters the function expects.
    *   `type` (string): Specifies the overall data type, such as `object`.
    *   `properties` (object): Lists individual parameters, each with:
        *   `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.
        *   `description` (string): A description of the parameter's purpose and format. Provide examples and constraints ("The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.").
        *   `enum` (array, optional): If the parameter values are from a fixed set, use "enum" to list the allowed values instead of just describing them in the description. This improves accuracy ("enum": \["daylight", "cool", "warm"\]).
    *   `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.

## Parallel function calling

In addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.

### Python

```
power_disco_ball = {
    "name": "power_disco_ball",
    "description": "Powers the spinning disco ball.",
    "parameters": {
        "type": "object",
        "properties": {
            "power": {
                "type": "boolean",
                "description": "Whether to turn the disco ball on or off.",
            }
        },
        "required": ["power"],
    },
}

start_music = {
    "name": "start_music",
    "description": "Play some music matching the specified parameters.",
    "parameters": {
        "type": "object",
        "properties": {
            "energetic": {
                "type": "boolean",
                "description": "Whether the music is energetic or not.",
            },
            "loud": {
                "type": "boolean",
                "description": "Whether the music is loud or not.",
            },
        },
        "required": ["energetic", "loud"],
    },
}

dim_lights = {
    "name": "dim_lights",
    "description": "Dim the lights.",
    "parameters": {
        "type": "object",
        "properties": {
            "brightness": {
                "type": "number",
                "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
            }
        },
        "required": ["brightness"],
    },
}
```

### JavaScript

```
import { Type } from '@google/genai';

const powerDiscoBall = {
  name: 'power_disco_ball',
  description: 'Powers the spinning disco ball.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      power: {
        type: Type.BOOLEAN,
        description: 'Whether to turn the disco ball on or off.'
      }
    },
    required: ['power']
  }
};

const startMusic = {
  name: 'start_music',
  description: 'Play some music matching the specified parameters.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      energetic: {
        type: Type.BOOLEAN,
        description: 'Whether the music is energetic or not.'
      },
      loud: {
        type: Type.BOOLEAN,
        description: 'Whether the music is loud or not.'
      }
    },
    required: ['energetic', 'loud']
  }
};

const dimLights = {
  name: 'dim_lights',
  description: 'Dim the lights.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      brightness: {
        type: Type.NUMBER,
        description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
      }
    },
    required: ['brightness']
  }
};
```

Configure the function calling mode to allow using all of the specified tools. To learn more, you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).

### Python

```
from google import genai
from google.genai import types

# Configure the client and tools
client = genai.Client()
house_tools = [
    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
]
config = types.GenerateContentConfig(
    tools=house_tools,
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
        disable=True
    ),
    # Force the model to call 'any' function, instead of chatting.
    tool_config=types.ToolConfig(
        function_calling_config=types.FunctionCallingConfig(mode='ANY')
    ),
)

chat = client.chats.create(model="gemini-2.5-flash", config=config)
response = chat.send_message("Turn this place into a party!")

# Print out each of the function calls requested from this single call
print("Example 1: Forced function calling")
for fn in response.function_calls:
    args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
    print(f"{fn.name}({args})")
```

### JavaScript

```
import { GoogleGenAI } from '@google/genai';

// Set up function declarations
const houseFns = [powerDiscoBall, startMusic, dimLights];

const config = {
    tools: [{
        functionDeclarations: houseFns
    }],
    // Force the model to call 'any' function, instead of chatting.
    toolConfig: {
        functionCallingConfig: {
            mode: 'any'
        }
    }
};

// Configure the client
const ai = new GoogleGenAI({});

// Create a chat session
const chat = ai.chats.create({
    model: 'gemini-2.5-flash',
    config: config
});
const response = await chat.sendMessage({message: 'Turn this place into a party!'});

// Print out each of the function calls requested from this single call
console.log("Example 1: Forced function calling");
for (const fn of response.functionCalls) {
    const args = Object.entries(fn.args)
        .map(([key, val]) => `${key}=${val}`)
        .join(', ');
    console.log(`${fn.name}(${args})`);
}
```

Each of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested.

The Python SDK supports [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only), which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.

### Python

```
from google import genai
from google.genai import types

# Actual function implementations
def power_disco_ball_impl(power: bool) -> dict:
    """Powers the spinning disco ball.

    Args:
        power: Whether to turn the disco ball on or off.

    Returns:
        A status dictionary indicating the current state.
    """
    return {"status": f"Disco ball powered {'on' if power else 'off'}"}

def start_music_impl(energetic: bool, loud: bool) -> dict:
    """Play some music matching the specified parameters.

    Args:
        energetic: Whether the music is energetic or not.
        loud: Whether the music is loud or not.

    Returns:
        A dictionary containing the music settings.
    """
    music_type = "energetic" if energetic else "chill"
    volume = "loud" if loud else "quiet"
    return {"music_type": music_type, "volume": volume}

def dim_lights_impl(brightness: float) -> dict:
    """Dim the lights.

    Args:
        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

    Returns:
        A dictionary containing the new brightness setting.
    """
    return {"brightness": brightness}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Do everything you need to this place into party!",
    config=config,
)

print("\nExample 2: Automatic function calling")
print(response.text)
# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!
```

## Compositional function calling

Compositional or sequential function calling allows Gemini to chain multiple function calls together to fulfill a complex request. For example, to answer "Get the temperature in my current location", the Gemini API might first invoke a `get_current_location()` function followed by a `get_weather()` function that takes the location as a parameter.

The following example demonstrates how to implement compositional function calling using the Python SDK and automatic function calling.

### Python

This example uses the automatic function calling feature of the `google-genai` Python SDK. The SDK automatically converts the Python functions to the required schema, executes the function calls when requested by the model, and sends the results back to the model to complete the task.

```
import os
from google import genai
from google.genai import types

# Example Functions
def get_weather_forecast(location: str) -> dict:
    """Gets the current weather temperature for a given location."""
    print(f"Tool Call: get_weather_forecast(location={location})")
    # TODO: Make API call
    print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
    return {"temperature": 25, "unit": "celsius"}  # Dummy response

def set_thermostat_temperature(temperature: int) -> dict:
    """Sets the thermostat to a desired temperature."""
    print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
    # TODO: Interact with a thermostat API
    print("Tool Response: {'status': 'success'}")
    return {"status": "success"}

# Configure the client and model
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_weather_forecast, set_thermostat_temperature]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
    config=config,
)

# Print the final, user-facing response
print(response.text)
```

**Expected Output**

When you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. I've set the thermostat to 20°C.
```

### JavaScript

This example shows how to use JavaScript/TypeScript SDK to do comopositional function calling using a manual execution loop.

```
import { GoogleGenAI, Type } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Example Functions
function get_weather_forecast({ location }) {
  console.log(`Tool Call: get_weather_forecast(location=${location})`);
  // TODO: Make API call
  console.log("Tool Response: {'temperature': 25, 'unit': 'celsius'}");
  return { temperature: 25, unit: "celsius" };
}

function set_thermostat_temperature({ temperature }) {
  console.log(
    `Tool Call: set_thermostat_temperature(temperature=${temperature})`,
  );
  // TODO: Make API call
  console.log("Tool Response: {'status': 'success'}");
  return { status: "success" };
}

const toolFunctions = {
  get_weather_forecast,
  set_thermostat_temperature,
};

const tools = [
  {
    functionDeclarations: [
      {
        name: "get_weather_forecast",
        description:
          "Gets the current weather temperature for a given location.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            location: {
              type: Type.STRING,
            },
          },
          required: ["location"],
        },
      },
      {
        name: "set_thermostat_temperature",
        description: "Sets the thermostat to a desired temperature.",
        parameters: {
          type: Type.OBJECT,
          properties: {
            temperature: {
              type: Type.NUMBER,
            },
          },
          required: ["temperature"],
        },
      },
    ],
  },
];

// Prompt for the model
let contents = [
  {
    role: "user",
    parts: [
      {
        text: "If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.",
      },
    ],
  },
];

// Loop until the model has no more function calls to make
while (true) {
  const result = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents,
    config: { tools },
  });

  if (result.functionCalls && result.functionCalls.length > 0) {
    const functionCall = result.functionCalls[0];

    const { name, args } = functionCall;

    if (!toolFunctions[name]) {
      throw new Error(`Unknown function call: ${name}`);
    }

    // Call the function and get the response.
    const toolResponse = toolFunctions[name](args);

    const functionResponsePart = {
      name: functionCall.name,
      response: {
        result: toolResponse,
      },
    };

    // Send the function response back to the model.
    contents.push({
      role: "model",
      parts: [
        {
          functionCall: functionCall,
        },
      ],
    });
    contents.push({
      role: "user",
      parts: [
        {
          functionResponse: functionResponsePart,
        },
      ],
    });
  } else {
    // No more function calls, break the loop.
    console.log(result.text);
    break;
  }
}
```

**Expected Output**

When you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.

```
Tool Call: get_weather_forecast(location=London)
Tool Response: {'temperature': 25, 'unit': 'celsius'}
Tool Call: set_thermostat_temperature(temperature=20)
Tool Response: {'status': 'success'}
OK. It's 25°C in London, so I've set the thermostat to 20°C.
```

Compositional function calling is a native [Live API](https://ai.google.dev/gemini-api/docs/live) feature. This means Live API can handle the function calling similar to the Python SDK.

### Python

```
# Light control schemas
turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

prompt = """
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
  """

tools = [
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
]

await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Light control schemas
const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };
const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };

const prompt = `
  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
`;

const tools = [
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }
];

await run(prompt, tools=tools, modality="AUDIO")
```

## Function calling modes

The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the.`function_calling_config`.

*   `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.
*   `ANY`: The model is constrained to always predict a function call and guarantees function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).
*   `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.
    

### Python

```
from google.genai import types

# Configure function calling mode
tool_config = types.ToolConfig(
    function_calling_config=types.FunctionCallingConfig(
        mode="ANY", allowed_function_names=["get_current_temperature"]
    )
)

# Create the generation config
config = types.GenerateContentConfig(
    tools=[tools],  # not defined here.
    tool_config=tool_config,
)
```

### JavaScript

```
import { FunctionCallingConfigMode } from '@google/genai';

// Configure function calling mode
const toolConfig = {
  functionCallingConfig: {
    mode: FunctionCallingConfigMode.ANY,
    allowedFunctionNames: ['get_current_temperature']
  }
};

// Create the generation config
const config = {
  tools: tools, // not defined here.
  toolConfig: toolConfig,
};
```

## Automatic function calling (Python only)

When using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and the response cycle for you. The Python SDK then automatically:

1.  Detects function call responses from the model.
2.  Call the corresponding Python function in your code.
3.  Sends the function response back to the model.
4.  Returns the model's final text response.

To use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:

### Python

```
from google import genai
from google.genai import types

# Define the function with type hints and docstring
def get_current_temperature(location: str) -> dict:
    """Gets the current temperature for a given location.

    Args:
        location: The city and state, e.g. San Francisco

    Returns:
        A dictionary containing the temperature and unit.
    """
    # ... (implementation) ...
    return {"temperature": 25, "unit": "Celsius"}

# Configure the client
client = genai.Client()
config = types.GenerateContentConfig(
    tools=[get_current_temperature]
)  # Pass the function itself

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="What's the temperature in Boston?",
    config=config,
)

print(response.text)  # The SDK handles the function call and returns the final text
```

You can disable automatic function calling with:

### Python

```
config = types.GenerateContentConfig(
    tools=[get_current_temperature],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
)
```

### Automatic function schema declaration

Automatic schema extraction from Python functions doesn't work in all cases. For example, it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:

### Python

```
AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])
```

To see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):

### Python

```
def multiply(a: float, b: float):
    """Returns a * b."""
    return a * b

fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

# to_json_dict() provides a clean JSON representation.
print(fn_decl.to_json_dict())
```

## Multi-tool use: Combine native tools with function calling

You can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).

### Python

```
# Multiple tasks example - combining lights, code execution, and search
prompt = """
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
  """

tools = [
    {'google_search': {}},
    {'code_execution': {}},
    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
]

# Execute the prompt with specified tools in audio modality
await run(prompt, tools=tools, modality="AUDIO")
```

### JavaScript

```
// Multiple tasks example - combining lights, code execution, and search
const prompt = `
  Hey, I need you to do three things for me.

    1.  Turn on the lights.
    2.  Then compute the largest prime palindrome under 100000.
    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

  Thanks!
`;

const tools = [
  { googleSearch: {} },
  { codeExecution: {} },
  { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
];

// Execute the prompt with specified tools in audio modality
await run(prompt, {tools: tools, modality: "AUDIO"});
```

Python developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).

## Model context protocol (MCP)

[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard for connecting AI applications with external tools and data. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts.

The Gemini SDKs have built-in support for the MCP, reducing boilerplate code and offering [automatic tool calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) for MCP tools. When the model generates an MCP tool call, the Python and JavaScript client SDK can automatically execute the MCP tool and send the response back to the model in a subsequent request, continuing this loop until no more tool calls are made by the model.

Here, you can find an example of how to use a local MCP server with Gemini and `mcp` SDK.

### Python

Make sure the latest version of the [`mcp` SDK](https://modelcontextprotocol.io/introduction) is installed on your platform of choice.

```
pip install mcp
```

```
import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool
                    # Uncomment if you **don't** want the SDK to automatically call the tool
                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
                    #     disable=True
                    # ),
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())
```

### JavaScript

Make sure the latest version of the `mcp` SDK is installed on your platform of choice.

```
npm install @modelcontextprotocol/sdk
```

```
import { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';
import { Client } from "@modelcontextprotocol/sdk/client/index.js";
import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

// Create server parameters for stdio connection
const serverParams = new StdioClientTransport({
  command: "npx", // Executable
  args: ["-y", "@philschmid/weather-mcp"] // MCP Server
});

const client = new Client(
  {
    name: "example-client",
    version: "1.0.0"
  }
);

// Configure the client
const ai = new GoogleGenAI({});

// Initialize the connection between client and server
await client.connect(serverParams);

// Send request to the model with MCP tools
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,
  config: {
    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool
    // Uncomment if you **don't** want the sdk to automatically call the tool
    // automaticFunctionCalling: {
    //   disable: true,
    // },
  },
});
console.log(response.text)

// Close the connection
await client.close();
```

### Limitations with built-in MCP support

Built-in MCP support is a [experimental](/gemini-api/docs/models#preview) feature in our SDKs and has the following limitations:

*   Only tools are supported, not resources nor prompts
*   It is available for the Python and JavaScript/TypeScript SDK.
*   Breaking changes might occur in future releases.

Manual integration of MCP servers is always an option if these limit what you're building.

## Supported models

This section lists models and their function calling capabilities. Experimental models are not included. You can find a comprehensive capabilities overview on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.

Model

Function Calling

Parallel Function Calling

Compositional Function Calling

Gemini 2.5 Pro

✔️

✔️

✔️

Gemini 2.5 Flash

✔️

✔️

✔️

Gemini 2.5 Flash-Lite

✔️

✔️

✔️

Gemini 2.0 Flash

✔️

✔️

✔️

Gemini 2.0 Flash-Lite

X

X

X

## Best practices

*   **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.
*   **Naming:** Use descriptive function names (without spaces, periods, or dashes).
*   **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.
*   **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.
*   **Prompt Engineering:**
    *   Provide context: Tell the model its role (e.g., "You are a helpful weather assistant.").
    *   Give instructions: Specify how and when to use functions (e.g., "Don't guess dates; always use a future date for forecasts.").
    *   Encourage clarification: Instruct the model to ask clarifying questions if needed.
*   **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.
*   **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.
*   **Error Handling**: Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.
*   **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.
*   **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.

## Notes and limitations

*   Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.
*   Supported parameter types in Python are limited.
*   Automatic function calling is a Python SDK feature only.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/prompting-intro

Presentamos las actualizaciones de nuestra familia de modelos de pensamiento 2.5. [Más información](https://ai.google.dev/gemini-api/docs/models?hl=es-419)

![](https://ai.google.dev/_static/images/translated.svg?hl=es-419) Se usó la [API de Cloud Translation](//cloud.google.com/translate/?hl=es-419) para traducir esta página.

*   [Página principal](https://ai.google.dev/?hl=es-419)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=es-419)
*   [Modelos](https://ai.google.dev/gemini-api/docs?hl=es-419)

Enviar comentarios

# Estrategias de diseño de instrucciones

El _diseño de instrucciones_ es el proceso de crear instrucciones o solicitudes de lenguaje natural que generen respuestas precisas y de alta calidad de un modelo de lenguaje.

En esta página, se presentan conceptos básicos, estrategias y prácticas recomendadas para comenzar a diseñar instrucciones y aprovechar al máximo los modelos de IA de Gemini.

## Guías de instrucciones específicas para temas

¿Buscas estrategias de indicaciones más específicas? Consulta nuestras otras guías de instrucciones sobre lo siguiente:

*   [Instrucciones con archivos multimedia](https://ai.google.dev/gemini-api/docs/files?hl=es-419#prompt-guide)
*   [Instrucciones para generar imágenes](https://ai.google.dev/gemini-api/docs/image-generation?hl=es-419#imagen-prompt-guide)
*   [Solicitud de generación de videos](https://ai.google.dev/gemini-api/docs/video?hl=es-419#prompt-guide)

Google AI Studio también aloja una [galería de indicaciones](https://aistudio.google.com/app/gallery?hl=es-419) destinada a mostrar de forma interactiva muchos de los conceptos que se comparten en esta guía.

## Instrucciones claras y específicas

Una forma eficaz y eficiente de personalizar el comportamiento del modelo es proporcionarle instrucciones claras y específicas. Las instrucciones pueden ser una pregunta, tareas paso a paso o tan complejas como mapear la experiencia y la mentalidad de un usuario.

### Entrada

La entrada es el texto obligatorio en la instrucción al que deseas que el modelo proporcione una respuesta. Las entradas pueden ser una pregunta que responda el modelo (entrada de pregunta), una tarea que realice el modelo (entrada de tarea), una entidad en la que opera el modelo (entrada de entidad) o una entrada parcial que completa el modelo o continúa (entrada de finalización).

**Tipo de entrada**

**Instrucción**

**Resultado generado**

Pregunta

    ```

    What's a good name for a flower shop that specializes in selling bouquets of
    dried flowers? Create a list of 5 options with just the names.
```

    ```

    Here are 10 names for a flower shop specializing in dried flowers:

    1. Everlasting Blooms
    2. Dried & Delightful
    3. The Petal Preserve
    4. Whispers of Wildflowers
    5. Forever Blooms
    
```
    

Tarea

    Give me a simple list of just the things that I must bring on a camping trip. The list should have 5 items.

    1. \*\*Tent:\*\* Your shelter from the elements.
    2. \*\*Sleeping Bag:\*\* Stay warm and comfortable.
    3. \*\*Sleeping Pad:\*\* Insulation and cushioning for a good night's sleep.
    4. \*\*Headlamp or Flashlight:\*\* Light for nighttime activities.
    5. \*\*First Aid Kit:\*\* Be prepared for minor injuries.

Entidad

    ```

    Classify the following items as [large, small]:
Elephant
Mouse
Snail
```

```

Here's the classification:

* **Elephant:** Large
* **Mouse:** Small
* **Snail:** Small
    
```
    

#### Completa la entrada de forma parcial

Los modelos generativos de lenguaje funcionan como una herramienta de autocompletado avanzada. Cuando proporcionas contenido parcial, el modelo puede proporcionar el resto del contenido o lo que cree que es una continuación de ese contenido como respuesta. Cuando lo haces, si incluyes ejemplos o contextos, el modelo puede tener en cuenta esos ejemplos o el contexto.

En el siguiente ejemplo, se proporciona un mensaje con una instrucción y una entrada de entidad:

**Prompt:**

For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or
drink, with the value being the quantity.

Order: A burger and a drink.
  **Respuesta:**

{
  "cheeseburger": 0,
  "hamburger": 1,
  "fries": 0,
  "drink": 1
}
  (gemini-2.5-flash)

Si bien el modelo hizo lo que se le indicó, escribir las instrucciones en lenguaje natural a veces puede ser difícil y deja mucho a la interpretación del modelo. Por ejemplo, el menú de un restaurante puede contener muchos elementos. Para reducir el tamaño de la respuesta JSON, es probable que quieras omitir los elementos que no se ordenaron. En este caso, puedes dar un ejemplo y un [prefijo de respuesta](#prefixes) y dejar que el modelo lo complete:

**Prompt:**

Valid fields are cheeseburger, hamburger, fries, and drink.
Order: Give me a cheeseburger and fries
Output:
\`\`\`
{
  "cheeseburger": 1,
  "fries": 1
}
\`\`\`
Order: I want two burgers, a drink, and fries.
Output:
  **Respuesta:**

\`\`\`
{
  "hamburger": 2,
  "drink": 1,
  "fries": 1
}
\`\`\`
  (gemini-2.5-flash)

Observa cómo se excluyó "hamburguesa con queso" del resultado porque no formaba parte del pedido.

### Limitaciones

Especifica cualquier restricción para leer el prompt o generar una respuesta. Puedes indicarle al modelo qué hacer y qué no hacer. Por ejemplo, puedes especificar una restricción en la instrucción sobre cuán extenso quieres que sea un resumen:

**Prompt:**

**Summarize this text in one sentence:**
Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially
faster than any modern traditional computer. At very tiny scales, physical matter acts as both
particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.
The operating principles of quantum devices is beyond the scope of classical physics. When deployed
at scale, quantum computers could be used in a wide variety of applications such as: in
cybersecurity to break existing encryption methods while helping researchers create new ones, in
meteorology to develop better weather forecasting etc. However, the current state of the art quantum
computers are still largely experimental and impractical.
    

**Respuesta:**

      Exploiting quantum mechanical phenomena, quantum computers can perform calculations exponentially faster than traditional computers for potential applications like cybersecurity and meteorology, although they are currently largely experimental and impractical.
    (gemini-2.5-flash)

### Formato de respuesta

Puedes proporcionar instrucciones que especifiquen el formato de la respuesta. Por ejemplo, puedes solicitar que la respuesta tenga el formato de una tabla, una lista con viñetas, una presentación breve, palabras clave, una oración o un párrafo. La siguiente instrucción del sistema le indica al modelo que sea más conversacional en su respuesta:

**Instrucciones del sistema**

    All questions should be answered comprehensively with details, unless the user requests a concise response specifically.
  **Consigna**

    What is a smart way to make a business that sells DVD's in 2025?
  **Respuesta:**

Starting a business selling DVDs in 2025 presents significant challenges due to the overwhelming dominance of digital streaming and on-demand services; a "smart" approach would therefore require a highly specialized niche, a strong value proposition beyond just the disc, and likely a focus on community or collector markets rather than mainstream consumption.

Here's a breakdown of what a smart DVD business in 2025 would likely entail:

1. Extreme Niche Specialization: You cannot compete with streaming libraries. Your business must focus on what digital doesn't offer easily or at all. This could include:

    \* Out-of-Print & Rare Titles: Sourcing films or TV shows that have never been released digitally or are currently unavailable on major streaming platforms.
    ....
  (gemini-2.5-flash)

#### Da formato a las respuestas con la estrategia de finalización

La [estrategia de finalización](#completion) también puede ayudar a dar formato a la respuesta. En el siguiente ejemplo, se solicita al modelo crear un esquema de ensayo:

**Prompt:**

Create an outline for an essay about hummingbirds.
  **Respuesta:**

Here is a possible outline for an essay about hummingbirds:

I. Introduction
A. Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
B. Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds.
C. Thesis Statement: State the main point of the essay, which could be about the remarkable adaptations, behaviors, and ecological importance of hummingbirds.

II. Appearance and Identification
.....
  (gemini-2.5-flash)

El mensaje no especificó el formato del esquema, y el modelo eligió un formato para ti. Para que el modelo muestre un esquema en un formato específico, puedes agregar texto que represente el inicio del esquema y dejar que el modelo lo complete según el patrón que iniciaste.

**Prompt:**

Create an outline for an essay about hummingbirds.
I. Introduction
   \*
  **Respuesta:**

Okay, here is an outline for an essay about hummingbirds, starting from the introduction you provided:

I. Introduction
\* Hook: Start with a captivating fact or description about hummingbirds (e.g., their speed, iridescent colors, or tiny size).
\* Background: Briefly introduce hummingbirds – where they are found (Americas), their reputation as unique birds, and their general characteristics.
\* Thesis Statement: State the main point of the essay, focusing on the remarkable adaptations, behaviors, and ecological significance that make hummingbirds extraordinary.
.....
  (gemini-2.5-flash)

## Comparación entre los mensajes de toma de cero y los de tomas individuales

Puedes incluir ejemplos en la instrucción que muestren cómo se ve el modelo. El modelo intenta identificar los patrones y las relaciones de los ejemplos y los aplica cuando se genera una respuesta. Las instrucciones que contienen algunos ejemplos se denominan instrucciones _con ejemplos limitados_, mientras que las instrucciones que no proporcionan ejemplos se denominan _instrucciones sin ejemplos_. Por lo general, se usan pocas instrucciones con ejemplos limitados para regular el formato, la frase, el alcance o el patrón general de las respuestas del modelo. Usa ejemplos específicos y variados para ayudar al modelo a reducir su enfoque y generar resultados más precisos.

Recomendamos que incluyas siempre ejemplos con pocos intentos en tus prompts. Es probable que las instrucciones sin ejemplos con pocos intentos sean menos eficaces. De hecho, puedes quitar instrucciones de tu instrucción si los ejemplos son lo suficientemente claros como para mostrar la tarea en cuestión.

La siguiente instrucción sin ejemplos le pide al modelo que elija la mejor explicación.

**Prompt:**

Please choose the best explanation to the question:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Respuesta:**

Explanation1 is the better explanation because it provides more detail on the
process, including how ice crystals combine and grow into snowflakes as they
fall through the atmosphere.
  (gemini-2.5-flash)

Si tu caso de uso requiere que el modelo produzca respuestas concisas, puedes incluir ejemplos en la instrucción que den preferencia a respuestas concisas.

En la siguiente instrucción, se proporcionan dos ejemplos que muestran una preferencia por las explicaciones más cortas. En la respuesta, puedes ver que los ejemplos guiaron al modelo para elegir la explicación más corta (`Explanation2`) en lugar de la explicación más larga (`Explanation1`) como lo hizo antes.

**Prompt:**

Below are some examples showing a question, explanation, and answer format:

Question: Why is the sky blue?
Explanation1: The sky appears blue because of Rayleigh scattering, which causes
shorter blue wavelengths of light to be scattered more easily than longer red
wavelengths, making the sky look blue.
Explanation2: Due to Rayleigh scattering effect.
Answer: Explanation2

Question: What is the cause of earthquakes?
Explanation1: Sudden release of energy in the Earth's crust.
Explanation2: Earthquakes happen when tectonic plates suddenly slip or break
apart, causing a release of energy that creates seismic waves that can shake the
ground and cause damage.
Answer: Explanation1

Now, Answer the following question given the example formats above:

Question: How is snow formed?
Explanation1: Snow is formed when water vapor in the air freezes into ice
crystals in the atmosphere, which can combine and grow into snowflakes as they
fall through the atmosphere and accumulate on the ground.
Explanation2: Water vapor freezes into ice crystals forming snow.
Answer:
  **Respuesta:**

Answer: Explanation2
  (gemini-2.5-flash)

### Cantidad óptima de ejemplos

Los modelos como Gemini a menudo pueden recoger patrones con algunos ejemplos, aunque es posible que debas experimentar con la cantidad de ejemplos que proporcionas en la instrucción para obtener los mejores resultados. Al mismo tiempo, si incluyes demasiados ejemplos, el modelo puede comenzar a [sobreajustar](https://developers.google.com/machine-learning/glossary?hl=es-419#overfitting) la respuesta a los ejemplos.

### Patrones y antipatrones

Usar ejemplos para mostrar al modelo un patrón a seguir es más eficaz que usar ejemplos para mostrar al modelo un patrón a evitar.

⛔ **Patrón negativo:**

**Ejemplo**

Don't end haikus with a question:
Haiku are fun
A short and simple poem
Don't you enjoy them?
    

✅ **Patrón positivo:**

**Ejemplo**

Always end haikus with an assertion:
Haiku are fun
A short and simple poem
A joy to write
    

### Formato coherente

Asegúrate de que la estructura y el formato de los ejemplos con pocos intentos sean los mismos para evitar respuestas con formatos no deseados. Uno de los objetivos principales de agregar ejemplos de pocas tomas en las instrucciones es mostrar al modelo el formato de respuesta. Por lo tanto, es esencial garantizar un formato coherente en todos los ejemplos, especialmente presta atención a las etiquetas XML, los espacios en blanco, los saltos de línea y los divisores de ejemplos.

## Agregar contexto

Puedes incluir instrucciones y la información que el modelo necesita para resolver un problema en lugar de suponer que tiene toda la información requerida. Esta información contextual ayuda a que el modelo comprenda las restricciones y los detalles de lo que le pides que haga.

En el siguiente ejemplo, se le pide al modelo que proporcione orientación para solucionar problemas de un router:

**Prompt:**

What should I do to fix my disconnected wifi? **The light on my Google
Wifi router is yellow and blinking slowly.**
  **Respuesta:**

  A yellow blinking light on your Google Wifi router usually indicates a problem
  with the internet connection. Here are some steps to troubleshoot and fix the
  issue:
  1. Restart your router and modem:
    - Unplug your modem from the power outlet and wait for 30 seconds.
    - Plug your modem back in and wait for it to fully boot up.
    - Unplug your Google Wifi router and wait for 30 seconds.
    - Plug your Google Wifi router back in and wait for it to fully boot up.
.....
  (gemini-2.5-flash)

La respuesta parece información de solución de problemas genérica que no es específica del router ni del estado de las luces indicadoras LED.

Para personalizar la respuesta del router específico, puedes agregar a la instrucción la guía de solución de problemas del router como contexto para que este haga referencia cuando se proporciona una respuesta.

**Prompt:**

Answer the question using the text below. Respond with only the text provided.
Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.

Text:
Color: Slowly pulsing yellow
What it means: There is a network error.
What to do:
Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.

Color: Fast blinking yellow
What it means: You are holding down the reset button and are factory resetting this device.
What to do:
If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.

Color: Solid yellow
What it means: Router is factory resetting.
What to do:
This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.

Color: Solid red
What it means: Something is wrong.
What to do:
Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.
  **Respuesta:**

Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.
  (gemini-2.5-flash)

## Agregar prefijos

Un prefijo es una palabra o frase que agregas al contenido de la instrucción que puede servir para varios propósitos, según dónde coloques el prefijo:

*   **Prefijo de entrada:** Agregar un prefijo a las señales de entrada para indicar partes semánticamente importantes de la entrada al modelo. Por ejemplo, los prefijos “inglés:” y “francés:” demarcan dos idiomas diferentes.
*   **Prefijo de salida:** Aunque el modelo genera el resultado, puedes agregar un prefijo para la salida en la instrucción. El prefijo de salida proporciona información al modelo sobre lo que se espera como respuesta. Por ejemplo, el prefijo de salida “JSON:” indica al modelo que el resultado debe estar en formato JSON.
*   **Prefijo de ejemplo:** En algunas instrucciones con ejemplos limitados, agregar prefijos a los ejemplos proporciona etiquetas que el modelo puede usar cuando se genera el resultado, lo que facilita el análisis del contenido de los resultados.

En el siguiente ejemplo, “Texto:” es el prefijo de entrada y “La respuesta es:” es el prefijo de salida.

**Prompt:**

Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail
The answer is: small
Text: Elephant
The answer is:
  **Respuesta:**

The answer is: large
  (gemini-2.5-flash)

## Desglosa las instrucciones en componentes

Para los casos de uso que requieren instrucciones complejas, puedes ayudar al modelo a administrar esta complejidad mediante el desglose en componentes más simples.

1.  **Desglosa las instrucciones:** En lugar de tener muchas instrucciones en un prompt, crea un prompt por instrucción. Puedes elegir qué prompt procesar según la entrada del usuario.
    
2.  **Encadena instrucciones:** Para tareas complejas que implican varios pasos secuenciales, haz que cada paso sea una instrucción y encadena las instrucciones en una secuencia. En esta cadena secuencial de instrucciones, el resultado de una instrucción de la secuencia se convierte en la entrada de la siguiente instrucción. El resultado de la última instrucción de la secuencia es el resultado final.
    
3.  **Agrega respuestas:** La agregación ocurre cuando deseas realizar diferentes tareas paralelas en diferentes partes de los datos y agregar los resultados para producir el resultado final. Por ejemplo, puedes indicarle al modelo que realice una operación en la primera parte de los datos, que realice otra operación en el resto de los datos y que agregue los resultados.
    

## Experimenta con los parámetros del modelo

Cada llamada que envías a un modelo incluye valores de parámetros que controlan cómo el modelo genera una respuesta. El modelo puede generar resultados diferentes para los valores de parámetros diferentes. Experimenta con diferentes valores de parámetros a fin de obtener los mejores valores para la tarea. Los parámetros disponibles para diferentes modelos pueden variar. Los más comunes son los siguientes:

1.  **Cantidad máxima de tokens de salida:** Especifica la cantidad máxima de tokens que se pueden generar en la respuesta. Un token tiene aproximadamente cuatro caracteres. 100 tokens corresponden a casi 60 a 80 palabras.
    
2.  **Temperatura:** La temperatura controla el grado de aleatoriedad en la selección de tokens. La temperatura se usa para las muestras durante la generación de respuesta, que se genera cuando se aplican `topP` y `topK`. Las temperaturas más bajas son buenas para las instrucciones que requieren una respuesta más determinística o menos abierta, mientras que las temperaturas más altas pueden generar resultados más diversos o creativos. Una temperatura de 0 es determinística, lo que significa que siempre se elige la respuesta de mayor probabilidad.
    
3.  **`topK`:** El parámetro `topK` cambia la forma en que el modelo selecciona tokens para la salida. Un `topK` de 1 significa que el token seleccionado es el más probable entre todos los tokens en el vocabulario del modelo (también llamado decodificación codiciosa), mientras que un `topK` de 3 significa que el siguiente token se selecciona de los 3 más probables con la temperatura. Para cada paso de selección de tokens, se muestrean los tokens `topK` con las probabilidades más altas. Luego, los tokens se filtran según `topP` con el token final seleccionado mediante el muestreo de temperatura.
    
4.  **`topP`:** El parámetro `topP` cambia la forma en que el modelo selecciona tokens para la salida. Los tokens se seleccionan del más al menos probable hasta que la suma de sus probabilidades sea igual al valor de `topP`. Por ejemplo, si los tokens A, B y C tienen una probabilidad de 0.3, 0.2 y 0.1, y el valor `topP` es 0.5, el modelo elegirá A o B como el siguiente token usando la temperatura y excluirá a C como candidato. El valor predeterminado de `topP` es 0.95.
    
5.  **`stop_sequences`:** Establece una secuencia de detención para indicarle al modelo que deje de generar contenido. Una secuencia de detención puede ser cualquier secuencia de caracteres. Intenta evitar usar una secuencia de caracteres que pueda aparecer en el contenido generado.
    

## Estrategias de iteración de mensajes

El diseño de instrucciones a veces puede requerir algunas iteraciones antes de obtener de forma coherente la respuesta que buscas. En esta sección, se proporciona orientación sobre algunos aspectos que puedes probar cuando se itera sobre las instrucciones:

1.  **Usa diferentes frases:** El uso de diferentes palabras o frases en tus instrucciones a menudo produce respuestas diferentes del modelo, aunque todas significan lo mismo. Si no obtienes los resultados esperados de la instrucción, intenta reformularla.
    
    Version 1:
    How do I bake a pie?
    
    Version 2:
    Suggest a recipe for a pie.
    
    Version 3:
    What's a good pie recipe?
        
    
2.  **Cambia a una tarea análoga:** Si no puedes hacer que el modelo siga las instrucciones de una tarea, intenta darle instrucciones para una tarea análoga que logre el mismo resultado.
    
    Esta instrucción le indica al modelo que clasifique un libro con categorías predefinidas:
    
    **Prompt:**
    
    Which category does The Odyssey belong to:
    thriller
    sci-fi
    mythology
    biography
      **Respuesta:**
    
        The Odyssey belongs to the category of \*\*mythology\*\*. 
    
        Here's why:
    
        \* \*\*Mythology:\*\* The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his
        journey home after the Trojan War. It features gods, monsters, and supernatural events common to
        Greek mythology.
    .....
      (gemini-2.5-flash)
    
    La respuesta es correcta, pero el modelo no se mantiene dentro de los límites de las opciones. También puedes modelar para que solo responda con una de las opciones, en lugar de hacerlo en una oración completa. En este caso, puedes reformular las instrucciones como una pregunta de opción múltiple y pedir al modelo que elija una opción.
    
    **Prompt:**
    
    Multiple choice problem: Which of the following options describes the book The Odyssey?
    Options:
    
    *   thriller
    *   sci-fi
    *   mythology
    *   biography
    **Respuesta:**
    
    The correct answer is **mythology**. 
    (gemini-2.5-flash)
    
3.  **Cambia el orden del contenido de las instrucciones:** A veces, el orden del contenido en las instrucciones puede afectar la respuesta. Intenta cambiar el orden del contenido y observa cómo eso afecta la respuesta.
    
    ```
    Version 1:
    [examples]
    [context]
    [input]
    
    Version 2:
    [input]
    [examples]
    [context]
    
    Version 3:
    [examples]
    [input]
    [context]
    ```
    

## Respuestas de resguardo

Una respuesta de resguardo es una respuesta que muestra el modelo cuando el mensaje o la respuesta activan un filtro de seguridad. Un ejemplo de una respuesta de resguardo es “No puedo ayudar con eso, solo soy un modelo de lenguaje”.

Si el modelo responde con una respuesta de resguardo, intenta aumentar la temperatura.

## Elementos que debes evitar

*   Evita depender de los modelos para generar información fáctica.
*   Úsalo con cuidado en problemas matemáticos y lógicos.

## Funcionamiento interno de los modelos generativos

El objetivo de esta sección es responder la pregunta: **_¿Hay aleatoriedad en las respuestas de los modelos generativos o son deterministas?_**

La respuesta breve es que sí a ambas. Cuando le das una instrucción a un modelo generativo, se genera una respuesta de texto en dos etapas. En la primera etapa, el modelo generativo procesa la instrucción de entrada y genera una **distribución de probabilidad** sobre los posibles tokens (palabras) que podrían aparecer a continuación. Por ejemplo, si le pides con el texto de entrada "El perro saltó sobre el ... ", el modelo generativo produzca un array de palabras probables a continuación:

```
[("fence", 0.77), ("ledge", 0.12), ("blanket", 0.03), ...]
```

Este proceso es determinístico; un modelo generativo producirá esta misma distribución cada vez que se ingrese el mismo texto de instrucción.

En la segunda etapa, el modelo generativo convierte estas distribuciones en respuestas de texto reales a través de una de varias estrategias de decodificación. Una estrategia de decodificación simple podría seleccionar el token más probable en cada paso. Este proceso siempre sería determinista. Sin embargo, puedes optar por generar una respuesta mediante el _muestreo aleatorio_ de la distribución que muestra el modelo. Este proceso sería estocástico (aleatorio). Establece la temperatura para controlar el grado de aleatorización permitido en este proceso de decodificación. Una temperatura de 0 significa que solo se seleccionan los tokens más probables y no hay aleatoriedad. Por el contrario, una temperatura alta inyecta un alto grado de aleatoriedad en los tokens que selecciona el modelo, lo que genera respuestas más inesperadas y sorprendentes.

## Próximos pasos

*   Ahora que tienes una comprensión más profunda del diseño de instrucciones, intenta escribir las tuyas con [Google AI Studio](http://aistudio.google.com?hl=es-419).
*   Para obtener información sobre las instrucciones multimodales, consulta [Cómo dar instrucciones con archivos multimedia](https://ai.google.dev/gemini-api/docs/files?hl=es-419#prompt-guide).
*   Para obtener información sobre las instrucciones de imagen, consulta la [Guía de instrucciones de Imagen](https://ai.google.dev/gemini-api/docs/image-generation?hl=es-419#imagen-prompt-guide).
*   Para obtener información sobre las instrucciones de video, consulta la [guía de instrucciones de Veo](https://ai.google.dev/gemini-api/docs/video?hl=es-419#prompt-guide).

Enviar comentarios

Salvo que se indique lo contrario, el contenido de esta página está sujeto a la [licencia Atribución 4.0 de Creative Commons](https://creativecommons.org/licenses/by/4.0/), y los ejemplos de código están sujetos a la [licencia Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Para obtener más información, consulta las [políticas del sitio de Google Developers](https://developers.google.com/site-policies?hl=es-419). Java es una marca registrada de Oracle o sus afiliados.

Última actualización: 2025-04-28 (UTC)

---

# Content from: https://ai.google.dev/gemini-api/docs/semantic_retrieval

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

# Gemini Developer API

[Get a Gemini API Key](https://aistudio.google.com/apikey)

Get a Gemini API key and make your first API request in minutes.

### Python

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Explain how AI works in a few words",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"
    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil))
    if err != nil {
        log.Fatal(err)
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("Explain how AI works in a few words"),
        nil,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### Java

```
package com.example;

import com.google.genai.Client;
import com.google.genai.types.GenerateContentResponse;

public class GenerateTextFromTextInput {
  public static void main(String[] args) {
    Client client = new Client();

    GenerateContentResponse response =
        client.models.generateContent(
            "gemini-2.5-flash",
            "Explain how AI works in a few words",
            null);

    System.out.println(response.text());
  }
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works in a few words"
          }
        ]
      }
    ]
  }'
```

## Meet the models

[Use Gemini in Google AI Studio](https://aistudio.google.com)

2.5 Pro spark

Our most powerful thinking model with features for complex reasoning and much more

[](/gemini-api/docs/models#gemini-2.5-pro)

2.5 Flash spark

Our newest multimodal model, with next generation features and improved capabilities

[](/gemini-api/docs/models/gemini#gemini-2.5-flash)

2.5 Flash-Lite spark

Our fastest and most cost-efficient multimodal model with great performance for high-frequency tasks

[](/gemini-api/docs/models/gemini#gemini-2.5-flash-lite)

## Explore the API

![](/static/site-assets/images/image-generation-index.png)

### Native Image Generation

Generate and edit highly contextual images natively with Gemini 2.0 Flash.

[](/gemini-api/docs/image-generation)

![](/static/site-assets/images/long-context-overview.png)

### Explore long context

Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.

[](/gemini-api/docs/long-context)

![](/static/site-assets/images/structured-outputs-index.png)

### Generate structured outputs

Constrain Gemini to respond with JSON, a structured data format suitable for automated processing.

[](/gemini-api/docs/structured-output)

### Start building with the Gemini API

[Get started](/gemini-api/docs/quickstart)

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/grounding

Memperkenalkan pembaruan pada rangkaian model pemikiran 2.5 kami. [Pelajari lebih lanjut](https://ai.google.dev/gemini-api/docs/models?hl=id)

![](https://ai.google.dev/_static/images/translated.svg?hl=id) Halaman ini diterjemahkan oleh [Cloud Translation API](//cloud.google.com/translate/?hl=id).

*   [Beranda](https://ai.google.dev/?hl=id)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=id)
*   [Model](https://ai.google.dev/gemini-api/docs?hl=id)

Kirim masukan

# Perujukan dengan Google Penelusuran

Grounding dengan Google Penelusuran menghubungkan model Gemini ke konten web real-time dan berfungsi dengan semua [bahasa yang tersedia](https://ai.google.dev/gemini-api/docs/models/gemini?hl=id#available-languages). Hal ini memungkinkan Gemini memberikan jawaban yang lebih akurat dan mengutip sumber yang dapat diverifikasi di luar batas pengetahuannya.

Grounding membantu Anda mem-build aplikasi yang dapat:

*   **Meningkatkan akurasi faktual:** Mengurangi halusinasi model dengan mendasarkan respons pada informasi dunia nyata.
*   **Mengakses informasi real-time:** Menjawab pertanyaan tentang peristiwa dan topik terbaru.
*   **Berikan kutipan:** Bangun kepercayaan pengguna dengan menampilkan sumber untuk klaim model.
    

### Python

```
from google import genai
from google.genai import types

# Configure the client
client = genai.Client()

# Define the grounding tool
grounding_tool = types.Tool(
    google_search=types.GoogleSearch()
)

# Configure generation settings
config = types.GenerateContentConfig(
    tools=[grounding_tool]
)

# Make the request
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Who won the euro 2024?",
    config=config,
)

# Print the grounded response
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

// Configure the client
const ai = new GoogleGenAI({});

// Define the grounding tool
const groundingTool = {
  googleSearch: {},
};

// Configure generation settings
const config = {
  tools: [groundingTool],
};

// Make the request
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

// Print the grounded response
console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {"text": "Who won the euro 2024?"}
        ]
      }
    ],
    "tools": [
      {
        "google_search": {}
      }
    ]
  }'
```

Anda dapat mempelajari lebih lanjut dengan mencoba [Notebook alat penelusuran](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb?hl=id).

## Cara kerja grounding dengan Google Penelusuran

Saat Anda mengaktifkan alat `google_search`, model akan menangani seluruh alur kerja penelusuran, pemrosesan, dan mengutip informasi secara otomatis.

![grounding-overview](https://ai.google.dev/static/gemini-api/docs/images/google-search-tool-overview.png?hl=id)

1.  **Perintah Pengguna:** Aplikasi Anda mengirimkan perintah pengguna ke Gemini API dengan alat `google_search` diaktifkan.
2.  **Analisis Perintah:** Model menganalisis perintah dan menentukan apakah Google Penelusuran dapat meningkatkan kualitas jawaban.
3.  **Google Penelusuran:** Jika diperlukan, model akan otomatis membuat satu atau beberapa kueri penelusuran dan menjalankannya.
4.  **Pemrosesan Hasil Penelusuran:** Model memproses hasil penelusuran, menyintesis informasi, dan merumuskan respons.
5.  **Respons Berdasar:** API menampilkan respons akhir yang mudah digunakan dan didasarkan pada hasil penelusuran. Respons ini mencakup jawaban teks model dan `groundingMetadata` dengan kueri penelusuran, hasil web, dan kutipan.

## Memahami Respons Grounding

Jika respons berhasil di-ground, respons akan menyertakan kolom `groundingMetadata`. Data terstruktur ini penting untuk memverifikasi klaim dan membuat pengalaman kutipan yang kaya di aplikasi Anda.

```
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
          }
        ],
        "role": "model"
      },
      "groundingMetadata": {
        "webSearchQueries": [
          "UEFA Euro 2024 winner",
          "who won euro 2024"
        ],
        "searchEntryPoint": {
          "renderedContent": "<!-- HTML and CSS for the search widget -->"
        },
        "groundingChunks": [
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
          {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
        ],
        "groundingSupports": [
          {
            "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
            "groundingChunkIndices": [0]
          },
          {
            "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
            "groundingChunkIndices": [0, 1]
          }
        ]
      }
    }
  ]
}
```

Gemini API menampilkan informasi berikut dengan `groundingMetadata`:

*   `webSearchQueries` : Array kueri penelusuran yang digunakan. Hal ini berguna untuk men-debug dan memahami proses penalaran model.
*   `searchEntryPoint` : Berisi HTML dan CSS untuk merender Saran Penelusuran yang diperlukan. Persyaratan penggunaan selengkapnya dijelaskan dalam [Persyaratan Layanan](https://ai.google.dev/gemini-api/terms?hl=id#grounding-with-google-search).
*   `groundingChunks` : Array objek yang berisi sumber web (`uri` dan `title`).
*   `groundingSupports` : Array potongan untuk menghubungkan respons model `text` ke sumber di `groundingChunks`. Setiap bagian menautkan `segment` teks (ditentukan oleh `startIndex` dan `endIndex`) ke satu atau beberapa `groundingChunkIndices`. Ini adalah kunci untuk membuat kutipan inline.

Grounding dengan Google Penelusuran juga dapat digunakan bersama dengan [alat konteks URL](https://ai.google.dev/gemini-api/docs/url-context?hl=id) untuk me-grounding respons dalam data web publik dan URL spesifik yang Anda berikan.

## Mengatribusikan Sumber dengan Kutipan Langsung

API ini menampilkan data kutipan terstruktur, sehingga Anda memiliki kontrol penuh atas cara menampilkan sumber di antarmuka pengguna. Anda dapat menggunakan kolom `groundingSupports` dan `groundingChunks` untuk menautkan pernyataan model langsung ke sumbernya. Berikut adalah pola umum untuk memproses metadata guna membuat respons dengan kutipan inline yang dapat diklik.

### Python

```
def add_citations(response):
    text = response.text
    supports = response.candidates[0].grounding_metadata.grounding_supports
    chunks = response.candidates[0].grounding_metadata.grounding_chunks

    # Sort supports by end_index in descending order to avoid shifting issues when inserting.
    sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

    for support in sorted_supports:
        end_index = support.segment.end_index
        if support.grounding_chunk_indices:
            # Create citation string like [1](link1)[2](link2)
            citation_links = []
            for i in support.grounding_chunk_indices:
                if i < len(chunks):
                    uri = chunks[i].web.uri
                    citation_links.append(f"[{i + 1}]({uri})")

            citation_string = ", ".join(citation_links)
            text = text[:end_index] + citation_string + text[end_index:]

    return text

# Assuming response with grounding metadata
text_with_citations = add_citations(response)
print(text_with_citations)
```

### JavaScript

```
function addCitations(response) {
    let text = response.text;
    const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
    const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

    // Sort supports by end_index in descending order to avoid shifting issues when inserting.
    const sortedSupports = [...supports].sort(
        (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
    );

    for (const support of sortedSupports) {
        const endIndex = support.segment?.endIndex;
        if (endIndex === undefined || !support.groundingChunkIndices?.length) {
        continue;
        }

        const citationLinks = support.groundingChunkIndices
        .map(i => {
            const uri = chunks[i]?.web?.uri;
            if (uri) {
            return `[${i + 1}](${uri})`;
            }
            return null;
        })
        .filter(Boolean);

        if (citationLinks.length > 0) {
        const citationString = citationLinks.join(", ");
        text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
        }
    }

    return text;
}

const textWithCitations = addCitations(response);
console.log(textWithCitations);
```

Respons baru dengan kutipan inline akan terlihat seperti ini:

```
Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)
```

## Harga

Saat Anda menggunakan Grounding dengan Google Penelusuran, project Anda ditagih per permintaan API yang menyertakan alat `google_search`. Jika model memutuskan untuk menjalankan beberapa kueri penelusuran untuk menjawab satu perintah (misalnya, menelusuri `"UEFA Euro 2024 winner"` dan `"Spain vs England Euro 2024 final score"` dalam panggilan API yang sama), hal ini dihitung sebagai satu penggunaan alat yang dapat ditagih untuk permintaan tersebut.

Untuk mengetahui informasi harga mendetail, lihat [halaman harga Gemini API](https://ai.google.dev/gemini-api/docs/pricing?hl=id).

## Model yang Didukung

Model Eksperimental dan Pratinjau tidak disertakan. Anda dapat menemukan kemampuannya di halaman [ringkasan model](https://ai.google.dev/gemini-api/docs/models?hl=id).

Model

Grounding dengan Google Penelusuran

Gemini 2.5 Pro

✔️

Gemini 2.5 Flash

✔️

Gemini 2.0 Flash

✔️

Gemini 1.5 Pro

✔️

Gemini 1.5 Flash

✔️

## Grounding dengan Model Gemini 1.5 (Lama)

Meskipun alat `google_search` direkomendasikan untuk Gemini 2.0 dan yang lebih baru, Gemini 1.5 mendukung alat lama bernama `google_search_retrieval`. Alat ini menyediakan mode `dynamic` yang memungkinkan model memutuskan apakah akan melakukan penelusuran berdasarkan keyakinannya bahwa perintah memerlukan informasi baru. Jika keyakinan model berada di atas `dynamic_threshold` yang Anda tetapkan (nilai antara 0,0 dan 1,0), model akan melakukan penelusuran.

### Python

```
# Note: This is a legacy approach for Gemini 1.5 models.
# The 'google_search' tool is recommended for all new development.
import os
from google import genai
from google.genai import types

client = genai.Client()

retrieval_tool = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            dynamic_threshold=0.7 # Only search if confidence > 70%
        )
    )
)

config = types.GenerateContentConfig(
    tools=[retrieval_tool]
)

response = client.models.generate_content(
    model='gemini-1.5-flash',
    contents="Who won the euro 2024?",
    config=config,
)
print(response.text)
if not response.candidates[0].grounding_metadata:
  print("\nModel answered from its own knowledge.")
```

### JavaScript

```
// Note: This is a legacy approach for Gemini 1.5 models.
// The 'googleSearch' tool is recommended for all new development.
import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

const ai = new GoogleGenAI({});

const retrievalTool = {
  googleSearchRetrieval: {
    dynamicRetrievalConfig: {
      mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
      dynamicThreshold: 0.7, // Only search if confidence > 70%
    },
  },
};

const config = {
  tools: [retrievalTool],
};

const response = await ai.models.generateContent({
  model: "gemini-1.5-flash",
  contents: "Who won the euro 2024?",
  config,
});

console.log(response.text);
if (!response.candidates?.[0]?.groundingMetadata) {
  console.log("\nModel answered from its own knowledge.");
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \

  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [
      {"parts": [{"text": "Who won the euro 2024?"}]}
    ],
    "tools": [{
      "google_search_retrieval": {
        "dynamic_retrieval_config": {
          "mode": "MODE_DYNAMIC",
          "dynamic_threshold": 0.7
        }
      }
    }]
  }'
```

## Langkah berikutnya

*   Coba [Pemberian dasar dengan Google Penelusuran di Gemini API Cookbook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb?hl=id).
*   Pelajari alat lain yang tersedia, seperti [Panggilan Fungsi](https://ai.google.dev/gemini-api/docs/function-calling?hl=id).
*   Pelajari cara menambahkan perintah dengan URL tertentu menggunakan [alat konteks URL](https://ai.google.dev/gemini-api/docs/url-context?hl=id).

Kirim masukan

Kecuali dinyatakan lain, konten di halaman ini dilisensikan berdasarkan [Lisensi Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/), sedangkan contoh kode dilisensikan berdasarkan [Lisensi Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Untuk mengetahui informasi selengkapnya, lihat [Kebijakan Situs Google Developers](https://developers.google.com/site-policies?hl=id). Java adalah merek dagang terdaftar dari Oracle dan/atau afiliasinya.

Terakhir diperbarui pada 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/prompting_with_media

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Files API

The Gemini family of artificial intelligence (AI) models is built to handle various types of input data, including text, images, and audio. Since these models can handle more than one type or _mode_ of data, the Gemini models are called _multimodal models_ or explained as having _multimodal capabilities_.

This guide shows you how to work with media files using the Files API. The basic operations are the same for audio files, images, videos, documents, and other supported file types.

For file prompting guidance, check out the [File prompt guide](/gemini-api/docs/files#prompt-guide) section.

## Upload a file

You can use the Files API to upload a media file. Always use the Files API when the total request size (including the files, text prompt, system instructions, etc.) is larger than 20 MB.

The following code uploads a file and then uses the file in a call to `generateContent`.

### Python

```
from google import genai

client = genai.Client()

myfile = client.files.upload(file="path/to/sample.mp3")

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=["Describe this audio clip", myfile]
)

print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
defer client.DeleteFile(ctx, file.Name)

model := client.GenerativeModel("gemini-2.0-flash")
resp, err := model.GenerateContent(ctx,
    genai.FileData{URI: file.URI},
    genai.Text("Describe this audio clip"))
if err != nil {
    log.Fatal(err)
}

printResponse(resp)
```

### REST

```
AUDIO_PATH="path/to/sample.mp3"
MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
DISPLAY_NAME=AUDIO

tmp_header_file=upload-header.tmp

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -D "${tmp_header_file}" \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri

# Now generate content using that file
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{
        "parts":[
          {"text": "Describe this audio clip"},
          {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
        }]
      }' 2> /dev/null > response.json

cat response.json
echo

jq ".candidates[].content.parts[].text" response.json
```

## Get metadata for a file

You can verify that the API successfully stored the uploaded file and get its metadata by calling `files.get`.

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
file_name = myfile.name
myfile = client.files.get(name=file_name)
print(myfile)
```

### JavaScript

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
const fetchedFile = await ai.files.get({ name: fileName });
console.log(fetchedFile);
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}

gotFile, err := client.GetFile(ctx, file.Name)
if err != nil {
    log.Fatal(err)
}
fmt.Println("Got file:", gotFile.Name)
```

### REST

```
# file_info.json was created in the upload example
name=$(jq ".file.name" file_info.json)
# Get the file of interest to check state
curl https://generativelanguage.googleapis.com/v1beta/files/$name \
-H "x-goog-api-key: $GEMINI_API_KEY" > file_info.json
# Print some information about the file you got
name=$(jq ".file.name" file_info.json)
echo name=$name
file_uri=$(jq ".file.uri" file_info.json)
echo file_uri=$file_uri
```

## List uploaded files

You can upload multiple files using the Files API. The following code gets a list of all the files uploaded:

### Python

```
print('My files:')
for f in client.files.list():
    print(' ', f.name)
```

### JavaScript

```
const listResponse = await ai.files.list({ config: { pageSize: 10 } });
for await (const file of listResponse) {
  console.log(file.name);
}
```

### Go

```
iter := client.ListFiles(ctx)
for {
    ifile, err := iter.Next()
    if err == iterator.Done {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(ifile.Name)
}
```

### REST

```
echo "My files: "

curl "https://generativelanguage.googleapis.com/v1beta/files" \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## Delete uploaded files

Files are automatically deleted after 48 hours. You can also manually delete an uploaded file:

### Python

```
myfile = client.files.upload(file='path/to/sample.mp3')
client.files.delete(name=myfile.name)
```

### JavaScript

```
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const fileName = myfile.name;
await ai.files.delete({ name: fileName });
```

### Go

```
file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
if err != nil {
    log.Fatal(err)
}
client.DeleteFile(ctx, file.Name)
```

### REST

```
curl --request "DELETE" https://generativelanguage.googleapis.com/v1beta/files/$name \
  -H "x-goog-api-key: $GEMINI_API_KEY"
```

## Usage info

You can use the Files API to upload and interact with media files. The Files API lets you store up to 20 GB of files per project, with a per-file maximum size of 2 GB. Files are stored for 48 hours. During that time, you can use the API to get metadata about the files, but you can't download the files. The Files API is available at no cost in all regions where the Gemini API is available.

## File prompting strategies

This section provides guidance and best practices for using media files with prompts for the Gemini API.

Being able to use various types of data in your prompts gives you more flexibility in terms of what tasks you can tackle with the Gemini API. For example, you can send the model a photo of a delicious meal and ask it to write a short blog about the meal.

**Prompt**

**Response**

Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.

  
![Cookies](/static/gemini-api/docs/images/image_prompt.png)

Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time.

This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option.

This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings!

If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied.

Here's to healthy and delicious meal prepping!

If you are having trouble getting the output you want from prompts that use media files, there are some strategies that can help you get the results you want. The following sections provide design approaches and troubleshooting tips for improving prompts that use multimodal input.

You can improve your multimodal prompts by following these best practices:

*   ### [Prompt design fundamentals](#specific-instructions)
    
    *   **Be specific in your instructions**: Craft clear and concise instructions that leave minimal room for misinterpretation.
    *   **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.
    *   **Break it down step-by-step**: Divide complex tasks into manageable sub-goals, guiding the model through the process.
    *   **Specify the output format**: In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more.
    *   **Put your image first for single-image prompts**: While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image (or video) is placed before the text prompt. However, for prompts that require images to be highly interleaved with texts to make sense, use whatever order is most natural.
*   ### [Troubleshooting your multimodal prompt](#troubleshooting)
    
    *   **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.
    *   **If the model output is too generic (not tailored enough to the image/video input):** At the start of the prompt, try asking the model to describe the image(s) or video before providing the task instruction, or try asking the model to refer to what's in the image.
    *   **To troubleshoot which part failed:** Ask the model to describe the image, or ask the model to explain its reasoning, to gauge the model's initial understanding.
    *   **If your prompt results in hallucinated content:** Try dialing down the temperature setting or asking the model for shorter descriptions so that it's less likely to extrapolate additional details.
    *   **Tuning the sampling parameters:** Experiment with different temperature settings and top-k selections to adjust the model's creativity.

### Be specific in your instructions

Prompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want.

For this image of an airport board, asking the model to just "describe this image" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt.

Prompt

Model response

![](/static/gemini-api/docs/images/timetable.png)

Describe this image.

The image shows an airport arrivals and departures board.

**Updated prompt**

**Improved response**

![Travel timetable](/static/gemini-api/docs/images/timetable.png)

Parse the time and city from the airport board shown in this image into a list.

*   10:50 Moscow
*   11:05 Edinburgh
*   11:05 London
*   11:10 Bucharest
*   11:30 Kiev
*   11:35 Dublin
*   11:45 East Midlands
*   12:15 Sofia
*   12:30 London
*   12:30 Newcastle
*   12:40 St Petersburg
*   12:40 London
*   12:45 Manchester

### Add a few examples

The Gemini model can accept multiple inputs which it can use as examples to understand the output you want. Adding these examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. This is also called "few-shot" learning.

In the example below, the initial output is written in sentence form, and also contains the country (Brazil). Suppose you want the output in a different format or style, and you want only the city, not the country. Adding few-shot examples to your prompt can steer the model to respond in the way you want.

**Prompt**

**Model response**

![Travel timetable](/static/gemini-api/docs/images/redeemer.png)

Determine the city along with the landmark.

The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.

**Updated prompt**

**Improved response**

Determine the city along with the landmark.

![colosseum](/static/gemini-api/docs/images/colosseum.png)

city: Rome, landmark: the Colosseum.

![Forbidden City](/static/gemini-api/docs/images/forbidden-city.png)

city: Beijing, landmark: Forbidden City

![forbidden-city](/static/gemini-api/docs/images/redeemer.png)

city: Rio de Janeiro, landmark: Christ the Redeemer statue

### Break it down step-by-step

For complex tasks like the ones that require both visual understanding and reasoning, it can be helpful to split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you directly ask the model to “think step by step” in your prompt.

Prompt

Model response

![](/static/gemini-api/docs/images/toilet-paper.jpg)

When will I run out of toilet paper?

Soon, you only have 3 rolls left.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/toilet-paper.jpg)

1\. First, count how many toilet paper rolls are in this picture.

2\. Then, determine how much toilet paper a typical person uses per day.

3\. Calculate how long these rolls of toilet paper will last.

1\. There are 3 rolls of toilet paper in this picture.

2\. A typical person uses about 20 sheets of toilet paper per day.

3\. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.

Math problems or other types of word problems are great candidates for asking the model to think step-by-step.

**Prompt**

**Response**

![](/static/gemini-api/docs/images/math.jpg)

What is the 4th term in the sequence?

\-135

The response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt.

**Prompt**

**Response**

![](/static/gemini-api/docs/images/math.jpg)

Parse the formula in the image first.

Then based on the formula, answer "what's the 4th term in the sequence?".

_Click to expand the result_

$$ b(n) = b(n-1) \* (-3) $$

$$ b(1) = 15 $$

1.  Substitute n = 4 into the formula:

$$ b(4) = b(4-1) \* (-3) $$

$$ b(4) = b(3) \* (-3) $$

4.  We don't know the value of b(3), so substitute n = 3 into the formula:

$$ b(3) = b(3-1) \* (-3) $$

$$ b(3) = b(2) \* (-3) $$

7.  We don't know the value of b(2), so substitute n = 2 into the formula:

$$ b(2) = b(2-1) \* (-3) $$

$$ b(2) = b(1) \* (-3) $$

10.  We know the value of b(1) is 15, so substitute it into the formula:

$$ b(2) = 15 \* (-3) $$

$$ b(2) = -45 $$

13.  Now that we know the value of b(2), substitute it into the formula for b(3):

$$ b(3) = b(2) \* (-3) $$

$$ b(3) = -45 \* (-3) $$

$$ b(3) = 135 $$

17.  Now that we know the value of b(3), substitute it into the formula for b(4):

$$ b(4) = b(3) \* (-3) $$

$$ b(4) = 135 \* (-3) $$

$$ b(4) = -405 $$

Therefore, the 4th term in the sequence is -405.

This model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/math.jpg)

What is the 4th term in the sequence? Think step by step.

The first term is 15.

The second term is b(1) \* (-3) = 15 \* (-3) = -45.

The third term is b(2) \* (-3) = -45 \* (-3) = 135.

The fourth term is b(3) \* (-3) = 135 \* (-3) = -405.

### Specify the output format

A common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples:

Prompt

Model response

![](/static/gemini-api/docs/images/excel-example.png)

Parse the table in this image into markdown format

| Subject | Status | Time | Start date | Due on |

| ------- | ------ | ---- | ---------- | ------ |

| Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 |

| Geography | Done | 30 min | 12/2/2023 | 12/4/2023 |

| Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 |

| Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |

**Prompt**

**Model response**

![](/static/gemini-api/docs/images/spanish-meal.png)

Provide a list of all the following attributes:

ingredients, type of cuisine, vegetarian or not, in JSON format

` ```json `

`   {  "ingredients": [  "rice",  "shrimp",  "clams",  "mussels",  "peas",  "tomatoes",  "onions",  "garlic",  "olive oil",  "paprika",  "salt",  "pepper"  ],  "type of cuisine": "Spanish",  "vegetarian": false  }  ```   `

### Put your image first for single-image prompts

While Gemini can interpret image and text in any order within a prompt, placing a single image before the text prompt might lead to better results. In the following samples, you'll notice the image comes first before the text in each prompt.

### Troubleshooting your multimodal prompt

You might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try.

#### If the model is not drawing information from the relevant part of the image

To get a more specific response, you can point out which aspects of the image you want the prompt to draw information from to inform its response.

Prompt

Model response

![](/static/gemini-api/docs/images/diapers.png)

How many days will these diapers last a baby?

The diapers will last for 198 days before they run out.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/diapers.png)

How long will these diapers last before I run out?

Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box.

Divide the total number by how many diapers the child goes through per day.

The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs.

The average child in this weight range is likely between 1 and 2 years old.

A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).

#### If the model output is too generic and not tailored enough to the image input

To help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task.

Prompt

Model response

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

What is in common between these images?

All four images are examples of landscape photography.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

First, describe what's in each image in detail. What's in common between these images?

The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea.

The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity.

The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world.

The fourth image shows the sea in between being calm and being stormy.

Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.

You can also try explicitly asking the model to refer to what's in the image.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/packing.jpg) ![](/static/gemini-api/docs/images/mall.jpg) ![](/static/gemini-api/docs/images/toilet-paper.jpg) ![](/static/gemini-api/docs/images/ocean.jpg)

What is in common between these images? Refer to what's in the images in your response.

All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea.

These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling.

The same tips work for creative output from images. In the following example with the rabbit, explicitly asking the model to include aspects from the image can improve the output.

Prompt

Model response

![](/static/gemini-api/docs/images/rabbit.jpg)

Can you write me a poem about this image?

Little bunny, so soft and brown,  
Hopping through the field, up and down.  
Your ears so long, your tail so small,  
You're the cutest creature of them all.

You nibble on the grass, so green and sweet,  
And hop away when you see me.  
But I'll always be your friend,  
Little bunny, until the very end.

**Updated prompt**

**Improved response**

![](/static/gemini-api/docs/images/rabbit.jpg)

Can you write me a descriptive and dramatic poem about this image and include the location?

Little bunny on the green,  
Ears so long, so keen.  
What do you see out there,  
In the distance, where you stare?

Is it the ocean blue,  
Sparkling in the sun's hue?  
Or the waves that crash and roar,  
A rhythm that you adore?

Little bunny, tell me true,  
What captures your attention so?  
Is it the salty breeze,  
Or the seagulls' cries that please?

Perhaps it's the vastness of it all,  
A world so big and small.  
A place where dreams can roam,  
And adventures can be sown.

Little bunny, I wonder what you dream,  
As you sit on the grass, so serene.  
Do you yearn to explore the deep,  
Or stay on land, where you can leap?

Whatever it may be, little bunny,  
Keep that spark of wonder burning brightly.  
For in your dreams and aspirations,  
Lies a world waiting for your creation.

#### Troubleshooting which part of the prompt failed

It can be hard to know whether a prompt failed because the model didn't **understand the image** to begin with, or if it did understand the image but did not perform the correct **reasoning steps** afterward. To disambiguate between those reasons, ask the model to describe what's in the image.

In the following example, if the model responds with a snack that seems surprising when paired with tea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized that the image contains tea.

Prompt

Prompt for troubleshooting

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this?

![](/static/gemini-api/docs/images/tea-set.png)

Describe what's in this image.

Another strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any.

Prompt

Prompt for troubleshooting

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this?

![](/static/gemini-api/docs/images/tea-set.png)

What's a snack I can make in 1 minute that would go well with this? Please explain why.

## What's next

*   Try writing your own multimodal prompts using [Google AI Studio](http://aistudio.google.com).
*   For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](/gemini-api/docs/vision), [Audio](/gemini-api/docs/audio), and [Document processing](/gemini-api/docs/document-processing) guides.
*   For more guidance on prompt design, like tuning sampling parameters, see the [Prompt strategies](/gemini-api/docs/prompting-strategies) page.

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/sdks

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini API libraries

This page provides information on downloading and installing the latest libraries for the Gemini API. If you're new to the Gemini API, get started with the [API quickstart](/gemini-api/docs/quickstart).

## Important note about our new libraries

Since late 2024, we have been progressively rolling out the **Google Gen AI SDK**, a new set of libraries that provide a unified interface for accessing Google's generative AI models through both the Gemini Developer API and Vertex AI.

The Google Gen AI SDK is now the recommended pathway for accessing all of Google's models and all code snippets in our documentation utilize these libraries.

**Key Library Updates**

  

Language

New library (Recommended)

Old library

**Python**

`google-genai`

`google-generativeai`

**JavaScript  
and TypeScript**

`@google/genai`

`@google/generative-ai`

**Go**

`google.golang.org/genai`

`google.golang.org/generative-ai`

**Java**

`google-genai`

N/A, did not exist before

We encourage all users to [migrate to the Google GenAI SDK](/gemini-api/docs/migrate), as it provides access to new features like multi-modal outputs (e.g., image and video generation), the Live API, and long term maintenance. We expect the feature gap between the old and new SDK to grow over time.

## Install a library

### Python

You can install our [Python library](https://pypi.org/project/google-genai) by running:

```
pip install google-genai
```

### JavaScript

You can install our [JavaScript and TypeScript library](https://www.npmjs.com/package/@google/genai) by running:

```
npm install @google/genai
```

### Go

You can install our [Go library](https://pkg.go.dev/google.golang.org/genai) by running:

```
go get google.golang.org/genai
```

### Java

If you're using Maven, you can install our [Java library](https://github.com/googleapis/java-genai) by adding the following to your dependencies:

```
<dependencies>
  <dependency>
    <groupId>com.google.genai</groupId>
    <artifactId>google-genai</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

## Previous libraries and SDKs

For platforms where a new library is available, we strongly recommend [migrating to the new libraries](/gemini-api/docs/migrate). Previous libraries won't receive new features and their specific support status varies, as detailed below.

For platforms where an official Google Gen AI library is not yet available, you can call our APIs directly, use community-maintained libraries, or use trusted third-party options. We are actively working to expand our official SDK coverage and will list new libraries here as they are released.

The following are the previous libraries:

*   [Python (previous)](https://github.com/google-gemini/deprecated-generative-ai-python): All support, including bug fixes, ends September 2025.
*   [Node.js (previous)](https://github.com/google-gemini/generative-ai-js): All support, including bug fixes, ends September 2025.
*   [Go (previous)](https://github.com/google/generative-ai-go): All support, including bug fixes, ends September 2025.
*   [Dart and Flutter (Previous)](https://pub.dev/packages/google_generative_ai/install): Not actively maintained
*   [Swift (Previous)](https://github.com/google/generative-ai-swift): Not actively maintained
*   [Android (Previous)](https://github.com/google-gemini/generative-ai-android): Not actively maintained

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-11 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/thinking-mode

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Gemini thinking

The [Gemini 2.5 series models](/gemini-api/docs/models) use an internal "thinking process" that significantly improves their reasoning and multi-step planning abilities, making them highly effective for complex tasks such as coding, advanced mathematics, and data analysis.

This guide shows you how to work with Gemini's thinking capabilities using the Gemini API.

## Before you begin

Ensure you use a supported 2.5 series model for thinking. You might find it beneficial to explore these models in AI Studio before diving into the API:

*   [Try Gemini 2.5 Flash in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash)
*   [Try Gemini 2.5 Pro in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro)
*   [Try Gemini 2.5 Flash-Lite Preview in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-lite-preview-06-17)

## Generating content with thinking

Initiating a request with a thinking model is similar to any other content generation request. The key difference lies in specifying one of the [models with thinking support](#supported-models) in the `model` field, as demonstrated in the following [text generation](/gemini-api/docs/text-generation#text-input) example:

### Python

```
from google import genai

client = genai.Client()
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents=prompt
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example.";

  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: prompt,
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example."
  model := "gemini-2.5-pro"

  resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil)

  fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
   "contents": [
     {
       "parts": [
         {
           "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example."
         }
       ]
     }
   ]
 }'
 ```
```

## Thinking budgets

The `thinkingBudget` parameter guides the model on the number of thinking tokens to use when generating a response. A higher token count generally allows for more detailed reasoning, which can be beneficial for tackling more [complex tasks](#tasks). If latency is more important, use a lower budget or disable thinking by setting `thinkingBudget` to 0. Setting the `thinkingBudget` to -1 turns on **dynamic thinking**, meaning the model will adjust the budget based on the complexity of the request.

The `thinkingBudget` is only [supported](#supported-models) in Gemini 2.5 Flash, 2.5 Pro, and 2.5 Flash-Lite. Depending on the prompt, the model might overflow or underflow the token budget.

The following are `thinkingBudget` configuration details for each model type.

    

Model

Default setting  
(Thinking budget is not set)

Range

Disable thinking

Turn on dynamic thinking

**2.5 Pro**

Dynamic thinking: Model decides when and how much to think

`128` to `32768`

N/A: Cannot disable thinking

`thinkingBudget = -1`

**2.5 Flash**

Dynamic thinking: Model decides when and how much to think

`0` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

**2.5 Flash Lite**

Model does not think

`512` to `24576`

`thinkingBudget = 0`

`thinkingBudget = -1`

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="Provide a list of 3 famous physicists and their key contributions",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=1024)
        # Turn off thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=0)
        # Turn on dynamic thinking:
        # thinking_config=types.ThinkingConfig(thinking_budget=-1)
    ),
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "Provide a list of 3 famous physicists and their key contributions",
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
        // Turn off thinking:
        // thinkingBudget: 0
        // Turn on dynamic thinking:
        // thinkingBudget: -1
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  thinkingBudgetVal := int32(1024)

  contents := genai.Text("Provide a list of 3 famous physicists and their key contributions")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      ThinkingBudget: &thinkingBudgetVal,
      // Turn off thinking:
      // ThinkingBudget: int32(0),
      // Turn on dynamic thinking:
      // ThinkingBudget: int32(-1),
    },
  })

fmt.Println(resp.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [
    {
      "parts": [
        {
          "text": "Provide a list of 3 famous physicists and their key contributions"
        }
      ]
    }
  ],
  "generationConfig": {
    "thinkingConfig": {
          "thinkingBudget": 1024
          # Thinking off:
          # "thinkingBudget": 0
          # Turn on dynamic thinking:
          # "thinkingBudget": -1
    }
  }
}'
```

## Thought summaries

Thought summaries are synthisized versions of the model's raw thoughts and offer insights into the model's internal reasoning process. Note that thinking budgets apply to the model's raw thoughts and not to thought summaries.

You can enable thought summaries by setting `includeThoughts` to `true` in your request configuration. You can then access the summary by iterating through the `response` parameter's `parts`, and checking the `thought` boolean.

Here's an example demonstrating how to enable and retrieve thought summaries without streaming, which returns a single, final thought summary with the response:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro",
  contents=prompt,
  config=types.GenerateContentConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought:
    print("Thought summary:")
    print(part.text)
    print()
  else:
    print("Answer:")
    print(part.text)
    print()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: "What is the sum of the first 50 prime numbers?",
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (!part.text) {
      continue;
    }
    else if (part.thought) {
      console.log("Thoughts summary:");
      console.log(part.text);
    }
    else {
      console.log("Answer:");
      console.log(part.text);
    }
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "google.golang.org/genai"
  "os"
)

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text("What is the sum of the first 50 prime numbers?")
  model := "gemini-2.5-pro"
  resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for _, part := range resp.Candidates[0].Content.Parts {
    if part.Text != "" {
      if part.Thought {
        fmt.Println("Thoughts Summary:")
        fmt.Println(part.Text)
      } else {
        fmt.Println("Answer:")
        fmt.Println(part.Text)
      }
    }
  }
}
```

And here is an example using thinking with streaming, which returns rolling, incremental summaries during generation:

### Python

```
from google import genai
from google.genai import types

client = genai.Client()

prompt = """
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
"""

thoughts = ""
answer = ""

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-pro",
    contents=prompt,
    config=types.GenerateContentConfig(
      thinking_config=types.ThinkingConfig(
        include_thoughts=True
      )
    )
):
  for part in chunk.candidates[0].content.parts:
    if not part.text:
      continue
    elif part.thought:
      if not thoughts:
        print("Thoughts summary:")
      print(part.text)
      thoughts += part.text
    else:
      if not answer:
        print("Thoughts summary:")
      print(part.text)
      answer += part.text
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

const prompt = `Alice, Bob, and Carol each live in a different house on the same
street: red, green, and blue. The person who lives in the red house owns a cat.
Bob does not live in the green house. Carol owns a dog. The green house is to
the left of the red house. Alice does not own a cat. Who lives in each house,
and what pet do they own?`;

let thoughts = "";
let answer = "";

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-pro",
    contents: prompt,
    config: {
      thinkingConfig: {
        includeThoughts: true,
      },
    },
  });

  for await (const chunk of response) {
    for (const part of chunk.candidates[0].content.parts) {
      if (!part.text) {
        continue;
      } else if (part.thought) {
        if (!thoughts) {
          console.log("Thoughts summary:");
        }
        console.log(part.text);
        thoughts = thoughts + part.text;
      } else {
        if (!answer) {
          console.log("Answer:");
        }
        console.log(part.text);
        answer = answer + part.text;
      }
    }
  }
}

await main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "log"
  "os"
  "google.golang.org/genai"
)

const prompt = `
Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue.
The person who lives in the red house owns a cat.
Bob does not live in the green house.
Carol owns a dog.
The green house is to the left of the red house.
Alice does not own a cat.
Who lives in each house, and what pet do they own?
`

func main() {
  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  contents := genai.Text(prompt)
  model := "gemini-2.5-pro"

  resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{
    ThinkingConfig: &genai.ThinkingConfig{
      IncludeThoughts: true,
    },
  })

  for chunk := range resp {
    for _, part := range chunk.Candidates[0].Content.Parts {
      if len(part.Text) == 0 {
        continue
      }

      if part.Thought {
        fmt.Printf("Thought: %s\n", part.Text)
      } else {
        fmt.Printf("Answer: %s\n", part.Text)
      }
    }
  }
}
```

## Pricing

When thinking is turned on, response pricing is the sum of output tokens and thinking tokens. You can get the total number of generated thinking tokens from the `thoughtsTokenCount` field.

### Python

```
# ...
print("Thoughts tokens:",response.usage_metadata.thoughts_token_count)
print("Output tokens:",response.usage_metadata.candidates_token_count)
```

### JavaScript

```
// ...
console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`);
console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`);
```

### Go

```
// ...
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
  log.Fatal(err)
}
fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count))
fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count))
```

Thinking models generate full thoughts to improve the quality of the final response, and then output [summaries](#summaries) to provide insight into the thought process. So, pricing is based on the full thought tokens the model needs to generate to create a summary, despite only the summary being output from the API.

You can learn more about tokens in the [Token counting](/gemini-api/docs/tokens) guide.

## Supported models

Thinking features are supported on all the 2.5 series models. You can find all model capabilities on the [model overview](/gemini-api/docs/models) page.

## Best practices

This section includes some guidance for using thinking models efficiently. As always, following our [prompting guidance and best practices](/gemini-api/docs/prompting-strategies) will get you the best results.

### Debugging and steering

*   **Review reasoning**: When you're not getting your expected response from the thinking models, it can help to carefully analyze Gemini's thought summaries. You can see how it broke down the task and arrived at its conclusion, and use that information to correct towards the right results.
    
*   **Provide Guidance in Reasoning**: If you're hoping for a particularly lengthy output, you may want to provide guidance in your prompt to constrain the [amount of thinking](#set-budget) the model uses. This lets you reserve more of the token output for your response.
    

### Task complexity

*   **Easy Tasks (Thinking could be OFF):** For straightforward requests where complex reasoning isn't required, such as fact retrieval or classification, thinking is not required. Examples include:
    *   "Where was DeepMind founded?"
    *   "Is this email asking for a meeting or just providing information?"
*   **Medium Tasks (Default/Some Thinking):** Many common requests benefit from a degree of step-by-step processing or deeper understanding. Gemini can flexibly use thinking capability for tasks like:
    *   Analogize photosynthesis and growing up.
    *   Compare and contrast electric cars and hybrid cars.
*   **Hard Tasks (Maximum Thinking Capability):** For truly complex challenges, such as solving complex math problems or coding tasks, we recommend setting a high thinking budget. These types of tasks require the model needs to engage its full reasoning and planning capabilities, often involving many internal steps before providing an answer. Examples include:
    *   Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for which 17b is a divisor of 97b.
    *   Write Python code for a web application that visualizes real-time stock market data, including user authentication. Make it as efficient as possible.

## Thinking with tools and capabilities

Thinking models work with all of Gemini's tools and capabilities. This allows the models to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.

*   The [search tool](/gemini-api/docs/grounding) allows the model to query Google Search to find up-to-date information or information beyond its training data. This is useful for questions about recent events or highly specific topics.
    
*   The [code execution tool](/gemini-api/docs/code-execution) enables the model to generate and run Python code to perform calculations, manipulate data, or solve problems that are best handled algorithmically. The model receives the code's output and can use it in its response.
    
*   With [structured output](/gemini-api/docs/structured-output), you can constrain Gemini to respond with JSON. This is particularly useful for integrating the model's output into applications.
    
*   [Function calling](/gemini-api/docs/function-calling) connects the thinking model to external tools and APIs, so it can reason about when to call the right function and what parameters to provide.
    
*   [URL Context](/gemini-api/docs/url-context) provides the model with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.
    

You can try examples of using tools with thinking models in the [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).

## What's next?

*   To work through more in depth examples, like:
    
    *   Using tools with thinking
    *   Streaming with thinking
    *   Adjusting the thinking budget for different results
    
    and more, try our [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).
    
*   Thinking coverage is now available in our [OpenAI Compatibility](/gemini-api/docs/openai#thinking) guide.
    
*   For more info about Gemini 2.5 Pro, Gemini Flash 2.5, and Gemini 2.5 Flash-Lite, visit the [model page](/gemini-api/docs/models).
    

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/json-mode

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)

Send feedback

# Structured output

You can configure Gemini for structured output instead of unstructured text, allowing precise extraction and standardization of information for further processing. For example, you can use structured output to extract information from resumes, standardize them to build a structured database.

Gemini can generate either [JSON](/gemini-api/docs/structured-output#generating-json) or [enum values](/gemini-api/docs/structured-output#generating-enums) as structured output.

## Generating JSON

There are two ways to generate JSON using the Gemini API:

*   Configure a schema on the model
*   Provide a schema in a text prompt

Configuring a schema on the model is the **recommended** way to generate JSON, because it constrains the model to output JSON.

### Configuring a schema (recommended)

To constrain the model to generate JSON, configure a `responseSchema`. The model will then respond to any prompt with JSON-formatted output.

### Python

```
from google import genai
from pydantic import BaseModel

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="List a few popular cookie recipes, and include the amounts of ingredients.",
    config={
        "response_mime_type": "application/json",
        "response_schema": list[Recipe],
    },
)
# Use the response as a JSON string.
print(response.text)

# Use instantiated objects.
my_recipes: list[Recipe] = response.parsed
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents:
      "List a few popular cookie recipes, and include the amounts of ingredients.",
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.ARRAY,
        items: {
          type: Type.OBJECT,
          properties: {
            recipeName: {
              type: Type.STRING,
            },
            ingredients: {
              type: Type.ARRAY,
              items: {
                type: Type.STRING,
              },
            },
          },
          propertyOrdering: ["recipeName", "ingredients"],
        },
      },
    },
  });

  console.log(response.text);
}

main();
```

### Go

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        ResponseMIMEType: "application/json",
        ResponseSchema: &genai.Schema{
            Type: genai.TypeArray,
            Items: &genai.Schema{
                Type: genai.TypeObject,
                Properties: map[string]*genai.Schema{
                    "recipeName": {Type: genai.TypeString},
                    "ingredients": {
                        Type:  genai.TypeArray,
                        Items: &genai.Schema{Type: genai.TypeString},
                    },
                },
                PropertyOrdering: []string{"recipeName", "ingredients"},
            },
        },
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("List a few popular cookie recipes, and include the amounts of ingredients."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
      "contents": [{
        "parts":[
          { "text": "List a few popular cookie recipes, and include the amounts of ingredients." }
        ]
      }],
      "generationConfig": {
        "responseMimeType": "application/json",
        "responseSchema": {
          "type": "ARRAY",
          "items": {
            "type": "OBJECT",
            "properties": {
              "recipeName": { "type": "STRING" },
              "ingredients": {
                "type": "ARRAY",
                "items": { "type": "STRING" }
              }
            },
            "propertyOrdering": ["recipeName", "ingredients"]
          }
        }
      }
}' 2> /dev/null | head
```

The output might look like this:

```
[
  {
    "recipeName": "Chocolate Chip Cookies",
    "ingredients": [
      "1 cup (2 sticks) unsalted butter, softened",
      "3/4 cup granulated sugar",
      "3/4 cup packed brown sugar",
      "1 teaspoon vanilla extract",
      "2 large eggs",
      "2 1/4 cups all-purpose flour",
      "1 teaspoon baking soda",
      "1 teaspoon salt",
      "2 cups chocolate chips"
    ]
  },
  ...
]
```

### Providing a schema in a text prompt

Instead of configuring a schema, you can supply a schema as natural language or pseudo-code in a text prompt. This method is **not recommended**, because it might produce lower quality output, and because the model is not constrained to follow the schema.

Here's a generic example of a schema provided in a text prompt:

```
List a few popular cookie recipes, and include the amounts of ingredients.

Produce JSON matching this specification:

Recipe = { "recipeName": string, "ingredients": array<string> }
Return: array<Recipe>
```

Since the model gets the schema from text in the prompt, you might have some flexibility in how you represent the schema. But when you supply a schema inline like this, the model is not actually constrained to return JSON. For a more deterministic, higher quality response, configure a schema on the model, and don't duplicate the schema in the text prompt.

## Generating enum values

In some cases you might want the model to choose a single option from a list of options. To implement this behavior, you can pass an _enum_ in your schema. You can use an enum option anywhere you could use a `string` in the `responseSchema`, because an enum is an array of strings. Like a JSON schema, an enum lets you constrain model output to meet the requirements of your application.

For example, assume that you're developing an application to classify musical instruments into one of five categories: `"Percussion"`, `"String"`, `"Woodwind"`, `"Brass"`, or "`"Keyboard"`". You could create an enum to help with this task.

In the following example, you pass an enum as the `responseSchema`, constraining the model to choose the most appropriate option.

### Python

```
from google import genai
import enum

class Instrument(enum.Enum):
  PERCUSSION = "Percussion"
  STRING = "String"
  WOODWIND = "Woodwind"
  BRASS = "Brass"
  KEYBOARD = "Keyboard"

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': Instrument,
    },
)

print(response.text)
# Woodwind
```

### JavaScript

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "What type of instrument is an oboe?",
    config: {
      responseMimeType: "text/x.enum",
      responseSchema: {
        type: Type.STRING,
        enum: ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
      },
    },
  });

console.log(response.text);
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
          "contents": [{
            "parts":[
              { "text": "What type of instrument is an oboe?" }
            ]
          }],
          "generationConfig": {
            "responseMimeType": "text/x.enum",
            "responseSchema": {
              "type": "STRING",
              "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
            }
          }
    }'
```

The Python library will translate the type declarations for the API. However, the API accepts a subset of the OpenAPI 3.0 schema ([Schema](https://ai.google.dev/api/caching#schema)).

There are two other ways to specify an enumeration. You can use a [`Literal`](https://docs.pydantic.dev/1.10/usage/types/#literal-type): \`\`\`

### Python

```
Literal["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
```

And you can also pass the schema as JSON:

### Python

```
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': {
            "type": "STRING",
            "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
        },
    },
)

print(response.text)
# Woodwind
```

Beyond basic multiple choice problems, you can use an enum anywhere in a JSON schema. For example, you could ask the model for a list of recipe titles and use a `Grade` enum to give each title a popularity grade:

### Python

```
from google import genai

import enum
from pydantic import BaseModel

class Grade(enum.Enum):
    A_PLUS = "a+"
    A = "a"
    B = "b"
    C = "c"
    D = "d"
    F = "f"

class Recipe(BaseModel):
  recipe_name: str
  rating: Grade

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='List 10 home-baked cookie recipes and give them grades based on tastiness.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': list[Recipe],
    },
)

print(response.text)
```

The response might look like this:

```
[
  {
    "recipe_name": "Chocolate Chip Cookies",
    "rating": "a+"
  },
  {
    "recipe_name": "Peanut Butter Cookies",
    "rating": "a"
  },
  {
    "recipe_name": "Oatmeal Raisin Cookies",
    "rating": "b"
  },
  ...
]
```

## About JSON schemas

Configuring the model for JSON output using `responseSchema` parameter relies on `Schema` object to define its structure. This object represents a select subset of the [OpenAPI 3.0 Schema object](https://spec.openapis.org/oas/v3.0.3#schema-object), and also adds a `propertyOrdering` field.

Here's a pseudo-JSON representation of all the `Schema` fields:

```
{
  "type": enum (Type),
  "format": string,
  "description": string,
  "nullable": boolean,
  "enum": [
    string
  ],
  "maxItems": integer,
  "minItems": integer,
  "properties": {
    string: {
      object (Schema)
    },
    ...
  },
  "required": [
    string
  ],
  "propertyOrdering": [
    string
  ],
  "items": {
    object (Schema)
  }
}
```

The `Type` of the schema must be one of the OpenAPI [Data Types](https://spec.openapis.org/oas/v3.0.3#data-types), or a union of those types (using `anyOf`). Only a subset of fields is valid for each `Type`. The following list maps each `Type` to a subset of the fields that are valid for that type:

*   `string` -> `enum`, `format`, `nullable`
*   `integer` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `number` -> `format`, `minimum`, `maximum`, `enum`, `nullable`
*   `boolean` -> `nullable`
*   `array` -> `minItems`, `maxItems`, `items`, `nullable`
*   `object` -> `properties`, `required`, `propertyOrdering`, `nullable`

Here are some example schemas showing valid type-and-field combinations:

```
{ "type": "string", "enum": ["a", "b", "c"] }

{ "type": "string", "format": "date-time" }

{ "type": "integer", "format": "int64" }

{ "type": "number", "format": "double" }

{ "type": "boolean" }

{ "type": "array", "minItems": 3, "maxItems": 3, "items": { "type": ... } }

{ "type": "object",
  "properties": {
    "a": { "type": ... },
    "b": { "type": ... },
    "c": { "type": ... }
  },
  "nullable": true,
  "required": ["c"],
  "propertyOrdering": ["c", "b", "a"]
}
```

For complete documentation of the Schema fields as they're used in the Gemini API, see the [Schema reference](/api/caching#Schema).

### Property ordering

When you're working with JSON schemas in the Gemini API, the order of properties is important. By default, the API orders properties alphabetically and does not preserve the order in which the properties are defined (although the [Google Gen AI SDKs](/gemini-api/docs/sdks) may preserve this order). If you're providing examples to the model with a schema configured, and the property ordering of the examples is not consistent with the property ordering of the schema, the output could be rambling or unexpected.

To ensure a consistent, predictable ordering of properties, you can use the optional `propertyOrdering[]` field.

```
"propertyOrdering": ["recipeName", "ingredients"]
```

`propertyOrdering[]` – not a standard field in the OpenAPI specification – is an array of strings used to determine the order of properties in the response. By specifying the order of properties and then providing examples with properties in that same order, you can potentially improve the quality of results. `propertyOrdering` is only supported when you manually create `types.Schema`.

### Schemas in Python

When you're using the Python library, the value of `response_schema` must be one of the following:

*   A type, as you would use in a type annotation (see the Python [`typing` module](https://docs.python.org/3/library/typing.html))
*   An instance of [`genai.types.Schema`](https://googleapis.github.io/python-genai/genai.html#genai.types.Schema)
*   The `dict` equivalent of `genai.types.Schema`

The easiest way to define a schema is with a Pydantic type (as shown in the previous example):

### Python

```
config={'response_mime_type': 'application/json',
        'response_schema': list[Recipe]}
```

When you use a Pydantic type, the Python library builds out a JSON schema for you and sends it to the API. For additional examples, see the [Python library docs](https://googleapis.github.io/python-genai/index.html#json-response-schema).

The Python library supports schemas defined with the following types (where `AllowedType` is any allowed type):

*   `int`
*   `float`
*   `bool`
*   `str`
*   `list[AllowedType]`
*   `AllowedType|AllowedType|...`
*   For structured types:
    *   `dict[str, AllowedType]`. This annotation declares all dict values to be the same type, but doesn't specify what keys should be included.
    *   User-defined [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/). This approach lets you specify the key names and define different types for the values associated with each of the keys, including nested structures.

### JSON Schema support

[JSON Schema](https://json-schema.org/) is a more recent specification than OpenAPI 3.0, which the [Schema](/api/caching#Schema) object is based on. Support for JSON Schema is available as a preview using the field [`responseJsonSchema`](/api/generate-content#FIELDS.response_json_schema) which accepts any JSON Schema with the following limitations:

*   It only works with Gemini 2.5.
*   While all JSON Schema properties can be passed, not all are supported. See the [documentation](/api/generate-content#FIELDS.response_json_schema) for the field for more details.
*   Recursive references can only be used as the value of a non-required object property.
*   Recursive references are unrolled to a finite degree, based on the size of the schema.
*   Schemas that contain `$ref` cannot contain any properties other than those starting with a `$`.

Here's an example of generating a JSON Schema with Pydantic and submitting it to the model:

```
curl "https://generativelanguage.googleapis.com/v1alpha/models/\
gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY"\
    -H 'Content-Type: application/json' \
    -d @- <<EOF
{
  "contents": [{
    "parts":[{
      "text": "Please give a random example following this schema"
    }]
  }],
  "generationConfig": {
    "response_mime_type": "application/json",
    "response_json_schema": $(python3 - << PYEOF
from enum import Enum
from typing import List, Optional, Union, Set
from pydantic import BaseModel, Field, ConfigDict
import json

class UserRole(str, Enum):
    ADMIN = "admin"
    VIEWER = "viewer"

class Address(BaseModel):
    street: str
    city: str

class UserProfile(BaseModel):
    username: str = Field(description="User's unique name")
    age: Optional[int] = Field(ge=0, le=120)
    roles: Set[UserRole] = Field(min_items=1)
    contact: Union[Address, str]
    model_config = ConfigDict(title="User Schema")

# Generate and print the JSON Schema
print(json.dumps(UserProfile.model_json_schema(), indent=2))
PYEOF
)
  }
}
EOF
```

Passing JSON Schema directly is not yet supported when using the SDK.

## Best practices

Keep the following considerations and best practices in mind when you're using a response schema:

*   The size of your response schema counts towards the input token limit.
*   By default, fields are optional, meaning the model can populate the fields or skip them. You can set fields as required to force the model to provide a value. If there's insufficient context in the associated input prompt, the model generates responses mainly based on the data it was trained on.
*   A complex schema can result in an `InvalidArgument: 400` error. Complexity might come from long property names, long array length limits, enums with many values, objects with lots of optional properties, or a combination of these factors.
    
    If you get this error with a valid schema, make one or more of the following changes to resolve the error:
    
    *   Shorten property names or enum names.
    *   Flatten nested arrays.
    *   Reduce the number of properties with constraints, such as numbers with minimum and maximum limits.
    *   Reduce the number of properties with complex constraints, such as properties with complex formats like `date-time`.
    *   Reduce the number of optional properties.
    *   Reduce the number of valid values for enums.
*   If you aren't seeing the results you expect, add more context to your input prompts or revise your response schema. For example, review the model's response without structured output to see how the model responds. You can then update your response schema so that it better fits the model's output.
    

## What's next

Now that you've learned how to generate structured output, you might want to try using Gemini API tools:

*   [Function calling](/gemini-api/docs/function-calling)
*   [Code execution](/gemini-api/docs/code-execution)
*   [Grounding with Google Search](/gemini-api/docs/grounding)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/imagen

אנחנו שמחים להציג עדכונים במשפחת המודלים שלנו לניתוח נתונים בגרסה 2.5. [מידע נוסף](https://ai.google.dev/gemini-api/docs/models?hl=he)

![](https://ai.google.dev/_static/images/translated.svg?hl=he) דף זה תורגם על ידי [Cloud Translation API](//cloud.google.com/translate/?hl=he).

*   [דף הבית](https://ai.google.dev/?hl=he)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=he)
*   [דגמים](https://ai.google.dev/gemini-api/docs?hl=he)

שליחת משוב

# יצירת תמונות

אתם יכולים ליצור תמונות באמצעות Gemini API באמצעות היכולות המולטימודליות המובנות של Gemini או באמצעות Imagen, המודלים של Google ליצירת תמונות. ברוב התרחישים לדוגמה, כדאי להתחיל עם [Gemini](https://ai.google.dev/gemini-api/docs/image-generation?hl=he#gemini). כדאי לבחור ב-[Imagen](https://ai.google.dev/gemini-api/docs/image-generation?hl=he#imagen) למשימות מיוחדות שבהן איכות התמונה קריטית. הנחיות נוספות זמינות בקטע [בחירת המודל המתאים](https://ai.google.dev/gemini-api/docs/image-generation?hl=he#choose-a-model).

כל התמונות שנוצרו כוללות [סימן מים של SynthID](https://ai.google.dev/responsible/docs/safeguards/synthid?hl=he).

## לפני שמתחילים

חשוב לוודא שאתם משתמשים בגרסת מודל נתמכת ליצירת תמונות:

*   ב-**Gemini**, משתמשים באפשרות 'יצירת תמונה של תצוגה מקדימה ב-Gemini 2.0 Flash'.
    
*   ב-**Imagen**, משתמשים באחד מהמודלים של Imagen‏ (Imagen 3, ‏ Imagen 4 או Imagen 4 Ultra).
    
    *   חשוב לדעת שהמודלים האלה זמינים רק ב[תוכנית בתשלום](https://ai.google.dev/gemini-api/docs/pricing?hl=he).

אפשר לגשת למודלים של Gemini ושל Imagen באמצעות אותן ספריות.

## יצירת תמונות באמצעות Gemini

Gemini יכול ליצור ולעבד תמונות בשיחה. אתם יכולים להנחות את Gemini באמצעות טקסט, תמונות או שילוב של שניהם כדי לבצע משימות שונות שקשורות לתמונות, כמו יצירת תמונות ועריכתן.

צריך לכלול את `responseModalities`: `["TEXT", "IMAGE"]` בהגדרה. אין תמיכה בפלט של תמונות בלבד במודלים האלה.

### יצירת תמונות (טקסט לתמונה)

הקוד הבא מדגים איך ליצור תמונה על סמך הנחיה תיאורית:

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO
import base64

client = genai.Client()

contents = ('Hi, can you create a 3d rendered image of a pig '
            'with wings and a top hat flying over a happy '
            'futuristic scifi city with lots of greenery?')

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=contents,
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.save('gemini-native-image.png')
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const contents =
    "Hi, can you create a 3d rendered image of a pig " +
    "with wings and a top hat flying over a happy " +
    "futuristic scifi city with lots of greenery?";

  // Set responseModalities to include "Image" so the model can generate  an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      ResponseModalities: []string{"TEXT", "IMAGE"},
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.0-flash-preview-image-generation",
      genai.Text("Hi, can you create a 3d rendered image of a pig " +
                 "with wings and a top hat flying over a happy " +
                 "futuristic scifi city with lots of greenery?"),
      config,
  )

  for _, part := range result.Candidates[0].Content.Parts {
      if part.Text != "" {
          fmt.Println(part.Text)
      } else if part.InlineData != nil {
          imageBytes := part.InlineData.Data
          outputFilename := "gemini_generated_image.png"
          _ = os.WriteFile(outputFilename, imageBytes, 0644)
      }
  }
}
```

### REST

```
curl -s -X POST 
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [
        {"text": "Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?"}
      ]
    }],
    "generationConfig":{"responseModalities":["TEXT","IMAGE"]}
  }' \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-native-image.png
```

![תמונה שנוצרה על ידי AI של חזיר מעופף פנטסטי](https://ai.google.dev/static/gemini-api/docs/images/flying-pig.png?hl=he)

תמונה שנוצרה על ידי AI של חזיר מעופף פנטסטי

### עריכת תמונות (טקסט ותמונה לתמונה)

כדי לערוך תמונה, מוסיפים תמונה כקלט. בדוגמה הבאה מוסבר איך מעלים תמונות בקידוד base64. כדי לשלוח כמה תמונות ועומסי נתונים גדולים יותר, אפשר לעיין בקטע [הזנת תמונה](https://ai.google.dev/gemini-api/docs/image-understanding?hl=he#image-input).

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

import PIL.Image

image = PIL.Image.open('/path/to/image.png')

client = genai.Client()

text_input = ('Hi, This is a picture of me.'
            'Can you add a llama next to me?',)

response = client.models.generate_content(
    model="gemini-2.0-flash-preview-image-generation",
    contents=[text_input, image],
    config=types.GenerateContentConfig(
      response_modalities=['TEXT', 'IMAGE']
    )
)

for part in response.candidates[0].content.parts:
  if part.text is not None:
    print(part.text)
  elif part.inline_data is not None:
    image = Image.open(BytesIO((part.inline_data.data)))
    image.show()
```

### JavaScript

```
import { GoogleGenAI, Modality } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  // Load the image from the local file system
  const imagePath = "path/to/image.png";
  const imageData = fs.readFileSync(imagePath);
  const base64Image = imageData.toString("base64");

  // Prepare the content parts
  const contents = [
    { text: "Can you add a llama next to the image?" },
    {
      inlineData: {
        mimeType: "image/png",
        data: base64Image,
      },
    },
  ];

  // Set responseModalities to include "Image" so the model can generate an image
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash-preview-image-generation",
    contents: contents,
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });
  for (const part of response.candidates[0].content.parts) {
    // Based on the part type, either show the text or save the image
    if (part.text) {
      console.log(part.text);
    } else if (part.inlineData) {
      const imageData = part.inlineData.data;
      const buffer = Buffer.from(imageData, "base64");
      fs.writeFileSync("gemini-native-image.png", buffer);
      console.log("Image saved as gemini-native-image.png");
    }
  }
}

main();
```

### Go

```
package main

import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
)

func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
     log.Fatal(err)
 }

 imagePath := "/path/to/image.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
   genai.NewPartFromText("Hi, This is a picture of me. Can you add a llama next to me?"),
   &genai.Part{
     InlineData: &genai.Blob{
       MIMEType: "image/png",
       Data:     imgData,
     },
   },
 }

 contents := []*genai.Content{
   genai.NewContentFromParts(parts, genai.RoleUser),
 }

 config := &genai.GenerateContentConfig{
     ResponseModalities: []string{"TEXT", "IMAGE"},
 }

 result, _ := client.Models.GenerateContent(
     ctx,
     "gemini-2.0-flash-preview-image-generation",
     contents,
     config,
 )

 for _, part := range result.Candidates[0].Content.Parts {
     if part.Text != "" {
         fmt.Println(part.Text)
     } else if part.InlineData != nil {
         imageBytes := part.InlineData.Data
         outputFilename := "gemini_generated_image.png"
         _ = os.WriteFile(outputFilename, imageBytes, 0644)
     }
 }
}
```

### REST

```
IMG_PATH=/path/to/your/image1.jpeg

if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
  B64FLAGS="--input"
else
  B64FLAGS="-w0"
fi

IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

curl -X POST \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-preview-image-generation:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d "{
      \"contents\": [{
        \"parts\":[
            {\"text\": \"'Hi, This is a picture of me. Can you add a llama next to me\"},
            {
              \"inline_data\": {
                \"mime_type\":\"image/jpeg\",
                \"data\": \"$IMG_BASE64\"
              }
            }
        ]
      }],
      \"generationConfig\": {\"responseModalities\": [\"TEXT\", \"IMAGE\"]}
    }"  \
  | grep -o '"data": "[^"]*"' \
  | cut -d'"' -f4 \
  | base64 --decode > gemini-edited-image.png
```

### מצבי יצירת תמונות אחרים

Gemini תומך במצבי אינטראקציה אחרים עם תמונות על סמך המבנה וההקשר של ההנחיה, כולל:

*   **טקסט לתמונות וטקסט (משולבים):** הפלט הוא תמונות עם טקסט קשור.
    *   הנחיה לדוגמה: "יצירת מתכון מאויר לפאייה".
*   **תמונות וטקסט לתמונות וטקסט (משולבים)**: המערכת משתמשת בתמונות ובטקסט שהזנתם כדי ליצור תמונות וטקסט קשורים חדשים.
    *   דוגמה להנחיה: (עם תמונה של חדר מרוהט) "אילו ספות בצבעים אחרים יתאימו למרחב שלי? יש לך אפשרות לעדכן את התמונה?"
*   **עריכת תמונות במספר שלבים (צ'אט):** אפשר להמשיך ליצור או לערוך תמונות בצורה רציפה.
    *   הנחיות לדוגמה: \[upload an image of a blue car.\] , "Turn this car into a convertible", "עכשיו משנים את הצבע לצהוב".

### מגבלות

*   כדי להשיג את הביצועים הטובים ביותר, מומלץ להשתמש בשפות הבאות: EN, ‏ es-MX, ‏ ja-JP, ‏ zh-CN,‏ hi-IN.
*   יצירת תמונות לא תומכת בקלט של אודיו או וידאו.
*   יכול להיות שיצירת התמונות לא תמיד תגרום לפעולות הבאות:
    *   המודל יכול להפיק פלט של טקסט בלבד. נסו לבקש פלט של תמונות באופן מפורש (למשל, 'יצירת תמונה', 'הוספת תמונות תוך כדי עבודה', 'עדכון התמונה').
    *   יכול להיות שהמודל יפסיק ליצור תמונות באמצע התהליך. אפשר לנסות שוב או לנסות הנחיה אחרת.
*   כשאתם יוצרים טקסט לתמונה, הכי טוב ליצור קודם את הטקסט ואז לבקש תמונה עם הטקסט.
*   יש אזורים או מדינות שבהם התכונה 'יצירת תמונות' לא זמינה. מידע נוסף זמין במאמר [מודלים](https://ai.google.dev/gemini-api/docs/models?hl=he).

## יצירת תמונות באמצעות המודלים של Imagen

בדוגמה הזו מוצגת יצירת תמונות באמצעות [מודל Imagen](https://deepmind.google/technologies/imagen-3/?hl=he):

### Python

```
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO

client = genai.Client()

response = client.models.generate_images(
    model='imagen-4.0-generate-preview-06-06',
    prompt='Robot holding a red skateboard',
    config=types.GenerateImagesConfig(
        number_of_images= 4,
    )
)
for generated_image in response.generated_images:
  generated_image.image.show()
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

async function main() {

  const ai = new GoogleGenAI({});

  const response = await ai.models.generateImages({
    model: 'imagen-4.0-generate-preview-06-06',
    prompt: 'Robot holding a red skateboard',
    config: {
      numberOfImages: 4,
    },
  });

  let idx = 1;
  for (const generatedImage of response.generatedImages) {
    let imgBytes = generatedImage.image.imageBytes;
    const buffer = Buffer.from(imgBytes, "base64");
    fs.writeFileSync(`imagen-${idx}.png`, buffer);
    idx++;
  }
}

main();
```

### Go

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateImagesConfig{
      NumberOfImages: 4,
  }

  response, _ := client.Models.GenerateImages(
      ctx,
      "imagen-4.0-generate-preview-06-06",
      "Robot holding a red skateboard",
      config,
  )

  for n, image := range response.GeneratedImages {
      fname := fmt.Sprintf("imagen-%d.png", n)
          _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
  }
}
```

### REST

```
curl -X POST \
    "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-preview-06-06:predict" \
    -H "x-goog-api-key: $GEMINI_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "instances": [
          {
            "prompt": "Robot holding a red skateboard"
          }
        ],
        "parameters": {
          "sampleCount": 4
        }
      }'
```

![תמונה שנוצרה על ידי AI של רובוט שמחזיק סקייטבורד אדום](https://ai.google.dev/static/gemini-api/docs/images/robot-skateboard.png?hl=he)

תמונה שנוצרה על ידי AI של רובוט עם סקייטבורד אדום

### הגדרת Imagen

בשלב הזה, Imagen תומך רק בהנחיות באנגלית ובפרמטרים הבאים:

*   `numberOfImages`: מספר התמונות שייווצרו, מ-1 עד 4 (כולל). ערך ברירת המחדל הוא 4. ב-Imagen 4 Ultra, הערך שמוגדר כברירת מחדל הוא 1 כי אפשר ליצור רק תמונה אחת בכל פעם.
*   `aspectRatio`: שינוי יחס הגובה-רוחב של התמונה שנוצרה. הערכים הנתמכים הם `"1:1"`,‏ `"3:4"`,‏ `"4:3"`,‏ `"9:16"` ו-`"16:9"`. ערך ברירת המחדל הוא `"1:1"`.
*   `personGeneration`: מתן הרשאה למודל ליצור תמונות של אנשים. יש תמיכה בערכים הבאים:
    
    *   `"dont_allow"`: חסימה של יצירת תמונות של אנשים.
    *   `"allow_adult"`: יצירת תמונות של מבוגרים, אבל לא של ילדים. זוהי ברירת המחדל.
    *   `"allow_all"`: יצירת תמונות שכוללות מבוגרים וילדים.

## בחירת המודל המתאים

כדאי לבחור ב-**Gemini** במקרים הבאים:

*   אתם צריכים תמונות שרלוונטיות להקשר, שמנצלות את הידע וההיגיון של העולם.
*   חשוב לשלב בצורה חלקה בין טקסט לתמונות.
*   אתם רוצים להטמיע רכיבים חזותיים מדויקים ברצפי טקסט ארוכים.
*   אתם רוצים לערוך תמונות בצורה שיחה תוך שמירה על ההקשר.

כדאי לבחור ב-**Imagen** במקרים הבאים:

*   איכות התמונה, ריאליזם צילום, פרטים אומנותיים או סגנונות ספציפיים (למשל, אימפרסיוניזם, אנימה) הם בעדיפות עליונה.
*   ביצוע משימות עריכה מיוחדות, כמו עדכוני רקע של מוצרים או התאמת תמונות לרזולוציה גבוהה יותר.
*   הוספת מיתוג, סגנון או יצירת סמלי לוגו ועיצובים של מוצרים.

מודל Imagen 4 הוא המודל המומלץ לשימוש כשמתחילים ליצור תמונות באמצעות Imagen. מומלץ לבחור ב-Imagen 4 Ultra לתרחישים מתקדמים של שימוש או כשצריך את איכות התמונה הטובה ביותר. חשוב לדעת ש-Imagen 4 Ultra יכול ליצור רק תמונה אחת בכל פעם.

## מדריך ליצירת הנחיות ב-Imagen

בקטע הזה במדריך של Imagen מוסבר איך שינוי של הנחיה להמרת טקסט לתמונה יכול להניב תוצאות שונות, ומוצגות דוגמאות לתמונות שאפשר ליצור.

### יסודות לכתיבת הנחיות

הנחיה טובה היא תיאורית וברורה, ומכילה מילות מפתח ומשתני שינוי משמעותיים. קודם כול, כדאי לחשוב על **הנושא**, **ההקשר** ו**הסגנון**.

![הנחיה עם הדגשה של הנושא, ההקשר והסגנון](https://ai.google.dev/static/gemini-api/docs/images/imagen/style-subject-context.png?hl=he)

טקסט תמונה: _רישום_ (**סגנון**) של _בניין דירות מודרני_ (**נושא**) מוקף _גורדי שחקים_ (**הקשר ורקע**).

1.  **נושא**: הדבר הראשון שצריך לחשוב עליו בכל הנחיה הוא _הנושא_: האובייקט, האדם, החיה או הנוף שרוצים לצלם.
    
2.  **הקשר ורקע:** חשוב לא פחות הוא _הרקע או ההקשר_ שבהם הנושא יוצג. נסו למקם את מושא הצילום במגוון רקעים. לדוגמה, סטודיו עם רקע לבן, סביבה חיצונית או פנימית.
    
3.  **סגנון:** לסיום, מוסיפים את סגנון התמונה הרצוי. _הסגנונות_ יכולים להיות כלליים (ציור, צילום, סקיצות) או ספציפיים מאוד (ציור בפסטל, ציור בפחם, איזומטרי תלת-ממדי). אפשר גם לשלב סגנונות.
    

אחרי שכותבים גרסה ראשונה של ההנחיה, מוסיפים פרטים נוספים כדי לשפר אותה עד שמגיעים לתמונה הרצויה. חשוב לבצע איטרציות. קודם מגדירים את הרעיון המרכזי, ואז משפרים ומרחיבים אותו עד שהתמונה שנוצרת קרובה לחזון שלכם.

![תמונה פוטוריאליסטית לדוגמה 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_short.png?hl=he)

הנחיה: פארק באביב ליד אגם

![תמונה פוטוריאליסטית לדוגמה 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_medium.png?hl=he)

הנחיה: פארק באביב ליד אגם, **השמש שוקעת מעבר לאגם, שעת הזהב**

![תמונה פוטוריאליסטית לדוגמה 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_long.png?hl=he)

הנחיה: פארק באביב ליד אגם, _**השמש שוקעת מעבר לאגם, שעת הזהב, פרחים אדומים של פרחים בר**_

מודלים של Imagen יכולים להפוך את הרעיונות שלכם לתמונות מפורטות, גם אם ההנחיות שלכם קצרות וגם אם הן ארוכות ומפורטות. תוכלו לשפר את התוצאה באמצעות הנחיות חוזרות, ולהוסיף פרטים עד שתגיעו לתוצאה המושלמת.

הנחיות קצרות מאפשרות לכם ליצור תמונה במהירות.

![דוגמה להנחיה קצרה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_short-prompt.png?hl=he)

הנחיה: תמונה מקרוב של אישה בשנות ה-20 לחייה, צילום רחוב, תמונה סטילס מסרט, גוונים כתומים חמים עמוקים

הנחיות ארוכות יותר מאפשרות לכם להוסיף פרטים ספציפיים ולבנות את התמונה.

![דוגמה להנחיה ארוכה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_long-prompt.png?hl=he)

הנחיה: תמונה מרתקת של אישה בשנות ה-20 שלה בסגנון צילום רחוב. התמונה אמורה להיראות כמו צילום סטילס מסרט עם גוונים כתומים חמים עמוקים.

טיפים נוספים לכתיבת הנחיות ב-Imagen:

*   **שימוש בשפה תיאורי**: השתמשו במילים מפורטות כמו תארים ומילות יחס כדי לתאר את התמונה בצורה ברורה ל-Imagen.
*   **הוספת הקשר**: אם צריך, מוסיפים מידע רקע כדי לעזור ל-AI להבין את התוכן.
*   **הפניה לאומנים או לסגנונות ספציפיים**: אם יש לכם חזון אסתטי מסוים, כדאי להיעזר בהפניה לאומנים או לתנועות אמנותיות ספציפיים.
*   **שימוש בכלים להנדסת הנחיות**: כדאי לבדוק את הכלים או המשאבים להנדסת הנחיות שיעזרו לכם לשפר את ההנחיות ולקבל תוצאות אופטימליות.
*   **שיפור פרטי הפנים בתמונות אישיות וקבוצתיות**: מציינים את פרטי הפנים כמיקוד התמונה (לדוגמה, משתמשים במילה 'פורטרט' בהנחיה).

### יצירת טקסט בתמונות

מודלים של Imagen יכולים להוסיף טקסט לתמונות, וכך פותחים אפשרויות יצירת תמונות יצירתיות יותר. כדי להפיק את המקסימום מהתכונה הזו, מומלץ לפעול לפי ההנחיות הבאות:

*   **חזרה על תהליך היצירה בביטחון**: יכול להיות שתצטרכו ליצור מחדש את התמונות עד שתגיעו למראה הרצוי. שילוב הטקסט ב-Imagen עדיין מתפתח, ולפעמים כמה ניסיונות מניב את התוצאות הטובות ביותר.
*   **קצר וקולע**: כדי שהיצירה תהיה אופטימלית, כדאי להגביל את הטקסט ל-25 תווים או פחות.
*   **כמה ביטויים**: אפשר לנסות להשתמש בשני או שלושה ביטויים נפרדים כדי לספק מידע נוסף. כדי ליצור קומפוזיציות נקיות יותר, מומלץ להימנע משימוש ביותר משלושה ביטויים.
    
    ![דוגמה לטקסט שנוצר על ידי Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_generate-text.png?hl=he)
    
    הנחיה: פוסטרים עם הטקסט 'Summerland' בגופן מודגש בתור כותרת, מתחת לטקסט הזה מופיע הסלוגן 'Summer never felt so good'
    
*   **הנחיה למיקום**: מערכת Imagen יכולה לנסות למקם את הטקסט לפי ההוראות, אבל יכול להיות שתבחינו בפעמים מסוימות בתנודות. אנחנו כל הזמן משפרים את התכונה הזו.
    
*   **השפעה על סגנון הגופן**: אפשר לציין סגנון גופן כללי כדי להשפיע בעדינות על הבחירות של Imagen. אל תצפו לשכפול מדויק של הגופן, אלא לפרשנויות יצירתיות.
    
*   **גודל גופן**: מציינים גודל גופן או אינדיקציה כללית לגודל (לדוגמה, _קטן_, _בינוני_, _גדול_) כדי להשפיע על יצירת גודל הגופן.
    

### הגדרת פרמטרים להנחיה

כדי לשלוט טוב יותר בתוצאות הפלט, כדאי להגדיר פרמטרים של הקלט ב-Imagen. לדוגמה, נניח שאתם רוצים לאפשר ללקוחות ליצור סמלי לוגו לעסק שלהם, ואתם רוצים לוודא שהסמלים תמיד נוצרים על רקע בצבע אחיד. בנוסף, אתם רוצים להגביל את האפשרויות שהלקוח יכול לבחור בתפריט.

בדוגמה הזו אפשר ליצור הנחיה עם פרמטרים, בדומה לדוגמה הבאה:

A {logo\_style} logo for a {company\_area} company on a solid color background. Include the text {company\_name}.

בממשק המשתמש המותאם אישית, הלקוח יכול להזין את הפרמטרים באמצעות תפריט, והערך שבחר מאכלס את ההנחיה ש-Imagen מקבלת.

לדוגמה:

1.  הנחיה: `A minimalist logo for a health care company on a solid color background. Include the text Journey.`
    
    ![דוגמה 1 להגדרת פרמטרים של הנחיה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_healthcare.png?hl=he)
    
2.  הנחיה: `A modern logo for a software company on a solid color background. Include the text Silo.`
    
    ![דוגמה 2 להגדרת פרמטרים של הנחיה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_software.png?hl=he)
    
3.  הנחיה: `A traditional logo for a baking company on a solid color background. Include the text Seed.`
    
    ![דוגמה 3 להגדרת פרמטרים של הנחיה ב-Imagen 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_baking.png?hl=he)
    

### שיטות מתקדמות לכתיבת הנחיות

תוכלו להיעזר בדוגמאות הבאות כדי ליצור הנחיות ספציפיות יותר על סמך מאפיינים כמו תיאורים של צילום, צורות וחומרים, תנועות היסטוריות באמנות ומאפייני שינוי של איכות התמונה.

#### צילום

*   ההנחיה כוללת: _"תמונה של..."_

כדי להשתמש בסגנון הזה, כדאי להתחיל להשתמש במילות מפתח שמציינות בבירור ל-Imagen שאתם מחפשים תמונה. נותנים הנחיות שמתחילות ב-_"תמונה של. . ‎."_. לדוגמה:

![תמונה פוטוריאליסטית לדוגמה 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_coffee-beans.png?hl=he)

הנחיה: **תמונה של** פולי קפה במטבח על משטח עץ

![תמונה פוטוריאליסטית לדוגמה 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_chocolate-bar.png?hl=he)

הנחיה: **תמונה של** חפיסת שוקולד על משטח מטבח

![תמונה פוטוריאליסטית לדוגמה 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_modern-building.png?hl=he)

הנחיה: **תמונה של** בניין מודרני עם מים ברקע

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

##### אביזרי צילום

בדוגמאות הבאות אפשר לראות כמה פרמטרים ומשתני אופן פעולה ספציפיים לצילום. אפשר לשלב כמה משתני אופן פעולה כדי לקבל שליטה מדויקת יותר.

1.  **קרבה למצלמה** – _צילום תקריב, שצולם מרחוק_
    
    ![תמונה לדוגמה של מצלמה מקרוב](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_close-up.png?hl=he)
    
    הנחיה: תמונה **מקרוב** של פולי קפה
    
    ![תמונה לדוגמה של מצלמה מורחקת](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_zoomed-out.png?hl=he)
    
    הנחיה: תמונה **לא מוגדלת** של שקית קטנה של  
    פולי קפה במטבח מבולגן
    
2.  **מיקום המצלמה** – _אווירי, מלמטה_
    
    ![תמונה לדוגמה של תצלום אווירי](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_aerial-photo.png?hl=he)
    
    הנחיה: **תמונה אווירית** של עיר עירונית עם גורדי שחקים
    
    ![תצוגה מלמטה של תמונה לדוגמה](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_from-below.png?hl=he)
    
    הנחיה: תמונה של חופת יער עם שמיים כחולים **מלמטה**
    
3.  **תאורה** – _טבעית, דרמטית, חמה, קרה_
    
    ![תמונה לדוגמה עם תאורה טבעית](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_natural-lighting.png?hl=he)
    
    הנחיה: תמונה בסטודיו של כורסה מודרנית, **תאורה טבעית**
    
    ![תמונה לדוגמה של תאורה דרמטית](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_dramatic-lighting.png?hl=he)
    
    הנחיה: תמונה בסטודיו של כורסה מודרנית, **תאורה דרמטית**
    
4.  **הגדרות המצלמה** _\- טשטוש תנועה, מיקוד רך, בוקה, דיוקן_
    
    ![תמונה לדוגמה של טשטוש תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_motion-blur.png?hl=he)
    
    הנחיה: תמונה של עיר עם גורדי שחקים מתוך מכונית עם **טשטוש תנועה**
    
    ![תמונה לדוגמה עם פוקוס רך](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_soft-focus.png?hl=he)
    
    הנחיה: תמונה עם **מיקוד רך** של גשר בעיר אורבנית בלילה
    
5.  **סוגי עדשות** – _35 מ"מ, 50 מ"מ, עדשת עין הדג, עדשת זווית רחבה, עדשת מאקרו_
    
    ![תמונה לדוגמה עם עדשת מאקרו](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_macro-lens.png?hl=he)
    
    הנחיה: תמונה של עלה, **עדשת מאקרו**
    
    ![תמונה לדוגמה של עדשת עין הדג](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_fisheye-lens.png?hl=he)
    
    הנחיה: צילום רחוב, ניו יורק, **עדשת עין הדג**
    
6.  **סוגי סרטים** – _שחור-לבן, פולרואיד_
    
    ![תמונה לדוגמה של תמונה בפורמט פולרואיד](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_polaroid-portrait.png?hl=he)
    
    הנחיה: **דיוקן פולרואיד** של כלב עם משקפי שמש
    
    ![תמונה לדוגמה של תמונה בשחור-לבן](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_bw-photo.png?hl=he)
    
    הנחיה: **תמונה בשחור-לבן** של כלב עם משקפי שמש
    

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

### איור ואומנות

*   ההנחיה כוללת: _"painting של..."_, _'sketch של…'_

סגנונות האמנות משתנים מסגנונות מונוכרום כמו סקיצות בעיפרון ועד לאומנות דיגיטלית היפר-ריאליסטית. לדוגמה, בתמונות הבאות נעשה שימוש באותה הנחיה עם סגנונות שונים:

_"\[art style or creation technique\] של מכונית סדאן ספורטיבית חשמלית עם גורדי שחקים ברקע"_

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1A.png?hl=he)

הנחיה: **רישום טכני בעיפרון** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1B.png?hl=he)

הנחיה: **ציור בפחם** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1C.png?hl=he)

הנחיה: **ציור בעיפרון צבעוני** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2E.png?hl=he)

הנחיה: **ציור בפסטל** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2F.png?hl=he)

הנחיה: **אומנות דיגיטלית** של...

![תמונות לדוגמה של גרפיקה](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2G.png?hl=he)

הנחיה: **פוסטר** בסגנון אר דקו של...

מקור התמונות: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 2.

##### צורות וחומרים

*   ההנחיה כוללת: _"...עשוי מ..."_, _'…בצורה של…'_

אחד מהיתרונות של הטכנולוגיה הזו הוא שאפשר ליצור תמונות שקשה או בלתי אפשרי ליצור אותן בדרכים אחרות. לדוגמה, תוכלו ליצור מחדש את הלוגו של החברה בחומרים ובמרקמים שונים.

![תמונה 1 של דוגמה לצורות ולחומרים](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_duffel.png?hl=he)

הנחיה: תיק דאפל **שעשוי** גבינה

![תמונה 2 של דוגמה לצורות ולחומרים](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_bird.png?hl=he)

הנחיה: צינורות ניאון **בצורת** ציפור

![תמונה 3 של דוגמה לצורות ולחומרים](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_paper.png?hl=he)

הנחיה: כורסה **עשויה נייר**, תמונה בסטודיו, בסגנון אוריגמי

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

#### מקורות היסטוריים לאמנות

*   ההנחיה כוללת: _"…בסגנון של…"_

סגנונות מסוימים הפכו לאיקונים עם השנים. ריכזנו כאן כמה רעיונות לציור היסטורי או לסגנונות אמנות היסטוריים שתוכלו לנסות.

_"generate an image in the style of \[art period or movement\] : a wind farm"_

![תמונה לדוגמה של אימפרסיוניזם](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_impressionism.png?hl=he)

הנחיה: ליצור תמונה **בסגנון של _ציור אימפרסיוניסטי_**: חוות רוח

![תמונה לדוגמה של הרנסנס](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_renaissance.png?hl=he)

הנחיה: ליצור תמונה **בסגנון של _ציור מתקופת הרנסנס_**: חוות רוח

![תמונה לדוגמה של פופ ארט](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_pop-art.png?hl=he)

הנחיה: יצירת תמונה **בסגנון _פופ ארט_**: חוות רוח

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

#### משתני איכות תמונה

מילות מפתח מסוימות יכולות להעביר למנוע החיפוש את המסר שאתם מחפשים נכס באיכות גבוהה. דוגמאות למשתני איכות:

*   **תוספי הגדרה כלליים** – _באיכות גבוהה, יפים, בסגנון_
*   **תמונות** – _4K, ‏ HDR, ‏ Studio Photo_
*   **אמנות, איור** – _מקצועי ומפורט_

בהמשך מפורטות כמה דוגמאות להנחיות ללא משתני איכות, ואותה הנחייה עם משתני איכות.

![תמונה לדוגמה של תירס ללא מודификаторים](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_no-mods.png?hl=he)

הנחיה (ללא מודפי איכות): תמונה של גבעול תירס

![תמונה לדוגמה של תירס עם מודификаторים](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_4k-hdr.png?hl=he)

הנחיה (עם משתני איכות): **תמונה יפה**  
ב-4K HDR של גבעול תירס **שצולמה על ידי  
צלם מקצועי**

מקור התמונה: כל תמונה נוצרה באמצעות ההנחיה הטקסטואלית התואמת שלה באמצעות מודל Imagen 3.

#### יחסי גובה-רוחב

יצירת תמונות באמצעות Imagen מאפשרת להגדיר חמישה יחסי גובה-רוחב שונים של תמונות.

1.  **ריבוע** (1:1, ברירת המחדל) – תמונה ריבועית רגילה. יחס גובה-רוחב נפוץ נוסף הוא 1:1. הוא מתאים לפוסטים ברשתות חברתיות.
2.  **מסך מלא** (4:3) – יחס הגובה-רוחב הזה נפוץ בתקשורת או בסרטים. אלה גם המידות של רוב הטלוויזיות הישנות (לא מסך רחב) ומצלמות בפורמט בינוני. ביחס גובה-רוחב כזה, אפשר לצלם יותר מהסצנה אופקית (בהשוואה ליחס גובה-רוחב של 1:1), ולכן זהו יחס הגובה-רוחב המועדף לצילום.
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_piano.png?hl=he)
    
    הנחיה: תקריב על אצבעות של מוזיקאי שמנגן בפסנתר, סרט בשחור לבן, וינטג'י (יחס גובה-רוחב 4:3)
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_fries.png?hl=he)
    
    הנחיה: תמונה מקצועית בסטודיו של צ'יפס למסעדה יוקרתית, בסגנון של מגזין אוכל (יחס גובה-רוחב של 4:3)
    
3.  **מסך מלא לאורך** (3:4) – זהו יחס הגובה-רוחב של המסך המלא, לאחר סיבוב של 90 מעלות. כך אפשר לצלם יותר מהסצנה אנכית בהשוואה ליחס גובה-רוחב של 1:1.
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_hiking.png?hl=he)
    
    הנחיה: אישה בטיול, תקריב של המגפיים שלה שמשתקפים בבור מים, הרים גדולים ברקע, בסגנון של מודעה, זוויות דרמטיות (יחס גובה-רוחב של 3:4)
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_valley.png?hl=he)
    
    הנחיה: צילום אווירי של נהר שזורם בעמק מיסטיקלי (יחס גובה-רוחב 3:4)
    
4.  **מסך רחב** (16:9) – היחס הזה החליף את היחס 4:3 והוא עכשיו יחס הגובה-רוחב הנפוץ ביותר בטלוויזיות, במסכים ובמסכי טלפונים ניידים (לרוחב). כדאי להשתמש ביחס גובה-רוחב כזה כשרוצים לצלם יותר מהרקע (לדוגמה, נופים).
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_16-9_man.png?hl=he)
    
    הנחיה: גבר לבוש לבן יושב על החוף, תקריב, תאורה בשעת הזהב (יחס גובה-רוחב 16:9)
    
5.  **לאורך** (9:16) – יחס גובה-רוחב של מסך רחב, אבל מסובב. זהו יחס גובה-רוחב חדש יחסית שנעשה פופולרי באפליקציות של סרטונים קצרים (לדוגמה, YouTube Shorts). משתמשים באפשרות הזו עבור אובייקטים גבוהים עם כיוון אנכי בולט, כמו בניינים, עצים, מפלים או אובייקטים דומים אחרים.
    
    ![דוגמה ליחס גובה-רוחב](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_9-16_skyscraper.png?hl=he)
    
    הנחיה: עיבוד דיגיטלי של גורד שחקים ענק, מודרני, מפואר ואפי, עם שקיעה יפהפייה ברקע (יחס גובה-רוחב של 9:16)
    

#### תמונות פוטוריאליסטיות

גרסאות שונות של מודל יצירת התמונות עשויות להציע שילוב של פלט אמנותי ופלט פוטו-ריאליסטי. כדי ליצור פלט ריאליסטי יותר, על סמך הנושא שרוצים ליצור, מומלץ להשתמש בניסוח הבא בהנחיות.

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אנשים (דיוקנאות)

Prime, zoom

24-35 מ״מ

סרט בשחור-לבן, סרט נואר, עומק שדה, דו-גוני (ציינו שני צבעים)

אוכל, חרקים, צמחים (אובייקטים, טבע דומם)

Macro

60-105 מ״מ

רמת פירוט גבוהה, מיקוד מדויק, תאורה מבוקרת

ספורט, חיות בר (תנועה)

זום טלפוטו

100-400 מ״מ

מהירות תריס מהירה, מעקב אחר תנועה או פעולה

אסטרונומי, לרוחב (רחב-זווית)

זווית רחבה

10-24 מ״מ

זמני חשיפה ארוכים, מיקוד חד, חשיפה ארוכה, מים או עננים חלקים

##### צילומי פורטרט

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אנשים (דיוקנאות)

Prime, zoom

24-35 מ״מ

סרט בשחור-לבן, סרט נואר, עומק שדה, דו-גוני (ציינו שני צבעים)

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את הדיוקנאות הבאים:

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray1.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray2.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray3.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray4.png?hl=he)

הנחיה: _אישה, דיוקן 35 מ"מ, דו-גווני כחול ואפור_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir1.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir2.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir3.png?hl=he)

![דוגמה לצילום דיוקן](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir4.png?hl=he)

הנחיה: _אישה, פורטרט 35 מ"מ, שחור-לבן_  
דגם: `imagen-3.0-generate-002`

##### אובייקטים

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אוכל, חרקים, צמחים (אובייקטים, טבע דומם)

Macro

60-105 מ״מ

רמת פירוט גבוהה, מיקוד מדויק, תאורה מבוקרת

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את תמונות האובייקטים הבאות:

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf1.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf2.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf3.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf4.png?hl=he)

הנחיה: _עלה של צמח תפילה, עדשת מאקרו, 60 מ"מ_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta1.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta2.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta3.png?hl=he)

![דוגמה לצילום אובייקט](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta4.png?hl=he)

הנחיה: _צלחת פסטה, עדשת מאקרו 100 מ"מ_  
דגם: `imagen-3.0-generate-002`

##### נסיעה, תנועה

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

ספורט, חיות בר (תנועה)

זום טלפוטו

100-400 מ״מ

מהירות תריס מהירה, מעקב אחר תנועה או פעולה

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את קובצי ה-GIF הבאים:

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football1.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football2.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football3.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football4.png?hl=he)

הנחיה: _טאצ'דאון מנצח, מהירות תריס מהירה, מעקב אחר תנועה_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer1.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer2.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer3.png?hl=he)

![דוגמה לצילום תנועה](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer4.png?hl=he)

הנחיה: _צבי שרץ ביער, מהירות תריס מהירה, מעקב אחר תנועה_  
דגם: `imagen-3.0-generate-002`

##### זווית רחבה

תרחיש לדוגמה

סוג העדשה

אורכי מוקד

פרטים נוספים

אסטרונומי, לרוחב (רחב-זווית)

זווית רחבה

10-24 מ״מ

זמני חשיפה ארוכים, מיקוד חד, חשיפה ארוכה, מים או עננים חלקים

בעזרת כמה מילות מפתח מהטבלה, Imagen יכול ליצור את התמונות הרחבות הבאות:

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain1.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain2.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain3.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain4.png?hl=he)

הנחיה: _רכס הרים רחב ידיים, זווית רחבה 10 מ"מ, פריסה לרוחב_  
דגם: `imagen-3.0-generate-002`

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro1.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro2.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro3.png?hl=he)

![דוגמה לצילום בזווית רחבה](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro4.png?hl=he)

הנחיה: _תמונה של הירח, צילום אסטרונומי, זווית רחבה 10mm_  
דגם: `imagen-3.0-generate-002`

## המאמרים הבאים

*   ב[מדריך Veo](https://ai.google.dev/gemini-api/docs/video?hl=he) מוסבר איך ליצור סרטונים באמצעות Gemini API.
*   מידע נוסף על מודלים של Gemini זמין במאמרים [מודלים של Gemini](https://ai.google.dev/gemini-api/docs/models/gemini?hl=he) ו[מודלים ניסיוניים](https://ai.google.dev/gemini-api/docs/models/experimental-models?hl=he).

שליחת משוב

אלא אם צוין אחרת, התוכן של דף זה הוא ברישיון [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) ודוגמאות הקוד הן ברישיון [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). לפרטים, ניתן לעיין ב[מדיניות האתר Google Developers‏](https://developers.google.com/site-policies?hl=he).‏ Java הוא סימן מסחרי רשום של חברת Oracle ו/או של השותפים העצמאיים שלה.

עדכון אחרון: 2025-06-27 (שעון UTC).

---

# Content from: https://ai.google.dev/gemini-api/docs/system-instructions

Представляем обновления для нашего семейства моделей мышления 2.5. [Узнать больше](https://ai.google.dev/gemini-api/docs/models?hl=ru)

![](https://ai.google.dev/_static/images/translated.svg?hl=ru) Эта страница переведена с помощью [Cloud Translation API](//cloud.google.com/translate/?hl=ru).

*   [Главная](https://ai.google.dev/?hl=ru)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=ru)
*   [Модели](https://ai.google.dev/gemini-api/docs?hl=ru)

Отправить отзыв

# Генерация текста

API Gemini может генерировать текстовый вывод из различных входных данных, включая текст, изображения, видео и аудио, используя модели Gemini.

Вот простой пример, требующий ввода одного текста:

### Питон

```
from google import genai

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?"
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Explain how AI works in a few words"),
      nil,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Думаем с Близнецами 2.5

В моделях 2.5 Flash и Pro по умолчанию включена [функция «мышления»](https://ai.google.dev/gemini-api/docs/thinking?hl=ru) для повышения качества, что может занять больше времени и увеличить использование токенов.

При использовании Flash 2.5 вы можете отключить мышление, установив бюджет мышления на ноль.

Более подробную информацию см. [в руководстве по мышлению](https://ai.google.dev/gemini-api/docs/thinking?hl=ru#set-budget) .

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking
    ),
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "How does AI work?",
    config: {
      thinkingConfig: {
        thinkingBudget: 0, // Disables thinking
      },
    }
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("How does AI work?"),
      &genai.GenerateContentConfig{
        ThinkingConfig: &genai.ThinkingConfig{
            ThinkingBudget: int32(0), // Disables thinking
        },
      }
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "How does AI work?"
          }
        ]
      }
    ],
    "generationConfig": {
      "thinkingConfig": {
        "thinkingBudget": 0
      }
    }
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'How AI does work?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Системные инструкции и другие конфигурации

Вы можете управлять поведением моделей Gemini с помощью системных инструкций. Для этого передайте объект [`GenerateContentConfig`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) .

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  config := &genai.GenerateContentConfig{
      SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Hello there"),
      config,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -d '{
    "system_instruction": {
      "parts": [
        {
          "text": "You are a cat. Your name is Neko."
        }
      ]
    },
    "contents": [
      {
        "parts": [
          {
            "text": "Hello there"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const systemInstruction = {
    parts: [{
      text: 'You are a cat. Your name is Neko.'
    }]
  };

  const payload = {
    systemInstruction,
    contents: [
      {
        parts: [
          { text: 'Hello there' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Объект [`GenerateContentConfig`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) также позволяет переопределять параметры генерации по умолчанию, такие как [температура](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) .

### Питон

```
from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"],
    config=types.GenerateContentConfig(
        temperature=0.1
    )
)
print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
    config: {
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  temp := float32(0.9)
  topP := float32(0.5)
  topK := float32(20.0)

  config := &genai.GenerateContentConfig{
    Temperature:       &temp,
    TopP:              &topP,
    TopK:              &topK,
    ResponseMIMEType:  "application/json",
  }

  result, _ := client.Models.GenerateContent(
    ctx,
    "gemini-2.5-flash",
    genai.Text("What is the average size of a swallow?"),
    config,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ],
    "generationConfig": {
      "stopSequences": [
        "Title"
      ],
      "temperature": 1.0,
      "topP": 0.8,
      "topK": 10
    }
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const generationConfig = {
    temperature: 1,
    topP: 0.95,
    topK: 40,
    responseMimeType: 'text/plain',
  };

  const payload = {
    generationConfig,
    contents: [
      {
        parts: [
          { text: 'Explain how AI works in a few words' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Полный список настраиваемых параметров и их описания см. в разделе [`GenerateContentConfig`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerationConfig) в нашем справочнике по API.

## Мультимодальные входы

API Gemini поддерживает мультимодальные входы, позволяя вам комбинировать текст с медиафайлами. Следующий пример демонстрирует предоставление изображения:

### Питон

```
from PIL import Image
from google import genai

client = genai.Client()

image = Image.open("/path/to/organ.png")
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[image, "Tell me about this instrument"]
)
print(response.text)
```

### JavaScript

```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  imagePath := "/path/to/organ.jpg"
  imgData, _ := os.ReadFile(imagePath)

  parts := []*genai.Part{
      genai.NewPartFromText("Tell me about this instrument"),
      &genai.Part{
          InlineData: &genai.Blob{
              MIMEType: "image/jpeg",
              Data:     imgData,
          },
      },
  }

  contents := []*genai.Content{
      genai.NewContentFromParts(parts, genai.RoleUser),
  }

  result, _ := client.Models.GenerateContent(
      ctx,
      "gemini-2.5-flash",
      contents,
      nil,
  )

  fmt.Println(result.Text())
}
```

### ОТДЫХ

```
# Use a temporary file to hold the base64 encoded image data
TEMP_B64=$(mktemp)
trap 'rm -f "$TEMP_B64"' EXIT
base64 $B64FLAGS $IMG_PATH > "$TEMP_B64"

# Use a temporary file to hold the JSON payload
TEMP_JSON=$(mktemp)
trap 'rm -f "$TEMP_JSON"' EXIT

cat > "$TEMP_JSON" << EOF
{
  "contents": [
    {
      "parts": [
        {
          "text": "Tell me about this instrument"
        },
        {
          "inline_data": {
            "mime_type": "image/jpeg",
            "data": "$(cat "$TEMP_B64")"
          }
        }
      ]
    }
  ]
}
EOF

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d "@$TEMP_JSON"
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const imageUrl = 'http://image/url';
  const image = getImageData(imageUrl);
  const payload = {
    contents: [
      {
        parts: [
          { image },
          { text: 'Tell me about this instrument' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}

function getImageData(url) {
  const blob = UrlFetchApp.fetch(url).getBlob();

  return {
    mimeType: blob.getContentType(),
    data: Utilities.base64Encode(blob.getBytes())
  };
}
```

Для альтернативных методов предоставления изображений и более продвинутой обработки изображений см. наше [руководство по пониманию изображений](https://ai.google.dev/gemini-api/docs/image-understanding?hl=ru) . API также поддерживает ввод и понимание [документов](https://ai.google.dev/gemini-api/docs/document-processing?hl=ru) , [видео](https://ai.google.dev/gemini-api/docs/video-understanding?hl=ru) и [аудио](https://ai.google.dev/gemini-api/docs/audio?hl=ru) .

## Потоковые ответы

По умолчанию модель возвращает ответ только после завершения всего процесса генерации.

Для более плавного взаимодействия используйте потоковую передачу, чтобы получать экземпляры [`GenerateContentResponse`](https://ai.google.dev/api/generate-content?hl=ru#v1beta.GenerateContentResponse) постепенно по мере их генерации.

### Питон

```
from google import genai

client = genai.Client()

response = client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  stream := client.Models.GenerateContentStream(
      ctx,
      "gemini-2.5-flash",
      genai.Text("Write a story about a magic backpack."),
      nil,
  )

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### ОТДЫХ

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  --no-buffer \
  -d '{
    "contents": [
      {
        "parts": [
          {
            "text": "Explain how AI works"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        parts: [
          { text: 'Explain how AI works' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Многократные беседы (чат)

Наши SDK предоставляют функционал для сбора нескольких раундов запросов и ответов в чат, предоставляя вам простой способ отслеживать историю разговоров.

### Питон

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  res, _ := chat.SendMessage(ctx, genai.Part{Text: "How many paws are in my house?"})

  if len(res.Candidates) > 0 {
      fmt.Println(res.Candidates[0].Content.Parts[0].Text)
  }
}
```

### ОТДЫХ

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

Потоковую передачу также можно использовать для многопоточных разговоров.

### Питон

```
from google import genai

client = genai.Client()
chat = client.chats.create(model="gemini-2.5-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")

for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.5-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
```

### Идти

```
package main

import (
  "context"
  "fmt"
  "os"
  "google.golang.org/genai"
)

func main() {

  ctx := context.Background()
  client, err := genai.NewClient(ctx, nil)
  if err != nil {
      log.Fatal(err)
  }

  history := []*genai.Content{
      genai.NewContentFromText("Hi nice to meet you! I have 2 dogs in my house.", genai.RoleUser),
      genai.NewContentFromText("Great to meet you. What would you like to know?", genai.RoleModel),
  }

  chat, _ := client.Chats.Create(ctx, "gemini-2.5-flash", nil, history)
  stream := chat.SendMessageStream(ctx, genai.Part{Text: "How many paws are in my house?"})

  for chunk, _ := range stream {
      part := chunk.Candidates[0].Content.Parts[0]
      fmt.Print(part.Text)
  }
}
```

### ОТДЫХ

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  -H 'Content-Type: application/json' \
  -X POST \
  -d '{
    "contents": [
      {
        "role": "user",
        "parts": [
          {
            "text": "Hello"
          }
        ]
      },
      {
        "role": "model",
        "parts": [
          {
            "text": "Great to meet you. What would you like to know?"
          }
        ]
      },
      {
        "role": "user",
        "parts": [
          {
            "text": "I have two dogs in my house. How many paws are in my house?"
          }
        ]
      }
    ]
  }'
```

### Скрипт приложений

```
// See https://developers.google.com/apps-script/guides/properties
// for instructions on how to set the API key.
const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');

function main() {
  const payload = {
    contents: [
      {
        role: 'user',
        parts: [
          { text: 'Hello' },
        ],
      },
      {
        role: 'model',
        parts: [
          { text: 'Great to meet you. What would you like to know?' },
        ],
      },
      {
        role: 'user',
        parts: [
          { text: 'I have two dogs in my house. How many paws are in my house?' },
        ],
      },
    ],
  };

  const url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent';
  const options = {
    method: 'POST',
    contentType: 'application/json',
    headers: {
      'x-goog-api-key': apiKey,
    },
    payload: JSON.stringify(payload)
  };

  const response = UrlFetchApp.fetch(url, options);
  const data = JSON.parse(response);
  const content = data['candidates'][0]['content']['parts'][0]['text'];
  console.log(content);
}
```

## Поддерживаемые модели

Все модели семейства Gemini поддерживают генерацию текста. Чтобы узнать больше о моделях и их возможностях, посетите страницу [Модели](https://ai.google.dev/gemini-api/docs/models?hl=ru) .

## Лучшие практики

### Полезные советы

Для создания простого текста часто бывает достаточно подсказки [с нуля](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=ru#few-shot) , без необходимости в примерах, системных инструкциях или специальном форматировании.

Для более индивидуальных результатов:

*   Используйте [системные инструкции](#system-instructions) для управления моделью.
*   Предоставьте несколько примеров входов и выходов для руководства моделью. Это часто называют подсказкой с [небольшим количеством выстрелов](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=ru#few-shot) .

Дополнительные советы можно найти в нашем [руководстве по быстрому проектированию](https://ai.google.dev/gemini/docs/prompting-strategies?hl=ru) .

### Структурированный вывод

В некоторых случаях вам может понадобиться структурированный вывод, такой как JSON. Обратитесь к нашему руководству [по структурированному выводу](https://ai.google.dev/gemini-api/docs/structured-output?hl=ru) , чтобы узнать, как это сделать.

## Что дальше?

*   Попробуйте [API Gemini, чтобы начать работу с Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb?hl=ru) .
*   Изучите возможности Gemini по распознаванию [изображений](https://ai.google.dev/gemini-api/docs/image-understanding?hl=ru) , [видео](https://ai.google.dev/gemini-api/docs/video-understanding?hl=ru) , [аудио](https://ai.google.dev/gemini-api/docs/audio?hl=ru) и [документов](https://ai.google.dev/gemini-api/docs/document-processing?hl=ru) .
*   Узнайте о [стратегиях мультимодального запроса файлов](https://ai.google.dev/gemini-api/docs/files?hl=ru#prompt-guide) .

Отправить отзыв

Если не указано иное, контент на этой странице предоставляется по [лицензии Creative Commons "С указанием авторства 4.0"](https://creativecommons.org/licenses/by/4.0/), а примеры кода – по [лицензии Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Подробнее об этом написано в [правилах сайта](https://developers.google.com/site-policies?hl=ru). Java – это зарегистрированный товарный знак корпорации Oracle и ее аффилированных лиц.

Последнее обновление: 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/models/gemini-v2

我们推出了 2.5 系列 Thinking 模型的更新。[了解详情](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)

![](https://ai.google.dev/_static/images/translated.svg?hl=zh-cn) 此页面由 [Cloud Translation API](//cloud.google.com/translate/?hl=zh-cn) 翻译。

*   [首页](https://ai.google.dev/?hl=zh-cn)
*   [Gemini API](https://ai.google.dev/gemini-api?hl=zh-cn)
*   [模型](https://ai.google.dev/gemini-api/docs?hl=zh-cn)

发送反馈

# Gemini 模型

2.5 Pro spark

我们最强大的思考型模型，回答准确性最高，性能最优

*   输入音频、图片、视频和文本，获取文本回答
*   解决棘手问题、分析大型数据库等
*   最适合进行复杂的编码、推理和多模态理解

[](#gemini-2.5-pro)

2.5 Flash spark

性价比最高的模型，提供全面的功能。

*   输入音频、图片、视频和文本，并获取文本回答
*   模型会根据需要进行思考；或者，您可以配置思考预算
*   最适合需要思考且低延迟的高数据量任务

[](#gemini-2.5-flash)

2.5 Flash-Lite experiment

一种 Gemini 2.5 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

*   输入音频、图片、视频和文本，并获取文本回答
*   最具成本效益且支持高吞吐量的模型
*   最适合实时低延迟应用场景

[](#gemini-2.5-flash-lite)

## 模型变体

Gemini API 提供针对特定应用场景进行了优化的不同模型。下面简要介绍了可用的 Gemini 变体：

模型变体

输入

输出

优化目标

[Gemini 2.5 Pro](#gemini-2.5-pro)  
`gemini-2.5-pro`

音频、图片、视频、文本和 PDF

文本

增强的思考和推理能力、多模态理解、高级编码等

[Gemini 2.5 Flash](#gemini-2.5-flash)  
`gemini-2.5-flash`

音频、图片、视频和文本

文本

自适应思维，成本效益高

[Gemini 2.5 Flash-Lite 预览版](#gemini-2.5-flash-lite)  
`gemini-2.5-flash-lite-preview-06-17`

文本、图片、视频、音频

文本

最具成本效益且支持高吞吐量的模型

[Gemini 2.5 Flash 原生音频](#gemini-2.5-flash-native-audio)  
`gemini-2.5-flash-preview-native-audio-dialog` 和  
`gemini-2.5-flash-exp-native-audio-thinking-dialog`

音频、视频和文本

文本和音频交错

无论是否思考，都能获得高质量、自然流畅的对话式音频输出

[Gemini 2.5 Flash 预览版 TTS](#gemini-2.5-flash-preview-tts)  
`gemini-2.5-flash-preview-tts`

文本

音频

低延迟、可控的单语音和多语音文字转语音音频生成

[Gemini 2.5 Pro 预览版 TTS](#gemini-2.5-pro-preview-tts)  
`gemini-2.5-pro-preview-tts`

文本

音频

低延迟、可控的单语音和多语音文字转语音音频生成

[Gemini 2.0 Flash](#gemini-2.0-flash)  
`gemini-2.0-flash`

音频、图片、视频和文本

文本

新一代功能、速度和实时在线播放。

[Gemini 2.0 Flash 预览图片生成](#gemini-2.0-flash-preview-image-generation)  
`gemini-2.0-flash-preview-image-generation`

音频、图片、视频和文本

文字、图片

对话式图片生成和编辑

[Gemini 2.0 Flash-Lite](#gemini-2.0-flash-lite)  
`gemini-2.0-flash-lite`

音频、图片、视频和文本

文本

成本效益高且延迟时间短

[Gemini 1.5 Flash](#gemini-1.5-flash)  
`gemini-1.5-flash`

音频、图片、视频和文本

文本

在各种任务中提供快速、多样化的性能

[Gemini 1.5 Flash-8B](#gemini-1.5-flash-8b)  
`gemini-1.5-flash-8b`

音频、图片、视频和文本

文本

量大且智能程度较低的任务

[Gemini 1.5 Pro](#gemini-1.5-pro)  
`gemini-1.5-pro`

音频、图片、视频和文本

文本

需要更多智能的复杂推理任务

[Gemini 嵌入](#gemini-embedding)  
`gemini-embedding-exp`

文本

文本嵌入

衡量文本字符串的相关性

[图片 4](#imagen-4)  
`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

文本

图片

我们最新的图片生成模型

[Imagen 3](#imagen-3)  
`imagen-3.0-generate-002`

文本

图片

高质量图片生成模型

[Veo 2](#veo-2)  
`veo-2.0-generate-001`

文字、图片

视频

生成高质量视频

[Gemini 2.5 Flash Live](#live-api)  
`gemini-live-2.5-flash-preview`

音频、视频和文本

文字、音频

低延迟的双向语音和视频互动

[Gemini 2.0 Flash 直播](#live-api-2.0)  
`gemini-2.0-flash-live-001`

音频、视频和文本

文字、音频

低延迟的双向语音和视频互动

您可以在[速率限制页面](https://ai.google.dev/gemini-api/docs/rate-limits?hl=zh-cn)上查看每种模型的速率限制。

### Gemini 2.5 Pro

Gemini 2.5 Pro 是我们最先进的思考型模型，能够推理编码、数学和 STEM 领域的复杂问题，还能使用长上下文分析大型数据集、代码库和文档。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-pro&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`gemini-2.5-pro`

保存支持的数据类型

**输入源**

音频、图片、视频、文本和 PDF

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

65536

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**利用 Google 搜索建立回答依据**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

**思考**

支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `Stable: gemini-2.5-pro`
*   `Preview: gemini-2.5-pro-preview-06-05`
*   `Preview: gemini-2.5-pro-preview-05-06`
*   `Preview: gemini-2.5-pro-preview-03-25`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash

性价比最高的模型，提供全面的功能。2.5 Flash 最适合用于大规模处理、需要思考的低延迟高数据量任务，以及代理用例。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash`

保存支持的数据类型

**输入源**

文本、图片、视频、音频

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

65536

handyman 功能

**音频生成**

不受支持

**缓存**

支持

**代码执行**

支持

**函数调用**

支持

**图片生成**

不受支持

**利用 Google 搜索建立回答依据**

支持

**结构化输出**

支持

**思考**

支持

**调音**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   稳定：`gemini-2.5-flash`
*   预览：`gemini-2.5-flash-preview-05-20`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash-Lite 预览版

一种 Gemini 2.5 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-lite-preview-06-17`

保存支持的数据类型

**输入源**

文本、图片、视频和音频

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,000,000

**输出 token 限制**

64,000

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**网址情境**

支持

**利用 Google 搜索建立回答依据**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

**思考**

支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.5-flash-lite-preview-06-17`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash 原生音频

我们的原生音频对话模型（有思考和无思考），可通过 [Live API](https://ai.google.dev/gemini-api/docs/live?hl=zh-cn) 使用。这些模型可提供交互式和非结构化对话式体验，并提供样式和控制提示。

[在 Google AI Studio 中试用原生音频](https://aistudio.google.com/app/live?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-preview-native-audio-dialog` 和  
`models/gemini-2.5-flash-exp-native-audio-thinking-dialog`

保存支持的数据类型

**输入源**

音频、视频、文字

**输出**

音频和文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

128,000

**输出 token 限制**

8000

handyman 功能

**音频生成**

支持

**缓存**

不受支持

**代码执行**

不受支持

**函数调用**

支持

**图片生成**

不受支持

**利用 Google 搜索建立回答依据**

支持

**结构化输出**

不受支持

**思考**

支持

**调音**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.5-flash-preview-05-20`
*   实验性：`gemini-2.5-flash-exp-native-audio-thinking-dialog`

calendar\_month最新更新

2025 年 5 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.5 Flash 预览版文本转语音

Gemini 2.5 Flash 预览版 TTS 是我们性价比最高的语音合成模型，可为播客生成、有声读物、客户服务等结构化工作流提供高度控制和透明度。 Gemini 2.5 Flash 是实验性/预览版模型，因此其速率限制更为严格。

[在 Google AI Studio 中试用](https://aistudio.google.com/generate-speech?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-flash-preview-tts`

保存支持的数据类型

**输入源**

文本

**输出**

音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8000

**输出 token 限制**

16,000

handyman 功能

**结构化输出**

不受支持

**缓存**

不受支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不受支持

**搜索**

不受支持

**音频生成**

支持

**Live API**

不受支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `gemini-2.5-flash-preview-tts`

calendar\_month最新更新

2025 年 5 月

### Gemini 2.5 Pro 预览版文本转语音

Gemini 2.5 Pro 预览版 TTS 是我们最强大的文本转语音模型，可为播客生成、有声读物、客户服务等结构化工作流提供高度控制和透明度。 Gemini 2.5 Pro 是实验性/预览版模型，因此速率限制更为严格。

[在 Google AI Studio 中试用](https://aistudio.google.com/generate-speech?hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.5-pro-preview-tts`

保存支持的数据类型

**输入源**

文本

**输出**

音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8000

**输出 token 限制**

16,000

handyman 功能

**结构化输出**

不受支持

**缓存**

不受支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不受支持

**搜索**

不受支持

**音频生成**

支持

**Live API**

不受支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   `gemini-2.5-pro-preview-tts`

calendar\_month最新更新

2025 年 5 月

### Gemini 2.0 Flash

Gemini 2.0 Flash 提供新一代功能和增强型功能，包括更快的速度、原生工具使用体验，以及 100 万个 token 的上下文窗口。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-001&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

支持

**思考**

实验性

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-2.0-flash`
*   稳定：`gemini-2.0-flash-001`
*   实验性：`gemini-2.0-flash-exp`

calendar\_month最新更新

2025 年 2 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 2.0 Flash 预览图片生成

Gemini 2.0 Flash 预览版图片生成功能提供了经过改进的图片生成功能，包括通过对话生成和编辑图片。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-preview-image-generation`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文字和图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

32000

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

不受支持

**代码执行**

不支持

**搜索**

不支持

**图片生成**

支持

**音频生成**

不受支持

**Live API**

不支持

**思考**

不支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.0-flash-preview-image-generation`

欧洲、中东和非洲的多个国家/地区目前不支持 gemini-2.0-flash-preview-image-generation

calendar\_month最新更新

2025 年 5 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 2.0 Flash-Lite

一种 Gemini 2.0 Flash 模型，经过优化，提高了成本效益并缩短了延迟时间。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-lite&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-lite`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

不受支持

**搜索**

不受支持

**图片生成**

不受支持

**音频生成**

不受支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-2.0-flash-lite`
*   稳定：`gemini-2.0-flash-lite-001`

calendar\_month最新更新

2025 年 2 月

cognition\_2 知识截点

2024 年 8 月

### Gemini 1.5 Flash

Gemini 1.5 Flash 是一种快速且多才多艺的多模态模型，可跨多种任务进行扩缩。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-flash`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

3600

**视频时长上限**

1 小时

**音频时长上限**

大约 9.5 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-flash-latest`
*   最新稳定版：`gemini-1.5-flash`
*   稳定版：

*   `gemini-1.5-flash-001`
*   `gemini-1.5-flash-002`

calendar\_month最新更新

2024 年 9 月

### Gemini 1.5 Flash-8B

Gemini 1.5 Flash-8B 是一款小型模型，专为处理低智能任务而设计。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-flash&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-flash-8b`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

3600

**视频时长上限**

1 小时

**音频时长上限**

大约 9.5 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-flash-8b-latest`
*   最新稳定版：`gemini-1.5-flash-8b`
*   稳定版：

*   `gemini-1.5-flash-8b-001`

calendar\_month最新更新

2024 年 10 月

### Gemini 1.5 Pro

试用 [Gemini 2.5 Pro 预览版](https://ai.google.dev/gemini-api/docs/models/experimental-models?hl=zh-cn#available-models)，这是我们迄今最先进的 Gemini 模型。

Gemini 1.5 Pro 是一款中型多模态模型，经过优化，可处理各种推理任务。1.5 Pro 可以一次处理大量数据，包括 2 小时的视频、19 小时的音频、6 万行代码的代码库或 2,000 页的文本。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-1.5-pro&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-1.5-pro`

保存支持的数据类型

**输入源**

音频、图片、视频和文本

**输出**

文本

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2,097,152

**输出 token 限制**

8192

movie\_info 音频/视觉规范

**每个问题的图片数量上限**

7,200

**视频时长上限**

2 小时

**音频时长上限**

大约 19 小时

handyman 功能

**系统指令**

支持

**JSON 模式**

支持

**JSON 架构**

支持

**可调节的安全设置**

支持

**缓存**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**Live API**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   最新电子邮件的接收日期：`gemini-1.5-pro-latest`
*   最新稳定版：`gemini-1.5-pro`
*   稳定版：

*   `gemini-1.5-pro-001`
*   `gemini-1.5-pro-002`

calendar\_month最新更新

2024 年 9 月

### Imagen 4

Imagen 4 是我们最新的图片模型，与之前的模型相比，能够生成光线丰富、细节丰富的图片，文字渲染效果显著提升，输出分辨率更高。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`imagen-4.0-generate-preview-06-06`  
`imagen-4.0-ultra-generate-preview-06-06`

保存支持的数据类型

**输入**

文本

**输出**

图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

480 个令牌（文本）

**输出图片**

1（超高）  
1 至 4（标准）

calendar\_month最新更新

2025 年 6 月

### Imagen 3

Imagen 3 是我们最出色的文本转图片模型，与之前的模型相比，它生成的图片细节更丰富、光线更丰富，干扰性伪影更少。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`imagen-3.0-generate-002`

保存支持的数据类型

**输入**

文本

**输出**

图片

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

不适用

**输出图片**

最多 4 项

calendar\_month最新更新

2025 年 2 月

### Veo 2

Veo 2 是我们的高质量文本和图片转视频模型，能够生成详细的视频，捕捉提示中的艺术细微之处。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`veo-2.0-generate-001`

保存支持的数据类型

**输入**

文字、图片

**输出**

视频

token\_auto 限制

**文本输入**

不适用

**图片输入**

任意图片分辨率和宽高比，文件大小不超过 20MB

**输出视频**

最多 2 个

calendar\_month最新更新

2025 年 4 月

### Gemini 2.5 Flash Live

Gemini 2.5 Flash Live 模型可与 Live API 搭配使用，支持与 Gemini 建立低延迟的双向语音和视频互动。该模型可以处理文本、音频和视频输入，并提供文本和音频输出。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-live-2.5-flash-preview&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-live-2.5-flash-preview`

保存支持的数据类型

**输入源**

音频、视频和文本

**输出**

文字和音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-live-2.5-flash-preview`

calendar\_month最新更新

2025 年 6 月

cognition\_2 知识截点

2025 年 1 月

### Gemini 2.0 Flash Live

Gemini 2.0 Flash Live 模型可与 Live API 搭配使用，支持与 Gemini 建立低延迟的双向语音和视频互动。该模型可以处理文本、音频和视频输入，并提供文本和音频输出。

[在 Google AI Studio 中试用](https://aistudio.google.com?model=gemini-2.0-flash-live-001&hl=zh-cn)

#### 模型详情

属性

说明

id\_card 模型代码

`models/gemini-2.0-flash-live-001`

保存支持的数据类型

**输入源**

音频、视频和文本

**输出**

文字和音频

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

1,048,576

**输出 token 限制**

8192

handyman 功能

**结构化输出**

支持

**调音**

不受支持

**函数调用**

支持

**代码执行**

支持

**搜索**

支持

**图片生成**

不受支持

**音频生成**

支持

**思考**

不受支持

123版本

如需了解详情，请参阅[模型版本模式](https://ai.google.dev/gemini-api/docs/models/gemini?hl=zh-cn#model-versions)。

*   预览：`gemini-2.0-flash-live-001`

calendar\_month最新更新

2025 年 4 月

cognition\_2 知识截点

2024 年 8 月

### Gemini Embedding Experimental

`Gemini embedding` 在代码、多语言和检索等许多关键维度上都取得了[世界领先的效果](https://deepmind.google/research/publications/157741/?hl=zh-cn)。 由于 Gemini 是实验性模型，因此嵌入率限制更为严格。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`gemini-embedding-exp-03-07`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

8192

**输出维度大小**

弹性，支持：3072、1536 或 768

calendar\_month最新更新

2025 年 3 月

### 文本嵌入和嵌入

#### 文本嵌入

试用我们全新的[实验性 Gemini 嵌入模型](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/)，该模型可实现最先进的性能。

[文本嵌入](https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-cn)用于衡量字符串的相关性，在许多 AI 应用中都得到了广泛的应用。

在标准 MTEB 嵌入基准测试中，`text-embedding-004` 在具有类似维度的情形下，[实现了更出色的检索性能，并超越了现有模型](https://arxiv.org/pdf/2403.20327)。

##### 模型详情

属性

说明

id\_card 模型代码

**Gemini API**

`models/text-embedding-004`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2048

**输出维度大小**

768

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

不受支持

calendar\_month最新更新

2024 年 4 月

#### 嵌入

您可以使用嵌入模型为输入文本生成[文本嵌入](https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-cn)。

该嵌入模型经过优化，可为最多 2,048 个令牌的文本创建 768 维的嵌入。

##### 嵌入模型详情

属性

说明

id\_card 模型代码

`models/embedding-001`

保存支持的数据类型

**输入**

文本

**输出**

文本嵌入

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

2048

**输出维度大小**

768

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

不受支持

calendar\_month最新更新

2023 年 12 月

### AQA

您可以使用 AQA 模型对文档、语料库或一组段落执行[归因式问答](https://ai.google.dev/gemini-api/docs/semantic_retrieval?hl=zh-cn) (AQA) 相关任务。AQA 模型会返回基于所提供来源的问题的答案，并估算可回答的概率。

#### 模型详情

属性

说明

id\_card 模型代码

`models/aqa`

保存支持的数据类型

**输入**

文本

**输出**

文本

language支持的语言

英语

token\_auto 令牌限制[\[\*\]](#token-size)

**输入令牌限制**

7168

**输出 token 限制**

1024

swap\_driving\_apps\_wheel 速率限制[\[\*\*\]](#rate-limits)

每分钟 1,500 个请求

已加密可调整的安全设置

支持

calendar\_month最新更新

2023 年 12 月

请参阅[示例](https://ai.google.dev/examples?hl=zh-cn)，探索这些模型变体的功能。

\[\*\] 对于 Gemini 模型，一个令牌相当于大约 4 个字符。100 个词元大约相当于 60-80 个英语单词。

## 模型版本名称模式

Gemini 模型有_稳定版_、_预览版_或_实验版_。在代码中，您可以使用以下任一模型名称格式来指定要使用的模型和版本。

### 最新稳定版

指向为指定的模型生成和变体发布的最新稳定版本。

如需指定最新的稳定版本，请使用以下模式：`<model>-<generation>-<variation>`。例如 `gemini-2.0-flash`。

### 稳定

指向特定的稳定型号。稳定型模型通常不会发生变化。大多数正式版应用都应使用特定的稳定型号。

如需指定稳定版本，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.0-flash-001`。

### 预览

指向预览版模型，可能不适合用于生产环境，速率限制更为严格，但可以启用结算功能。

如需指定预览版，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.5-pro-preview-06-05`。

### 实验性

指向可能不适合用于生产环境的实验性模型，并且具有更严格的速率限制。我们发布实验性模型是为了收集反馈，并让开发者能够快速获得最新的更新。

如需指定实验版本，请使用以下格式：`<model>-<generation>-<variation>-<version>`。例如 `gemini-2.0-pro-exp-02-05`。

## 实验性模型

除了稳定模型之外，Gemini API 还提供实验性模型，这些模型可能不适合用于生产环境，并且具有更严格的速率限制。

我们发布实验性模型是为了收集反馈、快速将最新动态交到开发者手中，并突出展示 Google 的创新步伐。我们会通过发布实验版积累经验，以便我们的正式版模型能更好地服务更多用户。实验性模型可以在不事先通知的情况下换成另一个模型。我们不保证实验性模型将来会成为稳定模型。

### 先前的实验性模型

随着新版本或稳定版的推出，我们会移除并替换实验性模型。您可以在下一部分中找到我们之前发布的实验性模型以及替换版本：

模型代码

基本模型

替换版本

`gemini-2.5-flash-preview-04-17`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-05-20`

`gemini-2.0-flash-exp-image-generation`

Gemini 2.0 Flash

`gemini-2.0-flash-preview-image-generation`

`gemini-2.5-pro-preview-06-05`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-05-06`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.5-pro-preview-03-25`

Gemini 2.5 Pro

`gemini-2.5-pro`

`gemini-2.0-flash-thinking-exp-01-21`

Gemini 2.5 Flash

`gemini-2.5-flash-preview-04-17`

`gemini-2.0-pro-exp-02-05`

Gemini 2.0 Pro Experimental

`gemini-2.5-pro-preview-03-25`

`gemini-2.0-flash-exp`

Gemini 2.0 Flash

`gemini-2.0-flash`

`gemini-exp-1206`

Gemini 2.0 Pro

`gemini-2.0-pro-exp-02-05`

`gemini-2.0-flash-thinking-exp-1219`

Gemini 2.0 Flash Thinking

`gemini-2.0-flash-thinking-exp-01-21`

`gemini-exp-1121`

Gemini

`gemini-exp-1206`

`gemini-exp-1114`

Gemini

`gemini-exp-1206`

`gemini-1.5-pro-exp-0827`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-pro-exp-0801`

Gemini 1.5 Pro

`gemini-exp-1206`

`gemini-1.5-flash-8b-exp-0924`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

`gemini-1.5-flash-8b-exp-0827`

Gemini 1.5 Flash-8B

`gemini-1.5-flash-8b`

## 支持的语言

Gemini 模型经过训练，可处理以下语言：

*   阿拉伯语 (`ar`)
*   孟加拉语 (`bn`)
*   保加利亚语 (`bg`)
*   简体中文和繁体中文 (`zh`)
*   克罗地亚语 (`hr`)
*   捷克语 (`cs`)
*   丹麦语 (`da`)
*   荷兰语 (`nl`)
*   英语 (`en`)
*   爱沙尼亚语 (`et`)
*   芬兰语 (`fi`)
*   法语 (`fr`)
*   德语 (`de`)
*   希腊语 (`el`)
*   希伯来语 (`iw`)
*   印地语 (`hi`)
*   匈牙利语 (`hu`)
*   印度尼西亚语 (`id`)
*   意大利语 (`it`)
*   日语 (`ja`)
*   韩语 (`ko`)
*   拉脱维亚语 (`lv`)
*   立陶宛语 (`lt`)
*   挪威语 (`no`)
*   波兰语 (`pl`)
*   葡萄牙语 (`pt`)
*   罗马尼亚语 (`ro`)
*   俄语 (`ru`)
*   塞尔维亚语 (`sr`)
*   斯洛伐克语 (`sk`)
*   斯洛维尼亚语 (`sl`)
*   西班牙语 (`es`)
*   斯瓦希里语 (`sw`)
*   瑞典语 (`sv`)
*   泰语 (`th`)
*   土耳其语 (`tr`)
*   乌克兰语 (`uk`)
*   越南语 (`vi`)

发送反馈

如未另行说明，那么本页面中的内容已根据[知识共享署名 4.0 许可](https://creativecommons.org/licenses/by/4.0/)获得了许可，并且代码示例已根据 [Apache 2.0 许可](https://www.apache.org/licenses/LICENSE-2.0)获得了许可。有关详情，请参阅 [Google 开发者网站政策](https://developers.google.com/site-policies?hl=zh-cn)。Java 是 Oracle 和/或其关联公司的注册商标。

最后更新时间 (UTC)：2025-06-26。

---

# Content from: https://ai.google.dev/gemini-api/docs/api-versions

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)
*   [API Reference](https://ai.google.dev/api)

Send feedback

# API versions explained

This document provides a high-level overview of the differences between the `v1` and `v1beta` versions of the Gemini API.

*   **v1**: Stable version of the API. Features in the stable version are fully-supported over the lifetime of the major version. If there are any breaking changes, then the next major version of the API will be created and the existing version will be deprecated after a reasonable period of time. Non-breaking changes may be introduced to the API without changing the major version.
*   **v1beta**: This version includes early-access features that may be under development and is subject to rapid and breaking changes. There is also no guarantee that the features in the Beta version will move to the stable version. Due to this instability, you should consider not launching production applications with this version.

Feature

v1

v1beta

Generate Content - Text-only input

Generate Content - Text-and-image input

Generate Content - Text output

Generate Content - Multi-turn conversations (chat)

Generate Content - Function calls

Generate Content - Streaming

Embed Content - Text-only input

Generate Answer

Semantic retriever

*   \- Supported
*   \- Will never be supported

## Configure API version in an SDK

The Gemini API SDK's default to `v1beta`, but you can opt to use other versions by setting the API version as shown in the following code sample:

### Python

```
from google import genai

client = genai.Client(http_options={'api_version': 'v1alpha'})

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents="Explain how AI works",
)

print(response.text)
```

### JavaScript

```
import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({
  httpOptions: { apiVersion: "v1alpha" },
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works",
  });
  console.log(response.text);
}

await main();
```

### REST

```
curl "https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-X POST \
-d '{
  "contents": [{
    "parts":[{"text": "Explain how AI works."}]
    }]
   }'
```

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-06-27 UTC.

---

# Content from: https://ai.google.dev/gemini-api/docs/api-overview

Introducing updates to our 2.5 family of thinking models. [Learn more](https://ai.google.dev/gemini-api/docs/models)

*   [Home](https://ai.google.dev/)
*   [Gemini API](https://ai.google.dev/gemini-api)
*   [Models](https://ai.google.dev/gemini-api/docs)
*   [API Reference](https://ai.google.dev/api)

Send feedback

# Gemini API reference

The Gemini API lets you access the latest generative models from Google. This API reference provides detailed information for the classes and methods available in the Gemini API SDKs. Pick a language and follow the setup steps to get started building.

Python JavaScript Go Apps Script

## Install the Gemini API library

## Make your first request

Use the [`generateContent`](/api/generate-content#method:-models.generatecontent) method to send a request to the Gemini API.

## What's next

If you're just getting started, check out the following guides, which will help you understand the Gemini API programming model:

*   [Gemini API quickstart](/gemini-api/docs/quickstart)
*   [Gemini model guide](/gemini-api/docs/models/gemini)
*   [Prompt design](/gemini-api/docs/prompting-intro)

You might also want to check out the capabilities guides, which introduce different Gemini API features and provide code examples:

*   [Text generation](/gemini-api/docs/text-generation)
*   [Context caching](/gemini-api/docs/caching)
*   [Embeddings](/gemini-api/docs/embeddings)
*   [Tuning](/gemini-api/docs/model-tuning)

Send feedback

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2025-05-12 UTC.